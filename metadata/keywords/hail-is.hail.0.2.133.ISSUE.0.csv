id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/4:163,Testability,Test,TestVCF,163,"_From @cseed on August 26, 2015 13:52_. See here:. https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/examples/src/main/java/org/seqdoop/hadoop_bam/examples/TestVCF.java. This allows us to stop using gatling. _Copied from original issue: cseed/hail#3_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4
https://github.com/hail-is/hail/issues/5:58,Deployability,patch,patching,58,"_From @cseed on August 26, 2015 14:0_. This might involve patching the Gradle Jacoco plugin. _Copied from original issue: cseed/hail#4_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5
https://github.com/hail-is/hail/issues/5:85,Modifiability,plugin,plugin,85,"_From @cseed on August 26, 2015 14:0_. This might involve patching the Gradle Jacoco plugin. _Copied from original issue: cseed/hail#4_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5
https://github.com/hail-is/hail/issues/10:187,Performance,concurren,concurrently,187,"_From @cseed on August 26, 2015 14:20_; - fix representation for variant information; - support upstream deletion allele; - normalize on import: left-align, split complex. Should be done concurrently with:; https://github.com/cseed/k3/issues/5. _Copied from original issue: cseed/hail#9_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10
https://github.com/hail-is/hail/issues/11:49,Performance,load,loading,49,"_From @cseed on August 26, 2015 14:23_. Requires loading `.ped`/`.fam` files. _Copied from original issue: cseed/hail#10_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/11
https://github.com/hail-is/hail/issues/15:114,Integrability,depend,dependency,114,"_From @cseed on August 26, 2015 14:32_. Need to test with gradle and IntelliJ. Last time I tried this there was a dependency conflict. I don't remember the details. _Copied from original issue: cseed/hail#14_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/15
https://github.com/hail-is/hail/issues/15:48,Testability,test,test,48,"_From @cseed on August 26, 2015 14:32_. Need to test with gradle and IntelliJ. Last time I tried this there was a dependency conflict. I don't remember the details. _Copied from original issue: cseed/hail#14_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/15
https://github.com/hail-is/hail/issues/16:77,Availability,mask,mask,77,"_From @cseed on August 26, 2015 14:37_. When designing, consider plinkseq `--mask` option:. https://atgu.mgh.harvard.edu/plinkseq/masks.shtml. and bcftools filter expressions:. https://samtools.github.io/bcftools/bcftools.html#expressions. _Copied from original issue: cseed/hail#15_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/16
https://github.com/hail-is/hail/issues/16:130,Availability,mask,masks,130,"_From @cseed on August 26, 2015 14:37_. When designing, consider plinkseq `--mask` option:. https://atgu.mgh.harvard.edu/plinkseq/masks.shtml. and bcftools filter expressions:. https://samtools.github.io/bcftools/bcftools.html#expressions. _Copied from original issue: cseed/hail#15_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/16
https://github.com/hail-is/hail/issues/18:40,Integrability,Depend,Depends,40,"_From @cseed on August 26, 2015 14:46_. Depends on:; https://github.com/cseed/k3/issues/4. _Copied from original issue: cseed/hail#17_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/18
https://github.com/hail-is/hail/issues/19:315,Performance,Load,LoadVCF,315,"_From @cseed on August 26, 2015 14:51_. Waiting on suitable machines (Intel spark cluster, cloud access, etc.); - measure size of stored data; - compressed vs uncompressed (gzip parquet, lz4 in SparkyVSM, etc.); - compute cost (or at least compute-hrs); - compare best-case (e.g. `gzip -cd file.vcf.gz | wc -l` vs `LoadVCF`). _Copied from original issue: cseed/hail#18_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/19
https://github.com/hail-is/hail/issues/19:97,Security,access,access,97,"_From @cseed on August 26, 2015 14:51_. Waiting on suitable machines (Intel spark cluster, cloud access, etc.); - measure size of stored data; - compressed vs uncompressed (gzip parquet, lz4 in SparkyVSM, etc.); - compute cost (or at least compute-hrs); - compare best-case (e.g. `gzip -cd file.vcf.gz | wc -l` vs `LoadVCF`). _Copied from original issue: cseed/hail#18_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/19
https://github.com/hail-is/hail/issues/28:260,Integrability,Depend,Depends,260,"_From @cseed on August 28, 2015 15:57_. We need to document k3 for users as a mathematician would, that is to say, precisely. It should include precise mathematical definitions for everything we can compute that can be cut-and-pasted into plots of our output. Depends on https://github.com/cseed/k3/issues/31. _Copied from original issue: cseed/hail#32_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/28
https://github.com/hail-is/hail/issues/32:164,Testability,log,logistic,164,"_From @cseed on September 1, 2015 15:16_. We should be on the lookout for an opportunity to write methods to generate synthetic data for method verification. Mixed logistic regression might be a good example. _Copied from original issue: cseed/hail#41_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/32
https://github.com/hail-is/hail/issues/33:68,Availability,error,error,68,"_From @cseed on September 1, 2015 15:48_. Need general strategy for error reporting. We can't use asserts for input data validation (e.g., reading tsv files, command line flags, etc.) Need to generate good error messages with feedback about line numbers, etc. _Copied from original issue: cseed/hail#42_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/33
https://github.com/hail-is/hail/issues/33:206,Availability,error,error,206,"_From @cseed on September 1, 2015 15:48_. Need general strategy for error reporting. We can't use asserts for input data validation (e.g., reading tsv files, command line flags, etc.) Need to generate good error messages with feedback about line numbers, etc. _Copied from original issue: cseed/hail#42_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/33
https://github.com/hail-is/hail/issues/33:212,Integrability,message,messages,212,"_From @cseed on September 1, 2015 15:48_. Need general strategy for error reporting. We can't use asserts for input data validation (e.g., reading tsv files, command line flags, etc.) Need to generate good error messages with feedback about line numbers, etc. _Copied from original issue: cseed/hail#42_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/33
https://github.com/hail-is/hail/issues/33:121,Security,validat,validation,121,"_From @cseed on September 1, 2015 15:48_. Need general strategy for error reporting. We can't use asserts for input data validation (e.g., reading tsv files, command line flags, etc.) Need to generate good error messages with feedback about line numbers, etc. _Copied from original issue: cseed/hail#42_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/33
https://github.com/hail-is/hail/issues/33:98,Testability,assert,asserts,98,"_From @cseed on September 1, 2015 15:48_. Need general strategy for error reporting. We can't use asserts for input data validation (e.g., reading tsv files, command line flags, etc.) Need to generate good error messages with feedback about line numbers, etc. _Copied from original issue: cseed/hail#42_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/33
https://github.com/hail-is/hail/issues/33:226,Usability,feedback,feedback,226,"_From @cseed on September 1, 2015 15:48_. Need general strategy for error reporting. We can't use asserts for input data validation (e.g., reading tsv files, command line flags, etc.) Need to generate good error messages with feedback about line numbers, etc. _Copied from original issue: cseed/hail#42_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/33
https://github.com/hail-is/hail/issues/34:1183,Availability,down,down,1183,"ython tool that can serve as model. From Kaitlin:. The most recent version is attached (3.93). The biggest issue that I've yet to resolve is how to handle multi-allelic lines above tri-allelic. Gets into nightmare territory quickly. To run this, you'll need three things:; 1) VCF of interest; 2) PED file for the families in the VCF; 3) The ESP variant counts file that I made (.gz for the moment since it is so large); You can find this file here: /humgen/atgu1/fs03/wip/kaitlin/all_ESP_counts_5.28.13.txt. The command line argument should look like this:; python de_novo_finder_3.py <VCF> <PED> all_ESP_counts_5.28.13.txt. I suggest specifying an output file. There are a few optional flags that you can use to adjust things in the script. -v, --annotatevariants_VEP: If you have VEP annotations in the ANNOTATION column of the VCF, this will pull out and print the gene name and mutation type. -t, --thresh: The PL threshold set for the next most likely genotype in the child. This gets after how confident you want the het call to be in the child. Default is a PL threshold of 20, but you can adjust that up or down if you'd like. -c, --minchildAB: I require that the heterozygous child has at least 20% alternative reads. You can adjust that with this. -d, --depthratio: I require that the depth of coverage in the child is at least 1/10th that of the combined parental depth. You can adjust that with this (integer input for the 1/x). -m, --prob(dn)metric: The minimum p(DN) that you will accept. I have it set at 0.05 and you could adjust it up. Due to the validation likelihoods that were added, you won't get anything below 0.05. -p, --maxparentAB: I require that the parents, who should both be homozygous reference, have no more than 5% alternative reads. You can adjust that with this flag. -a, --annotatevariants: If you have SnpEff annotations in the ANNOTATION column of the VCF, this will pull out and print the gene name and mutation type. _Copied from original issue: cseed/hail#44_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/34
https://github.com/hail-is/hail/issues/34:1632,Security,validat,validation,1632,"ython tool that can serve as model. From Kaitlin:. The most recent version is attached (3.93). The biggest issue that I've yet to resolve is how to handle multi-allelic lines above tri-allelic. Gets into nightmare territory quickly. To run this, you'll need three things:; 1) VCF of interest; 2) PED file for the families in the VCF; 3) The ESP variant counts file that I made (.gz for the moment since it is so large); You can find this file here: /humgen/atgu1/fs03/wip/kaitlin/all_ESP_counts_5.28.13.txt. The command line argument should look like this:; python de_novo_finder_3.py <VCF> <PED> all_ESP_counts_5.28.13.txt. I suggest specifying an output file. There are a few optional flags that you can use to adjust things in the script. -v, --annotatevariants_VEP: If you have VEP annotations in the ANNOTATION column of the VCF, this will pull out and print the gene name and mutation type. -t, --thresh: The PL threshold set for the next most likely genotype in the child. This gets after how confident you want the het call to be in the child. Default is a PL threshold of 20, but you can adjust that up or down if you'd like. -c, --minchildAB: I require that the heterozygous child has at least 20% alternative reads. You can adjust that with this. -d, --depthratio: I require that the depth of coverage in the child is at least 1/10th that of the combined parental depth. You can adjust that with this (integer input for the 1/x). -m, --prob(dn)metric: The minimum p(DN) that you will accept. I have it set at 0.05 and you could adjust it up. Due to the validation likelihoods that were added, you won't get anything below 0.05. -p, --maxparentAB: I require that the parents, who should both be homozygous reference, have no more than 5% alternative reads. You can adjust that with this flag. -a, --annotatevariants: If you have SnpEff annotations in the ANNOTATION column of the VCF, this will pull out and print the gene name and mutation type. _Copied from original issue: cseed/hail#44_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/34
https://github.com/hail-is/hail/issues/39:82,Testability,assert,assert,82,"_From @tpoterba on September 23, 2015 15:26_. It would be nice to be able to map `assert` in RDDs, this is impossible right now because assertions are not serializable. _Copied from original issue: cseed/hail#52_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/39
https://github.com/hail-is/hail/issues/39:136,Testability,assert,assertions,136,"_From @tpoterba on September 23, 2015 15:26_. It would be nice to be able to map `assert` in RDDs, this is impossible right now because assertions are not serializable. _Copied from original issue: cseed/hail#52_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/39
https://github.com/hail-is/hail/issues/43:114,Deployability,install,installDir,114,"_From @cseed on September 29, 2015 15:43_. Added gqbydp --plot option. Some issues to resolve:; - need to test; - installDir detection won't work with shadowJar; - handle case of output file in Hadoop or Hadoop URI. _Copied from original issue: cseed/hail#62_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/43
https://github.com/hail-is/hail/issues/43:125,Safety,detect,detection,125,"_From @cseed on September 29, 2015 15:43_. Added gqbydp --plot option. Some issues to resolve:; - need to test; - installDir detection won't work with shadowJar; - handle case of output file in Hadoop or Hadoop URI. _Copied from original issue: cseed/hail#62_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/43
https://github.com/hail-is/hail/issues/43:106,Testability,test,test,106,"_From @cseed on September 29, 2015 15:43_. Added gqbydp --plot option. Some issues to resolve:; - need to test; - installDir detection won't work with shadowJar; - handle case of output file in Hadoop or Hadoop URI. _Copied from original issue: cseed/hail#62_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/43
https://github.com/hail-is/hail/issues/44:264,Energy Efficiency,power,powerful,264,"_From @jbloom22 on September 29, 2015 17:15_. In computing statistics like sample and variant qc, we should not treat X/Y like the autosomes. A simple solution is to only compute on the autosome, but we should discuss with the community how to make the tools more powerful via sex awareness. for example, we could split all stats by sex, or just sex chromosome stats by sex, ... _Copied from original issue: cseed/hail#64_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/44
https://github.com/hail-is/hail/issues/44:144,Usability,simpl,simple,144,"_From @jbloom22 on September 29, 2015 17:15_. In computing statistics like sample and variant qc, we should not treat X/Y like the autosomes. A simple solution is to only compute on the autosome, but we should discuss with the community how to make the tools more powerful via sex awareness. for example, we could split all stats by sex, or just sex chromosome stats by sex, ... _Copied from original issue: cseed/hail#64_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/44
https://github.com/hail-is/hail/issues/45:111,Availability,error,errors,111,"_From @jbloom22 on September 29, 2015 17:21_. Once we handle multi-allelic sites, we will need to adapt mendel errors so that, for example, it does not double count errors in multi-allelic trios. _Copied from original issue: cseed/hail#65_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/45
https://github.com/hail-is/hail/issues/45:165,Availability,error,errors,165,"_From @jbloom22 on September 29, 2015 17:21_. Once we handle multi-allelic sites, we will need to adapt mendel errors so that, for example, it does not double count errors in multi-allelic trios. _Copied from original issue: cseed/hail#65_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/45
https://github.com/hail-is/hail/issues/45:98,Energy Efficiency,adapt,adapt,98,"_From @jbloom22 on September 29, 2015 17:21_. Once we handle multi-allelic sites, we will need to adapt mendel errors so that, for example, it does not double count errors in multi-allelic trios. _Copied from original issue: cseed/hail#65_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/45
https://github.com/hail-is/hail/issues/45:98,Modifiability,adapt,adapt,98,"_From @jbloom22 on September 29, 2015 17:21_. Once we handle multi-allelic sites, we will need to adapt mendel errors so that, for example, it does not double count errors in multi-allelic trios. _Copied from original issue: cseed/hail#65_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/45
https://github.com/hail-is/hail/issues/46:85,Testability,test,test,85,"_From @jbloom22 on September 30, 2015 20:21_. Currently in MendelErrorsSuite we only test the internal representation, and not the correctness of the output files. The same issue arises whenever we support write but not read. We should come up with a strategy for testing in such cases. _Copied from original issue: cseed/hail#67_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/46
https://github.com/hail-is/hail/issues/46:264,Testability,test,testing,264,"_From @jbloom22 on September 30, 2015 20:21_. Currently in MendelErrorsSuite we only test the internal representation, and not the correctness of the output files. The same issue arises whenever we support write but not read. We should come up with a strategy for testing in such cases. _Copied from original issue: cseed/hail#67_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/46
https://github.com/hail-is/hail/issues/47:172,Availability,error,errors,172,"_From @alexb-3 on September 30, 2015 20:42_. Note that we are using BibTeX with the common `bibfile.bib`, and that we need a make tool that runs PDFTeX and BibTeX until no errors remain. _Copied from original issue: cseed/hail#68_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/47
https://github.com/hail-is/hail/issues/50:78,Testability,test,testing,78,"_From @jbloom22 on October 14, 2015 1:23_. Cotton, I haven't implemented good testing yet, but would appreciate some initial feedback on the code as I take a break to write up the math. I've done some comparison with R and it checks out so far. It runs on the command line, but you'll need to create a covariate file from your PCs. _Copied from original issue: cseed/hail/pull/81_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50
https://github.com/hail-is/hail/issues/50:125,Usability,feedback,feedback,125,"_From @jbloom22 on October 14, 2015 1:23_. Cotton, I haven't implemented good testing yet, but would appreciate some initial feedback on the code as I take a break to write up the math. I've done some comparison with R and it checks out so far. It runs on the command line, but you'll need to create a covariate file from your PCs. _Copied from original issue: cseed/hail/pull/81_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/50
https://github.com/hail-is/hail/issues/51:177,Availability,toler,tolerance,177,"_From @alexb-3 on October 20, 2015 19:43_. These could be implemented as infix methods of a no-overhead RichDouble wrapper class, though we need some flexibility in setting the tolerance. For example, a computation like an exact test might use a stricter tolerance (say 1e-12) than a test (say 1e-6). _Copied from original issue: cseed/hail#84_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/51
https://github.com/hail-is/hail/issues/51:255,Availability,toler,tolerance,255,"_From @alexb-3 on October 20, 2015 19:43_. These could be implemented as infix methods of a no-overhead RichDouble wrapper class, though we need some flexibility in setting the tolerance. For example, a computation like an exact test might use a stricter tolerance (say 1e-12) than a test (say 1e-6). _Copied from original issue: cseed/hail#84_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/51
https://github.com/hail-is/hail/issues/51:115,Integrability,wrap,wrapper,115,"_From @alexb-3 on October 20, 2015 19:43_. These could be implemented as infix methods of a no-overhead RichDouble wrapper class, though we need some flexibility in setting the tolerance. For example, a computation like an exact test might use a stricter tolerance (say 1e-12) than a test (say 1e-6). _Copied from original issue: cseed/hail#84_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/51
https://github.com/hail-is/hail/issues/51:229,Testability,test,test,229,"_From @alexb-3 on October 20, 2015 19:43_. These could be implemented as infix methods of a no-overhead RichDouble wrapper class, though we need some flexibility in setting the tolerance. For example, a computation like an exact test might use a stricter tolerance (say 1e-12) than a test (say 1e-6). _Copied from original issue: cseed/hail#84_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/51
https://github.com/hail-is/hail/issues/51:284,Testability,test,test,284,"_From @alexb-3 on October 20, 2015 19:43_. These could be implemented as infix methods of a no-overhead RichDouble wrapper class, though we need some flexibility in setting the tolerance. For example, a computation like an exact test might use a stricter tolerance (say 1e-12) than a test (say 1e-6). _Copied from original issue: cseed/hail#84_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/51
https://github.com/hail-is/hail/issues/53:208,Availability,failure,failure,208,"_From @cseed on October 22, 2015 13:56_. Andrea is running into some problems with the Estonian dataset in sampleqc:. ```; Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 2689 tasks (2.0 GB) is bigger than spark.driver.maxResultSize (2.0 GB); ```. _Copied from original issue: cseed/hail#89_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/53
https://github.com/hail-is/hail/issues/53:187,Safety,abort,aborted,187,"_From @cseed on October 22, 2015 13:56_. Andrea is running into some problems with the Estonian dataset in sampleqc:. ```; Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 2689 tasks (2.0 GB) is bigger than spark.driver.maxResultSize (2.0 GB); ```. _Copied from original issue: cseed/hail#89_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/53
https://github.com/hail-is/hail/issues/55:271,Performance,perform,performance,271,"_From @tpoterba on October 23, 2015 17:9_. Array data only includes a genotype call, so >80% of the memory and unpacking cpu for genotypes is wasted on missing data. We could implement multiple types of Genotype instances, if there is a way to do this without sequencing performance suffering. _Copied from original issue: cseed/hail#91_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/55
https://github.com/hail-is/hail/issues/59:19,Testability,test,test,19,"Use the binomial p-test on allelic depths, but include a flag to allow for optionally setting a ref bias other than 0.5 (for example in exome it can be .53, which for high-depth sites is not negligible)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/59
https://github.com/hail-is/hail/pull/61:228,Deployability,install,install,228,"with profile75.vds:; linreg with 10 PCs: 58s, 59s, 59s; plink2: 38s, 39s, 39s; variantqc: 142s, 141s, 144s; sampleqc: 231s, 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/61
https://github.com/hail-is/hail/pull/61:1286,Performance,load,loaded,1286," 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/profile75.plink.assoc.linear ... done. real 0m38.837s; user 0m38.609s; sys 0m0.187s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/61
https://github.com/hail-is/hail/pull/61:1359,Performance,load,loaded,1359," 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/profile75.plink.assoc.linear ... done. real 0m38.837s; user 0m38.609s; sys 0m0.187s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/61
https://github.com/hail-is/hail/pull/61:1736,Performance,load,loaded,1736," 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/profile75.plink.assoc.linear ... done. real 0m38.837s; user 0m38.609s; sys 0m0.187s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/61
https://github.com/hail-is/hail/pull/61:1222,Safety,detect,detected,1222," 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/profile75.plink.assoc.linear ... done. real 0m38.837s; user 0m38.609s; sys 0m0.187s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/61
https://github.com/hail-is/hail/pull/61:963,Testability,Log,Logging,963,"with profile75.vds:; linreg with 10 PCs: 58s, 59s, 59s; plink2: 38s, 39s, 39s; variantqc: 142s, 141s, 144s; sampleqc: 231s, 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/61
https://github.com/hail-is/hail/pull/61:1006,Testability,log,log,1006,"file75.vds:; linreg with 10 PCs: 58s, 59s, 59s; plink2: 38s, 39s, 39s; variantqc: 142s, 141s, 144s; sampleqc: 231s, 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model associat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/61
https://github.com/hail-is/hail/pull/73:352,Availability,error,error-tolerant,352,"Added stats.LeveneHaldane and tests. Created doc folder, LeveneHaldane.tex, and bibfile.bib; Added docs/.gitignore. Added HWEPerVariant, test/resources/HWE_test.vcf and tests. Added Utils.time, now replaced by Utils.printTime. Used Option in ""r*"" (ratio) methods for missing values, now abstracted with Utils.divOption and Utils.someIf. Added rounding-error-tolerant comparison operators Utils.D_\* and used where appropriate. replaced closeEnough with D_==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/73
https://github.com/hail-is/hail/pull/73:30,Testability,test,tests,30,"Added stats.LeveneHaldane and tests. Created doc folder, LeveneHaldane.tex, and bibfile.bib; Added docs/.gitignore. Added HWEPerVariant, test/resources/HWE_test.vcf and tests. Added Utils.time, now replaced by Utils.printTime. Used Option in ""r*"" (ratio) methods for missing values, now abstracted with Utils.divOption and Utils.someIf. Added rounding-error-tolerant comparison operators Utils.D_\* and used where appropriate. replaced closeEnough with D_==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/73
https://github.com/hail-is/hail/pull/73:137,Testability,test,test,137,"Added stats.LeveneHaldane and tests. Created doc folder, LeveneHaldane.tex, and bibfile.bib; Added docs/.gitignore. Added HWEPerVariant, test/resources/HWE_test.vcf and tests. Added Utils.time, now replaced by Utils.printTime. Used Option in ""r*"" (ratio) methods for missing values, now abstracted with Utils.divOption and Utils.someIf. Added rounding-error-tolerant comparison operators Utils.D_\* and used where appropriate. replaced closeEnough with D_==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/73
https://github.com/hail-is/hail/pull/73:169,Testability,test,tests,169,"Added stats.LeveneHaldane and tests. Created doc folder, LeveneHaldane.tex, and bibfile.bib; Added docs/.gitignore. Added HWEPerVariant, test/resources/HWE_test.vcf and tests. Added Utils.time, now replaced by Utils.printTime. Used Option in ""r*"" (ratio) methods for missing values, now abstracted with Utils.divOption and Utils.someIf. Added rounding-error-tolerant comparison operators Utils.D_\* and used where appropriate. replaced closeEnough with D_==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/73
https://github.com/hail-is/hail/pull/76:52,Testability,Test,Test,52,Added multiallelic splitting to HtsjdkRecordReader. Test in SplitSuite. Also modified return type of AbstractRecordReader.readRecord,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/76
https://github.com/hail-is/hail/pull/80:52,Testability,Test,Test,52,"Added multiallelic splitting to HtsjdkRecordReader. Test in SplitSuite. Also modified return type of AbstractRecordReader.readRecord. Added ""wasSplit"" field to Variant and ""fakeRef"" field to Genotype",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/80
https://github.com/hail-is/hail/issues/81:156,Modifiability,plugin,plugin,156,Some links:. http://findbugs.sourceforge.net/; https://docs.gradle.org/current/userguide/findbugs_plugin.html; https://github.com/sksamuel/scalac-scapegoat-plugin; https://stackoverflow.com/questions/22617713/whats-the-current-state-of-static-analysis-tools-for-scala; https://stackoverflow.com/questions/1598882/are-there-any-tools-for-performing-static-analysis-of-scala-code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/81
https://github.com/hail-is/hail/issues/81:337,Performance,perform,performing-static-analysis-of-scala-code,337,Some links:. http://findbugs.sourceforge.net/; https://docs.gradle.org/current/userguide/findbugs_plugin.html; https://github.com/sksamuel/scalac-scapegoat-plugin; https://stackoverflow.com/questions/22617713/whats-the-current-state-of-static-analysis-tools-for-scala; https://stackoverflow.com/questions/1598882/are-there-any-tools-for-performing-static-analysis-of-scala-code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/81
https://github.com/hail-is/hail/issues/84:20,Availability,error,error,20,`hail: fatal: parse error in condition: reflective typecheck has failed: value DUMMY is not a member of __infoClass; `. There's got to be a better way to do this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/84
https://github.com/hail-is/hail/issues/90:93,Security,secur,security,93,"```; running: read -i /user/aganna/GPC.vcf.bgz; Exception in thread ""main"" org.apache.hadoop.security.AccessControlException: Permission denied: user=aganna, access=EXECUTE, inode=""/user/aganna/GPC.vcf.bgz"":aganna:supergroup:-rw-r--r—; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/90
https://github.com/hail-is/hail/issues/90:102,Security,Access,AccessControlException,102,"```; running: read -i /user/aganna/GPC.vcf.bgz; Exception in thread ""main"" org.apache.hadoop.security.AccessControlException: Permission denied: user=aganna, access=EXECUTE, inode=""/user/aganna/GPC.vcf.bgz"":aganna:supergroup:-rw-r--r—; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/90
https://github.com/hail-is/hail/issues/90:158,Security,access,access,158,"```; running: read -i /user/aganna/GPC.vcf.bgz; Exception in thread ""main"" org.apache.hadoop.security.AccessControlException: Permission denied: user=aganna, access=EXECUTE, inode=""/user/aganna/GPC.vcf.bgz"":aganna:supergroup:-rw-r--r—; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/90
https://github.com/hail-is/hail/pull/92:0,Testability,Test,Testing,0,Testing is sparse but my next step is property-based anyway.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/92
https://github.com/hail-is/hail/pull/93:53,Availability,down,downsamplevariants,53,Optimized sampleqc for (fixed) VSM structure. Added; downsamplevariants. Make sure all file IO goes through hadoop IO; interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/93
https://github.com/hail-is/hail/pull/93:119,Integrability,interface,interface,119,Optimized sampleqc for (fixed) VSM structure. Added; downsamplevariants. Make sure all file IO goes through hadoop IO; interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/93
https://github.com/hail-is/hail/pull/93:0,Performance,Optimiz,Optimized,0,Optimized sampleqc for (fixed) VSM structure. Added; downsamplevariants. Make sure all file IO goes through hadoop IO; interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/93
https://github.com/hail-is/hail/issues/94:10,Integrability,message,message,10,"With this message:. > java.util.NoSuchElementException: key not found: NA19438. I guess we should filter trios with kids not in the dataset, and make the parents None if they are missing and issue a warning?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/94
https://github.com/hail-is/hail/pull/98:55,Testability,test,tests,55,"Hi Cotton,. I ended up having to duplicate some of the tests to make sure everything got tested (ex: apply and applyIdentity, zip and zipIdentity). This way I know there won't be undetermined status and I know both cases got tested (correct and incorrect arguments).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/98
https://github.com/hail-is/hail/pull/98:89,Testability,test,tested,89,"Hi Cotton,. I ended up having to duplicate some of the tests to make sure everything got tested (ex: apply and applyIdentity, zip and zipIdentity). This way I know there won't be undetermined status and I know both cases got tested (correct and incorrect arguments).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/98
https://github.com/hail-is/hail/pull/98:225,Testability,test,tested,225,"Hi Cotton,. I ended up having to duplicate some of the tests to make sure everything got tested (ex: apply and applyIdentity, zip and zipIdentity). This way I know there won't be undetermined status and I know both cases got tested (correct and incorrect arguments).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/98
https://github.com/hail-is/hail/issues/100:191,Energy Efficiency,power,power,191,"1. We currently do not do a per-variant variance normalization. There should be an option to include normalization, or even a partial normalization dividing by standard deviation raised to a power between zero and one.; 2. Include an option to compute and output the ""other"" singular vectors, corresponding to the loadings of the PCs on variants rather than samples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/100
https://github.com/hail-is/hail/issues/100:314,Performance,load,loadings,314,"1. We currently do not do a per-variant variance normalization. There should be an option to include normalization, or even a partial normalization dividing by standard deviation raised to a power between zero and one.; 2. Include an option to compute and output the ""other"" singular vectors, corresponding to the loadings of the PCs on variants rather than samples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/100
https://github.com/hail-is/hail/issues/102:67,Testability,test,testing,67,"Given a FAM file:; - Identify founders (to use e.g. as filter when testing HWE); - Compute coefficients of relationship and inbreeding, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/102
https://github.com/hail-is/hail/issues/106:81,Deployability,install,installed,81,"We should be able to assume in tests that plink, gatk, bcftools, tabix and R are installed to test interoperability (e.g. vcf export).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/106
https://github.com/hail-is/hail/issues/106:99,Integrability,interoperab,interoperability,99,"We should be able to assume in tests that plink, gatk, bcftools, tabix and R are installed to test interoperability (e.g. vcf export).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/106
https://github.com/hail-is/hail/issues/106:31,Testability,test,tests,31,"We should be able to assume in tests that plink, gatk, bcftools, tabix and R are installed to test interoperability (e.g. vcf export).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/106
https://github.com/hail-is/hail/issues/106:94,Testability,test,test,94,"We should be able to assume in tests that plink, gatk, bcftools, tabix and R are installed to test interoperability (e.g. vcf export).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/106
https://github.com/hail-is/hail/pull/109:11,Testability,test,tests,11,Added TDT; tests still needed; for preliminary code review only. Merry Christmas!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/109
https://github.com/hail-is/hail/issues/112:213,Deployability,update,updated,213,"Add allele count (AC) and allele number (AN) in variantQC. These fields became important for filtering, since the AC and AN in the INFO field cannot be trust (e.g. if you remove an individual, the AC does not get updated).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/112
https://github.com/hail-is/hail/pull/113:137,Testability,test,tests,137,"Still need to complete genotype filtering based on option types, but all indication are that it'll work based on included FilterGenotype tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/113
https://github.com/hail-is/hail/issues/114:10,Modifiability,extend,extend,10,"We should extend filtervariants to take a file consisting of a list of positions (contig, start) or a list of variants (contig, start, ref, alt). In GoT2D, there are related .pos files of positions that pass various filters, and vcftools allows one to subset.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/114
https://github.com/hail-is/hail/issues/116:233,Testability,Assert,AssertionError,233,"Master fails with `read -i sample.vcf write -o sample.vds` in schema reflection in the Parquet writer:. ```; running: import -i /Users/cseed/sample.vcf; running: write -o /Users/cseed/sample.vds; Exception in thread ""main"" java.lang.AssertionError: assertion failed: Unsound substitution from List(type T) to List(); at scala.reflect.internal.Types$SubstMap.<init>(Types.scala:4644); at scala.reflect.internal.Types$SubstTypeMap.<init>(Types.scala:4761); at scala.reflect.internal.Types$Type.subst(Types.scala:796); at scala.reflect.internal.Types$TypeApiImpl.substituteTypes(Types.scala:321); at scala.reflect.internal.Types$TypeApiImpl.substituteTypes(Types.scala:298); ```. and so on. I'm guessing this has to do with the doubly-nested Maps. The following fixes the problem (but of course throws out all the annotations):. ```; diff --git a/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala b/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; index 2c95bd0..c480c86 100644; --- a/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; +++ b/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; @@ -30,7 +30,7 @@ object VariantSampleMatrix {; // val df = sqlContext.read.parquet(dirname + ""/rdd.parquet""); val df = sqlContext.parquetFile(dirname + ""/rdd.parquet""); new VariantSampleMatrix[Genotype](metadata, df.rdd.map(r =>; - (r.getVariant(0), r.getVariantAnnotations(1), r.getGenotypeStream(2)))); + (r.getVariant(0), Annotations.emptyOfData(), r.getGenotypeStream(1)))); }; }. @@ -326,7 +326,7 @@ class RichVDS(vds: VariantDataset) {. // rdd.toDF().write.parquet(dirname + ""/rdd.parquet""); vds.rdd; - .map { case (v, va, gs) => (v, va, gs.toGenotypeStream(v, compress)) }; + .map[(Variant, GenotypeStream)] { case (v, va, gs) => (v, gs.toGenotypeStream(v, compress)) }; .toDF(); .saveAsParquetFile(dirname + ""/rdd.parquet""); }; ```. You should probably serialize the Annotations in some way in the map before",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/116
https://github.com/hail-is/hail/issues/116:249,Testability,assert,assertion,249,"Master fails with `read -i sample.vcf write -o sample.vds` in schema reflection in the Parquet writer:. ```; running: import -i /Users/cseed/sample.vcf; running: write -o /Users/cseed/sample.vds; Exception in thread ""main"" java.lang.AssertionError: assertion failed: Unsound substitution from List(type T) to List(); at scala.reflect.internal.Types$SubstMap.<init>(Types.scala:4644); at scala.reflect.internal.Types$SubstTypeMap.<init>(Types.scala:4761); at scala.reflect.internal.Types$Type.subst(Types.scala:796); at scala.reflect.internal.Types$TypeApiImpl.substituteTypes(Types.scala:321); at scala.reflect.internal.Types$TypeApiImpl.substituteTypes(Types.scala:298); ```. and so on. I'm guessing this has to do with the doubly-nested Maps. The following fixes the problem (but of course throws out all the annotations):. ```; diff --git a/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala b/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; index 2c95bd0..c480c86 100644; --- a/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; +++ b/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; @@ -30,7 +30,7 @@ object VariantSampleMatrix {; // val df = sqlContext.read.parquet(dirname + ""/rdd.parquet""); val df = sqlContext.parquetFile(dirname + ""/rdd.parquet""); new VariantSampleMatrix[Genotype](metadata, df.rdd.map(r =>; - (r.getVariant(0), r.getVariantAnnotations(1), r.getGenotypeStream(2)))); + (r.getVariant(0), Annotations.emptyOfData(), r.getGenotypeStream(1)))); }; }. @@ -326,7 +326,7 @@ class RichVDS(vds: VariantDataset) {. // rdd.toDF().write.parquet(dirname + ""/rdd.parquet""); vds.rdd; - .map { case (v, va, gs) => (v, va, gs.toGenotypeStream(v, compress)) }; + .map[(Variant, GenotypeStream)] { case (v, va, gs) => (v, gs.toGenotypeStream(v, compress)) }; .toDF(); .saveAsParquetFile(dirname + ""/rdd.parquet""); }; ```. You should probably serialize the Annotations in some way in the map before",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/116
https://github.com/hail-is/hail/issues/116:2076,Testability,test,test,2076,"vcf; running: write -o /Users/cseed/sample.vds; Exception in thread ""main"" java.lang.AssertionError: assertion failed: Unsound substitution from List(type T) to List(); at scala.reflect.internal.Types$SubstMap.<init>(Types.scala:4644); at scala.reflect.internal.Types$SubstTypeMap.<init>(Types.scala:4761); at scala.reflect.internal.Types$Type.subst(Types.scala:796); at scala.reflect.internal.Types$TypeApiImpl.substituteTypes(Types.scala:321); at scala.reflect.internal.Types$TypeApiImpl.substituteTypes(Types.scala:298); ```. and so on. I'm guessing this has to do with the doubly-nested Maps. The following fixes the problem (but of course throws out all the annotations):. ```; diff --git a/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala b/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; index 2c95bd0..c480c86 100644; --- a/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; +++ b/src/main/scala/org/broadinstitute/hail/variant/VariantSampleMatrix.scala; @@ -30,7 +30,7 @@ object VariantSampleMatrix {; // val df = sqlContext.read.parquet(dirname + ""/rdd.parquet""); val df = sqlContext.parquetFile(dirname + ""/rdd.parquet""); new VariantSampleMatrix[Genotype](metadata, df.rdd.map(r =>; - (r.getVariant(0), r.getVariantAnnotations(1), r.getGenotypeStream(2)))); + (r.getVariant(0), Annotations.emptyOfData(), r.getGenotypeStream(1)))); }; }. @@ -326,7 +326,7 @@ class RichVDS(vds: VariantDataset) {. // rdd.toDF().write.parquet(dirname + ""/rdd.parquet""); vds.rdd; - .map { case (v, va, gs) => (v, va, gs.toGenotypeStream(v, compress)) }; + .map[(Variant, GenotypeStream)] { case (v, va, gs) => (v, gs.toGenotypeStream(v, compress)) }; .toDF(); .saveAsParquetFile(dirname + ""/rdd.parquet""); }; ```. You should probably serialize the Annotations in some way in the map before writing them out (and deserialize when reading them in). Ugh. Also: add a test that does write vds, read, compare! I can't believe we missed this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/116
https://github.com/hail-is/hail/issues/118:7,Testability,test,test,7,"Travis test processes sometimes die with a gradle exit code 137, internet research suggests that this is because Travis is issuing a SIGKILL to the process for using too many resources. No clear fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/118
https://github.com/hail-is/hail/issues/118:189,Usability,clear,clear,189,"Travis test processes sometimes die with a gradle exit code 137, internet research suggests that this is because Travis is issuing a SIGKILL to the process for using too many resources. No clear fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/118
https://github.com/hail-is/hail/issues/120:619,Availability,failure,failure,619,"Tim, when I run the following command I get the exception below. ~/hail/build/install/hail/bin/hail import -i ~/t2d/GoT2D.first10k.vcf filtervariants --keep -c ""true"" count. I get the following exception. ""[-80"" is not in the original vcf, so Cotton thinks it may be because htsjdk parses the info, then you converts them back to Strings, and then reparses them, so you might have trouble eating your own output. I'll share the vcf with you. I have no trouble filtering based on sample and interval lists, or doing qc or linreg. ```; Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NumberFormatException: For input string: ""[-80""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272); at scala.collection.immutable.StringOps.toInt(StringOps.scala:30); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:676,Availability,failure,failure,676,"Tim, when I run the following command I get the exception below. ~/hail/build/install/hail/bin/hail import -i ~/t2d/GoT2D.first10k.vcf filtervariants --keep -c ""true"" count. I get the following exception. ""[-80"" is not in the original vcf, so Cotton thinks it may be because htsjdk parses the info, then you converts them back to Strings, and then reparses them, so you might have trouble eating your own output. I'll share the vcf with you. I have no trouble filtering based on sample and interval lists, or doing qc or linreg. ```; Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NumberFormatException: For input string: ""[-80""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272); at scala.collection.immutable.StringOps.toInt(StringOps.scala:30); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:78,Deployability,install,install,78,"Tim, when I run the following command I get the exception below. ~/hail/build/install/hail/bin/hail import -i ~/t2d/GoT2D.first10k.vcf filtervariants --keep -c ""true"" count. I get the following exception. ""[-80"" is not in the original vcf, so Cotton thinks it may be because htsjdk parses the info, then you converts them back to Strings, and then reparses them, so you might have trouble eating your own output. I'll share the vcf with you. I have no trouble filtering based on sample and interval lists, or doing qc or linreg. ```; Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NumberFormatException: For input string: ""[-80""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272); at scala.collection.immutable.StringOps.toInt(StringOps.scala:30); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:3875,Energy Efficiency,schedul,scheduler,3875,org.broadinstitute.hail.methods.FilterVariantCondition.apply(Filter.scala:613); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:3946,Energy Efficiency,schedul,scheduler,3946,la:613); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:7,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4305,Energy Efficiency,schedul,scheduler,4305,n$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.sch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4345,Energy Efficiency,schedul,scheduler,4345,on.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4443,Energy Efficiency,schedul,scheduler,4443,$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4540,Energy Efficiency,schedul,scheduler,4540,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4791,Energy Efficiency,schedul,scheduler,4791,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4871,Energy Efficiency,schedul,scheduler,4871,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4976,Energy Efficiency,schedul,scheduler,4976,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:5124,Energy Efficiency,schedul,scheduler,5124,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:5212,Energy Efficiency,schedul,scheduler,5212,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:5307,Energy Efficiency,schedul,scheduler,5307,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:2008,Integrability,wrap,wrapper,2008,StringLike$class.toInt(StringLike.scala:272); at scala.collection.immutable.StringOps.toInt(StringOps.scala:30); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at scala.Option.map(Option.scala:146); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info.<init>(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1.<init>(<no source file>:23); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:32); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:2); at org.broadinstitute.hail.methods.FilterVariantCondition.apply(Filter.scala:613); at org.broadinstitute.hail.driver.FilterV,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:2178,Integrability,wrap,wrapper,2178,$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at scala.Option.map(Option.scala:146); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info.<init>(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1.<init>(<no source file>:23); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:32); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:2); at org.broadinstitute.hail.methods.FilterVariantCondition.apply(Filter.scala:613); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:2387,Integrability,wrap,wrapper,2387,e$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at scala.Option.map(Option.scala:146); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info.<init>(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1.<init>(<no source file>:23); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:32); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:2); at org.broadinstitute.hail.methods.FilterVariantCondition.apply(Filter.scala:613); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Ite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:2547,Integrability,wrap,wrapper,2547,ndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at scala.Option.map(Option.scala:146); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info.<init>(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1.<init>(<no source file>:23); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:32); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:2); at org.broadinstitute.hail.methods.FilterVariantCondition.apply(Filter.scala:613); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:2700,Integrability,wrap,wrapper,2700,llection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at scala.Option.map(Option.scala:146); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info.<init>(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1.<init>(<no source file>:23); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:32); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:2); at org.broadinstitute.hail.methods.FilterVariantCondition.apply(Filter.scala:613); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:2840,Integrability,wrap,wrapper,2840,oadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info$$anonfun$3.apply(<no source file>:11); at scala.Option.map(Option.scala:146); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1$__info.<init>(<no source file>:11); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1$__vaClass$1.<init>(<no source file>:23); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:32); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$anonfun$wrapper$1.apply(<no source file>:2); at org.broadinstitute.hail.methods.FilterVariantCondition.apply(Filter.scala:613); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4068,Performance,concurren,concurrent,4068,.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4152,Performance,concurren,concurrent,4152,nstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:598,Safety,abort,aborted,598,"Tim, when I run the following command I get the exception below. ~/hail/build/install/hail/bin/hail import -i ~/t2d/GoT2D.first10k.vcf filtervariants --keep -c ""true"" count. I get the following exception. ""[-80"" is not in the original vcf, so Cotton thinks it may be because htsjdk parses the info, then you converts them back to Strings, and then reparses them, so you might have trouble eating your own output. I'll share the vcf with you. I have no trouble filtering based on sample and interval lists, or doing qc or linreg. ```; Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NumberFormatException: For input string: ""[-80""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272); at scala.collection.immutable.StringOps.toInt(StringOps.scala:30); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4475,Safety,abort,abortStage,4475,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4572,Safety,abort,abortStage,4572,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/120:4814,Safety,abort,abortStage,4814,$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/120
https://github.com/hail-is/hail/issues/121:44,Security,expose,exposed,44,"Let's keep a list of things that need to be exposed in FilterOptions with high priority. From a chat yesterday with Andrea about info.ANN fields, we need string splitting. I've gone ahead and added this, but we should continue to iterate and keep a list open",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/121
https://github.com/hail-is/hail/pull/122:10,Testability,test,tested,10,Added and tested FOS.split.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/122
https://github.com/hail-is/hail/issues/133:4,Testability,log,logistic,4,and logistic when it is ready. See:. http://genome.sph.umich.edu/wiki/EPACTS#Currently_Supported_Statistical_Tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/133
https://github.com/hail-is/hail/pull/137:84,Testability,test,tests,84,Added symmetric support for Y chromosome in MendelErrors. Addresses Issue #91 . All tests pass locally.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/137
https://github.com/hail-is/hail/pull/141:6,Testability,test,test,6,Added test for `pAB == pAB() == pAB(.5)`; Added docs to explain the difference,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/141
https://github.com/hail-is/hail/issues/146:41,Integrability,message,message,41,When dividing INT by another INT print a message to inform the user that the output is an INT and so no decimals are printed. Or set as default that it prints a double.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/146
https://github.com/hail-is/hail/issues/147:92,Availability,error,error,92,"```; FilterGenotypes.run(state, Array(""--remove"", ""-c"", ""g.ad(0) < 5"")); hail: fatal: parse error in condition: org.broadinstitute.hail.methods.FilterOption[Array[Int]] does not take parameters; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/147
https://github.com/hail-is/hail/pull/149:80,Availability,error,errors,80,"To be merges AFTER jb_mendel_y. Here I have:; 1) added documentation for mendel errors; 2) modified the functions leading to the individual and family files so that the SNP count is recorded in addition to the total count. Talking to analysts, the SNP count is useful for QC because indels are so much more volatile. If analysts will typically want both, it seems better to avoid forcing them to run mendel errors twice (before and after filtering to SNPs). Adding the NSNP column in .imendel and .fmendel breaks the PLINK spec but only very gently: adding one additional (final) column. Once we have a better sense of users needs, we should break it completely to best suit them. I'd feel comfortable breaking up chr:pos:ref:alt into separate columns now if you think the time has come...calling that column SNP is especially confusing given that the variant may be an indel. Next up for mendel will be Issues #94 and #148",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/149
https://github.com/hail-is/hail/pull/149:407,Availability,error,errors,407,"To be merges AFTER jb_mendel_y. Here I have:; 1) added documentation for mendel errors; 2) modified the functions leading to the individual and family files so that the SNP count is recorded in addition to the total count. Talking to analysts, the SNP count is useful for QC because indels are so much more volatile. If analysts will typically want both, it seems better to avoid forcing them to run mendel errors twice (before and after filtering to SNPs). Adding the NSNP column in .imendel and .fmendel breaks the PLINK spec but only very gently: adding one additional (final) column. Once we have a better sense of users needs, we should break it completely to best suit them. I'd feel comfortable breaking up chr:pos:ref:alt into separate columns now if you think the time has come...calling that column SNP is especially confusing given that the variant may be an indel. Next up for mendel will be Issues #94 and #148",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/149
https://github.com/hail-is/hail/pull/149:374,Safety,avoid,avoid,374,"To be merges AFTER jb_mendel_y. Here I have:; 1) added documentation for mendel errors; 2) modified the functions leading to the individual and family files so that the SNP count is recorded in addition to the total count. Talking to analysts, the SNP count is useful for QC because indels are so much more volatile. If analysts will typically want both, it seems better to avoid forcing them to run mendel errors twice (before and after filtering to SNPs). Adding the NSNP column in .imendel and .fmendel breaks the PLINK spec but only very gently: adding one additional (final) column. Once we have a better sense of users needs, we should break it completely to best suit them. I'd feel comfortable breaking up chr:pos:ref:alt into separate columns now if you think the time has come...calling that column SNP is especially confusing given that the variant may be an indel. Next up for mendel will be Issues #94 and #148",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/149
https://github.com/hail-is/hail/pull/153:196,Availability,error,error,196,"To be merged AFTER jb_mendel_y and jb_mendel_docs. This modifies the behavior of Pedigree to discard those samples not in the variant data set when reading in a .fam file, rather than throwing an error when indexing fails, addressing Issue #94. I'd like advice on how best to throw a warning when samples are tossed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/153
https://github.com/hail-is/hail/issues/156:164,Performance,perform,performance,164,"Right now it's not entirely clear what to do to ensure that a build works on the dataflow cluster, as well as to generate a set of benchmarks that capture a set of performance statistics well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/156
https://github.com/hail-is/hail/issues/156:131,Testability,benchmark,benchmarks,131,"Right now it's not entirely clear what to do to ensure that a build works on the dataflow cluster, as well as to generate a set of benchmarks that capture a set of performance statistics well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/156
https://github.com/hail-is/hail/issues/156:28,Usability,clear,clear,28,"Right now it's not entirely clear what to do to ensure that a build works on the dataflow cluster, as well as to generate a set of benchmarks that capture a set of performance statistics well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/156
https://github.com/hail-is/hail/issues/157:43,Availability,down,downstream,43,"Spaces in sample names are a nightmare for downstream analysis tools, so when I get my callsets from Picard, I immediately remove spaces in sample names and re-write my VCF. It usually takes a day or two to re-write the entire VCF, depending on the size of the callset. I would be an extremely happy camper if I could immediately import my VCF from Picard and either fix sample names on the fly during the import, or fix sample names in the .vds after the import (as a separate step). Either way, this should save a substantial chunk of time right off the top in the QC process.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/157
https://github.com/hail-is/hail/issues/157:232,Integrability,depend,depending,232,"Spaces in sample names are a nightmare for downstream analysis tools, so when I get my callsets from Picard, I immediately remove spaces in sample names and re-write my VCF. It usually takes a day or two to re-write the entire VCF, depending on the size of the callset. I would be an extremely happy camper if I could immediately import my VCF from Picard and either fix sample names on the fly during the import, or fix sample names in the .vds after the import (as a separate step). Either way, this should save a substantial chunk of time right off the top in the QC process.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/157
https://github.com/hail-is/hail/issues/160:67,Availability,error,errors,67,Tabix just immediately creates a tiny index but does not throw any errors.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/160
https://github.com/hail-is/hail/issues/163:34,Availability,error,error,34,"Try to ""read"" a vcf, and get this error:. ```; running: read -i chr22.vcf.bgz; Exception in thread ""main"" java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:221); at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:23); at org.broadinstitute.hail.driver.Read$.run(Read.scala:21); at org.broadinstitute.hail.driver.Read$.run(Read.scala:6); at org.broadinstitute.hail.driver.Command.run(Command.scala:59); at org.broadinstitute.hail.driver.Main$$anonfun$main$2$$anonfun$4.apply(Main.scala:182); ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/163
https://github.com/hail-is/hail/issues/170:10,Testability,log,logging,10,"We need a logging system to organize warnings, info, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/170
https://github.com/hail-is/hail/issues/196:1491,Security,hash,hash,1491,"For Ricopili, Stephan creates a file in the user's home directory that records the exact command line used, the current working directory, the timestamp, and the task name plus the number of jobs submitted to the cluster. Example:; my_working_directory/pca pcaer_20 --noproject --prefercase --preferfam --out muhammad_pakistani_4pop pop_4pop_mix_SEQ.bim scz_ayub1_sas_jg-qc.bim prune2.000001 Fri_Dec_18_17:55:29_2015; my_working_directory/pca pcaer_20 --noproject --prefercase --preferfam --out muhammad_pakistani_4pop pop_4pop_mix_SEQ.bim scz_ayub1_sas_jg-qc.bim genome.000013 Fri_Dec_18_17:57:12_2015; my_working_directory/pca pcaer_20 --noproject --prefercase --preferfam --out muhammad_pakistani_4pop pop_4pop_mix_SEQ.bim scz_ayub1_sas_jg-qc.bim epca.000001 Fri_Dec_18_17:57:53_2015; my_working_directory/pca pcaer_20 --noproject --prefercase --preferfam --out muhammad_pakistani_4pop pop_4pop_mix_SEQ.bim scz_ayub1_sas_jg-qc.bim qqpl.000020 Fri_Dec_18_18:26:43_2015; my_working_directory/pca pcaer_20 --noproject --prefercase --preferfam --out muhammad_pakistani_4pop pop_4pop_mix_SEQ.bim scz_ayub1_sas_jg-qc.bim pcaplot.000001 Fri_Dec_18_18:28:15_2015; my_working_directory/pca pcaer_20 --noproject --prefercase --preferfam --out muhammad_pakistani_4pop pop_4pop_mix_SEQ.bim scz_ayub1_sas_jg-qc.bim finished Fri_Dec_18_18:32:17_2015. I think it would be good to record a timestamp, the command line used, possibly the working directory, and an identifier for the version of hail used (hash code) in one place (maybe ~/.hail_history??) that way someone can easily grep the file to figure out what they did for a particular dataset in the past.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/196
https://github.com/hail-is/hail/pull/198:57,Modifiability,evolve,evolve,57,Adding doc for linreg in its current form. This doc will evolve as we add more features.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/198
https://github.com/hail-is/hail/issues/200:30,Deployability,pipeline,pipeline,30,"I just found a bug in my GWAS pipeline where I was excluding most of the X-chromosome SNPs for cohorts with no gender reported because I used the flag ""--allow-no-sex"". Unfortunately, this meant HWE was calculated using both the unknown males and females (rather than females only). We should make sure HWE calculations in Hail on the X-chromosome are done smarter than what is done in PLINK!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/200
https://github.com/hail-is/hail/pull/202:217,Performance,load,loadings,217,Added detailed PCA docs in Pandoc format (Markdown+LaTeX).; Rewrote PCA command and SamplePCA method accordingly.; Passes preliminary testing including handling of missingness. Tests still needed.; Variant sorting in loadings output still needed.; Further issues flagged in the doc.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/202
https://github.com/hail-is/hail/pull/202:134,Testability,test,testing,134,Added detailed PCA docs in Pandoc format (Markdown+LaTeX).; Rewrote PCA command and SamplePCA method accordingly.; Passes preliminary testing including handling of missingness. Tests still needed.; Variant sorting in loadings output still needed.; Further issues flagged in the doc.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/202
https://github.com/hail-is/hail/pull/202:177,Testability,Test,Tests,177,Added detailed PCA docs in Pandoc format (Markdown+LaTeX).; Rewrote PCA command and SamplePCA method accordingly.; Passes preliminary testing including handling of missingness. Tests still needed.; Variant sorting in loadings output still needed.; Further issues flagged in the doc.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/202
https://github.com/hail-is/hail/issues/205:65,Performance,load,loadings,65,"1. Add options to store the scores as sample annotations and the loadings as variant annotations (or, eventually, store by default and write out optionally); 2. Once LD-pruning is implemented, should it be performed first automatically? Probably not, but perhaps an option and the doc should mention the issue.; 3. PLINK has an option to use X-chromosome variants. What is it doing exactly? There are several decisions around encoding hemizygous sites for males. More importantly, does anyone use it? Should we support it?; 4. What about PCA of things other than genotypes, such as missingness? Analysts have mentioned applications to QC and flagged the latter specifically, which is implemented in GCTA.; 5. Extension to multiallelics? Probably not so important as few variants have more than two common alleles and each individual variant generally contributes little. If we did it, a good approach is probably a one-hot encoding although the variance normalization needs some care. For microsatellites/STRs a quantitative rather than categorical encoding may be better.; 6. Support for outlier detection a la SmartPCA and/or EIGENSTRAT?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/205
https://github.com/hail-is/hail/issues/205:206,Performance,perform,performed,206,"1. Add options to store the scores as sample annotations and the loadings as variant annotations (or, eventually, store by default and write out optionally); 2. Once LD-pruning is implemented, should it be performed first automatically? Probably not, but perhaps an option and the doc should mention the issue.; 3. PLINK has an option to use X-chromosome variants. What is it doing exactly? There are several decisions around encoding hemizygous sites for males. More importantly, does anyone use it? Should we support it?; 4. What about PCA of things other than genotypes, such as missingness? Analysts have mentioned applications to QC and flagged the latter specifically, which is implemented in GCTA.; 5. Extension to multiallelics? Probably not so important as few variants have more than two common alleles and each individual variant generally contributes little. If we did it, a good approach is probably a one-hot encoding although the variance normalization needs some care. For microsatellites/STRs a quantitative rather than categorical encoding may be better.; 6. Support for outlier detection a la SmartPCA and/or EIGENSTRAT?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/205
https://github.com/hail-is/hail/issues/205:1097,Safety,detect,detection,1097,"1. Add options to store the scores as sample annotations and the loadings as variant annotations (or, eventually, store by default and write out optionally); 2. Once LD-pruning is implemented, should it be performed first automatically? Probably not, but perhaps an option and the doc should mention the issue.; 3. PLINK has an option to use X-chromosome variants. What is it doing exactly? There are several decisions around encoding hemizygous sites for males. More importantly, does anyone use it? Should we support it?; 4. What about PCA of things other than genotypes, such as missingness? Analysts have mentioned applications to QC and flagged the latter specifically, which is implemented in GCTA.; 5. Extension to multiallelics? Probably not so important as few variants have more than two common alleles and each individual variant generally contributes little. If we did it, a good approach is probably a one-hot encoding although the variance normalization needs some care. For microsatellites/STRs a quantitative rather than categorical encoding may be better.; 6. Support for outlier detection a la SmartPCA and/or EIGENSTRAT?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/205
https://github.com/hail-is/hail/issues/209:895,Deployability,Pipeline,PipelineSuite,895,Cotton thinks this is related to paths in Spark runner. java.lang.NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:110); at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:269); at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:195); at org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix.computeSVD(IndexedRowMatrix.scala:154); at org.broadinstitute.hail.methods.SamplePCA.apply(SamplePCA.scala:10); at org.broadinstitute.hail.driver.PCA$.run(PCA.scala:23); at org.broadinstitute.hail.driver.PCA$.run(PCA.scala:7); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:69); at org.broadinstitute.hail.driver.Command.run(Command.scala:73); at org.broadinstitute.hail.PipelineSuite.defaultPipeline(PipelineSuite.scala:22),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/209
https://github.com/hail-is/hail/issues/209:925,Deployability,Pipeline,PipelineSuite,925,Cotton thinks this is related to paths in Spark runner. java.lang.NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:110); at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:269); at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:195); at org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix.computeSVD(IndexedRowMatrix.scala:154); at org.broadinstitute.hail.methods.SamplePCA.apply(SamplePCA.scala:10); at org.broadinstitute.hail.driver.PCA$.run(PCA.scala:23); at org.broadinstitute.hail.driver.PCA$.run(PCA.scala:7); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:69); at org.broadinstitute.hail.driver.Command.run(Command.scala:73); at org.broadinstitute.hail.PipelineSuite.defaultPipeline(PipelineSuite.scala:22),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/209
https://github.com/hail-is/hail/pull/210:16,Testability,test,tested,16,"CallStreams are tested directly in HardCallSetSuite, but HardCallSet only indirectly in the final test of LinearRegressionSuite. So I think I should add a more direct test within HardCallSetSuite off of a small vcf.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/210
https://github.com/hail-is/hail/pull/210:98,Testability,test,test,98,"CallStreams are tested directly in HardCallSetSuite, but HardCallSet only indirectly in the final test of LinearRegressionSuite. So I think I should add a more direct test within HardCallSetSuite off of a small vcf.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/210
https://github.com/hail-is/hail/pull/210:167,Testability,test,test,167,"CallStreams are tested directly in HardCallSetSuite, but HardCallSet only indirectly in the final test of LinearRegressionSuite. So I think I should add a more direct test within HardCallSetSuite off of a small vcf.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/210
https://github.com/hail-is/hail/pull/215:6,Testability,test,test,6,"Added test for annotation.isMissing, fixed code in AST",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/215
https://github.com/hail-is/hail/pull/218:260,Testability,test,tests,260,"Rather than give you a million minor comments, I just made the changes I wanted to suggest. The code is looking really great. I will have at one more round of comments on the VCF code (ugh) and the annotation internals, both of which I'm still looking at. All tests pass. Feel free to fight back on any of these. Here's a rough summary of the changes:. Moved RichRow to utils.; Use named args for booleans (nullable in StructField, mostly).; Renamed signatures signature in various places.; Use convience functions consistently throughout.; Annotation clients shouldn't ever see Rows: various changes, toRow -> toAnnotation, Annotation(...) -> Row.fromSeq(Array(...)), etc.; Removed TAbstractStruct/TVdsStruct.; toRow goes on class, not object.; Don't store out Variant in GenotypeStream (duplicate).; Put back RichRow.toVariant, .toGenotypeStream for consistency with other Row client code.; Put schema on AltAllele, too.; Signature argument first in insert for consistency between Signature and VSM operations and between convience and main functions.; Formatting: always; } else; never; }; else; Some gratuitous renaming. Some not so gratuitous renaming.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/218
https://github.com/hail-is/hail/pull/219:407,Usability,simpl,simple,407,"Mostly minor. Let me know if you have any questions. If you merge this in to tp_annorework, I'll merge it in to master. Moved Annotation.emptySignature to Signature.empty.; Use convenience functions in more places (looked over all instances of List).; Rename Annotation.getSchema to .schema. Make schema virtual on Type. (Prefer virtual functions to match).; Some renaming in Signature.; Removed () on some simple functions (.getSchema(), .empty(), etc).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/219
https://github.com/hail-is/hail/issues/225:45,Deployability,install,install,45,VEP is currently untested. Need:; - Standard install location (with installation instructions) and vep.properties to run tests locally.; - Kyle has agreed to compare Hail and non-Hail pipelines. Migrate this comparison into automated tests.; - Option to set vep.properties in tests to run with VEP on the cluster.; - Konrad has promised checks to make sure VEP/Loftee is working. Make into automated test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/225
https://github.com/hail-is/hail/issues/225:68,Deployability,install,installation,68,VEP is currently untested. Need:; - Standard install location (with installation instructions) and vep.properties to run tests locally.; - Kyle has agreed to compare Hail and non-Hail pipelines. Migrate this comparison into automated tests.; - Option to set vep.properties in tests to run with VEP on the cluster.; - Konrad has promised checks to make sure VEP/Loftee is working. Make into automated test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/225
https://github.com/hail-is/hail/issues/225:184,Deployability,pipeline,pipelines,184,VEP is currently untested. Need:; - Standard install location (with installation instructions) and vep.properties to run tests locally.; - Kyle has agreed to compare Hail and non-Hail pipelines. Migrate this comparison into automated tests.; - Option to set vep.properties in tests to run with VEP on the cluster.; - Konrad has promised checks to make sure VEP/Loftee is working. Make into automated test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/225
https://github.com/hail-is/hail/issues/225:121,Testability,test,tests,121,VEP is currently untested. Need:; - Standard install location (with installation instructions) and vep.properties to run tests locally.; - Kyle has agreed to compare Hail and non-Hail pipelines. Migrate this comparison into automated tests.; - Option to set vep.properties in tests to run with VEP on the cluster.; - Konrad has promised checks to make sure VEP/Loftee is working. Make into automated test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/225
https://github.com/hail-is/hail/issues/225:234,Testability,test,tests,234,VEP is currently untested. Need:; - Standard install location (with installation instructions) and vep.properties to run tests locally.; - Kyle has agreed to compare Hail and non-Hail pipelines. Migrate this comparison into automated tests.; - Option to set vep.properties in tests to run with VEP on the cluster.; - Konrad has promised checks to make sure VEP/Loftee is working. Make into automated test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/225
https://github.com/hail-is/hail/issues/225:276,Testability,test,tests,276,VEP is currently untested. Need:; - Standard install location (with installation instructions) and vep.properties to run tests locally.; - Kyle has agreed to compare Hail and non-Hail pipelines. Migrate this comparison into automated tests.; - Option to set vep.properties in tests to run with VEP on the cluster.; - Konrad has promised checks to make sure VEP/Loftee is working. Make into automated test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/225
https://github.com/hail-is/hail/issues/225:400,Testability,test,test,400,VEP is currently untested. Need:; - Standard install location (with installation instructions) and vep.properties to run tests locally.; - Kyle has agreed to compare Hail and non-Hail pipelines. Migrate this comparison into automated tests.; - Option to set vep.properties in tests to run with VEP on the cluster.; - Konrad has promised checks to make sure VEP/Loftee is working. Make into automated test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/225
https://github.com/hail-is/hail/pull/227:113,Testability,assert,assert,113,"1. Allow floating-point literals to take the form "".99"" instead of ""0.99"".; 2. Fix typing of ""if"" expressions to assert that both the then and else ASTs have the same type, then return that type (was TBoolean); 3. Wrote a test for both these cases",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/227
https://github.com/hail-is/hail/pull/227:222,Testability,test,test,222,"1. Allow floating-point literals to take the form "".99"" instead of ""0.99"".; 2. Fix typing of ""if"" expressions to assert that both the then and else ASTs have the same type, then return that type (was TBoolean); 3. Wrote a test for both these cases",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/227
https://github.com/hail-is/hail/pull/228:56,Availability,error,error,56,"```; Lots of local cleanups.; Use `' for quoting inside error messages.; Added SuperCommand, ToplevelCommands. Use for annotatevariants and annotatesamples.; Try to make multiple instances of (essentially) same error message consistent.; Added option to LoadVCF, ImportVCF to skip genotypes. Use in importannotations.; Fixed some bugs in FatalException handling.; Moved Type.parse to trait Parsable.; Added Parser.parseAnnotationTypes.; Added `type_expr' non-terminal to Parser.; Prefer `import ...expr._' to `import ...expr' and `expr.Foo'.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/228
https://github.com/hail-is/hail/pull/228:211,Availability,error,error,211,"```; Lots of local cleanups.; Use `' for quoting inside error messages.; Added SuperCommand, ToplevelCommands. Use for annotatevariants and annotatesamples.; Try to make multiple instances of (essentially) same error message consistent.; Added option to LoadVCF, ImportVCF to skip genotypes. Use in importannotations.; Fixed some bugs in FatalException handling.; Moved Type.parse to trait Parsable.; Added Parser.parseAnnotationTypes.; Added `type_expr' non-terminal to Parser.; Prefer `import ...expr._' to `import ...expr' and `expr.Foo'.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/228
https://github.com/hail-is/hail/pull/228:62,Integrability,message,messages,62,"```; Lots of local cleanups.; Use `' for quoting inside error messages.; Added SuperCommand, ToplevelCommands. Use for annotatevariants and annotatesamples.; Try to make multiple instances of (essentially) same error message consistent.; Added option to LoadVCF, ImportVCF to skip genotypes. Use in importannotations.; Fixed some bugs in FatalException handling.; Moved Type.parse to trait Parsable.; Added Parser.parseAnnotationTypes.; Added `type_expr' non-terminal to Parser.; Prefer `import ...expr._' to `import ...expr' and `expr.Foo'.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/228
https://github.com/hail-is/hail/pull/228:217,Integrability,message,message,217,"```; Lots of local cleanups.; Use `' for quoting inside error messages.; Added SuperCommand, ToplevelCommands. Use for annotatevariants and annotatesamples.; Try to make multiple instances of (essentially) same error message consistent.; Added option to LoadVCF, ImportVCF to skip genotypes. Use in importannotations.; Fixed some bugs in FatalException handling.; Moved Type.parse to trait Parsable.; Added Parser.parseAnnotationTypes.; Added `type_expr' non-terminal to Parser.; Prefer `import ...expr._' to `import ...expr' and `expr.Foo'.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/228
https://github.com/hail-is/hail/pull/228:254,Performance,Load,LoadVCF,254,"```; Lots of local cleanups.; Use `' for quoting inside error messages.; Added SuperCommand, ToplevelCommands. Use for annotatevariants and annotatesamples.; Try to make multiple instances of (essentially) same error message consistent.; Added option to LoadVCF, ImportVCF to skip genotypes. Use in importannotations.; Fixed some bugs in FatalException handling.; Moved Type.parse to trait Parsable.; Added Parser.parseAnnotationTypes.; Added `type_expr' non-terminal to Parser.; Prefer `import ...expr._' to `import ...expr' and `expr.Foo'.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/228
https://github.com/hail-is/hail/pull/235:91,Testability,test,test,91,"Ported support (from t2d branch) for filtering variants with symbolic alleles, and added a test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/235
https://github.com/hail-is/hail/issues/236:362,Performance,perform,performance,362,"As per the discussion on Slack, Option should be the default and null is allowed in two places:; 1. At legacy/Java APIs that expect null. (Possibly) null values should be immediately converted to Option with `Option(expr)` and Options should be converted to possibly null values with `o.orNull`.; 2. When the scope of null values is well-defined and significant performance gains justify the added danger. Immediately two instances come to mind that can be improved:; - State has an nullable vds; - Args4j encourages the use of null and needs to go (for this and many reasons). We need to examine every instance of `null` and `= _` in the code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/236
https://github.com/hail-is/hail/pull/239:161,Testability,test,test,161,"vsm.annotatesamples had a really bad problem with it, where it was reducing the dimension of sampleAnnotations to the dimension of localSamples. Broke it with a test, and fixed the problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/239
https://github.com/hail-is/hail/pull/242:1289,Availability,avail,available,1289,"Adds `writekudu` and `readkudu` commands for storing a VariantSampleMatrix in Kudu. Note that only biallelic variants can be stored at the moment, so `splitmulti` should be used. Sample run. ```; SPARK_MASTER=yarn-client; # chr1 in 6m52.6s; spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; importvcf -f vcf-1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf splitmulti writekudu -o file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com --drop; # read (2 min or so); spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; readkudu -i file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com count ; ```. To install Kudu on a cluster, see http://www.cloudera.com/documentation/betas/kudu/0-5-0/topics/kudu_installation.html#concept_u4s_tbq_dt_unique_1, and follow the instructions for installing from parcels. The CDS file is available at http://archive.cloudera.com/beta/kudu/csd/. You can also run Kudu locally using a VM, see http://getkudu.io/docs/quickstart.html. This is suitable for running unit tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242
https://github.com/hail-is/hail/pull/242:1071,Deployability,install,install,1071,"Adds `writekudu` and `readkudu` commands for storing a VariantSampleMatrix in Kudu. Note that only biallelic variants can be stored at the moment, so `splitmulti` should be used. Sample run. ```; SPARK_MASTER=yarn-client; # chr1 in 6m52.6s; spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; importvcf -f vcf-1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf splitmulti writekudu -o file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com --drop; # read (2 min or so); spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; readkudu -i file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com count ; ```. To install Kudu on a cluster, see http://www.cloudera.com/documentation/betas/kudu/0-5-0/topics/kudu_installation.html#concept_u4s_tbq_dt_unique_1, and follow the instructions for installing from parcels. The CDS file is available at http://archive.cloudera.com/beta/kudu/csd/. You can also run Kudu locally using a VM, see http://getkudu.io/docs/quickstart.html. This is suitable for running unit tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242
https://github.com/hail-is/hail/pull/242:1248,Deployability,install,installing,1248,"Adds `writekudu` and `readkudu` commands for storing a VariantSampleMatrix in Kudu. Note that only biallelic variants can be stored at the moment, so `splitmulti` should be used. Sample run. ```; SPARK_MASTER=yarn-client; # chr1 in 6m52.6s; spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; importvcf -f vcf-1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf splitmulti writekudu -o file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com --drop; # read (2 min or so); spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; readkudu -i file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com count ; ```. To install Kudu on a cluster, see http://www.cloudera.com/documentation/betas/kudu/0-5-0/topics/kudu_installation.html#concept_u4s_tbq_dt_unique_1, and follow the instructions for installing from parcels. The CDS file is available at http://archive.cloudera.com/beta/kudu/csd/. You can also run Kudu locally using a VM, see http://getkudu.io/docs/quickstart.html. This is suitable for running unit tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242
https://github.com/hail-is/hail/pull/242:1466,Testability,test,tests,1466,"Adds `writekudu` and `readkudu` commands for storing a VariantSampleMatrix in Kudu. Note that only biallelic variants can be stored at the moment, so `splitmulti` should be used. Sample run. ```; SPARK_MASTER=yarn-client; # chr1 in 6m52.6s; spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; importvcf -f vcf-1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf splitmulti writekudu -o file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com --drop; # read (2 min or so); spark-submit \; --master $SPARK_MASTER \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; build/libs/hail-all-spark.jar \; readkudu -i file:///home/tom/sample.vds -t variants -m bottou06.sjc.cloudera.com count ; ```. To install Kudu on a cluster, see http://www.cloudera.com/documentation/betas/kudu/0-5-0/topics/kudu_installation.html#concept_u4s_tbq_dt_unique_1, and follow the instructions for installing from parcels. The CDS file is available at http://archive.cloudera.com/beta/kudu/csd/. You can also run Kudu locally using a VM, see http://getkudu.io/docs/quickstart.html. This is suitable for running unit tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242
https://github.com/hail-is/hail/pull/261:139,Modifiability,variab,variables,139,"Ready for review. The regex is working, though not sure where to place it in our code base. To properly match against nonNumeric() with no variables, there must be no groups (logically!)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/261
https://github.com/hail-is/hail/pull/261:175,Testability,log,logically,175,"Ready for review. The regex is working, though not sure where to place it in our code base. To properly match against nonNumeric() with no variables, there must be no groups (logically!)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/261
https://github.com/hail-is/hail/issues/262:107,Availability,error,error,107,"I argue `fatalIf(p, msg)` is less readable than `if (p) fatal(msg)` and not any shorter. It also causes an error since you can't control which fatal to call, e.g., `Utils.fatal` vs `Line.fatal`. @tpoterba Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/262
https://github.com/hail-is/hail/pull/269:54,Integrability,depend,dependency,54,BaseType compaion object => Type.; Removed ScalaCheck dependency.; Added Boolean generator.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/269
https://github.com/hail-is/hail/pull/272:57,Energy Efficiency,reduce,reduced,57,"mendelerrors and linreg has the longest help lines, so I reduced them to 80 characters, while preserving the content. Hopefully the top level help descriptions will be used as reminders of functionality rather than reference documentation, curious for your opinion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/272
https://github.com/hail-is/hail/issues/275:202,Availability,failure,failure,202,Trying to build on ubuntu ; Welcome to Ubuntu Xenial Xerus (development branch) (GNU/Linux 4.4.0-16-generic x86_64). This tree builds fine on mac using grade installDist. ; On ubuntu it gives the below failure message. We wondered if it might have something to do with case sensitivity issues in file/path naming (mac maintaining case but being case-insensitive). Let me know if you would like more info. ; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/275
https://github.com/hail-is/hail/issues/275:454,Availability,FAILURE,FAILURE,454,Trying to build on ubuntu ; Welcome to Ubuntu Xenial Xerus (development branch) (GNU/Linux 4.4.0-16-generic x86_64). This tree builds fine on mac using grade installDist. ; On ubuntu it gives the below failure message. We wondered if it might have something to do with case sensitivity issues in file/path naming (mac maintaining case but being case-insensitive). Let me know if you would like more info. ; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/275
https://github.com/hail-is/hail/issues/275:158,Deployability,install,installDist,158,Trying to build on ubuntu ; Welcome to Ubuntu Xenial Xerus (development branch) (GNU/Linux 4.4.0-16-generic x86_64). This tree builds fine on mac using grade installDist. ; On ubuntu it gives the below failure message. We wondered if it might have something to do with case sensitivity issues in file/path naming (mac maintaining case but being case-insensitive). Let me know if you would like more info. ; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/275
https://github.com/hail-is/hail/issues/275:545,Deployability,configurat,configuration,545,Trying to build on ubuntu ; Welcome to Ubuntu Xenial Xerus (development branch) (GNU/Linux 4.4.0-16-generic x86_64). This tree builds fine on mac using grade installDist. ; On ubuntu it gives the below failure message. We wondered if it might have something to do with case sensitivity issues in file/path naming (mac maintaining case but being case-insensitive). Let me know if you would like more info. ; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/275
https://github.com/hail-is/hail/issues/275:210,Integrability,message,message,210,Trying to build on ubuntu ; Welcome to Ubuntu Xenial Xerus (development branch) (GNU/Linux 4.4.0-16-generic x86_64). This tree builds fine on mac using grade installDist. ; On ubuntu it gives the below failure message. We wondered if it might have something to do with case sensitivity issues in file/path naming (mac maintaining case but being case-insensitive). Let me know if you would like more info. ; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/275
https://github.com/hail-is/hail/issues/275:545,Modifiability,config,configuration,545,Trying to build on ubuntu ; Welcome to Ubuntu Xenial Xerus (development branch) (GNU/Linux 4.4.0-16-generic x86_64). This tree builds fine on mac using grade installDist. ; On ubuntu it gives the below failure message. We wondered if it might have something to do with case sensitivity issues in file/path naming (mac maintaining case but being case-insensitive). Let me know if you would like more info. ; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/275
https://github.com/hail-is/hail/issues/293:177,Deployability,pipeline,pipeline,177,"- set up tests that evaluate performance tradeoffs; - build interface for controlling compression levels in import / write; - also test parquet LZ4 / snappy on write with Mitja pipeline, with size",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/293
https://github.com/hail-is/hail/issues/293:60,Integrability,interface,interface,60,"- set up tests that evaluate performance tradeoffs; - build interface for controlling compression levels in import / write; - also test parquet LZ4 / snappy on write with Mitja pipeline, with size",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/293
https://github.com/hail-is/hail/issues/293:29,Performance,perform,performance,29,"- set up tests that evaluate performance tradeoffs; - build interface for controlling compression levels in import / write; - also test parquet LZ4 / snappy on write with Mitja pipeline, with size",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/293
https://github.com/hail-is/hail/issues/293:9,Testability,test,tests,9,"- set up tests that evaluate performance tradeoffs; - build interface for controlling compression levels in import / write; - also test parquet LZ4 / snappy on write with Mitja pipeline, with size",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/293
https://github.com/hail-is/hail/issues/293:131,Testability,test,test,131,"- set up tests that evaluate performance tradeoffs; - build interface for controlling compression levels in import / write; - also test parquet LZ4 / snappy on write with Mitja pipeline, with size",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/293
https://github.com/hail-is/hail/pull/294:192,Availability,error,error,192,#290 - nocompress in vcf wasn't working; #283 - fam export now tab separated; #262 - fatalIf is dead; #246 - add g.fractionReadsAlt; #237 - give tsv annotators a delimiter option. Fixed fatal error in case of old VDS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/294
https://github.com/hail-is/hail/issues/295:103,Deployability,install,install,103,-- Takes 8 minutes per build. Probably because we are running Java over the network. Figure out how to install locally. -- Figure out how to build more than one job at a time. -- Fix reporting on email to only show Failed Tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/295
https://github.com/hail-is/hail/issues/295:222,Testability,Test,Tests,222,-- Takes 8 minutes per build. Probably because we are running Java over the network. Figure out how to install locally. -- Figure out how to build more than one job at a time. -- Fix reporting on email to only show Failed Tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/295
https://github.com/hail-is/hail/pull/299:0,Testability,Test,Test,0,Test by comparing with plink on random datasets with no missingness.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/299
https://github.com/hail-is/hail/pull/300:132,Testability,test,tests,132,"Read now takes --skip-genotypes to mirror importvcf.; filter{samples, variants} now take --all option.; Also, quiet plink output in tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/300
https://github.com/hail-is/hail/issues/301:124,Availability,Error,Error,124,hail-new importvcf /user/lfran/MacArthur_Merck_Finns.vcf.bgz splitmulti \; write -o /user/aganna/MacArthur_Merck_Finns.vds. Error: (1525 + 59) / 23758]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/222377/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/301
https://github.com/hail-is/hail/issues/301:219,Safety,abort,aborted,219,hail-new importvcf /user/lfran/MacArthur_Merck_Finns.vcf.bgz splitmulti \; write -o /user/aganna/MacArthur_Merck_Finns.vds. Error: (1525 + 59) / 23758]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/222377/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/301
https://github.com/hail-is/hail/issues/301:234,Testability,log,log,234,hail-new importvcf /user/lfran/MacArthur_Merck_Finns.vcf.bgz splitmulti \; write -o /user/aganna/MacArthur_Merck_Finns.vds. Error: (1525 + 59) / 23758]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/222377/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/301
https://github.com/hail-is/hail/issues/301:300,Testability,log,log,300,hail-new importvcf /user/lfran/MacArthur_Merck_Finns.vcf.bgz splitmulti \; write -o /user/aganna/MacArthur_Merck_Finns.vds. Error: (1525 + 59) / 23758]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/222377/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/301
https://github.com/hail-is/hail/issues/302:112,Modifiability,config,config,112,"So I did:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 100 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds . It almost immediately advanced to the write, then it sat there having tasks fail for two hours, then it said:; [Stage 1:> (0 + 35) / 100]; hail: write: caught exception: org.apache.spark.SparkException: Job aborted. log here: /humgen/atgu1/fs03/satterst/hail.jobaborted.log",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302
https://github.com/hail-is/hail/issues/302:426,Safety,abort,aborted,426,"So I did:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 100 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds . It almost immediately advanced to the write, then it sat there having tasks fail for two hours, then it said:; [Stage 1:> (0 + 35) / 100]; hail: write: caught exception: org.apache.spark.SparkException: Job aborted. log here: /humgen/atgu1/fs03/satterst/hail.jobaborted.log",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302
https://github.com/hail-is/hail/issues/302:435,Testability,log,log,435,"So I did:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 100 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds . It almost immediately advanced to the write, then it sat there having tasks fail for two hours, then it said:; [Stage 1:> (0 + 35) / 100]; hail: write: caught exception: org.apache.spark.SparkException: Job aborted. log here: /humgen/atgu1/fs03/satterst/hail.jobaborted.log",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302
https://github.com/hail-is/hail/issues/302:489,Testability,log,log,489,"So I did:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 100 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds . It almost immediately advanced to the write, then it sat there having tasks fail for two hours, then it said:; [Stage 1:> (0 + 35) / 100]; hail: write: caught exception: org.apache.spark.SparkException: Job aborted. log here: /humgen/atgu1/fs03/satterst/hail.jobaborted.log",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/302
https://github.com/hail-is/hail/issues/303:203,Availability,Error,Error,203,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:811,Deployability,deploy,deploy,811,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:848,Deployability,deploy,deploy,848,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:920,Deployability,deploy,deploy,920,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:996,Deployability,deploy,deploy,996,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:1067,Deployability,deploy,deploy,1067,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:1136,Deployability,deploy,deploy,1136,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:105,Modifiability,config,config,105,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:1357,Performance,load,loadClass,1357,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:1426,Performance,load,loadClass,1426,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:1481,Performance,load,loadClass,1481,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:1533,Testability,log,log,1533,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/303:1599,Testability,log,log,1599,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/303
https://github.com/hail-is/hail/issues/304:343,Availability,Error,Error,343,"command:. hail-new read -i /user/tpoterba/exac_reimport.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. hail: info: running: write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds; [Stage 2:> (0 + 72) / 14038]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/223029/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304
https://github.com/hail-is/hail/issues/304:536,Safety,abort,aborted,536,"command:. hail-new read -i /user/tpoterba/exac_reimport.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. hail: info: running: write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds; [Stage 2:> (0 + 72) / 14038]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/223029/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304
https://github.com/hail-is/hail/issues/304:551,Testability,log,log,551,"command:. hail-new read -i /user/tpoterba/exac_reimport.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. hail: info: running: write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds; [Stage 2:> (0 + 72) / 14038]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/223029/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304
https://github.com/hail-is/hail/issues/304:617,Testability,log,log,617,"command:. hail-new read -i /user/tpoterba/exac_reimport.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. hail: info: running: write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds; [Stage 2:> (0 + 72) / 14038]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/223029/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/304
https://github.com/hail-is/hail/issues/309:341,Availability,Error,Error,341,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:949,Deployability,deploy,deploy,949,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:986,Deployability,deploy,deploy,986,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1058,Deployability,deploy,deploy,1058,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1134,Deployability,deploy,deploy,1134,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1205,Deployability,deploy,deploy,1205,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1274,Deployability,deploy,deploy,1274,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1495,Performance,load,loadClass,1495,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1564,Performance,load,loadClass,1564,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1619,Performance,load,loadClass,1619,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1657,Testability,log,log,1657,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1677,Testability,log,log,1677,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/309:1743,Testability,log,log,1743,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/309
https://github.com/hail-is/hail/issues/317:1352,Availability,error,error,1352,"I'm running the following script: . /psych/genetics_data/working/cseed/bin/hail read -i ${input_vds} \; annotatevariants tsv file:///medpop/esp2/mzekavat/Estonia/UPDATED_TOOLS/dbNSFPv3.2/dbNSFP3.2a.ALLChr.bgz \; -r va.dbNSFP \; -t 'SIFT_pred: String, PROVEAN_pred: String, Polyphen2_HDIV_pred: String, Polyphen2_HVAR_pred: String, LRT_pred: String, MutationTaster_pred: String, MutationAssessor_pred: String, FATHMM_pred: String, MetaSVM_pred: String, MetaLR_pred: String, CADD_phred: Double, `Eigen-raw`: Double, `Eigen-phred`: Double, `Eigen-raw_rankscore`: Double' \; -v ""#chr,pos(1-based),ref,alt"" \; -m ""."" \; annotatevariants expr -c 'va.of8 = (if (""D"" ~ va.dbNSFP.SIFT_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.PROVEAN_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.Polyphen2_HDIV_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.Polyphen2_HVAR_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.LRT_pred) 1 else 0) + (if (""H"" ~ va.dbNSFP.MutationAssessor_pred || ""M"" ~ va.dbNSFP.MutationAssessor_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.MutationTaster_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.FATHMM_pred) 1 else 0)' \; exportvariants -c 'v.contig,v.start,v.ref,v.alt,va.of8,va.dbNSFP.MetaSVM_pred,va.dbNSFP.MetaLR_pred,va.dbNSFP.CADD_phred,va.dbNSFP.`Eigen-raw`,va.dbNSFP.`Eigen-phred`,va.dbNSFP.`Eigen-raw_rankscore`' -o /user/mzekavat/MiGen/dbNSFP.MiGen.tsv. and I'm getting an error here: /medpop/esp2/mzekavat/MiGen/Annotation/hail.log; Would greatly appreciate thoughts on this as soon as possible!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/317
https://github.com/hail-is/hail/issues/317:1408,Testability,log,log,1408,"I'm running the following script: . /psych/genetics_data/working/cseed/bin/hail read -i ${input_vds} \; annotatevariants tsv file:///medpop/esp2/mzekavat/Estonia/UPDATED_TOOLS/dbNSFPv3.2/dbNSFP3.2a.ALLChr.bgz \; -r va.dbNSFP \; -t 'SIFT_pred: String, PROVEAN_pred: String, Polyphen2_HDIV_pred: String, Polyphen2_HVAR_pred: String, LRT_pred: String, MutationTaster_pred: String, MutationAssessor_pred: String, FATHMM_pred: String, MetaSVM_pred: String, MetaLR_pred: String, CADD_phred: Double, `Eigen-raw`: Double, `Eigen-phred`: Double, `Eigen-raw_rankscore`: Double' \; -v ""#chr,pos(1-based),ref,alt"" \; -m ""."" \; annotatevariants expr -c 'va.of8 = (if (""D"" ~ va.dbNSFP.SIFT_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.PROVEAN_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.Polyphen2_HDIV_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.Polyphen2_HVAR_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.LRT_pred) 1 else 0) + (if (""H"" ~ va.dbNSFP.MutationAssessor_pred || ""M"" ~ va.dbNSFP.MutationAssessor_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.MutationTaster_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.FATHMM_pred) 1 else 0)' \; exportvariants -c 'v.contig,v.start,v.ref,v.alt,va.of8,va.dbNSFP.MetaSVM_pred,va.dbNSFP.MetaLR_pred,va.dbNSFP.CADD_phred,va.dbNSFP.`Eigen-raw`,va.dbNSFP.`Eigen-phred`,va.dbNSFP.`Eigen-raw_rankscore`' -o /user/mzekavat/MiGen/dbNSFP.MiGen.tsv. and I'm getting an error here: /medpop/esp2/mzekavat/MiGen/Annotation/hail.log; Would greatly appreciate thoughts on this as soon as possible!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/317
https://github.com/hail-is/hail/issues/319:103,Availability,error,error,103,"When using this command `filtervariants -c /user/xiaoli/LCR-hs37d5.interval_list --remove`, I got this error message: ; `hail: filtervariants: caught exception: scala.MatchError: [Ljava.lang.String;@56d822dc (of class [Ljava.lang.String;)`. The input file is formatted:; 1 1 10000; 1 10016 10464; 1 10656 10784; 1 28576 28603; 1 30852 30959; 1 31712 31733; 1 33440 33464; 1 33504 33541. It might be because that it cannot take tab delimited file with only three columns. At least we need a clear warning message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/319
https://github.com/hail-is/hail/issues/319:109,Integrability,message,message,109,"When using this command `filtervariants -c /user/xiaoli/LCR-hs37d5.interval_list --remove`, I got this error message: ; `hail: filtervariants: caught exception: scala.MatchError: [Ljava.lang.String;@56d822dc (of class [Ljava.lang.String;)`. The input file is formatted:; 1 1 10000; 1 10016 10464; 1 10656 10784; 1 28576 28603; 1 30852 30959; 1 31712 31733; 1 33440 33464; 1 33504 33541. It might be because that it cannot take tab delimited file with only three columns. At least we need a clear warning message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/319
https://github.com/hail-is/hail/issues/319:504,Integrability,message,message,504,"When using this command `filtervariants -c /user/xiaoli/LCR-hs37d5.interval_list --remove`, I got this error message: ; `hail: filtervariants: caught exception: scala.MatchError: [Ljava.lang.String;@56d822dc (of class [Ljava.lang.String;)`. The input file is formatted:; 1 1 10000; 1 10016 10464; 1 10656 10784; 1 28576 28603; 1 30852 30959; 1 31712 31733; 1 33440 33464; 1 33504 33541. It might be because that it cannot take tab delimited file with only three columns. At least we need a clear warning message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/319
https://github.com/hail-is/hail/issues/319:490,Usability,clear,clear,490,"When using this command `filtervariants -c /user/xiaoli/LCR-hs37d5.interval_list --remove`, I got this error message: ; `hail: filtervariants: caught exception: scala.MatchError: [Ljava.lang.String;@56d822dc (of class [Ljava.lang.String;)`. The input file is formatted:; 1 1 10000; 1 10016 10464; 1 10656 10784; 1 28576 28603; 1 30852 30959; 1 31712 31733; 1 33440 33464; 1 33504 33541. It might be because that it cannot take tab delimited file with only three columns. At least we need a clear warning message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/319
https://github.com/hail-is/hail/issues/320:3271,Availability,error,error,3271,"ind(c => c.consequence_terms.contains(va.vep.most_severe_consequence)).impact == ""HIGH"")' \; annotatevariants expr -c 'va.andrea.damaging = (va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""missense_variant"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""missense_variant"") || ; va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""inframe_deletion"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""inframe_deletion"") ||; va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""inframe_insertion"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""inframe_insertion"")) ; && ""D"" ~ va.dbNSFP.Polyphen2_HDIV_pred && ""D"" ~ va.dbNSFP.Polyphen2_HVAR_pred && ""D"" ~ va.dbNSFP.SIFT_pred && ""D"" ~ va.dbNSFP.LRT_pred && ""[AD]"" ~ va.dbNSFP.MutationTaster_pred && ""[HM]"" ~ va.dbNSFP.MutationAssessor_pred && ""D"" ~ va.dbNSFP.PROVEAN_pred ' \; annotatevariants expr -c 'va.andrea.synonymous = va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""synonymous_variant"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""synonymous_variant"")' \; annotatevariants expr -c 'va.andrea.genename = if (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing) va.vep.transcript_consequences.find(c => c.consequence_terms.contains(va.vep.most_severe_consequence)).gene_symbol else va.vep.transcript_consequences.find(c => c.canonical == 1).gene_symbol' \; write -o /user/aganna/IBD_ANNOT.vep.qced.otherann.vds. error:. hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/227035/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/320
https://github.com/hail-is/hail/issues/320:3347,Safety,abort,aborted,3347,"ind(c => c.consequence_terms.contains(va.vep.most_severe_consequence)).impact == ""HIGH"")' \; annotatevariants expr -c 'va.andrea.damaging = (va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""missense_variant"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""missense_variant"") || ; va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""inframe_deletion"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""inframe_deletion"") ||; va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""inframe_insertion"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""inframe_insertion"")) ; && ""D"" ~ va.dbNSFP.Polyphen2_HDIV_pred && ""D"" ~ va.dbNSFP.Polyphen2_HVAR_pred && ""D"" ~ va.dbNSFP.SIFT_pred && ""D"" ~ va.dbNSFP.LRT_pred && ""[AD]"" ~ va.dbNSFP.MutationTaster_pred && ""[HM]"" ~ va.dbNSFP.MutationAssessor_pred && ""D"" ~ va.dbNSFP.PROVEAN_pred ' \; annotatevariants expr -c 'va.andrea.synonymous = va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""synonymous_variant"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""synonymous_variant"")' \; annotatevariants expr -c 'va.andrea.genename = if (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing) va.vep.transcript_consequences.find(c => c.consequence_terms.contains(va.vep.most_severe_consequence)).gene_symbol else va.vep.transcript_consequences.find(c => c.canonical == 1).gene_symbol' \; write -o /user/aganna/IBD_ANNOT.vep.qced.otherann.vds. error:. hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/227035/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/320
https://github.com/hail-is/hail/issues/320:3362,Testability,log,log,3362,"ind(c => c.consequence_terms.contains(va.vep.most_severe_consequence)).impact == ""HIGH"")' \; annotatevariants expr -c 'va.andrea.damaging = (va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""missense_variant"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""missense_variant"") || ; va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""inframe_deletion"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""inframe_deletion"") ||; va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""inframe_insertion"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""inframe_insertion"")) ; && ""D"" ~ va.dbNSFP.Polyphen2_HDIV_pred && ""D"" ~ va.dbNSFP.Polyphen2_HVAR_pred && ""D"" ~ va.dbNSFP.SIFT_pred && ""D"" ~ va.dbNSFP.LRT_pred && ""[AD]"" ~ va.dbNSFP.MutationTaster_pred && ""[HM]"" ~ va.dbNSFP.MutationAssessor_pred && ""D"" ~ va.dbNSFP.PROVEAN_pred ' \; annotatevariants expr -c 'va.andrea.synonymous = va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""synonymous_variant"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""synonymous_variant"")' \; annotatevariants expr -c 'va.andrea.genename = if (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing) va.vep.transcript_consequences.find(c => c.consequence_terms.contains(va.vep.most_severe_consequence)).gene_symbol else va.vep.transcript_consequences.find(c => c.canonical == 1).gene_symbol' \; write -o /user/aganna/IBD_ANNOT.vep.qced.otherann.vds. error:. hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/227035/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/320
https://github.com/hail-is/hail/issues/320:3428,Testability,log,log,3428,"ind(c => c.consequence_terms.contains(va.vep.most_severe_consequence)).impact == ""HIGH"")' \; annotatevariants expr -c 'va.andrea.damaging = (va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""missense_variant"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""missense_variant"") || ; va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""inframe_deletion"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""inframe_deletion"") ||; va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""inframe_insertion"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""inframe_insertion"")) ; && ""D"" ~ va.dbNSFP.Polyphen2_HDIV_pred && ""D"" ~ va.dbNSFP.Polyphen2_HVAR_pred && ""D"" ~ va.dbNSFP.SIFT_pred && ""D"" ~ va.dbNSFP.LRT_pred && ""[AD]"" ~ va.dbNSFP.MutationTaster_pred && ""[HM]"" ~ va.dbNSFP.MutationAssessor_pred && ""D"" ~ va.dbNSFP.PROVEAN_pred ' \; annotatevariants expr -c 'va.andrea.synonymous = va.vep.transcript_consequences.find(c => c.canonical == 1).consequence_terms.contains(""synonymous_variant"") || (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing && va.vep.most_severe_consequence == ""synonymous_variant"")' \; annotatevariants expr -c 'va.andrea.genename = if (va.vep.transcript_consequences.find(c => c.canonical == 1).canonical.isMissing) va.vep.transcript_consequences.find(c => c.consequence_terms.contains(va.vep.most_severe_consequence)).gene_symbol else va.vep.transcript_consequences.find(c => c.canonical == 1).gene_symbol' \; write -o /user/aganna/IBD_ANNOT.vep.qced.otherann.vds. error:. hail: write: caught exception: org.apache.spark.SparkException: Job aborted. [hail.log.txt](https://github.com/broadinstitute/hail/files/227035/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/320
https://github.com/hail-is/hail/issues/321:628,Availability,error,error,628,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321
https://github.com/hail-is/hail/issues/321:758,Availability,Avail,Available,758,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321
https://github.com/hail-is/hail/issues/321:1097,Performance,concurren,concurrent,1097,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321
https://github.com/hail-is/hail/issues/321:1181,Performance,concurren,concurrent,1181,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321
https://github.com/hail-is/hail/issues/321:858,Safety,avoid,avoid,858,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321
https://github.com/hail-is/hail/issues/321:280,Testability,Log,Log,280,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321
https://github.com/hail-is/hail/issues/321:335,Testability,log,log,335,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321
https://github.com/hail-is/hail/issues/321:470,Testability,log,log,470,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/321
https://github.com/hail-is/hail/issues/325:626,Testability,log,log,626,Command: . hail-new read -i /user/aganna/CANCER.vep.vds \; renamesamples -i file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/key_old_new_names \; filtersamples --keep -c 'file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list' \; filtervariants --keep -c 'va.pass' \; filtergenotypes --remove -c '(g.isHet && ((g.ad[0] / g.dp) < 0.2 || (g.ad[0] / g.dp) > 0.8)) || g.gq < 20' \; variantqc \; filtervariants --keep -c 'va.qc.callRate > 0.80 && va.qc.MAC > 0' \; exportplink -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/CANCER \; count \; write -o /user/aganna/CANCER.vep.qced.vds. Attached log:. [hail.log.txt](https://github.com/broadinstitute/hail/files/228583/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/325
https://github.com/hail-is/hail/issues/325:638,Testability,log,log,638,Command: . hail-new read -i /user/aganna/CANCER.vep.vds \; renamesamples -i file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/key_old_new_names \; filtersamples --keep -c 'file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list' \; filtervariants --keep -c 'va.pass' \; filtergenotypes --remove -c '(g.isHet && ((g.ad[0] / g.dp) < 0.2 || (g.ad[0] / g.dp) > 0.8)) || g.gq < 20' \; variantqc \; filtervariants --keep -c 'va.qc.callRate > 0.80 && va.qc.MAC > 0' \; exportplink -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/CANCER \; count \; write -o /user/aganna/CANCER.vep.qced.vds. Attached log:. [hail.log.txt](https://github.com/broadinstitute/hail/files/228583/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/325
https://github.com/hail-is/hail/issues/325:704,Testability,log,log,704,Command: . hail-new read -i /user/aganna/CANCER.vep.vds \; renamesamples -i file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/key_old_new_names \; filtersamples --keep -c 'file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list' \; filtervariants --keep -c 'va.pass' \; filtergenotypes --remove -c '(g.isHet && ((g.ad[0] / g.dp) < 0.2 || (g.ad[0] / g.dp) > 0.8)) || g.gq < 20' \; variantqc \; filtervariants --keep -c 'va.qc.callRate > 0.80 && va.qc.MAC > 0' \; exportplink -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/CANCER \; count \; write -o /user/aganna/CANCER.vep.qced.vds. Attached log:. [hail.log.txt](https://github.com/broadinstitute/hail/files/228583/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/325
https://github.com/hail-is/hail/issues/327:76,Availability,error,error,76,"Due to a typo I tried to read a vds that doesn't exist. I got the following error message:; hail: fatal: read: corrupt VDS: no metadata.ser file. Recreate VDS. It should probably say something more specific, ideally:; hail: fatal: read: VDS does not exist. Or at least:; hail: fatal: read: non-existent or corrupt VDS: no metadata.ser file. Please (re)create VDS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/327
https://github.com/hail-is/hail/issues/327:82,Integrability,message,message,82,"Due to a typo I tried to read a vds that doesn't exist. I got the following error message:; hail: fatal: read: corrupt VDS: no metadata.ser file. Recreate VDS. It should probably say something more specific, ideally:; hail: fatal: read: VDS does not exist. Or at least:; hail: fatal: read: non-existent or corrupt VDS: no metadata.ser file. Please (re)create VDS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/327
https://github.com/hail-is/hail/issues/332:109,Usability,guid,guide,109,"Basically this is what CalculateGenotypePosteriors of GATK does. ([link](https://www.broadinstitute.org/gatk/guide/tooldocs/org_broadinstitute_gatk_tools_walkers_variantutils_CalculateGenotypePosteriors.php)) In common variant analysis, we need to improve GQ calculation based on a given panel, say, 1000 genomes panel. I see this module a common practice in variant QC for common variant analysis (GWAS/eQTL), so hope you can add this as a part of Hail. Thanks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/332
https://github.com/hail-is/hail/issues/335:370,Availability,failure,failures,370,"Three things:; 1. Jenkins build #116:. http://hail-ci.broadinstitute.org:8080/job/Hail%20-%20Test%20All%20Branches/116/. of master, revision 6d5fe392b32c7383370b2fc2ea259fb8cff2c1c6, failed several test cases, but I wasn't able to reproduce it on my laptop. Can we get more detailed logs, either by default or in another file, so we can debug build irreproducible build failures? Custom log4j properties for gradle might be a solution.; 1. I tried to rebuild #116 by clicking ""Rebuild"" on the left hand side menu. This resulted in build #119 . http://hail-ci.broadinstitute.org:8080/job/Hail%20-%20Test%20All%20Branches/119/. which is labeled ""rebuild of #116"", but it is a build of a different branch, origin/tp_dbnsfp_bug and a different revision. What's going on?; 1. When I noticed #116 failed, I wanted to find the last build of origin/master that succeeded. Is there a way to do that?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335
https://github.com/hail-is/hail/issues/335:198,Testability,test,test,198,"Three things:; 1. Jenkins build #116:. http://hail-ci.broadinstitute.org:8080/job/Hail%20-%20Test%20All%20Branches/116/. of master, revision 6d5fe392b32c7383370b2fc2ea259fb8cff2c1c6, failed several test cases, but I wasn't able to reproduce it on my laptop. Can we get more detailed logs, either by default or in another file, so we can debug build irreproducible build failures? Custom log4j properties for gradle might be a solution.; 1. I tried to rebuild #116 by clicking ""Rebuild"" on the left hand side menu. This resulted in build #119 . http://hail-ci.broadinstitute.org:8080/job/Hail%20-%20Test%20All%20Branches/119/. which is labeled ""rebuild of #116"", but it is a build of a different branch, origin/tp_dbnsfp_bug and a different revision. What's going on?; 1. When I noticed #116 failed, I wanted to find the last build of origin/master that succeeded. Is there a way to do that?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335
https://github.com/hail-is/hail/issues/335:283,Testability,log,logs,283,"Three things:; 1. Jenkins build #116:. http://hail-ci.broadinstitute.org:8080/job/Hail%20-%20Test%20All%20Branches/116/. of master, revision 6d5fe392b32c7383370b2fc2ea259fb8cff2c1c6, failed several test cases, but I wasn't able to reproduce it on my laptop. Can we get more detailed logs, either by default or in another file, so we can debug build irreproducible build failures? Custom log4j properties for gradle might be a solution.; 1. I tried to rebuild #116 by clicking ""Rebuild"" on the left hand side menu. This resulted in build #119 . http://hail-ci.broadinstitute.org:8080/job/Hail%20-%20Test%20All%20Branches/119/. which is labeled ""rebuild of #116"", but it is a build of a different branch, origin/tp_dbnsfp_bug and a different revision. What's going on?; 1. When I noticed #116 failed, I wanted to find the last build of origin/master that succeeded. Is there a way to do that?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/335
https://github.com/hail-is/hail/issues/336:433,Availability,failure,failure,433,"```; hail read -i profile.vds annotatesamples tsv -i sampleInfo.tsv -t 'Age: Int, Health: Double' -r sa.info filtersamples --keep -c 'sa.info.Health > 0.2' linreg -y sa.info.Health -c 'sa.info.Age' -r va.linreg exportvariants -c 'Variant=v, Beta = va.linreg.beta, Pval = va.linreg.pval' -o linreg.tsv; ```. ```; [Stage 1:> (0 + 7) / 7]hail: exportvariants: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 1.0 failed 1 times, most recent failure: Lost task 5.0 in stage 1.0 (TID 13, localhost): java.lang.ArrayIndexOutOfBoundsException: 357; at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply$mcVI$sp(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply(LinearRegression.scala:80); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156); at org.broadinstitute.hail.methods.LinRegBuilder.stats(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/336
https://github.com/hail-is/hail/issues/336:490,Availability,failure,failure,490,"```; hail read -i profile.vds annotatesamples tsv -i sampleInfo.tsv -t 'Age: Int, Health: Double' -r sa.info filtersamples --keep -c 'sa.info.Health > 0.2' linreg -y sa.info.Health -c 'sa.info.Age' -r va.linreg exportvariants -c 'Variant=v, Beta = va.linreg.beta, Pval = va.linreg.pval' -o linreg.tsv; ```. ```; [Stage 1:> (0 + 7) / 7]hail: exportvariants: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 1.0 failed 1 times, most recent failure: Lost task 5.0 in stage 1.0 (TID 13, localhost): java.lang.ArrayIndexOutOfBoundsException: 357; at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply$mcVI$sp(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply(LinearRegression.scala:80); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156); at org.broadinstitute.hail.methods.LinRegBuilder.stats(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/336
https://github.com/hail-is/hail/issues/336:2812,Energy Efficiency,schedul,scheduler,2812,ply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/336
https://github.com/hail-is/hail/issues/336:2883,Energy Efficiency,schedul,scheduler,2883,ply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/336
https://github.com/hail-is/hail/issues/336:3005,Performance,concurren,concurrent,3005,ply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/336
https://github.com/hail-is/hail/issues/336:3089,Performance,concurren,concurrent,3089,ply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/336
https://github.com/hail-is/hail/issues/336:412,Safety,abort,aborted,412,"```; hail read -i profile.vds annotatesamples tsv -i sampleInfo.tsv -t 'Age: Int, Health: Double' -r sa.info filtersamples --keep -c 'sa.info.Health > 0.2' linreg -y sa.info.Health -c 'sa.info.Age' -r va.linreg exportvariants -c 'Variant=v, Beta = va.linreg.beta, Pval = va.linreg.pval' -o linreg.tsv; ```. ```; [Stage 1:> (0 + 7) / 7]hail: exportvariants: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 1.0 failed 1 times, most recent failure: Lost task 5.0 in stage 1.0 (TID 13, localhost): java.lang.ArrayIndexOutOfBoundsException: 357; at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply$mcVI$sp(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinRegBuilder$$anonfun$stats$1.apply(LinearRegression.scala:80); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156); at org.broadinstitute.hail.methods.LinRegBuilder.stats(LinearRegression.scala:80); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:130); at org.broadinstitute.hail.methods.LinearRegression$$anonfun$apply$4.apply(LinearRegression.scala:129); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$41$$anonfun$apply$42.apply(PairRDDFunctions.scala:700); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$20.next(Iterator.scala:635); at scala.collection.Iterator$$anon$20.next(Iterator.scala:633); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/336
https://github.com/hail-is/hail/issues/342:21,Testability,assert,asserts,21,AST.scala:607 schema asserts fields.nonEmpty.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/342
https://github.com/hail-is/hail/issues/344:3,Testability,test,test,3,"To test, for example, catching and reporting SparkException inside Main.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/344
https://github.com/hail-is/hail/pull/345:13,Testability,test,test,13,"I've added a test which catches the bug (using row rather than sparse index of row to fill in missing genotype values with the mean), and fixed it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/345
https://github.com/hail-is/hail/issues/347:405,Availability,Error,Error,405,Commandline:. ```; hail-new -l /home/unix/gtiao/hail.rename.log \; read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds \; renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt \; write -o /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.no_spaces.vds. ```. Error message:. ```; hail: info: running: read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds; [Stage 0:=============================> (1 + 1) / 2]hail: info: running: renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt; hail: renamesamples: caught exception: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt at 175616 exp: -1352655701 got: 441984571. ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/347
https://github.com/hail-is/hail/issues/347:835,Availability,error,error,835,Commandline:. ```; hail-new -l /home/unix/gtiao/hail.rename.log \; read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds \; renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt \; write -o /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.no_spaces.vds. ```. Error message:. ```; hail: info: running: read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds; [Stage 0:=============================> (1 + 1) / 2]hail: info: running: renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt; hail: renamesamples: caught exception: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt at 175616 exp: -1352655701 got: 441984571. ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/347
https://github.com/hail-is/hail/issues/347:411,Integrability,message,message,411,Commandline:. ```; hail-new -l /home/unix/gtiao/hail.rename.log \; read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds \; renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt \; write -o /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.no_spaces.vds. ```. Error message:. ```; hail: info: running: read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds; [Stage 0:=============================> (1 + 1) / 2]hail: info: running: renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt; hail: renamesamples: caught exception: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt at 175616 exp: -1352655701 got: 441984571. ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/347
https://github.com/hail-is/hail/issues/347:807,Security,Checksum,ChecksumException,807,Commandline:. ```; hail-new -l /home/unix/gtiao/hail.rename.log \; read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds \; renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt \; write -o /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.no_spaces.vds. ```. Error message:. ```; hail: info: running: read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds; [Stage 0:=============================> (1 + 1) / 2]hail: info: running: renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt; hail: renamesamples: caught exception: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt at 175616 exp: -1352655701 got: 441984571. ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/347
https://github.com/hail-is/hail/issues/347:826,Security,Checksum,Checksum,826,Commandline:. ```; hail-new -l /home/unix/gtiao/hail.rename.log \; read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds \; renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt \; write -o /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.no_spaces.vds. ```. Error message:. ```; hail: info: running: read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds; [Stage 0:=============================> (1 + 1) / 2]hail: info: running: renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt; hail: renamesamples: caught exception: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt at 175616 exp: -1352655701 got: 441984571. ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/347
https://github.com/hail-is/hail/issues/347:60,Testability,log,log,60,Commandline:. ```; hail-new -l /home/unix/gtiao/hail.rename.log \; read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds \; renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt \; write -o /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.no_spaces.vds. ```. Error message:. ```; hail: info: running: read -i /user/gtiao/37k/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.vds; [Stage 0:=============================> (1 + 1) / 2]hail: info: running: renamesamples -i file:///xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt; hail: renamesamples: caught exception: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/xchip/cga_home/gtiao/37k/Hail/germline_cancer_joint_calling.no_restricted.GQ20_AB.split.VEP.sample_id_map.txt at 175616 exp: -1352655701 got: 441984571. ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/347
https://github.com/hail-is/hail/issues/348:121,Deployability,release,release,121,Might be related to phasing. The FORMAT fields is GT and a typical genotype is 0|1. There is a copy here: /broad/1kg/ftp/release/20130502.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/348
https://github.com/hail-is/hail/issues/349:0,Performance,Load,Load,0,"Load VCF PP as Hail PL. Load GT and GQ normally, but for the purpose of input validation, interpret them with respect to PP rather than PL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/349
https://github.com/hail-is/hail/issues/349:24,Performance,Load,Load,24,"Load VCF PP as Hail PL. Load GT and GQ normally, but for the purpose of input validation, interpret them with respect to PP rather than PL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/349
https://github.com/hail-is/hail/issues/349:78,Security,validat,validation,78,"Load VCF PP as Hail PL. Load GT and GQ normally, but for the purpose of input validation, interpret them with respect to PP rather than PL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/349
https://github.com/hail-is/hail/issues/350:26,Integrability,depend,dependent,26,"Many annotations are base-dependent rather than variant-dependant. For these, we want to annotate bases or intervals with multiple annotations from the same file. This should support multiple types (String, Int, Double)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/350
https://github.com/hail-is/hail/issues/350:56,Integrability,depend,dependant,56,"Many annotations are base-dependent rather than variant-dependant. For these, we want to annotate bases or intervals with multiple annotations from the same file. This should support multiple types (String, Int, Double)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/350
https://github.com/hail-is/hail/issues/353:80,Deployability,pipeline,pipelines,80,So people can write/distribute their own analyses which can be called from Hail pipelines. javacmd org.braodinstitue.laurent.MyAwesomeAnalysis options...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/353
https://github.com/hail-is/hail/issues/359:202,Modifiability,flexible,flexible,202,"With new aggregators, users could e.g. access Apache Math3 libraries to do arbitrary analyses based on case/control genotype counts.; Furthermore having ability to add new jars to classpath would allow flexible addition of new functions by users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/359
https://github.com/hail-is/hail/issues/359:39,Security,access,access,39,"With new aggregators, users could e.g. access Apache Math3 libraries to do arbitrary analyses based on case/control genotype counts.; Furthermore having ability to add new jars to classpath would allow flexible addition of new functions by users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/359
https://github.com/hail-is/hail/issues/361:9,Availability,error,error,9,Got this error message:. [Stage 0:> (0 + 120) / 2941]hail: write: fatal: hdfs://dataflow01.broadinstitute.org:8020/user/aganna/DIABETES.vcf.bgz: caught java.lang.IllegalArgumentException: requirement failed; offending line: 1 23735206 var_1_23735206 C T 4044.57PASS AC=1;AF=1.971e-0… . which according to Tim can be improved (there is no separation between QUAL and INFO filed). My main point however is that for these malformed files we should be able to remove the malformed lines instead of just not allowing to import the .vcf. This might be an important vcf and cannot / or difficult to find a better version.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/361
https://github.com/hail-is/hail/issues/361:15,Integrability,message,message,15,Got this error message:. [Stage 0:> (0 + 120) / 2941]hail: write: fatal: hdfs://dataflow01.broadinstitute.org:8020/user/aganna/DIABETES.vcf.bgz: caught java.lang.IllegalArgumentException: requirement failed; offending line: 1 23735206 var_1_23735206 C T 4044.57PASS AC=1;AF=1.971e-0… . which according to Tim can be improved (there is no separation between QUAL and INFO filed). My main point however is that for these malformed files we should be able to remove the malformed lines instead of just not allowing to import the .vcf. This might be an important vcf and cannot / or difficult to find a better version.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/361
https://github.com/hail-is/hail/issues/363:56,Testability,test,tests,56,"Hello, ; It would be really great to have the following tests in Hail, as detailed on EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Test. Most important for immediate analyses (with ~10K individuals WGS) are q.emmax, emmaxCMC, and mmskat, which all use mixed models (with kinship matrices or GRMs). . Furthermore, one step beyond running analysis is doing conditional analysis. Right now, in EPACTS, doing conditional analysis requires adding separate columns in the .ped file corresponding to GTs for each variant you'd like to condition on. Ideally, we'd be able to just list the variants (maybe in tab-delimited format with Chr, Pos, Ref, Alt. quantitative traits of interest: ; q.emmax ; mmskat. binary traits of interest: ; b.score (or b.wald); b.collapse ; emmaxCMC. Thanks again!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/363
https://github.com/hail-is/hail/issues/367:460,Availability,Error,Error,460,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/367
https://github.com/hail-is/hail/issues/367:584,Availability,failure,failure,584,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/367
https://github.com/hail-is/hail/issues/367:643,Availability,failure,failure,643,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/367
https://github.com/hail-is/hail/issues/367:563,Safety,abort,aborted,563,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/367
https://github.com/hail-is/hail/issues/367:454,Testability,test,test,454,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/367
https://github.com/hail-is/hail/issues/367:770,Testability,log,log,770,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/367
https://github.com/hail-is/hail/issues/367:836,Testability,log,log,836,"Command:. hail read -i /user/aganna/annotated_test22.vds \; filtervariants expr \; --keep -c 'v.contig == ""22""' \; annotatevariants expr -c 'va.andrea.URV = (va.qc.nNonRef == 1 && va.exac.info.AC.isEmpty)' \; annotatevariants expr -c 'va.andrea.URVEXAC = (va.qc.nNonRef == 1 && (va.exac.info.AC.isEmpty || va.exac.info.AC[1] < 3))' \; exportvariants -c 'v, va.andrea.URV ,va.andrea.URVEXAC, va.qc.nNonRef' -o file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/test. Error:. [Stage 1:> (268 + 184) / 14326]Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 1.0 failed 30 times, most recent failure: Lost task 37.29 in stage 1.0 (TID 3338, dataflow02.broadinstitute.org): java.lang.IndexOutOfBoundsException: 1. [hail.log.txt](https://github.com/broadinstitute/hail/files/250349/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/367
https://github.com/hail-is/hail/pull/369:38,Integrability,message,messages,38,Added nicer IndexOutOfBoundsException messages in expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/369
https://github.com/hail-is/hail/pull/372:495,Availability,error,errors,495,"**Things still to do:**; 1. Docs for importbgen, importgen, dosage representation, info score in variant qc; 2. Make sure info score is computed properly -- either implement correctly for non-autosomal variants or return None; 3. Add tests for info score (once we finalized how we're computing); 4. Remove null variant in GenotypeBuilder (from import plink block reader code); 5. Decide how to handle fake ref for multiallelics when original genotype call was null (could be because of rounding errors we get same integer value for close doubles such as 0.4035 and 0.4021); 6. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 7. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 8. Update the readFam function in import plink to utilize functionality Jon wrote already. **Questions:**; 1. I set the default value of --no-compress to true for `importplink`, `importgen`, and `importbgen`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/372
https://github.com/hail-is/hail/pull/372:819,Deployability,Update,Update,819,"**Things still to do:**; 1. Docs for importbgen, importgen, dosage representation, info score in variant qc; 2. Make sure info score is computed properly -- either implement correctly for non-autosomal variants or return None; 3. Add tests for info score (once we finalized how we're computing); 4. Remove null variant in GenotypeBuilder (from import plink block reader code); 5. Decide how to handle fake ref for multiallelics when original genotype call was null (could be because of rounding errors we get same integer value for close doubles such as 0.4035 and 0.4021); 6. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 7. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 8. Update the readFam function in import plink to utilize functionality Jon wrote already. **Questions:**; 1. I set the default value of --no-compress to true for `importplink`, `importgen`, and `importbgen`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/372
https://github.com/hail-is/hail/pull/372:234,Testability,test,tests,234,"**Things still to do:**; 1. Docs for importbgen, importgen, dosage representation, info score in variant qc; 2. Make sure info score is computed properly -- either implement correctly for non-autosomal variants or return None; 3. Add tests for info score (once we finalized how we're computing); 4. Remove null variant in GenotypeBuilder (from import plink block reader code); 5. Decide how to handle fake ref for multiallelics when original genotype call was null (could be because of rounding errors we get same integer value for close doubles such as 0.4035 and 0.4021); 6. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 7. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 8. Update the readFam function in import plink to utilize functionality Jon wrote already. **Questions:**; 1. I set the default value of --no-compress to true for `importplink`, `importgen`, and `importbgen`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/372
https://github.com/hail-is/hail/issues/373:91,Testability,log,log,91,"I tried to run LOFTEE in Hail for ~2 million variants. It ran for about 2 days and failed (log attached, seems to be killed for running too long?). Just wonder if anything could be done to improve the efficiency. Thanks!. [hail.log.txt](https://github.com/broadinstitute/hail/files/252702/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/373
https://github.com/hail-is/hail/issues/373:228,Testability,log,log,228,"I tried to run LOFTEE in Hail for ~2 million variants. It ran for about 2 days and failed (log attached, seems to be killed for running too long?). Just wonder if anything could be done to improve the efficiency. Thanks!. [hail.log.txt](https://github.com/broadinstitute/hail/files/252702/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/373
https://github.com/hail-is/hail/issues/373:294,Testability,log,log,294,"I tried to run LOFTEE in Hail for ~2 million variants. It ran for about 2 days and failed (log attached, seems to be killed for running too long?). Just wonder if anything could be done to improve the efficiency. Thanks!. [hail.log.txt](https://github.com/broadinstitute/hail/files/252702/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/373
https://github.com/hail-is/hail/issues/374:68,Availability,error,error,68,"Now it seems you should use string.isMissing, while this returns an error.; I suggest isMissing(a), which makes clear the proper use of the syntax.; In Brief: update help here: https://github.com/broadinstitute/hail/blob/master/docs/HailExpressionLanguage.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/374
https://github.com/hail-is/hail/issues/374:159,Deployability,update,update,159,"Now it seems you should use string.isMissing, while this returns an error.; I suggest isMissing(a), which makes clear the proper use of the syntax.; In Brief: update help here: https://github.com/broadinstitute/hail/blob/master/docs/HailExpressionLanguage.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/374
https://github.com/hail-is/hail/issues/374:112,Usability,clear,clear,112,"Now it seems you should use string.isMissing, while this returns an error.; I suggest isMissing(a), which makes clear the proper use of the syntax.; In Brief: update help here: https://github.com/broadinstitute/hail/blob/master/docs/HailExpressionLanguage.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/374
https://github.com/hail-is/hail/issues/375:153,Modifiability,variab,variable,153,"Current implementation allows to specify covariates, but does not output coefficients for the covariates.; I suggest implementing a linereg, where the X variable of interest can be specified.; E.g. . linreg -y sa.pheno.height -c sa.cov.age,sa.cov.isMale -x sa.isblueEyes. In this case, it just run 1 linear regression without using the genotype data. another important addition would be to specify the link function.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/375
https://github.com/hail-is/hail/issues/381:32,Availability,error,error,32,It may be good to have a better error message when users forget to add the file:/// to a unix file path when using hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/381
https://github.com/hail-is/hail/issues/381:38,Integrability,message,message,38,It may be good to have a better error message when users forget to add the file:/// to a unix file path when using hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/381
https://github.com/hail-is/hail/issues/382:370,Availability,Error,Error,370,"Commandline:. ```; hail -l /xchip/cga_home/gtiao/Hail/hail.re-import.log importvcf $VCF \; filtervariants all \; count \; filtersamples list -i 'file:///xchip/cga_home/gtiao/37k/germline_cancer_joint_calling.restricted_samples.sample_list' --remove \; count \; exportsamples -c 's.id' -o file:///xchip/cga_home/gtiao/37k/Hail/samples_after_removing_restricted.txt; ```. Error message:. ```; hail: info: running: exportsamples -c s.id -o file:///xchip/cga_home/gtiao/37k/Hail/samples_after_removing_restricted.txt; hail: exportsamples: fatal: does not support multiallelics.; Run `splitmulti' first.; ```. It works if I insert ""splitmulti"" after the import command, but then I drop all variants, so this seems a very silly requirement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/382
https://github.com/hail-is/hail/issues/382:376,Integrability,message,message,376,"Commandline:. ```; hail -l /xchip/cga_home/gtiao/Hail/hail.re-import.log importvcf $VCF \; filtervariants all \; count \; filtersamples list -i 'file:///xchip/cga_home/gtiao/37k/germline_cancer_joint_calling.restricted_samples.sample_list' --remove \; count \; exportsamples -c 's.id' -o file:///xchip/cga_home/gtiao/37k/Hail/samples_after_removing_restricted.txt; ```. Error message:. ```; hail: info: running: exportsamples -c s.id -o file:///xchip/cga_home/gtiao/37k/Hail/samples_after_removing_restricted.txt; hail: exportsamples: fatal: does not support multiallelics.; Run `splitmulti' first.; ```. It works if I insert ""splitmulti"" after the import command, but then I drop all variants, so this seems a very silly requirement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/382
https://github.com/hail-is/hail/issues/382:69,Testability,log,log,69,"Commandline:. ```; hail -l /xchip/cga_home/gtiao/Hail/hail.re-import.log importvcf $VCF \; filtervariants all \; count \; filtersamples list -i 'file:///xchip/cga_home/gtiao/37k/germline_cancer_joint_calling.restricted_samples.sample_list' --remove \; count \; exportsamples -c 's.id' -o file:///xchip/cga_home/gtiao/37k/Hail/samples_after_removing_restricted.txt; ```. Error message:. ```; hail: info: running: exportsamples -c s.id -o file:///xchip/cga_home/gtiao/37k/Hail/samples_after_removing_restricted.txt; hail: exportsamples: fatal: does not support multiallelics.; Run `splitmulti' first.; ```. It works if I insert ""splitmulti"" after the import command, but then I drop all variants, so this seems a very silly requirement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/382
https://github.com/hail-is/hail/issues/383:327,Availability,error,error,327,"I was trying to filtervariants with an interval_list, and I wrote:. filtervariants list --keep -i /user/satterst/exome_evaluation_regions.v1.interval_list count; (when clearly I should have done filtervariants intervals... instead of filtervariants list...). and it kept getting most of the way done and then failing, with the error message:; hail: count: fatal: invalid variant. Invalid variant? I double-checked that my dataset was OK by running count on the whole thing, telling me that the filtering was the problem, so I created different versions of the interval_list file, trying to figure out if something was specified incorrectly in the file... I was literally troubleshooting the interval_list file for over half an hour before I realized what the problem was. And it's not the first time I've made this mistake. . I humbly submit that a better error message here would be helpful, something like:; filtervariants: fatal: list expected but intervals encountered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/383
https://github.com/hail-is/hail/issues/383:856,Availability,error,error,856,"I was trying to filtervariants with an interval_list, and I wrote:. filtervariants list --keep -i /user/satterst/exome_evaluation_regions.v1.interval_list count; (when clearly I should have done filtervariants intervals... instead of filtervariants list...). and it kept getting most of the way done and then failing, with the error message:; hail: count: fatal: invalid variant. Invalid variant? I double-checked that my dataset was OK by running count on the whole thing, telling me that the filtering was the problem, so I created different versions of the interval_list file, trying to figure out if something was specified incorrectly in the file... I was literally troubleshooting the interval_list file for over half an hour before I realized what the problem was. And it's not the first time I've made this mistake. . I humbly submit that a better error message here would be helpful, something like:; filtervariants: fatal: list expected but intervals encountered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/383
https://github.com/hail-is/hail/issues/383:333,Integrability,message,message,333,"I was trying to filtervariants with an interval_list, and I wrote:. filtervariants list --keep -i /user/satterst/exome_evaluation_regions.v1.interval_list count; (when clearly I should have done filtervariants intervals... instead of filtervariants list...). and it kept getting most of the way done and then failing, with the error message:; hail: count: fatal: invalid variant. Invalid variant? I double-checked that my dataset was OK by running count on the whole thing, telling me that the filtering was the problem, so I created different versions of the interval_list file, trying to figure out if something was specified incorrectly in the file... I was literally troubleshooting the interval_list file for over half an hour before I realized what the problem was. And it's not the first time I've made this mistake. . I humbly submit that a better error message here would be helpful, something like:; filtervariants: fatal: list expected but intervals encountered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/383
https://github.com/hail-is/hail/issues/383:862,Integrability,message,message,862,"I was trying to filtervariants with an interval_list, and I wrote:. filtervariants list --keep -i /user/satterst/exome_evaluation_regions.v1.interval_list count; (when clearly I should have done filtervariants intervals... instead of filtervariants list...). and it kept getting most of the way done and then failing, with the error message:; hail: count: fatal: invalid variant. Invalid variant? I double-checked that my dataset was OK by running count on the whole thing, telling me that the filtering was the problem, so I created different versions of the interval_list file, trying to figure out if something was specified incorrectly in the file... I was literally troubleshooting the interval_list file for over half an hour before I realized what the problem was. And it's not the first time I've made this mistake. . I humbly submit that a better error message here would be helpful, something like:; filtervariants: fatal: list expected but intervals encountered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/383
https://github.com/hail-is/hail/issues/383:168,Usability,clear,clearly,168,"I was trying to filtervariants with an interval_list, and I wrote:. filtervariants list --keep -i /user/satterst/exome_evaluation_regions.v1.interval_list count; (when clearly I should have done filtervariants intervals... instead of filtervariants list...). and it kept getting most of the way done and then failing, with the error message:; hail: count: fatal: invalid variant. Invalid variant? I double-checked that my dataset was OK by running count on the whole thing, telling me that the filtering was the problem, so I created different versions of the interval_list file, trying to figure out if something was specified incorrectly in the file... I was literally troubleshooting the interval_list file for over half an hour before I realized what the problem was. And it's not the first time I've made this mistake. . I humbly submit that a better error message here would be helpful, something like:; filtervariants: fatal: list expected but intervals encountered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/383
https://github.com/hail-is/hail/issues/384:122,Security,access,access,122,"https://github.com/broadinstitute/hail/blob/master/docs/Representation.md#genotype. this page should specify that you can access the PLs by g.pl[0] = PL(HomRef), etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/384
https://github.com/hail-is/hail/pull/386:416,Availability,error,errors,416,"1. I ended up removing the test with the VCF from 1000 genomes; 2. I couldn't figure out a good way to get the number of variants used in the computation. It's either in the annotations as ""sa.imputegender.T"" or we'd have to do RDD.count() ; 3. I'm open to naming suggestions if you think something else is better. I didn't want to use ""sex check"" because that implies comparing to the reported gender and reporting errors -- not what I have implemented here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/386
https://github.com/hail-is/hail/pull/386:27,Testability,test,test,27,"1. I ended up removing the test with the VCF from 1000 genomes; 2. I couldn't figure out a good way to get the number of variants used in the computation. It's either in the annotations as ""sa.imputegender.T"" or we'd have to do RDD.count() ; 3. I'm open to naming suggestions if you think something else is better. I didn't want to use ""sex check"" because that implies comparing to the reported gender and reporting errors -- not what I have implemented here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/386
https://github.com/hail-is/hail/issues/389:255,Availability,error,error,255,"Currently if there are duplicated chr:pos:ref:alt in an annotation file, for example read by `annotatevariants table`, also the variants in the vds get duplicated. This is clearly not nice. So instead think on a better behaviour. But do not just issue an error, because otherwise is a pain.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/389
https://github.com/hail-is/hail/issues/389:172,Usability,clear,clearly,172,"Currently if there are duplicated chr:pos:ref:alt in an annotation file, for example read by `annotatevariants table`, also the variants in the vds get duplicated. This is clearly not nice. So instead think on a better behaviour. But do not just issue an error, because otherwise is a pain.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/389
https://github.com/hail-is/hail/issues/390:645,Usability,Clear,Clearly,645,"Let's say I read an external file with `annotateglobal table -r global.all_scores`. I then need to assign each column to a genset , for example `global.GWAS_height = global.all_scores.filter(x => x.GWAS_HEIGHT == ""1"").map(x => x.V1).toSet` then create a per-variant annotation checking if the gene is in the gene-set `va.andrea.GWAS_height = global.GWAS_height.contains(va.andrea.genename)` and then I need to count the number of variants per individuals `sa.andrea.GWAS_height = gs.statsif(va.andrea.URV && va.andrea.GWAS_height,g.nNonRefAlleles).sum`. And I want to do this for each of the column in the file read with `annotateglobal table`. Clearly loop is needed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/390
https://github.com/hail-is/hail/issues/391:18,Availability,Error,Error,18,See attached log. Error not clear:. `[Stage 0:==========> (596 + 168) / 2836]hail: write: caught exception: Job aborted.`. [hail.log.txt](https://github.com/broadinstitute/hail/files/269500/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/391
https://github.com/hail-is/hail/issues/391:112,Safety,abort,aborted,112,See attached log. Error not clear:. `[Stage 0:==========> (596 + 168) / 2836]hail: write: caught exception: Job aborted.`. [hail.log.txt](https://github.com/broadinstitute/hail/files/269500/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/391
https://github.com/hail-is/hail/issues/391:13,Testability,log,log,13,See attached log. Error not clear:. `[Stage 0:==========> (596 + 168) / 2836]hail: write: caught exception: Job aborted.`. [hail.log.txt](https://github.com/broadinstitute/hail/files/269500/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/391
https://github.com/hail-is/hail/issues/391:129,Testability,log,log,129,See attached log. Error not clear:. `[Stage 0:==========> (596 + 168) / 2836]hail: write: caught exception: Job aborted.`. [hail.log.txt](https://github.com/broadinstitute/hail/files/269500/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/391
https://github.com/hail-is/hail/issues/391:195,Testability,log,log,195,See attached log. Error not clear:. `[Stage 0:==========> (596 + 168) / 2836]hail: write: caught exception: Job aborted.`. [hail.log.txt](https://github.com/broadinstitute/hail/files/269500/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/391
https://github.com/hail-is/hail/issues/391:28,Usability,clear,clear,28,See attached log. Error not clear:. `[Stage 0:==========> (596 + 168) / 2836]hail: write: caught exception: Job aborted.`. [hail.log.txt](https://github.com/broadinstitute/hail/files/269500/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/391
https://github.com/hail-is/hail/issues/393:498,Usability,simpl,simply,498,"Instead of writing the following to count genotypes with GQ<20, GQ>=20, and total nonmissing GQs in the callset:. ```; annotatevariants expr -c 'va.gq_less_20 = gs.count(g.gq < 20), va.gq_greater_20 = gs.count(g.gq >= 20), va.gq_total_nonmissing = gs.count(isDefined(g.gq))'; annotateglobal expr -c 'global.gq_less_20 = variants.sum(va.gq_less_20), global.gq_greater_20 = variants.sum(va.gq_greater_20), global.gq_total_nonmissing = variants.sum(va.gq_total_nonmissing)'; ```. It would be great to simply write:. ```; annotateglobal expr -c 'global.gq_less_20 = gs.count(g.gq < 20), global.gq_greater_20 = gs.count(g.gq >= 20), global.gq_total_nonmissing = gs.count(isDefined(g.gq))'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/393
https://github.com/hail-is/hail/pull/398:116,Deployability,update,updated,116,"#313 : changed the Interval created from BED file so [Start, End) in BED file becomes [Start + 1, End] in Hail. See updated docs.; #257 : added annotatesamples list which creates a new boolean annotation based on whether a sample id is in the input file or not; #319 : changed regular expression parsing to handle contig\tstart\tend in addition to config:start-end",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/398
https://github.com/hail-is/hail/pull/398:348,Modifiability,config,config,348,"#313 : changed the Interval created from BED file so [Start, End) in BED file becomes [Start + 1, End] in Hail. See updated docs.; #257 : added annotatesamples list which creates a new boolean annotation based on whether a sample id is in the input file or not; #319 : changed regular expression parsing to handle contig\tstart\tend in addition to config:start-end",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/398
https://github.com/hail-is/hail/issues/402:131,Availability,error,error,131,"It would be great if it was possible to have Hail skip rows that don't have the correct number of fields, and just report them via error messages (without crashing), so that the annotation files (such as EIGEN) can still be used.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/402
https://github.com/hail-is/hail/issues/402:137,Integrability,message,messages,137,"It would be great if it was possible to have Hail skip rows that don't have the correct number of fields, and just report them via error messages (without crashing), so that the annotation files (such as EIGEN) can still be used.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/402
https://github.com/hail-is/hail/issues/404:509,Security,access,access,509,"I'd like to be able to export genotypes that have genotype calls but have AD=0 in order to investigate what's going on which these calls. (Also to have something to show Laura Gauthier when I complain to her about this.). Currently, exportgenotypes does not print homref or missing genotypes. Unfortunately, genotypes with AD=0 are treated as ""missing"" (at least in the filtering module), so I can't print these out. Cotton has suggested adding an -f flag in the exportgenotypes module that would allow me to access these genotypes -- e.g., . `exportgenotypes -f 'g.gq >= 20 && isMissing(g.fractionReadsRef)' -c 'sample=s, …'``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/404
https://github.com/hail-is/hail/pull/409:29,Testability,test,test,29,Duplicated R code for fisher.test and C code from here: https://github.com/wch/r-source/blob/e5b21d0397c607883ff25cca379687b86933d730/src/library/stats/src/zeroin.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/409
https://github.com/hail-is/hail/issues/413:49,Availability,error,error,49,"When commands are submitted there's currently an error when I don't provide the proper arguments for a function. This way I don't waste time running things only to have the job crash after it's been going for a while. There should be similar functionality when I submit a command that tries to filter by/annotate with a file that doesn't exist. If you can distinguish when this file will be automatically generated by earlier commands in the script, then that would be nice. If not, then you could just have a warning if the command line has a function using a file that doesn't currently exist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/413
https://github.com/hail-is/hail/issues/419:107,Usability,clear,clear,107,"Feature/Comment. Once documented the invocation of hail for running over a Spark cluster it proves crystal clear how to proceed, via `shadowJar` compilation and `spark-submit --master`. However it is not fully intuitive to have a `hail` command with a `--master` switch and a `shadowJar` compilation for `spark-submit`. A unifying script (eg. `bin/hail-submit`) that infers from command line options whether it is a local or cluster run, and from defaults infers the location of either the hail script or the `shadowJar` jar, could make the invocation simpler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/419
https://github.com/hail-is/hail/issues/419:210,Usability,intuit,intuitive,210,"Feature/Comment. Once documented the invocation of hail for running over a Spark cluster it proves crystal clear how to proceed, via `shadowJar` compilation and `spark-submit --master`. However it is not fully intuitive to have a `hail` command with a `--master` switch and a `shadowJar` compilation for `spark-submit`. A unifying script (eg. `bin/hail-submit`) that infers from command line options whether it is a local or cluster run, and from defaults infers the location of either the hail script or the `shadowJar` jar, could make the invocation simpler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/419
https://github.com/hail-is/hail/issues/419:552,Usability,simpl,simpler,552,"Feature/Comment. Once documented the invocation of hail for running over a Spark cluster it proves crystal clear how to proceed, via `shadowJar` compilation and `spark-submit --master`. However it is not fully intuitive to have a `hail` command with a `--master` switch and a `shadowJar` compilation for `spark-submit`. A unifying script (eg. `bin/hail-submit`) that infers from command line options whether it is a local or cluster run, and from defaults infers the location of either the hail script or the `shadowJar` jar, could make the invocation simpler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/419
https://github.com/hail-is/hail/issues/425:149,Availability,ERROR,ERROR,149,cmd line:. ```; hail: info: running: importvcf TT.head.vcf.bgz; hail: importvcf: caught exception: null; ```. in hail.log:. ```; 2016-06-16 11:40:11 ERROR Hail:88 - hail: importvcf: caught exception: java.util.zip.ZipException: null; at org.broadinstitute.hail.io.compress.BGzipInputStream$BGzipHeader.<init>(BGzipInputStream.java:31); at org.broadinstitute.hail.io.compress.BGzipInputStream.decompressNextBlock(BGzipInputStream.java:139); at org.broadinstitute.hail.io.compress.BGzipInputStream.read(BGzipInputStream.java:170); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/425
https://github.com/hail-is/hail/issues/425:118,Testability,log,log,118,cmd line:. ```; hail: info: running: importvcf TT.head.vcf.bgz; hail: importvcf: caught exception: null; ```. in hail.log:. ```; 2016-06-16 11:40:11 ERROR Hail:88 - hail: importvcf: caught exception: java.util.zip.ZipException: null; at org.broadinstitute.hail.io.compress.BGzipInputStream$BGzipHeader.<init>(BGzipInputStream.java:31); at org.broadinstitute.hail.io.compress.BGzipInputStream.decompressNextBlock(BGzipInputStream.java:139); at org.broadinstitute.hail.io.compress.BGzipInputStream.read(BGzipInputStream.java:170); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/425
https://github.com/hail-is/hail/issues/427:166,Availability,error,error,166,"I want to have a functionality when import VCF, disable the filter based on `sum(AD) != DP`. Although this is a good sanity check, but since we are not clear if this error will lead to inaccurate genotype calls, there will be an option for analyst to keep those calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/427
https://github.com/hail-is/hail/issues/427:117,Safety,sanity check,sanity check,117,"I want to have a functionality when import VCF, disable the filter based on `sum(AD) != DP`. Although this is a good sanity check, but since we are not clear if this error will lead to inaccurate genotype calls, there will be an option for analyst to keep those calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/427
https://github.com/hail-is/hail/issues/427:152,Usability,clear,clear,152,"I want to have a functionality when import VCF, disable the filter based on `sum(AD) != DP`. Although this is a good sanity check, but since we are not clear if this error will lead to inaccurate genotype calls, there will be an option for analyst to keep those calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/427
https://github.com/hail-is/hail/issues/430:338,Availability,failure,failure,338,"Happened on VDS with small number of partitions (18) but large number of variants (~150mio). [Stage 0:=============================> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:396,Availability,failure,failure,396,"Happened on VDS with small number of partitions (18) but large number of variants (~150mio). [Stage 0:=============================> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:1733,Energy Efficiency,schedul,scheduler,1733,"==========> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:1804,Energy Efficiency,schedul,scheduler,1804,"==========> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:180,Modifiability,config,config,180,"Happened on VDS with small number of partitions (18) but large number of variants (~150mio). [Stage 0:=============================> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:1214,Performance,Cache,CacheManager,1214,"==========> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:1240,Performance,Cache,CacheManager,1240,"==========> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:1926,Performance,concurren,concurrent,1926,"==========> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:2010,Performance,concurren,concurrent,2010,"==========> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/430:317,Safety,abort,aborted,317,"Happened on VDS with small number of partitions (18) but large number of variants (~150mio). [Stage 0:=============================> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/430
https://github.com/hail-is/hail/issues/431:28,Testability,test,test,28,"For example, fisher's exact test should look something like this:. fet(0, 100, 5, 1000, t, oddsRatio = 2, confidence_level = 0.1, alternative = ""two.sided"")",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/431
https://github.com/hail-is/hail/pull/432:70,Safety,avoid,avoid,70,Add option to use reference population frequency -- more accurate and avoid reading genotypes twice; Use annotations as intermediate allele frequency storage,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/432
https://github.com/hail-is/hail/issues/433:69,Usability,clear,clear,69,The current help in `Representation of sequence data in Hail` is not clear regarding the `altAllele`; for example make clear that `aa.ref` should be `v.altAllele.ref`.; Cheers,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/433
https://github.com/hail-is/hail/issues/433:119,Usability,clear,clear,119,The current help in `Representation of sequence data in Hail` is not clear regarding the `altAllele`; for example make clear that `aa.ref` should be `v.altAllele.ref`.; Cheers,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/433
https://github.com/hail-is/hail/issues/440:226,Integrability,depend,depend,226,"From @cseed: . > We're going to add a Reference class with a list of all the contigs and; > their sizes soon. We should use this instead of looking at the contig lines; > (which are unfortunately optional in the VCF spec.) We depend on the; > reference in a few places (e.g. Variant.inParX) and currently assume b37,; > but will soon have datasets with both b37 and b38.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/440
https://github.com/hail-is/hail/issues/442:55,Modifiability,variab,variable,55,"With EIGENSTRAT's smartpca, you can specify a grouping variable in the indiv file then specify which group to call PCs on using -w flag & project onto the rest. Would be great to have this feature as this will allow projection with 1000G samples, related samples, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/442
https://github.com/hail-is/hail/pull/444:49,Integrability,Depend,Depends,49,Added command `aggregateintervals` for monkol. . Depends on #441 (that should be merged / reviewed first),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/444
https://github.com/hail-is/hail/issues/445:252,Integrability,message,message,252,"The title is self-explanatory. Some kind of average of the fifth column for this command: `hdfs dfs -ls CANCER.vep.qced.otherann.vds/rdd.parquet` and maybe the difference between this metric and 128MB. If the difference is larger than x, then report a message saying that you need to repartition. Even more cool the command can suggest the number of partitions you should use.; cheers",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/445
https://github.com/hail-is/hail/issues/452:458,Availability,error,error,458,"As the docs state, inParX should be ""true if in pseudo-autosomal region on chromosome X."" Likewise, inParY should be ""true if in pseudo-autosomal region on chromosome Y."" However, these flags are currently being applied regardless of chromosome. That is, currently any variant that meets (60001 <= start && start <= 2699520) || (154931044 <= start && start <= 155260560) gets inParX = true, and similarly for inParY. This is confusing and, I would guess, in error. inParX should only be able to be true if v.contig == ""X"" is true, and inParY should only be able to be true if v.contig == ""Y"" is true.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/452
https://github.com/hail-is/hail/issues/453:733,Availability,error,errors,733,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453
https://github.com/hail-is/hail/issues/453:769,Availability,FAILURE,FAILURE,769,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453
https://github.com/hail-is/hail/issues/453:110,Deployability,install,installDist,110,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453
https://github.com/hail-is/hail/issues/453:238,Energy Efficiency,schedul,scheduled,238,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453
https://github.com/hail-is/hail/issues/453:147,Testability,test,testing,147,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453
https://github.com/hail-is/hail/issues/453:1006,Testability,log,log,1006,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/453
https://github.com/hail-is/hail/issues/454:1656,Availability,error,errors,1656,"When buliding hail , There are several problems ，please help，thanks. [root@**\* hail]# gradle shadowJar; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:135: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:153: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:162: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:661: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:753: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 39.537 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/454
https://github.com/hail-is/hail/issues/454:1692,Availability,FAILURE,FAILURE,1692,"When buliding hail , There are several problems ，please help，thanks. [root@**\* hail]# gradle shadowJar; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:135: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:153: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:162: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:661: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:753: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 39.537 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/454
https://github.com/hail-is/hail/issues/454:220,Energy Efficiency,schedul,scheduled,220,"When buliding hail , There are several problems ，please help，thanks. [root@**\* hail]# gradle shadowJar; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:135: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:153: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:162: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:661: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:753: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 39.537 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/454
https://github.com/hail-is/hail/issues/454:129,Testability,test,testing,129,"When buliding hail , There are several problems ，please help，thanks. [root@**\* hail]# gradle shadowJar; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:135: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:153: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:162: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:661: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:753: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 39.537 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/454
https://github.com/hail-is/hail/issues/454:1929,Testability,log,log,1929,"When buliding hail , There are several problems ，please help，thanks. [root@**\* hail]# gradle shadowJar; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:135: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:153: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:162: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:661: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; /**_/hail/src/main/scala/org/broadinstitute/hail/expr/AST.scala:753: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 39.537 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/454
https://github.com/hail-is/hail/issues/457:1719,Availability,FAILURE,FAILURE,1719,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:33,Deployability,install,installDist,33,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:498,Energy Efficiency,schedul,scheduled,498,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:271,Testability,test,tests,271,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:298,Testability,test,test,298,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:406,Testability,test,testing,406,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:850,Testability,test,testClasses,850,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:875,Testability,test,test,875,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:923,Testability,test,test,923,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:979,Testability,test,testBiallelic,979,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1069,Testability,test,test,1069,"y:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1075,Testability,Test,Test,1075,"y:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1087,Testability,test,test,1087,"y:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1180,Testability,test,test,1180,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1227,Testability,test,test,1227,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1300,Testability,test,test,1300,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1306,Testability,Test,Test,1306,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1318,Testability,test,testGenotypeStream,1318,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1433,Testability,test,test,1433,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1487,Testability,test,testImputeSexPlinkVersion,1487,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1587,Testability,test,test,1587,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1593,Testability,Test,Test,1593,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1605,Testability,test,test,1605,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1678,Testability,test,tests,1678,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1706,Testability,test,test,1706,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1809,Testability,test,test,1809,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1840,Testability,test,tests,1840,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:1898,Testability,test,tests,1898,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/457:2023,Testability,log,log,2023,"oot@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9 mins 25.533 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/457
https://github.com/hail-is/hail/issues/463:602,Modifiability,variab,variables,602,"In R, when you load a data table, it auto-detects whether each column is a character vs. numeric type. It would be super if this could be implemented in Hail. I'm guessing it would take the form of ""if none of the fields in the column contain special characters or letters, then it's numeric, else it's character,"" (but maybe it's not so straight forward, not so sure...). . Anyways, when you have over 30 annotations that are numeric, it's a bit of a pain to have to go through writing all the -t flag options in Hail, so if it could be auto-detected, that would be super! . In the case where 'dummy' variables are used (like 1-5 for Batch), then the user should be able to say that that's a string or a ""factor"" as it is in R (or a character/string, which is essentially the same), for the purposes of analysis in linear regression.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/463
https://github.com/hail-is/hail/issues/463:15,Performance,load,load,15,"In R, when you load a data table, it auto-detects whether each column is a character vs. numeric type. It would be super if this could be implemented in Hail. I'm guessing it would take the form of ""if none of the fields in the column contain special characters or letters, then it's numeric, else it's character,"" (but maybe it's not so straight forward, not so sure...). . Anyways, when you have over 30 annotations that are numeric, it's a bit of a pain to have to go through writing all the -t flag options in Hail, so if it could be auto-detected, that would be super! . In the case where 'dummy' variables are used (like 1-5 for Batch), then the user should be able to say that that's a string or a ""factor"" as it is in R (or a character/string, which is essentially the same), for the purposes of analysis in linear regression.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/463
https://github.com/hail-is/hail/issues/463:42,Safety,detect,detects,42,"In R, when you load a data table, it auto-detects whether each column is a character vs. numeric type. It would be super if this could be implemented in Hail. I'm guessing it would take the form of ""if none of the fields in the column contain special characters or letters, then it's numeric, else it's character,"" (but maybe it's not so straight forward, not so sure...). . Anyways, when you have over 30 annotations that are numeric, it's a bit of a pain to have to go through writing all the -t flag options in Hail, so if it could be auto-detected, that would be super! . In the case where 'dummy' variables are used (like 1-5 for Batch), then the user should be able to say that that's a string or a ""factor"" as it is in R (or a character/string, which is essentially the same), for the purposes of analysis in linear regression.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/463
https://github.com/hail-is/hail/issues/463:543,Safety,detect,detected,543,"In R, when you load a data table, it auto-detects whether each column is a character vs. numeric type. It would be super if this could be implemented in Hail. I'm guessing it would take the form of ""if none of the fields in the column contain special characters or letters, then it's numeric, else it's character,"" (but maybe it's not so straight forward, not so sure...). . Anyways, when you have over 30 annotations that are numeric, it's a bit of a pain to have to go through writing all the -t flag options in Hail, so if it could be auto-detected, that would be super! . In the case where 'dummy' variables are used (like 1-5 for Batch), then the user should be able to say that that's a string or a ""factor"" as it is in R (or a character/string, which is essentially the same), for the purposes of analysis in linear regression.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/463
https://github.com/hail-is/hail/pull/481:246,Usability,feedback,feedback,246,"Please look over the `FAQ.md` page and let me know if this is what you had in mind in terms of layout, organization, question complexity, and range of topics. I haven't done a thorough pass through of the Hail slack channel yet. Once I have your feedback, I'll continue adding and refining examples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/481
https://github.com/hail-is/hail/pull/483:29,Performance,load,loading,29,Added additional support for loading (implicit parent data) and exporting (in the context of exportplink) fam files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/483
https://github.com/hail-is/hail/issues/489:417,Modifiability,plugin,plugin,417,"Would it be possible to add an option to annotate with --tab and --pick_allele, as below:; This would be super helpful!. <hail.vep.perl>; <hail.vep.location>; --format vcf; --tab; --pick_allele; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.cache_dir>/homo_sapiens/81_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa; --minimal; --assembly GRCh37; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/489
https://github.com/hail-is/hail/issues/489:240,Performance,cache,cache,240,"Would it be possible to add an option to annotate with --tab and --pick_allele, as below:; This would be super helpful!. <hail.vep.perl>; <hail.vep.location>; --format vcf; --tab; --pick_allele; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.cache_dir>/homo_sapiens/81_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa; --minimal; --assembly GRCh37; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/489
https://github.com/hail-is/hail/issues/493:113,Testability,Assert,AssertionError,113,"Example: ```$ hail ... exportvariants -o foo.btsv -c '""this is a \""'`. ```; Exception in thread ""main"" java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:165); at org.broadinstitute.hail.expr.Parser$$anonfun$stringLiteral$2$$anonfun$apply$136.apply(Parser.scala:326); at org.broadinstitute.hail.expr.Parser$$anonfun$stringLiteral$2$$anonfun$apply$136.apply(Parser.scala:323); at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136); ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/493
https://github.com/hail-is/hail/issues/493:129,Testability,assert,assertion,129,"Example: ```$ hail ... exportvariants -o foo.btsv -c '""this is a \""'`. ```; Exception in thread ""main"" java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:165); at org.broadinstitute.hail.expr.Parser$$anonfun$stringLiteral$2$$anonfun$apply$136.apply(Parser.scala:326); at org.broadinstitute.hail.expr.Parser$$anonfun$stringLiteral$2$$anonfun$apply$136.apply(Parser.scala:323); at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136); ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/493
https://github.com/hail-is/hail/issues/493:164,Testability,assert,assert,164,"Example: ```$ hail ... exportvariants -o foo.btsv -c '""this is a \""'`. ```; Exception in thread ""main"" java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:165); at org.broadinstitute.hail.expr.Parser$$anonfun$stringLiteral$2$$anonfun$apply$136.apply(Parser.scala:326); at org.broadinstitute.hail.expr.Parser$$anonfun$stringLiteral$2$$anonfun$apply$136.apply(Parser.scala:323); at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136); ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/493
https://github.com/hail-is/hail/pull/497:127,Availability,error,error,127,"Customize export variant- and genotype-level fields.; Upgrade to SorlJ 6, add dependencies.; Support SolrCloud.; Retry on Solr error.; Only insert non-null fields.; Only export called non-ref genotypes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/497
https://github.com/hail-is/hail/pull/497:54,Deployability,Upgrade,Upgrade,54,"Customize export variant- and genotype-level fields.; Upgrade to SorlJ 6, add dependencies.; Support SolrCloud.; Retry on Solr error.; Only insert non-null fields.; Only export called non-ref genotypes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/497
https://github.com/hail-is/hail/pull/497:78,Integrability,depend,dependencies,78,"Customize export variant- and genotype-level fields.; Upgrade to SorlJ 6, add dependencies.; Support SolrCloud.; Retry on Solr error.; Only insert non-null fields.; Only export called non-ref genotypes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/497
https://github.com/hail-is/hail/pull/499:104,Availability,redundant,redundant,104,"this small PR addresses #452 . alternatively, we could only have inParX and inParY, but this introduces redundant contig checks in two of three usages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499
https://github.com/hail-is/hail/pull/499:104,Safety,redund,redundant,104,"this small PR addresses #452 . alternatively, we could only have inParX and inParY, but this introduces redundant contig checks in two of three usages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499
https://github.com/hail-is/hail/pull/511:173,Deployability,update,updated,173,- added sort on Arrays in expr; - extended sort and sortBy to take an optional Boolean parameter for ascending; - modifed behavior to always place null values at the end; - updated HailExpressionLanguage.md; - added tests for sort and sortBy to ExprSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511
https://github.com/hail-is/hail/pull/511:34,Modifiability,extend,extended,34,- added sort on Arrays in expr; - extended sort and sortBy to take an optional Boolean parameter for ascending; - modifed behavior to always place null values at the end; - updated HailExpressionLanguage.md; - added tests for sort and sortBy to ExprSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511
https://github.com/hail-is/hail/pull/511:216,Testability,test,tests,216,- added sort on Arrays in expr; - extended sort and sortBy to take an optional Boolean parameter for ascending; - modifed behavior to always place null values at the end; - updated HailExpressionLanguage.md; - added tests for sort and sortBy to ExprSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511
https://github.com/hail-is/hail/pull/512:12,Usability,undo,undocumented,12,Also remove undocumented fileDate header entry in exportvcf.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/512
https://github.com/hail-is/hail/pull/513:21,Usability,guid,guide,21,Also converted style guide to markdown.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/513
https://github.com/hail-is/hail/pull/516:137,Security,access,accessor,137,- Added a new annotate module due to demand from users to be able to annotate variants without ref/alt.; - Added `Locus` constructor and accessor methods in the expr language,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/516
https://github.com/hail-is/hail/pull/517:45,Testability,test,test,45,- removed LinearRegressionFromHcsCommand and test; - removed extraneous JSON files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/517
https://github.com/hail-is/hail/issues/519:28,Availability,error,errors,28,"Some of the examples create errors in the new version (ex: need to use ""importannotations table"", and the -r flag doesn't exist anymore). (also -- would be great to add a link to this page from the main page!)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/519
https://github.com/hail-is/hail/issues/520:136,Availability,error,errors,136,"ex: in importing annotations from a dbNSFP table, using --impute imputes the Chromosome as an Int as opposed to a String, and brings up errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/520
https://github.com/hail-is/hail/pull/523:0,Testability,Test,Test,0,Test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/523
https://github.com/hail-is/hail/pull/525:45,Testability,test,test,45,- removed LinearRegressionFromHcsCommand and test; - removed extraneous JSON files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/525
https://github.com/hail-is/hail/pull/526:0,Modifiability,Refactor,Refactored,0,"Refactored RDD[(Variant, Annotation, Iterable[Genotype])] to; RDD[(Variant, (Annotation, Iterable[Genotype]))]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/526
https://github.com/hail-is/hail/issues/527:32,Testability,log,log,32,"Would be nice to be able to use log/log10 - this way, if I filter by pHWE, I won't have to type out the full float and potentially miss a zero",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/527
https://github.com/hail-is/hail/pull/532:65,Testability,test,tested,65,"Fixes: https://github.com/broadinstitute/hail/issues/321. I just tested it on the Cray against 20K exomes. Took ~6m, worked like a charm.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/532
https://github.com/hail-is/hail/pull/536:477,Testability,test,test,477,Resolves broadinstitute/hail#535 by choosing uniformly(?) a partition of the given size. The partition algorithm is [defined in Gen](https://github.com/danking/hail/blob/4dcde7e15c0cb9de402e0a2307f7f8e8a56054bd/src/main/scala/org/broadinstitute/hail/check/Gen.scala#L23-L34). It takes `size` balls and randomly places each one in a bin. The algorithm @jbloom22 described to me last night would draw O(partition) random numbers. This approximately halved the run-time of `grade test` on my machine.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/536
https://github.com/hail-is/hail/issues/539:74,Availability,failure,failure,74,Treat them analogously to ref. I think there's potential for an assertion failure now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/539
https://github.com/hail-is/hail/issues/539:64,Testability,assert,assertion,64,Treat them analogously to ref. I think there's potential for an assertion failure now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/539
https://github.com/hail-is/hail/pull/542:407,Performance,load,load,407,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/542
https://github.com/hail-is/hail/pull/542:449,Performance,load,load,449,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/542
https://github.com/hail-is/hail/pull/542:497,Performance,load,load,497,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/542
https://github.com/hail-is/hail/pull/542:249,Security,access,accessing,249,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/542
https://github.com/hail-is/hail/pull/543:72,Security,access,access,72,"Add new command to aggregate and export statistics over intervals with; access to a 'variants' aggregator. Takes an interval list as input,; takes an export command (see exportvariants), and an output path. Exposed `Interval` in the expr language, which has `start`, `end`,; and `contains` (all locus-based). Reworked property-based testing for annotation impexes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/543
https://github.com/hail-is/hail/pull/543:207,Security,Expose,Exposed,207,"Add new command to aggregate and export statistics over intervals with; access to a 'variants' aggregator. Takes an interval list as input,; takes an export command (see exportvariants), and an output path. Exposed `Interval` in the expr language, which has `start`, `end`,; and `contains` (all locus-based). Reworked property-based testing for annotation impexes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/543
https://github.com/hail-is/hail/pull/543:333,Testability,test,testing,333,"Add new command to aggregate and export statistics over intervals with; access to a 'variants' aggregator. Takes an interval list as input,; takes an export command (see exportvariants), and an output path. Exposed `Interval` in the expr language, which has `start`, `end`,; and `contains` (all locus-based). Reworked property-based testing for annotation impexes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/543
https://github.com/hail-is/hail/issues/549:274,Availability,error,error,274,"Having some issues with `map` in `annotateglobal`. The following command:. ```; annotateglobal expr -c 'global.pops = [""AFR"", ""NFE""]' \; annotateglobal expr -c 'global.pop_counts = global.pops.map(x => samples.count(sa.meta_test.POP == x))' \; showglobals; ```. return this error:. ```; hail: fatal: annotateglobal expr: symbol `x' not found; <input>:1:global.nfes = global.pops.map(x => samples.count(sa.meta_test.POP == x)); ```. @tpoterba seemed to suggest aggregators don't work inside map?. Anyway, to provide context: what I'd love to do is eventually be able to iterate through arrays to summarize data (rather than having to specify all my criteria once in each annotate command). Perhaps this is better done in some `group_by` type functionality, but I'm not sure where that is on the roadmap.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/549
https://github.com/hail-is/hail/issues/551:149,Deployability,update,update,149,"Create a filter alleles command. The interface should be modeled off the current filter commands (variant, sample, genotype). One wrinkle is that to update the variant annotations, which must be done by the user, they must have access to information about what alleles were filtered. Rough sketch of the interface:. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(i => va.info.AC[i]), va.info.AN = ...'; ```. where `va.alleleQuality` is a hypothetical annotation of type `Array[Double]`. It also has a `--remove` option. `-c` is the filter condition. It has type `Boolean` and `v`, `va`, and `aIndex` in scope, where `aIndex` is the index of the allele being evaluated. `-a` is an annotation expression which updates variant annotations analogous to the `-c` argument of `annotatevariants expr`. It has `v`, `va` and `aIndices`, where `v` is the _new_ variant (so `v.altAlleles.size` is the number of alleles being kept), `va` is the old variant annotations which are being updated, and `aIndices` is a map from the new to old allele indices. Only alternate alleles should be filtered, although all indices should be 0-based, counting the reference. If no alternate alleles remain, the variant should be filtered (no monomorphic variants). Step one should be to sketch the command docs so we can get feedback on the interface. We should work with @konradjk and Monkol to make examples that handle the ExACv2 use case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551
https://github.com/hail-is/hail/issues/551:760,Deployability,update,updates,760,"Create a filter alleles command. The interface should be modeled off the current filter commands (variant, sample, genotype). One wrinkle is that to update the variant annotations, which must be done by the user, they must have access to information about what alleles were filtered. Rough sketch of the interface:. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(i => va.info.AC[i]), va.info.AN = ...'; ```. where `va.alleleQuality` is a hypothetical annotation of type `Array[Double]`. It also has a `--remove` option. `-c` is the filter condition. It has type `Boolean` and `v`, `va`, and `aIndex` in scope, where `aIndex` is the index of the allele being evaluated. `-a` is an annotation expression which updates variant annotations analogous to the `-c` argument of `annotatevariants expr`. It has `v`, `va` and `aIndices`, where `v` is the _new_ variant (so `v.altAlleles.size` is the number of alleles being kept), `va` is the old variant annotations which are being updated, and `aIndices` is a map from the new to old allele indices. Only alternate alleles should be filtered, although all indices should be 0-based, counting the reference. If no alternate alleles remain, the variant should be filtered (no monomorphic variants). Step one should be to sketch the command docs so we can get feedback on the interface. We should work with @konradjk and Monkol to make examples that handle the ExACv2 use case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551
https://github.com/hail-is/hail/issues/551:1025,Deployability,update,updated,1025,"Create a filter alleles command. The interface should be modeled off the current filter commands (variant, sample, genotype). One wrinkle is that to update the variant annotations, which must be done by the user, they must have access to information about what alleles were filtered. Rough sketch of the interface:. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(i => va.info.AC[i]), va.info.AN = ...'; ```. where `va.alleleQuality` is a hypothetical annotation of type `Array[Double]`. It also has a `--remove` option. `-c` is the filter condition. It has type `Boolean` and `v`, `va`, and `aIndex` in scope, where `aIndex` is the index of the allele being evaluated. `-a` is an annotation expression which updates variant annotations analogous to the `-c` argument of `annotatevariants expr`. It has `v`, `va` and `aIndices`, where `v` is the _new_ variant (so `v.altAlleles.size` is the number of alleles being kept), `va` is the old variant annotations which are being updated, and `aIndices` is a map from the new to old allele indices. Only alternate alleles should be filtered, although all indices should be 0-based, counting the reference. If no alternate alleles remain, the variant should be filtered (no monomorphic variants). Step one should be to sketch the command docs so we can get feedback on the interface. We should work with @konradjk and Monkol to make examples that handle the ExACv2 use case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551
https://github.com/hail-is/hail/issues/551:37,Integrability,interface,interface,37,"Create a filter alleles command. The interface should be modeled off the current filter commands (variant, sample, genotype). One wrinkle is that to update the variant annotations, which must be done by the user, they must have access to information about what alleles were filtered. Rough sketch of the interface:. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(i => va.info.AC[i]), va.info.AN = ...'; ```. where `va.alleleQuality` is a hypothetical annotation of type `Array[Double]`. It also has a `--remove` option. `-c` is the filter condition. It has type `Boolean` and `v`, `va`, and `aIndex` in scope, where `aIndex` is the index of the allele being evaluated. `-a` is an annotation expression which updates variant annotations analogous to the `-c` argument of `annotatevariants expr`. It has `v`, `va` and `aIndices`, where `v` is the _new_ variant (so `v.altAlleles.size` is the number of alleles being kept), `va` is the old variant annotations which are being updated, and `aIndices` is a map from the new to old allele indices. Only alternate alleles should be filtered, although all indices should be 0-based, counting the reference. If no alternate alleles remain, the variant should be filtered (no monomorphic variants). Step one should be to sketch the command docs so we can get feedback on the interface. We should work with @konradjk and Monkol to make examples that handle the ExACv2 use case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551
https://github.com/hail-is/hail/issues/551:304,Integrability,interface,interface,304,"Create a filter alleles command. The interface should be modeled off the current filter commands (variant, sample, genotype). One wrinkle is that to update the variant annotations, which must be done by the user, they must have access to information about what alleles were filtered. Rough sketch of the interface:. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(i => va.info.AC[i]), va.info.AN = ...'; ```. where `va.alleleQuality` is a hypothetical annotation of type `Array[Double]`. It also has a `--remove` option. `-c` is the filter condition. It has type `Boolean` and `v`, `va`, and `aIndex` in scope, where `aIndex` is the index of the allele being evaluated. `-a` is an annotation expression which updates variant annotations analogous to the `-c` argument of `annotatevariants expr`. It has `v`, `va` and `aIndices`, where `v` is the _new_ variant (so `v.altAlleles.size` is the number of alleles being kept), `va` is the old variant annotations which are being updated, and `aIndices` is a map from the new to old allele indices. Only alternate alleles should be filtered, although all indices should be 0-based, counting the reference. If no alternate alleles remain, the variant should be filtered (no monomorphic variants). Step one should be to sketch the command docs so we can get feedback on the interface. We should work with @konradjk and Monkol to make examples that handle the ExACv2 use case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551
https://github.com/hail-is/hail/issues/551:1367,Integrability,interface,interface,1367,"Create a filter alleles command. The interface should be modeled off the current filter commands (variant, sample, genotype). One wrinkle is that to update the variant annotations, which must be done by the user, they must have access to information about what alleles were filtered. Rough sketch of the interface:. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(i => va.info.AC[i]), va.info.AN = ...'; ```. where `va.alleleQuality` is a hypothetical annotation of type `Array[Double]`. It also has a `--remove` option. `-c` is the filter condition. It has type `Boolean` and `v`, `va`, and `aIndex` in scope, where `aIndex` is the index of the allele being evaluated. `-a` is an annotation expression which updates variant annotations analogous to the `-c` argument of `annotatevariants expr`. It has `v`, `va` and `aIndices`, where `v` is the _new_ variant (so `v.altAlleles.size` is the number of alleles being kept), `va` is the old variant annotations which are being updated, and `aIndices` is a map from the new to old allele indices. Only alternate alleles should be filtered, although all indices should be 0-based, counting the reference. If no alternate alleles remain, the variant should be filtered (no monomorphic variants). Step one should be to sketch the command docs so we can get feedback on the interface. We should work with @konradjk and Monkol to make examples that handle the ExACv2 use case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551
https://github.com/hail-is/hail/issues/551:228,Security,access,access,228,"Create a filter alleles command. The interface should be modeled off the current filter commands (variant, sample, genotype). One wrinkle is that to update the variant annotations, which must be done by the user, they must have access to information about what alleles were filtered. Rough sketch of the interface:. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(i => va.info.AC[i]), va.info.AN = ...'; ```. where `va.alleleQuality` is a hypothetical annotation of type `Array[Double]`. It also has a `--remove` option. `-c` is the filter condition. It has type `Boolean` and `v`, `va`, and `aIndex` in scope, where `aIndex` is the index of the allele being evaluated. `-a` is an annotation expression which updates variant annotations analogous to the `-c` argument of `annotatevariants expr`. It has `v`, `va` and `aIndices`, where `v` is the _new_ variant (so `v.altAlleles.size` is the number of alleles being kept), `va` is the old variant annotations which are being updated, and `aIndices` is a map from the new to old allele indices. Only alternate alleles should be filtered, although all indices should be 0-based, counting the reference. If no alternate alleles remain, the variant should be filtered (no monomorphic variants). Step one should be to sketch the command docs so we can get feedback on the interface. We should work with @konradjk and Monkol to make examples that handle the ExACv2 use case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551
https://github.com/hail-is/hail/issues/551:1351,Usability,feedback,feedback,1351,"Create a filter alleles command. The interface should be modeled off the current filter commands (variant, sample, genotype). One wrinkle is that to update the variant annotations, which must be done by the user, they must have access to information about what alleles were filtered. Rough sketch of the interface:. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(i => va.info.AC[i]), va.info.AN = ...'; ```. where `va.alleleQuality` is a hypothetical annotation of type `Array[Double]`. It also has a `--remove` option. `-c` is the filter condition. It has type `Boolean` and `v`, `va`, and `aIndex` in scope, where `aIndex` is the index of the allele being evaluated. `-a` is an annotation expression which updates variant annotations analogous to the `-c` argument of `annotatevariants expr`. It has `v`, `va` and `aIndices`, where `v` is the _new_ variant (so `v.altAlleles.size` is the number of alleles being kept), `va` is the old variant annotations which are being updated, and `aIndices` is a map from the new to old allele indices. Only alternate alleles should be filtered, although all indices should be 0-based, counting the reference. If no alternate alleles remain, the variant should be filtered (no monomorphic variants). Step one should be to sketch the command docs so we can get feedback on the interface. We should work with @konradjk and Monkol to make examples that handle the ExACv2 use case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551
https://github.com/hail-is/hail/issues/561:183,Availability,error,error,183,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561
https://github.com/hail-is/hail/issues/561:252,Availability,error,error,252,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561
https://github.com/hail-is/hail/issues/561:376,Availability,down,download,376,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561
https://github.com/hail-is/hail/issues/561:189,Integrability,message,message,189,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561
https://github.com/hail-is/hail/issues/561:341,Testability,test,test,341,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561
https://github.com/hail-is/hail/issues/561:358,Testability,test,test,358,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561
https://github.com/hail-is/hail/issues/561:440,Testability,test,test,440,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/561
https://github.com/hail-is/hail/pull/564:72,Security,access,access,72,"Add new command to aggregate and export statistics over intervals with; access to a 'variants' aggregator. Takes an interval list as input,; takes an export command (see exportvariants), and an output path. Exposed `Interval` in the expr language, which has `start`, `end`,; and `contains` (all locus-based). Reworked property-based testing for annotation impexes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/564
https://github.com/hail-is/hail/pull/564:207,Security,Expose,Exposed,207,"Add new command to aggregate and export statistics over intervals with; access to a 'variants' aggregator. Takes an interval list as input,; takes an export command (see exportvariants), and an output path. Exposed `Interval` in the expr language, which has `start`, `end`,; and `contains` (all locus-based). Reworked property-based testing for annotation impexes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/564
https://github.com/hail-is/hail/pull/564:333,Testability,test,testing,333,"Add new command to aggregate and export statistics over intervals with; access to a 'variants' aggregator. Takes an interval list as input,; takes an export command (see exportvariants), and an output path. Exposed `Interval` in the expr language, which has `start`, `end`,; and `contains` (all locus-based). Reworked property-based testing for annotation impexes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/564
https://github.com/hail-is/hail/issues/565:133,Availability,ERROR,ERROR,133,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:343,Availability,error,error,343,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:624,Availability,error,error,624,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:746,Availability,FAILURE,FAILURE,746,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:17,Deployability,update,updated,17,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:262,Deployability,install,installDist,262,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:383,Testability,test,test,383,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:444,Testability,test,testNonNumericPheno,444,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:480,Testability,test,test,480,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:486,Testability,Test,Test,486,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:498,Testability,test,testWithImportFam,498,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:733,Testability,test,test,733,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:836,Testability,test,test,836,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:864,Testability,Test,Test,864,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/565:1027,Testability,log,log,1027,"```; Hi, Today I updated the new verion of hail ，and try to re-build it, but I encountered some issues. ; ```; ## （1） “gradle check” ERROR ：/tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. The “~/hail $ gradle installDist” went successfully，but when I do ""~/hail $ gradle check"",it got some error:. ........; Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.testNonNumericPheno PASSED; Running test: Test method testWithImportFam(org.broadinstitute.hail.methods.LinearRegressionSuite); /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 5 mins 6.833 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/565
https://github.com/hail-is/hail/issues/566:229,Availability,failure,failure,229,"Replicable bug:. ```; hail -b 0 importvcf src/test/resources/multipleChromosomes.vcf -n 10 exportvcf -o /tmp/out.vcf.bgz importvcf /tmp/out.vcf.bgz -n 10 count --genotypes. hail: count: caught exception: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4, localhost): htsjdk.samtools.SAMFormatException: Invalid GZIP header; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:72); at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:410); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:392); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:127); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.readWithinBlock(BGZFSplitCompressionInputStream.java:81); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.read(BGZFSplitCompressionInputStream.java:48); at java.io.InputStream.read(InputStream.java:101); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130); at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216); at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/566:286,Availability,failure,failure,286,"Replicable bug:. ```; hail -b 0 importvcf src/test/resources/multipleChromosomes.vcf -n 10 exportvcf -o /tmp/out.vcf.bgz importvcf /tmp/out.vcf.bgz -n 10 count --genotypes. hail: count: caught exception: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4, localhost): htsjdk.samtools.SAMFormatException: Invalid GZIP header; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:72); at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:410); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:392); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:127); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.readWithinBlock(BGZFSplitCompressionInputStream.java:81); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.read(BGZFSplitCompressionInputStream.java:48); at java.io.InputStream.read(InputStream.java:101); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130); at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216); at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/566:726,Availability,avail,available,726,"Replicable bug:. ```; hail -b 0 importvcf src/test/resources/multipleChromosomes.vcf -n 10 exportvcf -o /tmp/out.vcf.bgz importvcf /tmp/out.vcf.bgz -n 10 count --genotypes. hail: count: caught exception: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4, localhost): htsjdk.samtools.SAMFormatException: Invalid GZIP header; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:72); at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:410); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:392); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:127); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.readWithinBlock(BGZFSplitCompressionInputStream.java:81); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.read(BGZFSplitCompressionInputStream.java:48); at java.io.InputStream.read(InputStream.java:101); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130); at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216); at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/566:2907,Energy Efficiency,schedul,scheduler,2907,er.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/566:2978,Energy Efficiency,schedul,scheduler,2978,er.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/566:3100,Performance,concurren,concurrent,3100,er.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/566:3184,Performance,concurren,concurrent,3184,er.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/566:208,Safety,abort,aborted,208,"Replicable bug:. ```; hail -b 0 importvcf src/test/resources/multipleChromosomes.vcf -n 10 exportvcf -o /tmp/out.vcf.bgz importvcf /tmp/out.vcf.bgz -n 10 count --genotypes. hail: count: caught exception: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4, localhost): htsjdk.samtools.SAMFormatException: Invalid GZIP header; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:72); at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:410); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:392); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:127); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.readWithinBlock(BGZFSplitCompressionInputStream.java:81); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.read(BGZFSplitCompressionInputStream.java:48); at java.io.InputStream.read(InputStream.java:101); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130); at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216); at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/566:46,Testability,test,test,46,"Replicable bug:. ```; hail -b 0 importvcf src/test/resources/multipleChromosomes.vcf -n 10 exportvcf -o /tmp/out.vcf.bgz importvcf /tmp/out.vcf.bgz -n 10 count --genotypes. hail: count: caught exception: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4, localhost): htsjdk.samtools.SAMFormatException: Invalid GZIP header; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:72); at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:410); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:392); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:127); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.readWithinBlock(BGZFSplitCompressionInputStream.java:81); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.read(BGZFSplitCompressionInputStream.java:48); at java.io.InputStream.read(InputStream.java:101); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130); at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216); at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/566
https://github.com/hail-is/hail/issues/567:68,Availability,avail,available,68,"things like VEP properties locations, hail launch script locations, available clusters, etc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/567
https://github.com/hail-is/hail/pull/569:133,Deployability,update,update,133,"Style guide draft with 3 sample commands and HTML/JS code . Not intended for merging to master. If you're happy with this, then I'll update the rest of the commands with the style guide I specified. The style guide does not address how to format tables (work in progress). **Style Guide:**; - docs/DocsStyleGuide.md. **Example Markdown Command Files to look at:**; - docs/commands/annotateglobal_expr; - docs/commands/annotateglobal_list; - docs/commands/annotateglobal_table. **HTML/JS code:**; - docs/index.html; - docs/buildDocs.js",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/569
https://github.com/hail-is/hail/pull/569:6,Usability,guid,guide,6,"Style guide draft with 3 sample commands and HTML/JS code . Not intended for merging to master. If you're happy with this, then I'll update the rest of the commands with the style guide I specified. The style guide does not address how to format tables (work in progress). **Style Guide:**; - docs/DocsStyleGuide.md. **Example Markdown Command Files to look at:**; - docs/commands/annotateglobal_expr; - docs/commands/annotateglobal_list; - docs/commands/annotateglobal_table. **HTML/JS code:**; - docs/index.html; - docs/buildDocs.js",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/569
https://github.com/hail-is/hail/pull/569:180,Usability,guid,guide,180,"Style guide draft with 3 sample commands and HTML/JS code . Not intended for merging to master. If you're happy with this, then I'll update the rest of the commands with the style guide I specified. The style guide does not address how to format tables (work in progress). **Style Guide:**; - docs/DocsStyleGuide.md. **Example Markdown Command Files to look at:**; - docs/commands/annotateglobal_expr; - docs/commands/annotateglobal_list; - docs/commands/annotateglobal_table. **HTML/JS code:**; - docs/index.html; - docs/buildDocs.js",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/569
https://github.com/hail-is/hail/pull/569:209,Usability,guid,guide,209,"Style guide draft with 3 sample commands and HTML/JS code . Not intended for merging to master. If you're happy with this, then I'll update the rest of the commands with the style guide I specified. The style guide does not address how to format tables (work in progress). **Style Guide:**; - docs/DocsStyleGuide.md. **Example Markdown Command Files to look at:**; - docs/commands/annotateglobal_expr; - docs/commands/annotateglobal_list; - docs/commands/annotateglobal_table. **HTML/JS code:**; - docs/index.html; - docs/buildDocs.js",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/569
https://github.com/hail-is/hail/pull/569:281,Usability,Guid,Guide,281,"Style guide draft with 3 sample commands and HTML/JS code . Not intended for merging to master. If you're happy with this, then I'll update the rest of the commands with the style guide I specified. The style guide does not address how to format tables (work in progress). **Style Guide:**; - docs/DocsStyleGuide.md. **Example Markdown Command Files to look at:**; - docs/commands/annotateglobal_expr; - docs/commands/annotateglobal_list; - docs/commands/annotateglobal_table. **HTML/JS code:**; - docs/index.html; - docs/buildDocs.js",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/569
https://github.com/hail-is/hail/pull/573:407,Performance,load,load,407,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder. Fix indentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/573
https://github.com/hail-is/hail/pull/573:449,Performance,load,load,449,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder. Fix indentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/573
https://github.com/hail-is/hail/pull/573:497,Performance,load,load,497,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder. Fix indentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/573
https://github.com/hail-is/hail/pull/573:249,Security,access,accessing,249,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder. Fix indentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/573
https://github.com/hail-is/hail/issues/577:92,Availability,error,error,92,Right now dependencies to include the in shadowJar are managed by hand and it is incredibly error prone. There has to be a better way to do this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/577
https://github.com/hail-is/hail/issues/577:10,Integrability,depend,dependencies,10,Right now dependencies to include the in shadowJar are managed by hand and it is incredibly error prone. There has to be a better way to do this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/577
https://github.com/hail-is/hail/pull/585:34,Testability,test,tests,34,"- support for wald, lr, and score tests; - added logreg doc; - added LogisticRegressionCommand; - added LogisticRegression; - added LogisticRegressionModel; - added LogisticRegressionSuite including Epacts comparison; - added LogisticRegressionModelSuite including R comparison; - added supporting resource files regressionLogistic*; - added chiSquaredTail function to Utils; - added chiSquaredTail teses to UtilsSuite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585
https://github.com/hail-is/hail/pull/585:49,Testability,log,logreg,49,"- support for wald, lr, and score tests; - added logreg doc; - added LogisticRegressionCommand; - added LogisticRegression; - added LogisticRegressionModel; - added LogisticRegressionSuite including Epacts comparison; - added LogisticRegressionModelSuite including R comparison; - added supporting resource files regressionLogistic*; - added chiSquaredTail function to Utils; - added chiSquaredTail teses to UtilsSuite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585
https://github.com/hail-is/hail/pull/585:69,Testability,Log,LogisticRegressionCommand,69,"- support for wald, lr, and score tests; - added logreg doc; - added LogisticRegressionCommand; - added LogisticRegression; - added LogisticRegressionModel; - added LogisticRegressionSuite including Epacts comparison; - added LogisticRegressionModelSuite including R comparison; - added supporting resource files regressionLogistic*; - added chiSquaredTail function to Utils; - added chiSquaredTail teses to UtilsSuite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585
https://github.com/hail-is/hail/pull/585:104,Testability,Log,LogisticRegression,104,"- support for wald, lr, and score tests; - added logreg doc; - added LogisticRegressionCommand; - added LogisticRegression; - added LogisticRegressionModel; - added LogisticRegressionSuite including Epacts comparison; - added LogisticRegressionModelSuite including R comparison; - added supporting resource files regressionLogistic*; - added chiSquaredTail function to Utils; - added chiSquaredTail teses to UtilsSuite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585
https://github.com/hail-is/hail/pull/585:132,Testability,Log,LogisticRegressionModel,132,"- support for wald, lr, and score tests; - added logreg doc; - added LogisticRegressionCommand; - added LogisticRegression; - added LogisticRegressionModel; - added LogisticRegressionSuite including Epacts comparison; - added LogisticRegressionModelSuite including R comparison; - added supporting resource files regressionLogistic*; - added chiSquaredTail function to Utils; - added chiSquaredTail teses to UtilsSuite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585
https://github.com/hail-is/hail/pull/585:165,Testability,Log,LogisticRegressionSuite,165,"- support for wald, lr, and score tests; - added logreg doc; - added LogisticRegressionCommand; - added LogisticRegression; - added LogisticRegressionModel; - added LogisticRegressionSuite including Epacts comparison; - added LogisticRegressionModelSuite including R comparison; - added supporting resource files regressionLogistic*; - added chiSquaredTail function to Utils; - added chiSquaredTail teses to UtilsSuite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585
https://github.com/hail-is/hail/pull/585:226,Testability,Log,LogisticRegressionModelSuite,226,"- support for wald, lr, and score tests; - added logreg doc; - added LogisticRegressionCommand; - added LogisticRegression; - added LogisticRegressionModel; - added LogisticRegressionSuite including Epacts comparison; - added LogisticRegressionModelSuite including R comparison; - added supporting resource files regressionLogistic*; - added chiSquaredTail function to Utils; - added chiSquaredTail teses to UtilsSuite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585
https://github.com/hail-is/hail/pull/586:0,Testability,Test,Tests,0,"Tests now take ~2m40 on my laptop. With -Dcheck.size=100 -Dcheck.count=1, they take about ~1m20. A huge part of the test time was (re)creating the SparkContext. I disabled testVSMGenIsLinearSpaceInSizeParameter. It doesn't quite make sense to me to be spending half our test time budget verifying ... that our tests aren't taking too long. Happy to discuss it on Monday. Following Jackie's original code, the duplicated logic and defaults for check settings in Prop and gradle are needed to include the check settings (in particular, the seed) in the gradle output when you're not using `--info` or similar. I couldn't figure out another way to do it. Changes:. Set seed with -Dcheck.seed=seed, random with -Dcheck.seed=random.; Added SparkManager to store sc and sqlContext.; Reuse sc between tests.; Added -Dcheck.size=size and -Dcheck.count=count.; Removed Prop.check with count, size options.; Disabled testVSMGenIsLinearSpaceInSizeParameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586
https://github.com/hail-is/hail/pull/586:116,Testability,test,test,116,"Tests now take ~2m40 on my laptop. With -Dcheck.size=100 -Dcheck.count=1, they take about ~1m20. A huge part of the test time was (re)creating the SparkContext. I disabled testVSMGenIsLinearSpaceInSizeParameter. It doesn't quite make sense to me to be spending half our test time budget verifying ... that our tests aren't taking too long. Happy to discuss it on Monday. Following Jackie's original code, the duplicated logic and defaults for check settings in Prop and gradle are needed to include the check settings (in particular, the seed) in the gradle output when you're not using `--info` or similar. I couldn't figure out another way to do it. Changes:. Set seed with -Dcheck.seed=seed, random with -Dcheck.seed=random.; Added SparkManager to store sc and sqlContext.; Reuse sc between tests.; Added -Dcheck.size=size and -Dcheck.count=count.; Removed Prop.check with count, size options.; Disabled testVSMGenIsLinearSpaceInSizeParameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586
https://github.com/hail-is/hail/pull/586:172,Testability,test,testVSMGenIsLinearSpaceInSizeParameter,172,"Tests now take ~2m40 on my laptop. With -Dcheck.size=100 -Dcheck.count=1, they take about ~1m20. A huge part of the test time was (re)creating the SparkContext. I disabled testVSMGenIsLinearSpaceInSizeParameter. It doesn't quite make sense to me to be spending half our test time budget verifying ... that our tests aren't taking too long. Happy to discuss it on Monday. Following Jackie's original code, the duplicated logic and defaults for check settings in Prop and gradle are needed to include the check settings (in particular, the seed) in the gradle output when you're not using `--info` or similar. I couldn't figure out another way to do it. Changes:. Set seed with -Dcheck.seed=seed, random with -Dcheck.seed=random.; Added SparkManager to store sc and sqlContext.; Reuse sc between tests.; Added -Dcheck.size=size and -Dcheck.count=count.; Removed Prop.check with count, size options.; Disabled testVSMGenIsLinearSpaceInSizeParameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586
https://github.com/hail-is/hail/pull/586:270,Testability,test,test,270,"Tests now take ~2m40 on my laptop. With -Dcheck.size=100 -Dcheck.count=1, they take about ~1m20. A huge part of the test time was (re)creating the SparkContext. I disabled testVSMGenIsLinearSpaceInSizeParameter. It doesn't quite make sense to me to be spending half our test time budget verifying ... that our tests aren't taking too long. Happy to discuss it on Monday. Following Jackie's original code, the duplicated logic and defaults for check settings in Prop and gradle are needed to include the check settings (in particular, the seed) in the gradle output when you're not using `--info` or similar. I couldn't figure out another way to do it. Changes:. Set seed with -Dcheck.seed=seed, random with -Dcheck.seed=random.; Added SparkManager to store sc and sqlContext.; Reuse sc between tests.; Added -Dcheck.size=size and -Dcheck.count=count.; Removed Prop.check with count, size options.; Disabled testVSMGenIsLinearSpaceInSizeParameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586
https://github.com/hail-is/hail/pull/586:310,Testability,test,tests,310,"Tests now take ~2m40 on my laptop. With -Dcheck.size=100 -Dcheck.count=1, they take about ~1m20. A huge part of the test time was (re)creating the SparkContext. I disabled testVSMGenIsLinearSpaceInSizeParameter. It doesn't quite make sense to me to be spending half our test time budget verifying ... that our tests aren't taking too long. Happy to discuss it on Monday. Following Jackie's original code, the duplicated logic and defaults for check settings in Prop and gradle are needed to include the check settings (in particular, the seed) in the gradle output when you're not using `--info` or similar. I couldn't figure out another way to do it. Changes:. Set seed with -Dcheck.seed=seed, random with -Dcheck.seed=random.; Added SparkManager to store sc and sqlContext.; Reuse sc between tests.; Added -Dcheck.size=size and -Dcheck.count=count.; Removed Prop.check with count, size options.; Disabled testVSMGenIsLinearSpaceInSizeParameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586
https://github.com/hail-is/hail/pull/586:420,Testability,log,logic,420,"Tests now take ~2m40 on my laptop. With -Dcheck.size=100 -Dcheck.count=1, they take about ~1m20. A huge part of the test time was (re)creating the SparkContext. I disabled testVSMGenIsLinearSpaceInSizeParameter. It doesn't quite make sense to me to be spending half our test time budget verifying ... that our tests aren't taking too long. Happy to discuss it on Monday. Following Jackie's original code, the duplicated logic and defaults for check settings in Prop and gradle are needed to include the check settings (in particular, the seed) in the gradle output when you're not using `--info` or similar. I couldn't figure out another way to do it. Changes:. Set seed with -Dcheck.seed=seed, random with -Dcheck.seed=random.; Added SparkManager to store sc and sqlContext.; Reuse sc between tests.; Added -Dcheck.size=size and -Dcheck.count=count.; Removed Prop.check with count, size options.; Disabled testVSMGenIsLinearSpaceInSizeParameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586
https://github.com/hail-is/hail/pull/586:794,Testability,test,tests,794,"Tests now take ~2m40 on my laptop. With -Dcheck.size=100 -Dcheck.count=1, they take about ~1m20. A huge part of the test time was (re)creating the SparkContext. I disabled testVSMGenIsLinearSpaceInSizeParameter. It doesn't quite make sense to me to be spending half our test time budget verifying ... that our tests aren't taking too long. Happy to discuss it on Monday. Following Jackie's original code, the duplicated logic and defaults for check settings in Prop and gradle are needed to include the check settings (in particular, the seed) in the gradle output when you're not using `--info` or similar. I couldn't figure out another way to do it. Changes:. Set seed with -Dcheck.seed=seed, random with -Dcheck.seed=random.; Added SparkManager to store sc and sqlContext.; Reuse sc between tests.; Added -Dcheck.size=size and -Dcheck.count=count.; Removed Prop.check with count, size options.; Disabled testVSMGenIsLinearSpaceInSizeParameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586
https://github.com/hail-is/hail/pull/586:907,Testability,test,testVSMGenIsLinearSpaceInSizeParameter,907,"Tests now take ~2m40 on my laptop. With -Dcheck.size=100 -Dcheck.count=1, they take about ~1m20. A huge part of the test time was (re)creating the SparkContext. I disabled testVSMGenIsLinearSpaceInSizeParameter. It doesn't quite make sense to me to be spending half our test time budget verifying ... that our tests aren't taking too long. Happy to discuss it on Monday. Following Jackie's original code, the duplicated logic and defaults for check settings in Prop and gradle are needed to include the check settings (in particular, the seed) in the gradle output when you're not using `--info` or similar. I couldn't figure out another way to do it. Changes:. Set seed with -Dcheck.seed=seed, random with -Dcheck.seed=random.; Added SparkManager to store sc and sqlContext.; Reuse sc between tests.; Added -Dcheck.size=size and -Dcheck.count=count.; Removed Prop.check with count, size options.; Disabled testVSMGenIsLinearSpaceInSizeParameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/586
https://github.com/hail-is/hail/pull/591:93,Testability,test,tests,93,- added methods to AST for Array and Set; - added docs in HailExpressionLangauge.md; - added tests to ExprSuite; - addresses Issue #556,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/591
https://github.com/hail-is/hail/pull/593:152,Availability,ping,pinging,152,"Includes a general version of Ward's algorithm, a common hierarchical clustering technique important for implementing the UNICORN model. . Specifically pinging @alexb-3 for code review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/593
https://github.com/hail-is/hail/issues/594:345,Availability,FAILURE,FAILURE,345,"The 'build' docs page implies that the only requirement for running hail is Gradle. However, I've just tried to build hail on Debian Jessie and Ubuntu 16.04, and both failed in different ways. On Jessie, I was able to figure out that the version of Gradle was too old. On Ubuntu 16.04, I get. ```; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; ```. A quick Google around doesn't reveal any obvious answers to this. What version of Gradle is needed? Is Scala a prerequisite? It would be very useful to provide detailed instructions on how to build hail from scratch on a fresh installation of some Linux distribution.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594
https://github.com/hail-is/hail/issues/594:435,Deployability,configurat,configuration,435,"The 'build' docs page implies that the only requirement for running hail is Gradle. However, I've just tried to build hail on Debian Jessie and Ubuntu 16.04, and both failed in different ways. On Jessie, I was able to figure out that the version of Gradle was too old. On Ubuntu 16.04, I get. ```; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; ```. A quick Google around doesn't reveal any obvious answers to this. What version of Gradle is needed? Is Scala a prerequisite? It would be very useful to provide detailed instructions on how to build hail from scratch on a fresh installation of some Linux distribution.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594
https://github.com/hail-is/hail/issues/594:900,Deployability,install,installation,900,"The 'build' docs page implies that the only requirement for running hail is Gradle. However, I've just tried to build hail on Debian Jessie and Ubuntu 16.04, and both failed in different ways. On Jessie, I was able to figure out that the version of Gradle was too old. On Ubuntu 16.04, I get. ```; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; ```. A quick Google around doesn't reveal any obvious answers to this. What version of Gradle is needed? Is Scala a prerequisite? It would be very useful to provide detailed instructions on how to build hail from scratch on a fresh installation of some Linux distribution.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594
https://github.com/hail-is/hail/issues/594:435,Modifiability,config,configuration,435,"The 'build' docs page implies that the only requirement for running hail is Gradle. However, I've just tried to build hail on Debian Jessie and Ubuntu 16.04, and both failed in different ways. On Jessie, I was able to figure out that the version of Gradle was too old. On Ubuntu 16.04, I get. ```; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; ```. A quick Google around doesn't reveal any obvious answers to this. What version of Gradle is needed? Is Scala a prerequisite? It would be very useful to provide detailed instructions on how to build hail from scratch on a fresh installation of some Linux distribution.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594
https://github.com/hail-is/hail/issues/594:642,Testability,log,log,642,"The 'build' docs page implies that the only requirement for running hail is Gradle. However, I've just tried to build hail on Debian Jessie and Ubuntu 16.04, and both failed in different ways. On Jessie, I was able to figure out that the version of Gradle was too old. On Ubuntu 16.04, I get. ```; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; ```. A quick Google around doesn't reveal any obvious answers to this. What version of Gradle is needed? Is Scala a prerequisite? It would be very useful to provide detailed instructions on how to build hail from scratch on a fresh installation of some Linux distribution.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594
https://github.com/hail-is/hail/issues/597:397,Availability,error,errors,397,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597
https://github.com/hail-is/hail/issues/597:410,Availability,error,error,410,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597
https://github.com/hail-is/hail/issues/597:500,Availability,error,errors,500,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597
https://github.com/hail-is/hail/issues/597:557,Availability,error,error,557,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597
https://github.com/hail-is/hail/issues/597:584,Availability,error,errors,584,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597
https://github.com/hail-is/hail/issues/597:823,Availability,error,error,823,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597
https://github.com/hail-is/hail/issues/597:107,Modifiability,inherit,inheritance,107,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/597
https://github.com/hail-is/hail/pull/610:59,Availability,Error,Error,59,isDosage defaults to false if not found in metadata JSON.; Error if wrong type in metadata JSON.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/610
https://github.com/hail-is/hail/pull/612:3635,Integrability,interface,interface,3635,"collection.immutable.MapLike$$anon$2, value: Map(hello -> 5)); - element of array (index: 2); - array (class [Ljava.lang.Object;, size 16); - field (class: scala.collection.mutable.ArrayBuffer, name: array, type: class [Ljava.lang.Object;); - object (class scala.collection.mutable.ArrayBuffer, ArrayBuffer(null, null, Map(hello -> 5))); - field (class: org.broadinstitute.hail.expr.EvalContext, name: a, type: class scala.collection.mutable.ArrayBuffer); - object (class org.broadinstitute.hail.expr.EvalContext, EvalContext(Map(v -> (0,Variant), va -> (1,Locus), global -> (2,Dict[Int]), gs -> (-1,Aggregable)),ArrayBuffer(null, null, Map(hello -> 5)),ArrayBuffer())); - field (class: org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2, name: ec$1, type: class org.broadinstitute.hail.expr.EvalContext); - object (class org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2, <function3>); - field (class: org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1, name: f$1, type: interface scala.Function3); - object (class org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1, <function1>); at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84); at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122); at org.apache.spark.SparkContext.clean(SparkContext.scala:2021); at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:314); at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:313); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/612
https://github.com/hail-is/hail/issues/620:363,Performance,optimiz,optimize,363,"Could there be some functionality to make it easier to run GWAS on a large set of phenotypes at once? For instance, in metabolomics data sets there can be around 10,000 phenotypes (many of which are highly correlated or chemically related) and you'd like to see GWAS results from all of these. Rather than submitting 10,000 separate jobs, could there be a way to optimize this analysis by considering them all together? I'm sure this would be useful in other fields besides metabolomics.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/620
https://github.com/hail-is/hail/issues/621:1989,Integrability,interface,interfaces,1989,elation.scala:404); at scala.Option.orElse(Option.scala:257); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:561); at org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:560); at org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:31); at org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:389); at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:267); at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:132); at org.broadinstitute.hail.driver.Read$.run(Read.scala:29); at org.broadinstitute.hail.driver.Read$.run(Read.scala:6); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:238); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:86); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:111); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:111); at org.broadinstitute.hail.Utils$.time(Utils.scala:1185); at org.broadinstitute.hail.driver.Main$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/621:2068,Integrability,interface,interfaces,2068,.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:561); at org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:560); at org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:31); at org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:389); at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:267); at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:132); at org.broadinstitute.hail.driver.Read$.run(Read.scala:29); at org.broadinstitute.hail.driver.Read$.run(Read.scala:6); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:238); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:86); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:111); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:111); at org.broadinstitute.hail.Utils$.time(Utils.scala:1185); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:110); at org.broadinstitute.hail.driver.Main,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/621:24,Testability,test,test,24,"```; hail importvcf src/test/resources/sample2.vcf filtervariants all write -o /tmp/out.vds read -i /tmp/out.vds count; hail: info: running: importvcf src/test/resources/sample2.vcf; hail: info: running: filtervariants all; hail: info: running: write -o /tmp/out.vds; hail: info: running: read -i /tmp/out.vds; Exception in thread ""main"" java.lang.AssertionError: assertion failed: No predefined schema found, and no Parquet data files or summary files found under file:/tmp/out.vds/rdd.parquet.; at scala.Predef$.assert(Predef.scala:179); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$MetadataCache$$readSchema(ParquetRelation.scala:478); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at scala.Option.orElse(Option.scala:257); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/621:155,Testability,test,test,155,"```; hail importvcf src/test/resources/sample2.vcf filtervariants all write -o /tmp/out.vds read -i /tmp/out.vds count; hail: info: running: importvcf src/test/resources/sample2.vcf; hail: info: running: filtervariants all; hail: info: running: write -o /tmp/out.vds; hail: info: running: read -i /tmp/out.vds; Exception in thread ""main"" java.lang.AssertionError: assertion failed: No predefined schema found, and no Parquet data files or summary files found under file:/tmp/out.vds/rdd.parquet.; at scala.Predef$.assert(Predef.scala:179); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$MetadataCache$$readSchema(ParquetRelation.scala:478); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at scala.Option.orElse(Option.scala:257); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/621:348,Testability,Assert,AssertionError,348,"```; hail importvcf src/test/resources/sample2.vcf filtervariants all write -o /tmp/out.vds read -i /tmp/out.vds count; hail: info: running: importvcf src/test/resources/sample2.vcf; hail: info: running: filtervariants all; hail: info: running: write -o /tmp/out.vds; hail: info: running: read -i /tmp/out.vds; Exception in thread ""main"" java.lang.AssertionError: assertion failed: No predefined schema found, and no Parquet data files or summary files found under file:/tmp/out.vds/rdd.parquet.; at scala.Predef$.assert(Predef.scala:179); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$MetadataCache$$readSchema(ParquetRelation.scala:478); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at scala.Option.orElse(Option.scala:257); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/621:364,Testability,assert,assertion,364,"```; hail importvcf src/test/resources/sample2.vcf filtervariants all write -o /tmp/out.vds read -i /tmp/out.vds count; hail: info: running: importvcf src/test/resources/sample2.vcf; hail: info: running: filtervariants all; hail: info: running: write -o /tmp/out.vds; hail: info: running: read -i /tmp/out.vds; Exception in thread ""main"" java.lang.AssertionError: assertion failed: No predefined schema found, and no Parquet data files or summary files found under file:/tmp/out.vds/rdd.parquet.; at scala.Predef$.assert(Predef.scala:179); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$MetadataCache$$readSchema(ParquetRelation.scala:478); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at scala.Option.orElse(Option.scala:257); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/621:514,Testability,assert,assert,514,"```; hail importvcf src/test/resources/sample2.vcf filtervariants all write -o /tmp/out.vds read -i /tmp/out.vds count; hail: info: running: importvcf src/test/resources/sample2.vcf; hail: info: running: filtervariants all; hail: info: running: write -o /tmp/out.vds; hail: info: running: read -i /tmp/out.vds; Exception in thread ""main"" java.lang.AssertionError: assertion failed: No predefined schema found, and no Parquet data files or summary files found under file:/tmp/out.vds/rdd.parquet.; at scala.Predef$.assert(Predef.scala:179); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$MetadataCache$$readSchema(ParquetRelation.scala:478); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404); at scala.Option.orElse(Option.scala:257); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/621:2137,Testability,Log,LogicalRelation,2137,tRelation.scala:404); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:561); at org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:560); at org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:31); at org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:389); at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:267); at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:132); at org.broadinstitute.hail.driver.Read$.run(Read.scala:29); at org.broadinstitute.hail.driver.Read$.run(Read.scala:6); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:238); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:86); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:111); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:111); at org.broadinstitute.hail.Utils$.time(Utils.scala:1185); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:110); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:104); at scala.collection.IndexedS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/621:2160,Testability,Log,LogicalRelation,2160,); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196); at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:561); at org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:560); at org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:31); at org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:389); at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:267); at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:132); at org.broadinstitute.hail.driver.Read$.run(Read.scala:29); at org.broadinstitute.hail.driver.Read$.run(Read.scala:6); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:238); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:86); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:111); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:111); at org.broadinstitute.hail.Utils$.time(Utils.scala:1185); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:110); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:104); at scala.collection.IndexedSeqOptimized$class.fo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/621
https://github.com/hail-is/hail/issues/624:593,Availability,down,downstream,593,"Interface change:. ``` scala; abstract class Type[T] extends BaseType {; def coerce(a: Any): T; // ...; }; ```. Note the two major changes:; - Every `Type` now must correspond to a Scala type; - Every `Type` must know how to convert appropriate values to their associated Scala type. We may then naturally modify methods like `evalCompose`:. ``` scala; def evalCompose[T](ec: EvalContext, typ: Type[T])(subexpr: AST); (g: (T) => Any): () => Any = {; val f = subexpr.eval(ec); () => {; val x = f(); if (x != null); g(typ.coerce(x)); else; null; }; }; ```. which will hopefully induce or enable downstream simplifications.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/624
https://github.com/hail-is/hail/issues/624:0,Integrability,Interface,Interface,0,"Interface change:. ``` scala; abstract class Type[T] extends BaseType {; def coerce(a: Any): T; // ...; }; ```. Note the two major changes:; - Every `Type` now must correspond to a Scala type; - Every `Type` must know how to convert appropriate values to their associated Scala type. We may then naturally modify methods like `evalCompose`:. ``` scala; def evalCompose[T](ec: EvalContext, typ: Type[T])(subexpr: AST); (g: (T) => Any): () => Any = {; val f = subexpr.eval(ec); () => {; val x = f(); if (x != null); g(typ.coerce(x)); else; null; }; }; ```. which will hopefully induce or enable downstream simplifications.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/624
https://github.com/hail-is/hail/issues/624:53,Modifiability,extend,extends,53,"Interface change:. ``` scala; abstract class Type[T] extends BaseType {; def coerce(a: Any): T; // ...; }; ```. Note the two major changes:; - Every `Type` now must correspond to a Scala type; - Every `Type` must know how to convert appropriate values to their associated Scala type. We may then naturally modify methods like `evalCompose`:. ``` scala; def evalCompose[T](ec: EvalContext, typ: Type[T])(subexpr: AST); (g: (T) => Any): () => Any = {; val f = subexpr.eval(ec); () => {; val x = f(); if (x != null); g(typ.coerce(x)); else; null; }; }; ```. which will hopefully induce or enable downstream simplifications.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/624
https://github.com/hail-is/hail/issues/624:604,Usability,simpl,simplifications,604,"Interface change:. ``` scala; abstract class Type[T] extends BaseType {; def coerce(a: Any): T; // ...; }; ```. Note the two major changes:; - Every `Type` now must correspond to a Scala type; - Every `Type` must know how to convert appropriate values to their associated Scala type. We may then naturally modify methods like `evalCompose`:. ``` scala; def evalCompose[T](ec: EvalContext, typ: Type[T])(subexpr: AST); (g: (T) => Any): () => Any = {; val f = subexpr.eval(ec); () => {; val x = f(); if (x != null); g(typ.coerce(x)); else; null; }; }; ```. which will hopefully induce or enable downstream simplifications.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/624
https://github.com/hail-is/hail/pull/630:439,Availability,down,downloads,439,"For #604: I changed the max-width to 80em from 45em. If this is not wide enough, then we should probably remove the max-width property. For #605: It was extremely difficult to replicate the issue, but I believe it's because the mathjax and jquery operations are running asynchronously and the mathjax finishes before the jquery code has finished populating the DOM. I added a ""defer"" attribute to the mathjax script loading, so the script downloads in the background, but doesn't get executed until the DOM has been populated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630
https://github.com/hail-is/hail/pull/630:416,Performance,load,loading,416,"For #604: I changed the max-width to 80em from 45em. If this is not wide enough, then we should probably remove the max-width property. For #605: It was extremely difficult to replicate the issue, but I believe it's because the mathjax and jquery operations are running asynchronously and the mathjax finishes before the jquery code has finished populating the DOM. I added a ""defer"" attribute to the mathjax script loading, so the script downloads in the background, but doesn't get executed until the DOM has been populated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/630
https://github.com/hail-is/hail/issues/631:163,Energy Efficiency,allocate,allocates,163,"Currently ExportVCF calls Genotype.toString to format the per-sample genotype fields. ExportVCF uses a StringBuilder to minimize allocation, but Genotype.toString allocates a StringBuilder and the result. Datasets can have 100s of millions of variants and hundreds of thousands of samples. That's 10s of trillions of extra allocations. ExportVCF should have its own routine that formats the Genotype according to the VCF spec directly into its StringBuilder. Genotype.toString shouldn't be required to conform to the VCF spec. In particular, it currently doesn't indicate Phred-scaled likelihoods vs dosage (which in the VCF are indicated in the format field are indicated by PL vs GP) nor the fakeRef (which isn't indicated in the VCF at all). Maybe something like `0/1*:10,10,0:22:99:PL=100,0,100` where `*` indicates fakeRef?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/631
https://github.com/hail-is/hail/issues/631:366,Integrability,rout,routine,366,"Currently ExportVCF calls Genotype.toString to format the per-sample genotype fields. ExportVCF uses a StringBuilder to minimize allocation, but Genotype.toString allocates a StringBuilder and the result. Datasets can have 100s of millions of variants and hundreds of thousands of samples. That's 10s of trillions of extra allocations. ExportVCF should have its own routine that formats the Genotype according to the VCF spec directly into its StringBuilder. Genotype.toString shouldn't be required to conform to the VCF spec. In particular, it currently doesn't indicate Phred-scaled likelihoods vs dosage (which in the VCF are indicated in the format field are indicated by PL vs GP) nor the fakeRef (which isn't indicated in the VCF at all). Maybe something like `0/1*:10,10,0:22:99:PL=100,0,100` where `*` indicates fakeRef?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/631
https://github.com/hail-is/hail/pull/633:32,Integrability,wrap,wrapping,32,Pandoc normally tries to manage wrapping in tables by setting column widths with a colgroup and width attributes. This isn't necessary for HTML. This change suppresses the colgroups. Got the idea from here:. https://github.com/jgm/pandoc/issues/2574,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/633
https://github.com/hail-is/hail/issues/635:238,Availability,down,down,238,"Each webpage should have a single `h1` matching the `title` text. Individual command documentation begins with `#`/`h1`. The single doc page has a `title`/`h1` Hail Documentation, so headings in individual command docs should get shifted down a level, `hi => h(i+1)`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/635
https://github.com/hail-is/hail/pull/644:214,Energy Efficiency,reduce,reduced,214,1. `inline-block` on `code` tags fixes code tags inside; pre blocks having a little extra pading on the first line; by applying the padding to each line; 2. `font-size` is applied once using the `code` tag and is; reduced to 80%; 3. Some combination of the above broke the indentation hack; for synposes; setting the `text-index` to -25 seems to restore; the previous behavior.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/644
https://github.com/hail-is/hail/pull/649:46,Testability,test,test,46,Actually call `check` on `Prop`s. Also adds a test; verifying equality of `gtPair` and `gtPairSqrt`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/649
https://github.com/hail-is/hail/issues/655:16,Safety,avoid,avoid,16,AKA: How can we avoid using Jenkins?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/655
https://github.com/hail-is/hail/pull/656:35,Testability,test,test,35,Do not merge this. Should fail one test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/656
https://github.com/hail-is/hail/issues/660:54,Availability,error,error,54,"While I was running the script shown below. I got the error message. I attach the log file. Do you know what cause the errors ? Thanks . ```; hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/*.bgz splitmulti \; filtervariants expr -c 'v.contig == ""X"" || v.contig == ""Y"" || v.contig == ""MT""' --remove \; filtersamples list -i file:///medpop/afib/schoi/projects/TOPMed/Result/TopMed_nodup.6998.sample.map --keep \; sampleqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/SampleQC/chrom/TOPMed.PreQC.sampleqc.chr22.tsv \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.PreQC.variantqc.chr22.tsv \; annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' \; filtergenotypes -c '(g.ad[0] + g.ad[1]) / g.dp < 0.9 || (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) || (g.isHet && (g.ad[1] / g.dp < 0.20 || g.pl[0] < 20)) || (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) || g.dp > 200' --remove \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep1.variantqc.Chr22.tsv \; annotatevariants intervals -r va.isLCF -i file:///medpop/afib/schoi/projects/TOPMed/Data/LCR/LCR.interval_list \; annotatevariants expr -c 'va.AC1 = va.qc.AC,; va.good = ((va.info.QD > 3 && v.altAllele.isIndel) || (va.info.QD > 2 && v.altAllele.isSNP)) && (va.qc.callRate > 0.95)' \; annotateglobal expr -c 'global.badVQSLOD = variants.count(va.pass),; global.badQD = variants.count((va.info.QD <= 3 && v.altAllele.isIndel) || (va.info.QD <= 2 && v.altAllele.isSNP)),; global.badCallRate = variants.count(va.qc.callRate <= 0.95),; global.nIndel_1 = variants.count(v.altAllele.isIndel && va.AC1 > 0),; global.nSNP_1 = variants.count(v.altAllele.isSNP && va.AC1 > 0),; global.nIndel_2 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass),; global.nSNP_2 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass),; gl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660
https://github.com/hail-is/hail/issues/660:119,Availability,error,errors,119,"While I was running the script shown below. I got the error message. I attach the log file. Do you know what cause the errors ? Thanks . ```; hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/*.bgz splitmulti \; filtervariants expr -c 'v.contig == ""X"" || v.contig == ""Y"" || v.contig == ""MT""' --remove \; filtersamples list -i file:///medpop/afib/schoi/projects/TOPMed/Result/TopMed_nodup.6998.sample.map --keep \; sampleqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/SampleQC/chrom/TOPMed.PreQC.sampleqc.chr22.tsv \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.PreQC.variantqc.chr22.tsv \; annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' \; filtergenotypes -c '(g.ad[0] + g.ad[1]) / g.dp < 0.9 || (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) || (g.isHet && (g.ad[1] / g.dp < 0.20 || g.pl[0] < 20)) || (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) || g.dp > 200' --remove \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep1.variantqc.Chr22.tsv \; annotatevariants intervals -r va.isLCF -i file:///medpop/afib/schoi/projects/TOPMed/Data/LCR/LCR.interval_list \; annotatevariants expr -c 'va.AC1 = va.qc.AC,; va.good = ((va.info.QD > 3 && v.altAllele.isIndel) || (va.info.QD > 2 && v.altAllele.isSNP)) && (va.qc.callRate > 0.95)' \; annotateglobal expr -c 'global.badVQSLOD = variants.count(va.pass),; global.badQD = variants.count((va.info.QD <= 3 && v.altAllele.isIndel) || (va.info.QD <= 2 && v.altAllele.isSNP)),; global.badCallRate = variants.count(va.qc.callRate <= 0.95),; global.nIndel_1 = variants.count(v.altAllele.isIndel && va.AC1 > 0),; global.nSNP_1 = variants.count(v.altAllele.isSNP && va.AC1 > 0),; global.nIndel_2 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass),; global.nSNP_2 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass),; gl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660
https://github.com/hail-is/hail/issues/660:60,Integrability,message,message,60,"While I was running the script shown below. I got the error message. I attach the log file. Do you know what cause the errors ? Thanks . ```; hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/*.bgz splitmulti \; filtervariants expr -c 'v.contig == ""X"" || v.contig == ""Y"" || v.contig == ""MT""' --remove \; filtersamples list -i file:///medpop/afib/schoi/projects/TOPMed/Result/TopMed_nodup.6998.sample.map --keep \; sampleqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/SampleQC/chrom/TOPMed.PreQC.sampleqc.chr22.tsv \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.PreQC.variantqc.chr22.tsv \; annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' \; filtergenotypes -c '(g.ad[0] + g.ad[1]) / g.dp < 0.9 || (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) || (g.isHet && (g.ad[1] / g.dp < 0.20 || g.pl[0] < 20)) || (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) || g.dp > 200' --remove \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep1.variantqc.Chr22.tsv \; annotatevariants intervals -r va.isLCF -i file:///medpop/afib/schoi/projects/TOPMed/Data/LCR/LCR.interval_list \; annotatevariants expr -c 'va.AC1 = va.qc.AC,; va.good = ((va.info.QD > 3 && v.altAllele.isIndel) || (va.info.QD > 2 && v.altAllele.isSNP)) && (va.qc.callRate > 0.95)' \; annotateglobal expr -c 'global.badVQSLOD = variants.count(va.pass),; global.badQD = variants.count((va.info.QD <= 3 && v.altAllele.isIndel) || (va.info.QD <= 2 && v.altAllele.isSNP)),; global.badCallRate = variants.count(va.qc.callRate <= 0.95),; global.nIndel_1 = variants.count(v.altAllele.isIndel && va.AC1 > 0),; global.nSNP_1 = variants.count(v.altAllele.isSNP && va.AC1 > 0),; global.nIndel_2 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass),; global.nSNP_2 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass),; gl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660
https://github.com/hail-is/hail/issues/660:82,Testability,log,log,82,"While I was running the script shown below. I got the error message. I attach the log file. Do you know what cause the errors ? Thanks . ```; hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/*.bgz splitmulti \; filtervariants expr -c 'v.contig == ""X"" || v.contig == ""Y"" || v.contig == ""MT""' --remove \; filtersamples list -i file:///medpop/afib/schoi/projects/TOPMed/Result/TopMed_nodup.6998.sample.map --keep \; sampleqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/SampleQC/chrom/TOPMed.PreQC.sampleqc.chr22.tsv \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.PreQC.variantqc.chr22.tsv \; annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' \; filtergenotypes -c '(g.ad[0] + g.ad[1]) / g.dp < 0.9 || (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) || (g.isHet && (g.ad[1] / g.dp < 0.20 || g.pl[0] < 20)) || (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) || g.dp > 200' --remove \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep1.variantqc.Chr22.tsv \; annotatevariants intervals -r va.isLCF -i file:///medpop/afib/schoi/projects/TOPMed/Data/LCR/LCR.interval_list \; annotatevariants expr -c 'va.AC1 = va.qc.AC,; va.good = ((va.info.QD > 3 && v.altAllele.isIndel) || (va.info.QD > 2 && v.altAllele.isSNP)) && (va.qc.callRate > 0.95)' \; annotateglobal expr -c 'global.badVQSLOD = variants.count(va.pass),; global.badQD = variants.count((va.info.QD <= 3 && v.altAllele.isIndel) || (va.info.QD <= 2 && v.altAllele.isSNP)),; global.badCallRate = variants.count(va.qc.callRate <= 0.95),; global.nIndel_1 = variants.count(v.altAllele.isIndel && va.AC1 > 0),; global.nSNP_1 = variants.count(v.altAllele.isSNP && va.AC1 > 0),; global.nIndel_2 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass),; global.nSNP_2 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass),; gl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660
https://github.com/hail-is/hail/issues/660:192,Testability,log,log,192,"While I was running the script shown below. I got the error message. I attach the log file. Do you know what cause the errors ? Thanks . ```; hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/*.bgz splitmulti \; filtervariants expr -c 'v.contig == ""X"" || v.contig == ""Y"" || v.contig == ""MT""' --remove \; filtersamples list -i file:///medpop/afib/schoi/projects/TOPMed/Result/TopMed_nodup.6998.sample.map --keep \; sampleqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/SampleQC/chrom/TOPMed.PreQC.sampleqc.chr22.tsv \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.PreQC.variantqc.chr22.tsv \; annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' \; filtergenotypes -c '(g.ad[0] + g.ad[1]) / g.dp < 0.9 || (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) || (g.isHet && (g.ad[1] / g.dp < 0.20 || g.pl[0] < 20)) || (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) || g.dp > 200' --remove \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep1.variantqc.Chr22.tsv \; annotatevariants intervals -r va.isLCF -i file:///medpop/afib/schoi/projects/TOPMed/Data/LCR/LCR.interval_list \; annotatevariants expr -c 'va.AC1 = va.qc.AC,; va.good = ((va.info.QD > 3 && v.altAllele.isIndel) || (va.info.QD > 2 && v.altAllele.isSNP)) && (va.qc.callRate > 0.95)' \; annotateglobal expr -c 'global.badVQSLOD = variants.count(va.pass),; global.badQD = variants.count((va.info.QD <= 3 && v.altAllele.isIndel) || (va.info.QD <= 2 && v.altAllele.isSNP)),; global.badCallRate = variants.count(va.qc.callRate <= 0.95),; global.nIndel_1 = variants.count(v.altAllele.isIndel && va.AC1 > 0),; global.nSNP_1 = variants.count(v.altAllele.isSNP && va.AC1 > 0),; global.nIndel_2 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass),; global.nSNP_2 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass),; gl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660
https://github.com/hail-is/hail/issues/660:216,Testability,log,log,216,"While I was running the script shown below. I got the error message. I attach the log file. Do you know what cause the errors ? Thanks . ```; hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/*.bgz splitmulti \; filtervariants expr -c 'v.contig == ""X"" || v.contig == ""Y"" || v.contig == ""MT""' --remove \; filtersamples list -i file:///medpop/afib/schoi/projects/TOPMed/Result/TopMed_nodup.6998.sample.map --keep \; sampleqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/SampleQC/chrom/TOPMed.PreQC.sampleqc.chr22.tsv \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.PreQC.variantqc.chr22.tsv \; annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' \; filtergenotypes -c '(g.ad[0] + g.ad[1]) / g.dp < 0.9 || (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) || (g.isHet && (g.ad[1] / g.dp < 0.20 || g.pl[0] < 20)) || (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) || g.dp > 200' --remove \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep1.variantqc.Chr22.tsv \; annotatevariants intervals -r va.isLCF -i file:///medpop/afib/schoi/projects/TOPMed/Data/LCR/LCR.interval_list \; annotatevariants expr -c 'va.AC1 = va.qc.AC,; va.good = ((va.info.QD > 3 && v.altAllele.isIndel) || (va.info.QD > 2 && v.altAllele.isSNP)) && (va.qc.callRate > 0.95)' \; annotateglobal expr -c 'global.badVQSLOD = variants.count(va.pass),; global.badQD = variants.count((va.info.QD <= 3 && v.altAllele.isIndel) || (va.info.QD <= 2 && v.altAllele.isSNP)),; global.badCallRate = variants.count(va.qc.callRate <= 0.95),; global.nIndel_1 = variants.count(v.altAllele.isIndel && va.AC1 > 0),; global.nSNP_1 = variants.count(v.altAllele.isSNP && va.AC1 > 0),; global.nIndel_2 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass),; global.nSNP_2 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass),; gl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660
https://github.com/hail-is/hail/issues/660:3454,Testability,log,log,3454,"ariants.count((va.info.QD <= 3 && v.altAllele.isIndel) || (va.info.QD <= 2 && v.altAllele.isSNP)),; global.badCallRate = variants.count(va.qc.callRate <= 0.95),; global.nIndel_1 = variants.count(v.altAllele.isIndel && va.AC1 > 0),; global.nSNP_1 = variants.count(v.altAllele.isSNP && va.AC1 > 0),; global.nIndel_2 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass),; global.nSNP_2 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass),; global.nIndel_3 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass && !va.isLCF),; global.nSNP_3 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass && !va.isLCF),; global.nIndel_4 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass && !va.isLCF && va.qc.callRate > 0.80),; global.nSNP_4 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass && !va.isLCF && va.qc.callRate > 0.80),; global.nIndel_5 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass && !va.isLCF && va.good),; global.nSNP_5 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass && !va.isLCF && va.good)' \; showglobals -o file:///medpop/afib/schoi/projects/TOPMed/Result/Global/chrom/TOPMed_showglobalsQC.chr22.txt \; filtervariants expr -c 'va.AC1 > 0 && va.pass && !va.isLCF && va.good' --keep \; sampleqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/SampleQC/chrom/TOPMed.QCstep2.sampleqc.chr22.tsv \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep2.variantqc.chr22.tsv \; exportvariants -c 'Chrom=v.contig,Pos=v.start,Ref=v.ref,Alt=v.alt, rsID = va.rsid,PASS = va.pass, MISSINGNESS = 1 - va.qc.callRate,QD = va.info.QD, InbreedingCoeff = va.info.InbreedingCoeff, VQSLOD = va.info.VQSLOD,MAC=va.qc.AC,MAF=va.qc.AF' -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep2.variantqcv2.chr22.tsv \; write -o TOPMed.6998.chr22.vds; ```. [TopMed.Chr22.QC.vds.log.txt](https://github.com/broadinstitute/hail/files/432676/TopMed.Chr22.QC.vds.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660
https://github.com/hail-is/hail/issues/660:3535,Testability,log,log,3535,"ariants.count((va.info.QD <= 3 && v.altAllele.isIndel) || (va.info.QD <= 2 && v.altAllele.isSNP)),; global.badCallRate = variants.count(va.qc.callRate <= 0.95),; global.nIndel_1 = variants.count(v.altAllele.isIndel && va.AC1 > 0),; global.nSNP_1 = variants.count(v.altAllele.isSNP && va.AC1 > 0),; global.nIndel_2 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass),; global.nSNP_2 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass),; global.nIndel_3 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass && !va.isLCF),; global.nSNP_3 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass && !va.isLCF),; global.nIndel_4 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass && !va.isLCF && va.qc.callRate > 0.80),; global.nSNP_4 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass && !va.isLCF && va.qc.callRate > 0.80),; global.nIndel_5 = variants.count(v.altAllele.isIndel && va.AC1 > 0 && va.pass && !va.isLCF && va.good),; global.nSNP_5 = variants.count(v.altAllele.isSNP && va.AC1 > 0 && va.pass && !va.isLCF && va.good)' \; showglobals -o file:///medpop/afib/schoi/projects/TOPMed/Result/Global/chrom/TOPMed_showglobalsQC.chr22.txt \; filtervariants expr -c 'va.AC1 > 0 && va.pass && !va.isLCF && va.good' --keep \; sampleqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/SampleQC/chrom/TOPMed.QCstep2.sampleqc.chr22.tsv \; variantqc -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep2.variantqc.chr22.tsv \; exportvariants -c 'Chrom=v.contig,Pos=v.start,Ref=v.ref,Alt=v.alt, rsID = va.rsid,PASS = va.pass, MISSINGNESS = 1 - va.qc.callRate,QD = va.info.QD, InbreedingCoeff = va.info.InbreedingCoeff, VQSLOD = va.info.VQSLOD,MAC=va.qc.AC,MAF=va.qc.AF' -o file:///medpop/afib/schoi/projects/TOPMed/Result/VariantQC/chrom/TOPMed.QCstep2.variantqcv2.chr22.tsv \; write -o TOPMed.6998.chr22.vds; ```. [TopMed.Chr22.QC.vds.log.txt](https://github.com/broadinstitute/hail/files/432676/TopMed.Chr22.QC.vds.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/660
https://github.com/hail-is/hail/issues/669:10,Availability,error,error,10,I got the error message while importing VCFs in dataflow01. `hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.test.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.fold,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:2690,Deployability,deploy,deploy,2690,.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:2727,Deployability,deploy,deploy,2727,.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:2799,Deployability,deploy,deploy,2799,.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:2875,Deployability,deploy,deploy,2875,.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:2946,Deployability,deploy,deploy,2946,.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:3015,Deployability,deploy,deploy,3015,.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:16,Integrability,message,message,16,I got the error message while importing VCFs in dataflow01. `hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.test.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.fold,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:1215,Performance,Load,LoadVCF,1215,2/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:1230,Performance,Load,LoadVCF,1230,853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:111,Testability,log,log,111,I got the error message while importing VCFs in dataflow01. `hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.test.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.fold,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:135,Testability,test,test,135,I got the error message while importing VCFs in dataflow01. `hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.test.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.fold,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/669:140,Testability,log,log,140,I got the error message while importing VCFs in dataflow01. `hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.test.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.fold,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/669
https://github.com/hail-is/hail/issues/673:2694,Deployability,deploy,deploy,2694,d.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:2731,Deployability,deploy,deploy,2731,d.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:2803,Deployability,deploy,deploy,2803,d.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:2879,Deployability,deploy,deploy,2879,d.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:2950,Deployability,deploy,deploy,2950,d.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:3019,Deployability,deploy,deploy,3019,d.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:1219,Performance,Load,LoadVCF,1219,==============================================>(1049 + 1) / 1050]hail: info: Ordering unsorted dataset with network shuffle[A^[[A; [Stage 1:====================================================>(1043 + 7) / 1050]hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:1234,Performance,Load,LoadVCF,1234,==================================>(1049 + 1) / 1050]hail: info: Ordering unsorted dataset with network shuffle[A^[[A; [Stage 1:====================================================>(1043 + 7) / 1050]hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:60,Testability,Log,Log,60,"Been having problems running hail on dataflow, the latest:. Log file: /humgen/atgu1/fs03/jkoskela/hail.log. hail: info: running: importvcf /user/jkoskela/ibd/vcf/99percent_finns_plus_AD_IBD_NFID_ALL.vcf.bgz; [Stage 0:====================================================>(1049 + 1) / 1050]hail: info: Ordering unsorted dataset with network shuffle[A^[[A; [Stage 1:====================================================>(1043 + 7) / 1050]hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/673:103,Testability,log,log,103,"Been having problems running hail on dataflow, the latest:. Log file: /humgen/atgu1/fs03/jkoskela/hail.log. hail: info: running: importvcf /user/jkoskela/ibd/vcf/99percent_finns_plus_AD_IBD_NFID_ALL.vcf.bgz; [Stage 0:====================================================>(1049 + 1) / 1050]hail: info: Ordering unsorted dataset with network shuffle[A^[[A; [Stage 1:====================================================>(1043 + 7) / 1050]hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/673
https://github.com/hail-is/hail/issues/675:30,Security,access,access,30,"In addition to setting up SSL access to TeamCity (see #674), we should harden the TeamCity instance against attacks:; 1. change the user running and owning TeamCity to a new user called `teamcity`; 2. review the [Security Notes](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-TeamCitySecurityNotes) to ensure we don't have any other remaining security holes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/675
https://github.com/hail-is/hail/issues/675:108,Security,attack,attacks,108,"In addition to setting up SSL access to TeamCity (see #674), we should harden the TeamCity instance against attacks:; 1. change the user running and owning TeamCity to a new user called `teamcity`; 2. review the [Security Notes](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-TeamCitySecurityNotes) to ensure we don't have any other remaining security holes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/675
https://github.com/hail-is/hail/issues/675:213,Security,Secur,Security,213,"In addition to setting up SSL access to TeamCity (see #674), we should harden the TeamCity instance against attacks:; 1. change the user running and owning TeamCity to a new user called `teamcity`; 2. review the [Security Notes](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-TeamCitySecurityNotes) to ensure we don't have any other remaining security holes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/675
https://github.com/hail-is/hail/issues/675:376,Security,secur,security,376,"In addition to setting up SSL access to TeamCity (see #674), we should harden the TeamCity instance against attacks:; 1. change the user running and owning TeamCity to a new user called `teamcity`; 2. review the [Security Notes](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-TeamCitySecurityNotes) to ensure we don't have any other remaining security holes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/675
https://github.com/hail-is/hail/issues/683:82,Availability,error,errors,82,"Hi, I am a staff in DCH. Now, we are testing the hail software and meet some test errors .; 1. How to check the results of some commands? such as, annotatesamples. For example, if you run this command:; hail importvcf sample.vcf annotatesamples expr -c 'sa.nHet = gs.count(g.isHet)‘ exportsamples –c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnota",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683
https://github.com/hail-is/hail/issues/683:798,Availability,error,errors,798,"Hi, I am a staff in DCH. Now, we are testing the hail software and meet some test errors .; 1. How to check the results of some commands? such as, annotatesamples. For example, if you run this command:; hail importvcf sample.vcf annotatesamples expr -c 'sa.nHet = gs.count(g.isHet)‘ exportsamples –c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnota",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683
https://github.com/hail-is/hail/issues/683:1338,Availability,error,error,1338,"–c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; 2. PCA error; If you run the command:; hail read -i tmp.vds pca –o pca_tmp.tsv ; you will get the error information: hail: pca: caught exception: requirement failed: Requested k singular values but got k=10 and numCols=0. Why?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683
https://github.com/hail-is/hail/issues/683:2078,Availability,error,error,2078,"–c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; 2. PCA error; If you run the command:; hail read -i tmp.vds pca –o pca_tmp.tsv ; you will get the error information: hail: pca: caught exception: requirement failed: Requested k singular values but got k=10 and numCols=0. Why?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683
https://github.com/hail-is/hail/issues/683:2169,Availability,error,error,2169,"–c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; 2. PCA error; If you run the command:; hail read -i tmp.vds pca –o pca_tmp.tsv ; you will get the error information: hail: pca: caught exception: requirement failed: Requested k singular values but got k=10 and numCols=0. Why?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683
https://github.com/hail-is/hail/issues/683:37,Testability,test,testing,37,"Hi, I am a staff in DCH. Now, we are testing the hail software and meet some test errors .; 1. How to check the results of some commands? such as, annotatesamples. For example, if you run this command:; hail importvcf sample.vcf annotatesamples expr -c 'sa.nHet = gs.count(g.isHet)‘ exportsamples –c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnota",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683
https://github.com/hail-is/hail/issues/683:77,Testability,test,test,77,"Hi, I am a staff in DCH. Now, we are testing the hail software and meet some test errors .; 1. How to check the results of some commands? such as, annotatesamples. For example, if you run this command:; hail importvcf sample.vcf annotatesamples expr -c 'sa.nHet = gs.count(g.isHet)‘ exportsamples –c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnota",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683
https://github.com/hail-is/hail/issues/683:769,Testability,test,test,769,"Hi, I am a staff in DCH. Now, we are testing the hail software and meet some test errors .; 1. How to check the results of some commands? such as, annotatesamples. For example, if you run this command:; hail importvcf sample.vcf annotatesamples expr -c 'sa.nHet = gs.count(g.isHet)‘ exportsamples –c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnota",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/683
https://github.com/hail-is/hail/pull/686:57,Integrability,depend,dependencies,57,"Moved main code from VSM.coalesce to OrderedRDD.; Handle dependencies and preferred locations.; In non-shuffle case, interpret requested partitions as maximum.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/686
https://github.com/hail-is/hail/pull/690:76,Deployability,update,update,76,I kept things simple. Next round we should add a richer example. Note we'll update 1.5.2 to 1.6.2 once fix is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/690
https://github.com/hail-is/hail/pull/690:14,Usability,simpl,simple,14,I kept things simple. Next round we should add a richer example. Note we'll update 1.5.2 to 1.6.2 once fix is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/690
https://github.com/hail-is/hail/pull/693:0,Modifiability,Refactor,Refactored,0,"Refactored VariantRecord to RecordDecoder, made genotype decoding; lazy. This allows us to get out fastKeys and the genotypes with the; same abstraction. Refactored multiple BGEN file handling to BgenLoader, where it should be. Added assertions that we silently relied on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/693
https://github.com/hail-is/hail/pull/693:154,Modifiability,Refactor,Refactored,154,"Refactored VariantRecord to RecordDecoder, made genotype decoding; lazy. This allows us to get out fastKeys and the genotypes with the; same abstraction. Refactored multiple BGEN file handling to BgenLoader, where it should be. Added assertions that we silently relied on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/693
https://github.com/hail-is/hail/pull/693:234,Testability,assert,assertions,234,"Refactored VariantRecord to RecordDecoder, made genotype decoding; lazy. This allows us to get out fastKeys and the genotypes with the; same abstraction. Refactored multiple BGEN file handling to BgenLoader, where it should be. Added assertions that we silently relied on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/693
https://github.com/hail-is/hail/issues/695:4,Availability,avail,available,4,Now available in Dataproc 1.1: https://cloud.google.com/dataproc/docs/concepts/dataproc-versions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/695
https://github.com/hail-is/hail/pull/696:163,Integrability,depend,depends,163,This function permits developers to preserve ordering; when flatMaping a function which produces elements; montonically related to the source element. This change depends on PR #706. Merging that commit will simplify this diff.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/696
https://github.com/hail-is/hail/pull/696:208,Usability,simpl,simplify,208,This function permits developers to preserve ordering; when flatMaping a function which produces elements; montonically related to the source element. This change depends on PR #706. Merging that commit will simplify this diff.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/696
https://github.com/hail-is/hail/pull/697:0,Integrability,Depend,Depends,0,Depends on pull request #696. Merge that first to get a simpler diff.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/697
https://github.com/hail-is/hail/pull/697:56,Usability,simpl,simpler,56,Depends on pull request #696. Merge that first to get a simpler diff.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/697
https://github.com/hail-is/hail/issues/700:204,Availability,error,error,204,"So this evening I noticed that one of my vds files (written four days ago) makes Hail crash when I try to read it. When I do:; hail read -i /user/satterst/DBS_v3/DBS_v3_split_vep.vds. I get the following error message: ; hail: read: caught exception: java.lang.IllegalArgumentException: requirement failed; and then a big stack trace, captured here:. /mnt/lustre/satterst/hail.crash.log. I'd be interested to know what's up. One line in the log says:; 2016-08-27 20:16:41 WARN AbstractLifeCycle:204 - FAILED SelectChannelConnector@0.0.0.0:54054: java.net.BindException: Address already in use; but I don't know what this means or if it's relevant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/700
https://github.com/hail-is/hail/issues/700:210,Integrability,message,message,210,"So this evening I noticed that one of my vds files (written four days ago) makes Hail crash when I try to read it. When I do:; hail read -i /user/satterst/DBS_v3/DBS_v3_split_vep.vds. I get the following error message: ; hail: read: caught exception: java.lang.IllegalArgumentException: requirement failed; and then a big stack trace, captured here:. /mnt/lustre/satterst/hail.crash.log. I'd be interested to know what's up. One line in the log says:; 2016-08-27 20:16:41 WARN AbstractLifeCycle:204 - FAILED SelectChannelConnector@0.0.0.0:54054: java.net.BindException: Address already in use; but I don't know what this means or if it's relevant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/700
https://github.com/hail-is/hail/issues/700:383,Testability,log,log,383,"So this evening I noticed that one of my vds files (written four days ago) makes Hail crash when I try to read it. When I do:; hail read -i /user/satterst/DBS_v3/DBS_v3_split_vep.vds. I get the following error message: ; hail: read: caught exception: java.lang.IllegalArgumentException: requirement failed; and then a big stack trace, captured here:. /mnt/lustre/satterst/hail.crash.log. I'd be interested to know what's up. One line in the log says:; 2016-08-27 20:16:41 WARN AbstractLifeCycle:204 - FAILED SelectChannelConnector@0.0.0.0:54054: java.net.BindException: Address already in use; but I don't know what this means or if it's relevant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/700
https://github.com/hail-is/hail/issues/700:441,Testability,log,log,441,"So this evening I noticed that one of my vds files (written four days ago) makes Hail crash when I try to read it. When I do:; hail read -i /user/satterst/DBS_v3/DBS_v3_split_vep.vds. I get the following error message: ; hail: read: caught exception: java.lang.IllegalArgumentException: requirement failed; and then a big stack trace, captured here:. /mnt/lustre/satterst/hail.crash.log. I'd be interested to know what's up. One line in the log says:; 2016-08-27 20:16:41 WARN AbstractLifeCycle:204 - FAILED SelectChannelConnector@0.0.0.0:54054: java.net.BindException: Address already in use; but I don't know what this means or if it's relevant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/700
https://github.com/hail-is/hail/issues/709:110,Availability,error,error,110,"When loading a plink binary file, if the delimiter is incorrect it currently fails with the following cryptic error:. caught scala.MatchError: [Ljava.lang.String;@459f703f (of class [Ljava.lang.String;). This was a file with spaces (where the default is tab). As space seems to be the default for plink2, which was used to make these files, it may be worth allowing either by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/709
https://github.com/hail-is/hail/issues/709:5,Performance,load,loading,5,"When loading a plink binary file, if the delimiter is incorrect it currently fails with the following cryptic error:. caught scala.MatchError: [Ljava.lang.String;@459f703f (of class [Ljava.lang.String;). This was a file with spaces (where the default is tab). As space seems to be the default for plink2, which was used to make these files, it may be worth allowing either by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/709
https://github.com/hail-is/hail/issues/714:281,Availability,toler,tolerance,281,"We tried loading a large bgen file (chr21 - 950k Variants, 150k samples) from UK Biobank. It seemed to proceed ok and generate a VDS file that was loadable, but upon looking at variantqc, it appears that at the large majority of sites, all homref calls are missing. There were no 'tolerance too low' messages while loading, and this property seems to be retained across several runs with difference tolerances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/714
https://github.com/hail-is/hail/issues/714:399,Availability,toler,tolerances,399,"We tried loading a large bgen file (chr21 - 950k Variants, 150k samples) from UK Biobank. It seemed to proceed ok and generate a VDS file that was loadable, but upon looking at variantqc, it appears that at the large majority of sites, all homref calls are missing. There were no 'tolerance too low' messages while loading, and this property seems to be retained across several runs with difference tolerances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/714
https://github.com/hail-is/hail/issues/714:300,Integrability,message,messages,300,"We tried loading a large bgen file (chr21 - 950k Variants, 150k samples) from UK Biobank. It seemed to proceed ok and generate a VDS file that was loadable, but upon looking at variantqc, it appears that at the large majority of sites, all homref calls are missing. There were no 'tolerance too low' messages while loading, and this property seems to be retained across several runs with difference tolerances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/714
https://github.com/hail-is/hail/issues/714:9,Performance,load,loading,9,"We tried loading a large bgen file (chr21 - 950k Variants, 150k samples) from UK Biobank. It seemed to proceed ok and generate a VDS file that was loadable, but upon looking at variantqc, it appears that at the large majority of sites, all homref calls are missing. There were no 'tolerance too low' messages while loading, and this property seems to be retained across several runs with difference tolerances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/714
https://github.com/hail-is/hail/issues/714:147,Performance,load,loadable,147,"We tried loading a large bgen file (chr21 - 950k Variants, 150k samples) from UK Biobank. It seemed to proceed ok and generate a VDS file that was loadable, but upon looking at variantqc, it appears that at the large majority of sites, all homref calls are missing. There were no 'tolerance too low' messages while loading, and this property seems to be retained across several runs with difference tolerances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/714
https://github.com/hail-is/hail/issues/714:315,Performance,load,loading,315,"We tried loading a large bgen file (chr21 - 950k Variants, 150k samples) from UK Biobank. It seemed to proceed ok and generate a VDS file that was loadable, but upon looking at variantqc, it appears that at the large majority of sites, all homref calls are missing. There were no 'tolerance too low' messages while loading, and this property seems to be retained across several runs with difference tolerances.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/714
https://github.com/hail-is/hail/issues/715:84,Availability,error,error,84,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:427,Availability,failure,failure,427,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:485,Availability,failure,failure,485,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:2245,Energy Efficiency,schedul,scheduler,2245,": caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:2316,Energy Efficiency,schedul,scheduler,2316,": caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:11,Performance,load,loading,11,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:406,Safety,abort,aborted,406,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:771,Security,Checksum,ChecksumFileSystem,771,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:820,Security,Checksum,ChecksumFileSystem,820,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/issues/715:101,Testability,log,log,101,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/715
https://github.com/hail-is/hail/pull/718:13,Testability,assert,asserts,13,asOrderedRDD asserts the VSM is an OrderedRDD or partitioned by a OrderedPartitioner.; added new toOrderedRDD that takes ranges and always shuffles.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/718
https://github.com/hail-is/hail/pull/722:136,Testability,test,tests,136,"Fix bug in Genotype.gtFromLinear. We had a destructive bug in this function that; caused dosages to never result in a HomRef call.; Our tests were inadequate because we used this; function both in the generator and import, i.e.; testing it against itself. I added a unit test for; this method. Fixes #714",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/722
https://github.com/hail-is/hail/pull/722:229,Testability,test,testing,229,"Fix bug in Genotype.gtFromLinear. We had a destructive bug in this function that; caused dosages to never result in a HomRef call.; Our tests were inadequate because we used this; function both in the generator and import, i.e.; testing it against itself. I added a unit test for; this method. Fixes #714",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/722
https://github.com/hail-is/hail/pull/722:271,Testability,test,test,271,"Fix bug in Genotype.gtFromLinear. We had a destructive bug in this function that; caused dosages to never result in a HomRef call.; Our tests were inadequate because we used this; function both in the generator and import, i.e.; testing it against itself. I added a unit test for; this method. Fixes #714",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/722
https://github.com/hail-is/hail/issues/723:19,Integrability,depend,dependencies,19,"Shuffles have wide dependencies, and it appears that Spark does not enforce that the Nth partition of a shuffled RDD is computed on the same node as the Nth partition of its parent. To keep RDDs `Ordered`, we sometimes need to shuffle, even though the typical key doesn't move at all. It would likely provide a sizable performance gain to enforce that partitions stay on the same node after the shuffle: this way, network traffic (often the rate-limiting step) is kept to a minimum. To do this, we need to optionally override the `getPreferredLocations` function of the ShuffledRDD created in `OrderedRDD.apply` to provide the preferred locations of the parent RDD. This flag should be used in `splitmulti`. **NB:** it's possible this won't actually help much, since if there are 3 preferred hosts for the parent, we may only have a 1/3 chance of landing on the same one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/723
https://github.com/hail-is/hail/issues/723:319,Performance,perform,performance,319,"Shuffles have wide dependencies, and it appears that Spark does not enforce that the Nth partition of a shuffled RDD is computed on the same node as the Nth partition of its parent. To keep RDDs `Ordered`, we sometimes need to shuffle, even though the typical key doesn't move at all. It would likely provide a sizable performance gain to enforce that partitions stay on the same node after the shuffle: this way, network traffic (often the rate-limiting step) is kept to a minimum. To do this, we need to optionally override the `getPreferredLocations` function of the ShuffledRDD created in `OrderedRDD.apply` to provide the preferred locations of the parent RDD. This flag should be used in `splitmulti`. **NB:** it's possible this won't actually help much, since if there are 3 preferred hosts for the parent, we may only have a 1/3 chance of landing on the same one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/723
https://github.com/hail-is/hail/pull/729:41,Performance,load,load,41,"The serialization changed briefly, still load VDSes written with that verison.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/729
https://github.com/hail-is/hail/issues/730:36,Security,access,accessible,36,"The altAllele.nMismatch field isn't accessible trough AST. Given the current implementation, it probably shouldn't be and should be removed from the docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/730
https://github.com/hail-is/hail/pull/732:198,Modifiability,extend,extends,198,"I added a `PropertySuite` and deleted the `check` business. I feel this is an improvement, but also that we can do better. You can still have an orphaned `Prop` by writing:. ```; class MyProperties extends PropertySuite {; forAll ... // no property(""name"") = ...; }; ```. I think better would be for `PropretySuite` to declare `forAll` and make `forAll` take a name. `PropertySuite` extends `SparkSuite`. I was seeing some strange behavior that I don't fully understand if I made it extend `TestNGSuite` and then mixed `PropertySuite` and `SparkSuite` in a test suite. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/732
https://github.com/hail-is/hail/pull/732:383,Modifiability,extend,extends,383,"I added a `PropertySuite` and deleted the `check` business. I feel this is an improvement, but also that we can do better. You can still have an orphaned `Prop` by writing:. ```; class MyProperties extends PropertySuite {; forAll ... // no property(""name"") = ...; }; ```. I think better would be for `PropretySuite` to declare `forAll` and make `forAll` take a name. `PropertySuite` extends `SparkSuite`. I was seeing some strange behavior that I don't fully understand if I made it extend `TestNGSuite` and then mixed `PropertySuite` and `SparkSuite` in a test suite. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/732
https://github.com/hail-is/hail/pull/732:483,Modifiability,extend,extend,483,"I added a `PropertySuite` and deleted the `check` business. I feel this is an improvement, but also that we can do better. You can still have an orphaned `Prop` by writing:. ```; class MyProperties extends PropertySuite {; forAll ... // no property(""name"") = ...; }; ```. I think better would be for `PropretySuite` to declare `forAll` and make `forAll` take a name. `PropertySuite` extends `SparkSuite`. I was seeing some strange behavior that I don't fully understand if I made it extend `TestNGSuite` and then mixed `PropertySuite` and `SparkSuite` in a test suite. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/732
https://github.com/hail-is/hail/pull/732:491,Testability,Test,TestNGSuite,491,"I added a `PropertySuite` and deleted the `check` business. I feel this is an improvement, but also that we can do better. You can still have an orphaned `Prop` by writing:. ```; class MyProperties extends PropertySuite {; forAll ... // no property(""name"") = ...; }; ```. I think better would be for `PropretySuite` to declare `forAll` and make `forAll` take a name. `PropertySuite` extends `SparkSuite`. I was seeing some strange behavior that I don't fully understand if I made it extend `TestNGSuite` and then mixed `PropertySuite` and `SparkSuite` in a test suite. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/732
https://github.com/hail-is/hail/pull/732:557,Testability,test,test,557,"I added a `PropertySuite` and deleted the `check` business. I feel this is an improvement, but also that we can do better. You can still have an orphaned `Prop` by writing:. ```; class MyProperties extends PropertySuite {; forAll ... // no property(""name"") = ...; }; ```. I think better would be for `PropretySuite` to declare `forAll` and make `forAll` take a name. `PropertySuite` extends `SparkSuite`. I was seeing some strange behavior that I don't fully understand if I made it extend `TestNGSuite` and then mixed `PropertySuite` and `SparkSuite` in a test suite. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/732
https://github.com/hail-is/hail/issues/739:56,Energy Efficiency,green,green,56,"If a commit is rebuilt and fails, the docs link will be green and point to the previous successful commit. If a commit fails, no docs link exists. There are no pending docs statuses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/739
https://github.com/hail-is/hail/pull/740:33,Modifiability,refactor,refactoring,33,"Significantly cleaned up code by refactoring; and deleting org.broadinstitute.hail.Utils:; 1. Rich classes are moved to utils.richUtils._,; and implicit conversions are held in a trait; utils.richUtils.Implicits; 2. Standalone classes in Utils have been made; independent classes under utils; 3. Miscellaneous methods have been moved to; utils package object",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/740
https://github.com/hail-is/hail/issues/742:102,Testability,test,tests,102,Build against:; - google cloud / latest 1.X version: 1.6.2; - cray version; - data flow version. just tests against latest 1.X version.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/742
https://github.com/hail-is/hail/pull/748:76,Availability,error,error,76,"It also fixes numeric promotion of TInt to TLong, which threw an; assertion error before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/748
https://github.com/hail-is/hail/pull/748:66,Testability,assert,assertion,66,"It also fixes numeric promotion of TInt to TLong, which threw an; assertion error before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/748
https://github.com/hail-is/hail/pull/749:38,Testability,test,test,38,Workflow:. ```; $ hdfs dfs -rm -r src/test/resources; $ hdfs dfs -put src/test/resources src/test; $ ./gradlew shadowTestJar; $ SPARK_CLASSPATH=./build/libs/hail-all-spark-test.jar spark-submit --total-executor-cores 2 --class org.testng.TestNG ./build/libs/hail-all-spark-test.jar ./testng.xml; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/749
https://github.com/hail-is/hail/pull/749:74,Testability,test,test,74,Workflow:. ```; $ hdfs dfs -rm -r src/test/resources; $ hdfs dfs -put src/test/resources src/test; $ ./gradlew shadowTestJar; $ SPARK_CLASSPATH=./build/libs/hail-all-spark-test.jar spark-submit --total-executor-cores 2 --class org.testng.TestNG ./build/libs/hail-all-spark-test.jar ./testng.xml; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/749
https://github.com/hail-is/hail/pull/749:93,Testability,test,test,93,Workflow:. ```; $ hdfs dfs -rm -r src/test/resources; $ hdfs dfs -put src/test/resources src/test; $ ./gradlew shadowTestJar; $ SPARK_CLASSPATH=./build/libs/hail-all-spark-test.jar spark-submit --total-executor-cores 2 --class org.testng.TestNG ./build/libs/hail-all-spark-test.jar ./testng.xml; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/749
https://github.com/hail-is/hail/pull/749:172,Testability,test,test,172,Workflow:. ```; $ hdfs dfs -rm -r src/test/resources; $ hdfs dfs -put src/test/resources src/test; $ ./gradlew shadowTestJar; $ SPARK_CLASSPATH=./build/libs/hail-all-spark-test.jar spark-submit --total-executor-cores 2 --class org.testng.TestNG ./build/libs/hail-all-spark-test.jar ./testng.xml; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/749
https://github.com/hail-is/hail/pull/749:231,Testability,test,testng,231,Workflow:. ```; $ hdfs dfs -rm -r src/test/resources; $ hdfs dfs -put src/test/resources src/test; $ ./gradlew shadowTestJar; $ SPARK_CLASSPATH=./build/libs/hail-all-spark-test.jar spark-submit --total-executor-cores 2 --class org.testng.TestNG ./build/libs/hail-all-spark-test.jar ./testng.xml; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/749
https://github.com/hail-is/hail/pull/749:238,Testability,Test,TestNG,238,Workflow:. ```; $ hdfs dfs -rm -r src/test/resources; $ hdfs dfs -put src/test/resources src/test; $ ./gradlew shadowTestJar; $ SPARK_CLASSPATH=./build/libs/hail-all-spark-test.jar spark-submit --total-executor-cores 2 --class org.testng.TestNG ./build/libs/hail-all-spark-test.jar ./testng.xml; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/749
https://github.com/hail-is/hail/pull/749:273,Testability,test,test,273,Workflow:. ```; $ hdfs dfs -rm -r src/test/resources; $ hdfs dfs -put src/test/resources src/test; $ ./gradlew shadowTestJar; $ SPARK_CLASSPATH=./build/libs/hail-all-spark-test.jar spark-submit --total-executor-cores 2 --class org.testng.TestNG ./build/libs/hail-all-spark-test.jar ./testng.xml; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/749
https://github.com/hail-is/hail/pull/749:284,Testability,test,testng,284,Workflow:. ```; $ hdfs dfs -rm -r src/test/resources; $ hdfs dfs -put src/test/resources src/test; $ ./gradlew shadowTestJar; $ SPARK_CLASSPATH=./build/libs/hail-all-spark-test.jar spark-submit --total-executor-cores 2 --class org.testng.TestNG ./build/libs/hail-all-spark-test.jar ./testng.xml; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/749
https://github.com/hail-is/hail/pull/753:96,Modifiability,inherit,inherited,96,"Implemented the transmission disequilibrium test (TDT) in hail. TDT tests for variants that are inherited more or less than what would be expected by chance (i.e., 50%).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753
https://github.com/hail-is/hail/pull/753:44,Testability,test,test,44,"Implemented the transmission disequilibrium test (TDT) in hail. TDT tests for variants that are inherited more or less than what would be expected by chance (i.e., 50%).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753
https://github.com/hail-is/hail/pull/753:68,Testability,test,tests,68,"Implemented the transmission disequilibrium test (TDT) in hail. TDT tests for variants that are inherited more or less than what would be expected by chance (i.e., 50%).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753
https://github.com/hail-is/hail/pull/758:76,Availability,error,error,76,"It also fixes numeric promotion of TInt to TLong, which threw an; assertion error before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/758
https://github.com/hail-is/hail/pull/758:66,Testability,assert,assertion,66,"It also fixes numeric promotion of TInt to TLong, which threw an; assertion error before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/758
https://github.com/hail-is/hail/issues/759:215,Usability,simpl,simplify,215,"`[0, 0.toFloat][0].toInt` results in a ClassCastException. We should fix the type promotion method, and should also replace the `makeLong` and `makeDouble` methods on TNumeric with a `conv: NumericConversion[_]` to simplify things and be consistent. @khernyo has claimed this!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/759
https://github.com/hail-is/hail/issues/773:18,Availability,error,error,18,And generate nice error message on Java 7.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/773
https://github.com/hail-is/hail/issues/773:24,Integrability,message,message,24,And generate nice error message on Java 7.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/773
https://github.com/hail-is/hail/issues/785:269,Availability,failure,failure,269,"reproduces with this command:. ``` text; hail importvcf src/test/resources/sample.vcf annotatevariants expr -c 'va.filters = ""HELLO""' exportvcf -o /tmp/out.vcf; ```. ``` text; hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/785
https://github.com/hail-is/hail/issues/785:326,Availability,failure,failure,326,"reproduces with this command:. ``` text; hail importvcf src/test/resources/sample.vcf annotatevariants expr -c 'va.filters = ""HELLO""' exportvcf -o /tmp/out.vcf; ```. ``` text; hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/785
https://github.com/hail-is/hail/issues/785:1891,Energy Efficiency,schedul,scheduler,1891," in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/785
https://github.com/hail-is/hail/issues/785:1962,Energy Efficiency,schedul,scheduler,1962," in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/785
https://github.com/hail-is/hail/issues/785:2084,Performance,concurren,concurrent,2084," in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/785
https://github.com/hail-is/hail/issues/785:2168,Performance,concurren,concurrent,2168," in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/785
https://github.com/hail-is/hail/issues/785:248,Safety,abort,aborted,248,"reproduces with this command:. ``` text; hail importvcf src/test/resources/sample.vcf annotatevariants expr -c 'va.filters = ""HELLO""' exportvcf -o /tmp/out.vcf; ```. ``` text; hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/785
https://github.com/hail-is/hail/issues/785:60,Testability,test,test,60,"reproduces with this command:. ``` text; hail importvcf src/test/resources/sample.vcf annotatevariants expr -c 'va.filters = ""HELLO""' exportvcf -o /tmp/out.vcf; ```. ``` text; hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/785
https://github.com/hail-is/hail/pull/787:26,Performance,load,load,26,This permits shuffle-free load of files where; multiallelics are split across partitions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/787
https://github.com/hail-is/hail/pull/788:132,Availability,error,error,132,When the user fails to provide a lambda to an aggregation; they now receive a suggestion to use a lambda rather than; a Scala match error. @tpoterba this should hold us over until we have something nicer like that `projectT` function. resolves #786,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/788
https://github.com/hail-is/hail/issues/789:152,Integrability,message,message,152,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/789
https://github.com/hail-is/hail/issues/789:198,Integrability,message,message,198,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/789
https://github.com/hail-is/hail/issues/789:285,Integrability,message,message,285,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/789
https://github.com/hail-is/hail/issues/789:452,Testability,assert,assert,452,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/789
https://github.com/hail-is/hail/pull/793:52,Testability,Test,Tests,52,Reorder partitions based on ParquetInputSplit path. Tests pass on GCP.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/793
https://github.com/hail-is/hail/issues/798:21,Deployability,pipeline,pipeline,21,See Maryam's example pipeline posted to Slack #hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/798
https://github.com/hail-is/hail/pull/799:26,Performance,load,load,26,This permits shuffle-free load of files where; multiallelics are split across partitions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/799
https://github.com/hail-is/hail/pull/800:10,Deployability,install,install,10,"- Need to install npm on build server; - Install following node packages: jsdom, jquery, mathjax-node; - Set the Global Environment Variable for where Node packages are: $NODE_PATH",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/800
https://github.com/hail-is/hail/pull/800:41,Deployability,Install,Install,41,"- Need to install npm on build server; - Install following node packages: jsdom, jquery, mathjax-node; - Set the Global Environment Variable for where Node packages are: $NODE_PATH",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/800
https://github.com/hail-is/hail/pull/800:132,Modifiability,Variab,Variable,132,"- Need to install npm on build server; - Install following node packages: jsdom, jquery, mathjax-node; - Set the Global Environment Variable for where Node packages are: $NODE_PATH",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/800
https://github.com/hail-is/hail/pull/805:82,Testability,test,tests,82,"not sure if this is worth documenting here, but I had this question, and did some tests to figure out the answer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/805
https://github.com/hail-is/hail/issues/806:87,Performance,load,loadings,87,"Francesco found that after filtering to the purcell 5k (6k sites after a `count`), the loadings file produced by PCA only contained 400 variants.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/806
https://github.com/hail-is/hail/pull/807:1088,Modifiability,extend,extends,1088,"Resolves issue #763.; ### Simple Types. The Function Registry distinguishes between fields and functions because they were distinguished in the existing `AST.scala`. Moreover, for unary functions, there are registration methods for both pure functions and computations in the `Option` monad. Registration requires only a name and an implementation. Unfortunately, the Scala compiler fails to infer the type parameters from an expression like `_.isHomRef`. . ``` scala; registerOptionField(""dosage"", { (x: Genotype) => x.dosage.map(a => a: IndexedSeq[Double]) }); registerField(""isHomRef"", { (x: Genotype) => x.isHomRef }); ```. ``` scala; register(""Variant"", { (x: String) =>; val Array(chr, pos, ref, alts) = x.split("":""); Variant(chr, pos.toInt, ref, alts.split("","")); }); register(""Variant"", { (x: String, y: Int, z: String, a: String) => Variant(x, y, z, a) }); ```. The `HailRep` type class associates Scala types with Hail expression types. For example, the function registry knows that `Variant` returns a `TVariant` because of this implicit:. ``` scala; implicit object variantHr extends HailRep[Variant] {; def typ = TVariant; }; ```; ### Polymorphic Types. I don't have an answer for the various kinds of polymorphism present in the Hail expression language. There is unbounded polymorphism:. ``` scala; case (t: TArray, ""length"") => TInt; ```. as well as bounded polymorphism:. ``` scala; case (""pow"", _) => TDouble; args.map(_.`type`) match {; case Array(a: TNumeric, b: TNumeric) => TDouble; // ...; }; ```. Both of these are still handled by explicit case matching.; ### Struct Types. Functions returning structs can use `registerAnn` to specifically provide a return type. ``` scala; registerAnn(""foo"", TStruct((""bar"", TDouble)), { (x: Int) => Annotation(x / 2.0) } ; ```. In general, the `register` `HailRep` implicits can be overridden as well, but this case is common enough to merit a concise alternative.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/807
https://github.com/hail-is/hail/pull/807:1148,Modifiability,Polymorphi,Polymorphic,1148,"Resolves issue #763.; ### Simple Types. The Function Registry distinguishes between fields and functions because they were distinguished in the existing `AST.scala`. Moreover, for unary functions, there are registration methods for both pure functions and computations in the `Option` monad. Registration requires only a name and an implementation. Unfortunately, the Scala compiler fails to infer the type parameters from an expression like `_.isHomRef`. . ``` scala; registerOptionField(""dosage"", { (x: Genotype) => x.dosage.map(a => a: IndexedSeq[Double]) }); registerField(""isHomRef"", { (x: Genotype) => x.isHomRef }); ```. ``` scala; register(""Variant"", { (x: String) =>; val Array(chr, pos, ref, alts) = x.split("":""); Variant(chr, pos.toInt, ref, alts.split("","")); }); register(""Variant"", { (x: String, y: Int, z: String, a: String) => Variant(x, y, z, a) }); ```. The `HailRep` type class associates Scala types with Hail expression types. For example, the function registry knows that `Variant` returns a `TVariant` because of this implicit:. ``` scala; implicit object variantHr extends HailRep[Variant] {; def typ = TVariant; }; ```; ### Polymorphic Types. I don't have an answer for the various kinds of polymorphism present in the Hail expression language. There is unbounded polymorphism:. ``` scala; case (t: TArray, ""length"") => TInt; ```. as well as bounded polymorphism:. ``` scala; case (""pow"", _) => TDouble; args.map(_.`type`) match {; case Array(a: TNumeric, b: TNumeric) => TDouble; // ...; }; ```. Both of these are still handled by explicit case matching.; ### Struct Types. Functions returning structs can use `registerAnn` to specifically provide a return type. ``` scala; registerAnn(""foo"", TStruct((""bar"", TDouble)), { (x: Int) => Annotation(x / 2.0) } ; ```. In general, the `register` `HailRep` implicits can be overridden as well, but this case is common enough to merit a concise alternative.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/807
https://github.com/hail-is/hail/pull/807:1215,Modifiability,polymorphi,polymorphism,1215,"Resolves issue #763.; ### Simple Types. The Function Registry distinguishes between fields and functions because they were distinguished in the existing `AST.scala`. Moreover, for unary functions, there are registration methods for both pure functions and computations in the `Option` monad. Registration requires only a name and an implementation. Unfortunately, the Scala compiler fails to infer the type parameters from an expression like `_.isHomRef`. . ``` scala; registerOptionField(""dosage"", { (x: Genotype) => x.dosage.map(a => a: IndexedSeq[Double]) }); registerField(""isHomRef"", { (x: Genotype) => x.isHomRef }); ```. ``` scala; register(""Variant"", { (x: String) =>; val Array(chr, pos, ref, alts) = x.split("":""); Variant(chr, pos.toInt, ref, alts.split("","")); }); register(""Variant"", { (x: String, y: Int, z: String, a: String) => Variant(x, y, z, a) }); ```. The `HailRep` type class associates Scala types with Hail expression types. For example, the function registry knows that `Variant` returns a `TVariant` because of this implicit:. ``` scala; implicit object variantHr extends HailRep[Variant] {; def typ = TVariant; }; ```; ### Polymorphic Types. I don't have an answer for the various kinds of polymorphism present in the Hail expression language. There is unbounded polymorphism:. ``` scala; case (t: TArray, ""length"") => TInt; ```. as well as bounded polymorphism:. ``` scala; case (""pow"", _) => TDouble; args.map(_.`type`) match {; case Array(a: TNumeric, b: TNumeric) => TDouble; // ...; }; ```. Both of these are still handled by explicit case matching.; ### Struct Types. Functions returning structs can use `registerAnn` to specifically provide a return type. ``` scala; registerAnn(""foo"", TStruct((""bar"", TDouble)), { (x: Int) => Annotation(x / 2.0) } ; ```. In general, the `register` `HailRep` implicits can be overridden as well, but this case is common enough to merit a concise alternative.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/807
https://github.com/hail-is/hail/pull/807:1288,Modifiability,polymorphi,polymorphism,1288,"Resolves issue #763.; ### Simple Types. The Function Registry distinguishes between fields and functions because they were distinguished in the existing `AST.scala`. Moreover, for unary functions, there are registration methods for both pure functions and computations in the `Option` monad. Registration requires only a name and an implementation. Unfortunately, the Scala compiler fails to infer the type parameters from an expression like `_.isHomRef`. . ``` scala; registerOptionField(""dosage"", { (x: Genotype) => x.dosage.map(a => a: IndexedSeq[Double]) }); registerField(""isHomRef"", { (x: Genotype) => x.isHomRef }); ```. ``` scala; register(""Variant"", { (x: String) =>; val Array(chr, pos, ref, alts) = x.split("":""); Variant(chr, pos.toInt, ref, alts.split("","")); }); register(""Variant"", { (x: String, y: Int, z: String, a: String) => Variant(x, y, z, a) }); ```. The `HailRep` type class associates Scala types with Hail expression types. For example, the function registry knows that `Variant` returns a `TVariant` because of this implicit:. ``` scala; implicit object variantHr extends HailRep[Variant] {; def typ = TVariant; }; ```; ### Polymorphic Types. I don't have an answer for the various kinds of polymorphism present in the Hail expression language. There is unbounded polymorphism:. ``` scala; case (t: TArray, ""length"") => TInt; ```. as well as bounded polymorphism:. ``` scala; case (""pow"", _) => TDouble; args.map(_.`type`) match {; case Array(a: TNumeric, b: TNumeric) => TDouble; // ...; }; ```. Both of these are still handled by explicit case matching.; ### Struct Types. Functions returning structs can use `registerAnn` to specifically provide a return type. ``` scala; registerAnn(""foo"", TStruct((""bar"", TDouble)), { (x: Int) => Annotation(x / 2.0) } ; ```. In general, the `register` `HailRep` implicits can be overridden as well, but this case is common enough to merit a concise alternative.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/807
https://github.com/hail-is/hail/pull/807:1374,Modifiability,polymorphi,polymorphism,1374,"Resolves issue #763.; ### Simple Types. The Function Registry distinguishes between fields and functions because they were distinguished in the existing `AST.scala`. Moreover, for unary functions, there are registration methods for both pure functions and computations in the `Option` monad. Registration requires only a name and an implementation. Unfortunately, the Scala compiler fails to infer the type parameters from an expression like `_.isHomRef`. . ``` scala; registerOptionField(""dosage"", { (x: Genotype) => x.dosage.map(a => a: IndexedSeq[Double]) }); registerField(""isHomRef"", { (x: Genotype) => x.isHomRef }); ```. ``` scala; register(""Variant"", { (x: String) =>; val Array(chr, pos, ref, alts) = x.split("":""); Variant(chr, pos.toInt, ref, alts.split("","")); }); register(""Variant"", { (x: String, y: Int, z: String, a: String) => Variant(x, y, z, a) }); ```. The `HailRep` type class associates Scala types with Hail expression types. For example, the function registry knows that `Variant` returns a `TVariant` because of this implicit:. ``` scala; implicit object variantHr extends HailRep[Variant] {; def typ = TVariant; }; ```; ### Polymorphic Types. I don't have an answer for the various kinds of polymorphism present in the Hail expression language. There is unbounded polymorphism:. ``` scala; case (t: TArray, ""length"") => TInt; ```. as well as bounded polymorphism:. ``` scala; case (""pow"", _) => TDouble; args.map(_.`type`) match {; case Array(a: TNumeric, b: TNumeric) => TDouble; // ...; }; ```. Both of these are still handled by explicit case matching.; ### Struct Types. Functions returning structs can use `registerAnn` to specifically provide a return type. ``` scala; registerAnn(""foo"", TStruct((""bar"", TDouble)), { (x: Int) => Annotation(x / 2.0) } ; ```. In general, the `register` `HailRep` implicits can be overridden as well, but this case is common enough to merit a concise alternative.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/807
https://github.com/hail-is/hail/pull/807:26,Usability,Simpl,Simple,26,"Resolves issue #763.; ### Simple Types. The Function Registry distinguishes between fields and functions because they were distinguished in the existing `AST.scala`. Moreover, for unary functions, there are registration methods for both pure functions and computations in the `Option` monad. Registration requires only a name and an implementation. Unfortunately, the Scala compiler fails to infer the type parameters from an expression like `_.isHomRef`. . ``` scala; registerOptionField(""dosage"", { (x: Genotype) => x.dosage.map(a => a: IndexedSeq[Double]) }); registerField(""isHomRef"", { (x: Genotype) => x.isHomRef }); ```. ``` scala; register(""Variant"", { (x: String) =>; val Array(chr, pos, ref, alts) = x.split("":""); Variant(chr, pos.toInt, ref, alts.split("","")); }); register(""Variant"", { (x: String, y: Int, z: String, a: String) => Variant(x, y, z, a) }); ```. The `HailRep` type class associates Scala types with Hail expression types. For example, the function registry knows that `Variant` returns a `TVariant` because of this implicit:. ``` scala; implicit object variantHr extends HailRep[Variant] {; def typ = TVariant; }; ```; ### Polymorphic Types. I don't have an answer for the various kinds of polymorphism present in the Hail expression language. There is unbounded polymorphism:. ``` scala; case (t: TArray, ""length"") => TInt; ```. as well as bounded polymorphism:. ``` scala; case (""pow"", _) => TDouble; args.map(_.`type`) match {; case Array(a: TNumeric, b: TNumeric) => TDouble; // ...; }; ```. Both of these are still handled by explicit case matching.; ### Struct Types. Functions returning structs can use `registerAnn` to specifically provide a return type. ``` scala; registerAnn(""foo"", TStruct((""bar"", TDouble)), { (x: Int) => Annotation(x / 2.0) } ; ```. In general, the `register` `HailRep` implicits can be overridden as well, but this case is common enough to merit a concise alternative.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/807
https://github.com/hail-is/hail/pull/809:39,Availability,error,errors,39,resolves #791 . This should cause type errors much sooner if a signature disagrees with the actual types. Much of the sample annotating code uses multiple inserters and therefore cannot be rewritten to use `annotateSamples`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/809
https://github.com/hail-is/hail/pull/811:150,Usability,guid,guide,150,- Markdown files by category in `docs/faq` directory; - Uses NodeJS to compile markdown files into single page `build/docs/faq.html`; - See the style guide `docs/FAQStyleGuide.md`; - Modified Gradle `createDocs` task,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/811
https://github.com/hail-is/hail/issues/812:904,Deployability,update,updated,904,"See the FAQ Style Guide. **Annotations**; - [ ] Do I need to define the types when using `annotatesamples table`?; - [ ] How does Hail annotate variants overlapping different intervals in an interval list?; - [ ] How do I input phenotype information into Hail?; - [ ] Is there a way to see all annotations present in the dataset?. **Expression Language**; - [ ] Can I use regular expressions in the Hail expression language?; - [ ] how can i filter samples based on whether or not they have a particular variant?. **Data Representation**; - [ ] How are insertion and deletion variants coded in the VDS?; - [ ] How are the boundaries for Pseudo-autosomal variants determined?. **Exporting Data**; - [ ] How can I export all global annotations to a file?; - [ ] How do I export my data so there are separate VCFs per chromosome?; - [ ] How do I export my annotations as a JSON file?; - [ ] How do I export updated call statistics (AC, AF) to the info field of the VCF?. **Developer Tools**; - [ ] Is there a style guide I should use for IntelliJ?. **Importing Data**; - [ ] How do I import data from a VCF file?; - [ ] How do I import annotations in JSON format?; - [ ] Is the UCSC file 0 or 1 based?. **Methods**; - [ ] Does Hail handle sex chromosomes differently in variantqc and sampleqc?; - [ ] How do I parse the variant annotations from VEP to find the worst functional consequence?; - [ ] How do I find all variants where the functional change on the canonical transcript results in a missense mutation?; - [ ] Is rHetHom calculated over indels+SNPs or just SNPs?; - [ ] Are sampleqc and variantqc calculated only on PASS variants?. **Optimize Pipeline**; - [ ] When should I write my data to a VDS file?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/812
https://github.com/hail-is/hail/issues/812:1650,Deployability,Pipeline,Pipeline,1650,"See the FAQ Style Guide. **Annotations**; - [ ] Do I need to define the types when using `annotatesamples table`?; - [ ] How does Hail annotate variants overlapping different intervals in an interval list?; - [ ] How do I input phenotype information into Hail?; - [ ] Is there a way to see all annotations present in the dataset?. **Expression Language**; - [ ] Can I use regular expressions in the Hail expression language?; - [ ] how can i filter samples based on whether or not they have a particular variant?. **Data Representation**; - [ ] How are insertion and deletion variants coded in the VDS?; - [ ] How are the boundaries for Pseudo-autosomal variants determined?. **Exporting Data**; - [ ] How can I export all global annotations to a file?; - [ ] How do I export my data so there are separate VCFs per chromosome?; - [ ] How do I export my annotations as a JSON file?; - [ ] How do I export updated call statistics (AC, AF) to the info field of the VCF?. **Developer Tools**; - [ ] Is there a style guide I should use for IntelliJ?. **Importing Data**; - [ ] How do I import data from a VCF file?; - [ ] How do I import annotations in JSON format?; - [ ] Is the UCSC file 0 or 1 based?. **Methods**; - [ ] Does Hail handle sex chromosomes differently in variantqc and sampleqc?; - [ ] How do I parse the variant annotations from VEP to find the worst functional consequence?; - [ ] How do I find all variants where the functional change on the canonical transcript results in a missense mutation?; - [ ] Is rHetHom calculated over indels+SNPs or just SNPs?; - [ ] Are sampleqc and variantqc calculated only on PASS variants?. **Optimize Pipeline**; - [ ] When should I write my data to a VDS file?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/812
https://github.com/hail-is/hail/issues/812:1641,Performance,Optimiz,Optimize,1641,"See the FAQ Style Guide. **Annotations**; - [ ] Do I need to define the types when using `annotatesamples table`?; - [ ] How does Hail annotate variants overlapping different intervals in an interval list?; - [ ] How do I input phenotype information into Hail?; - [ ] Is there a way to see all annotations present in the dataset?. **Expression Language**; - [ ] Can I use regular expressions in the Hail expression language?; - [ ] how can i filter samples based on whether or not they have a particular variant?. **Data Representation**; - [ ] How are insertion and deletion variants coded in the VDS?; - [ ] How are the boundaries for Pseudo-autosomal variants determined?. **Exporting Data**; - [ ] How can I export all global annotations to a file?; - [ ] How do I export my data so there are separate VCFs per chromosome?; - [ ] How do I export my annotations as a JSON file?; - [ ] How do I export updated call statistics (AC, AF) to the info field of the VCF?. **Developer Tools**; - [ ] Is there a style guide I should use for IntelliJ?. **Importing Data**; - [ ] How do I import data from a VCF file?; - [ ] How do I import annotations in JSON format?; - [ ] Is the UCSC file 0 or 1 based?. **Methods**; - [ ] Does Hail handle sex chromosomes differently in variantqc and sampleqc?; - [ ] How do I parse the variant annotations from VEP to find the worst functional consequence?; - [ ] How do I find all variants where the functional change on the canonical transcript results in a missense mutation?; - [ ] Is rHetHom calculated over indels+SNPs or just SNPs?; - [ ] Are sampleqc and variantqc calculated only on PASS variants?. **Optimize Pipeline**; - [ ] When should I write my data to a VDS file?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/812
https://github.com/hail-is/hail/issues/812:18,Usability,Guid,Guide,18,"See the FAQ Style Guide. **Annotations**; - [ ] Do I need to define the types when using `annotatesamples table`?; - [ ] How does Hail annotate variants overlapping different intervals in an interval list?; - [ ] How do I input phenotype information into Hail?; - [ ] Is there a way to see all annotations present in the dataset?. **Expression Language**; - [ ] Can I use regular expressions in the Hail expression language?; - [ ] how can i filter samples based on whether or not they have a particular variant?. **Data Representation**; - [ ] How are insertion and deletion variants coded in the VDS?; - [ ] How are the boundaries for Pseudo-autosomal variants determined?. **Exporting Data**; - [ ] How can I export all global annotations to a file?; - [ ] How do I export my data so there are separate VCFs per chromosome?; - [ ] How do I export my annotations as a JSON file?; - [ ] How do I export updated call statistics (AC, AF) to the info field of the VCF?. **Developer Tools**; - [ ] Is there a style guide I should use for IntelliJ?. **Importing Data**; - [ ] How do I import data from a VCF file?; - [ ] How do I import annotations in JSON format?; - [ ] Is the UCSC file 0 or 1 based?. **Methods**; - [ ] Does Hail handle sex chromosomes differently in variantqc and sampleqc?; - [ ] How do I parse the variant annotations from VEP to find the worst functional consequence?; - [ ] How do I find all variants where the functional change on the canonical transcript results in a missense mutation?; - [ ] Is rHetHom calculated over indels+SNPs or just SNPs?; - [ ] Are sampleqc and variantqc calculated only on PASS variants?. **Optimize Pipeline**; - [ ] When should I write my data to a VDS file?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/812
https://github.com/hail-is/hail/issues/812:1012,Usability,guid,guide,1012,"See the FAQ Style Guide. **Annotations**; - [ ] Do I need to define the types when using `annotatesamples table`?; - [ ] How does Hail annotate variants overlapping different intervals in an interval list?; - [ ] How do I input phenotype information into Hail?; - [ ] Is there a way to see all annotations present in the dataset?. **Expression Language**; - [ ] Can I use regular expressions in the Hail expression language?; - [ ] how can i filter samples based on whether or not they have a particular variant?. **Data Representation**; - [ ] How are insertion and deletion variants coded in the VDS?; - [ ] How are the boundaries for Pseudo-autosomal variants determined?. **Exporting Data**; - [ ] How can I export all global annotations to a file?; - [ ] How do I export my data so there are separate VCFs per chromosome?; - [ ] How do I export my annotations as a JSON file?; - [ ] How do I export updated call statistics (AC, AF) to the info field of the VCF?. **Developer Tools**; - [ ] Is there a style guide I should use for IntelliJ?. **Importing Data**; - [ ] How do I import data from a VCF file?; - [ ] How do I import annotations in JSON format?; - [ ] Is the UCSC file 0 or 1 based?. **Methods**; - [ ] Does Hail handle sex chromosomes differently in variantqc and sampleqc?; - [ ] How do I parse the variant annotations from VEP to find the worst functional consequence?; - [ ] How do I find all variants where the functional change on the canonical transcript results in a missense mutation?; - [ ] Is rHetHom calculated over indels+SNPs or just SNPs?; - [ ] Are sampleqc and variantqc calculated only on PASS variants?. **Optimize Pipeline**; - [ ] When should I write my data to a VDS file?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/812
https://github.com/hail-is/hail/issues/813:75,Availability,avail,available,75,We'll likely use the [ASM library](http://asm.ow2.org/index.html) which is available in [maven](https://mvnrepository.com/artifact/org.ow2.asm/asm-parent/5.1). For:. ```; if cond then cnsq else altr; ```. the emitter should produce the byte code:. ```; COMPILE[[cond]]; ifnull (lengthOf(COMPILE[[cnsq]]) + lengthOf(COMPILE[[altr]]) + 4); checkcast offset_to_java_lang_Boolean_class; invokevirtual offset_to_java_lang_Boolean_booleanValue; ifne (lengthOf(COMPILE[[cnsq]]) + 1); COMPILE[[cnsq]]; COMPILE[[altr]]; checkcast offset_to_resultType; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/813
https://github.com/hail-is/hail/pull/824:161,Performance,load,loading,161,"TextTableReader.read takes (optional) nPartitions.; importannotations: new option -n/--npartitions.; Added hintPartition to OrderedRDD coerce and friends.; When loading variant annotations, load with as many partitions as the RDD we're going to join with, and hint with its partitioner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824
https://github.com/hail-is/hail/pull/824:190,Performance,load,load,190,"TextTableReader.read takes (optional) nPartitions.; importannotations: new option -n/--npartitions.; Added hintPartition to OrderedRDD coerce and friends.; When loading variant annotations, load with as many partitions as the RDD we're going to join with, and hint with its partitioner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/824
https://github.com/hail-is/hail/issues/825:58,Availability,error,error,58,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:2182,Availability,error,error,2182,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:2465,Availability,echo,echo,2465,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:2531,Availability,echo,echo,2531,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:1740,Deployability,deploy,deploy,1740,"et.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:1777,Deployability,deploy,deploy,1777,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:1849,Deployability,deploy,deploy,1849,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:1925,Deployability,deploy,deploy,1925,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:1996,Deployability,deploy,deploy,1996,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:2065,Deployability,deploy,deploy,2065,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:2158,Modifiability,config,configured,2158,"$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to java version, so I re-configured java,but the error still appear, How can I solve it ?. --------------------My java version and path; [root@***-3 opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode). [root@***-3 opt]# echo $JAVA_HOME; /opt/BioDir/jdk/jdk1.8.0_91. [root@bio-x-3 opt]# echo $PATH; /opt/BioDir/plink_1.9:/opt/BioDir/jdk/jdk1.8.0_91/bin:/opt/BioDir/gradle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. My OS is CentOS7; [root@***-3 opt]# uname -a; Linux bio-x-3 3.10.0-229.14.1.el7.x86_64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:1066,Performance,load,loadClass,1066,"park-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.Spark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:1124,Performance,load,loadClass,1124,"ss org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). I think this may relate to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:601,Security,secur,security,601,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:610,Security,Secur,SecureClassLoader,610,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:640,Security,Secur,SecureClassLoader,640,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:761,Security,access,access,761,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:923,Security,secur,security,923,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:932,Security,Access,AccessController,932,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/825:14,Testability,test,test,14,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825
https://github.com/hail-is/hail/issues/828:99,Testability,test,tests,99,"The goal is to have a branch that supports making more radical changes and doesn't require all the tests to pass. When merging to experimental, tests that throw `UnimplementedException` should be marked as ""skipped"" (or something) rather than failing. Figure out how to do this with TestNG/gradle.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/828
https://github.com/hail-is/hail/issues/828:144,Testability,test,tests,144,"The goal is to have a branch that supports making more radical changes and doesn't require all the tests to pass. When merging to experimental, tests that throw `UnimplementedException` should be marked as ""skipped"" (or something) rather than failing. Figure out how to do this with TestNG/gradle.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/828
https://github.com/hail-is/hail/issues/828:283,Testability,Test,TestNG,283,"The goal is to have a branch that supports making more radical changes and doesn't require all the tests to pass. When merging to experimental, tests that throw `UnimplementedException` should be marked as ""skipped"" (or something) rather than failing. Figure out how to do this with TestNG/gradle.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/828
https://github.com/hail-is/hail/issues/829:133,Usability,clear,clearly,133,"For example, documentation for `g.isHomRef` says ""true if this call is 0/0"". No mention of when it can be `false` or missing. Should clearly reflection the option vs non-option register variants in @danking new function registry code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/829
https://github.com/hail-is/hail/pull/830:150,Usability,guid,guide,150,- Markdown files by category in `docs/faq` directory; - Uses NodeJS to compile markdown files into single page `build/docs/faq.html`; - See the style guide `docs/FAQStyleGuide.md`; - Modified Gradle `createDocs` task,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/830
https://github.com/hail-is/hail/issues/838:30,Availability,error,error,30,Annotates no variants without error. See gitter discussion with @lescai.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/838
https://github.com/hail-is/hail/issues/840:137,Availability,down,downloading,137,"There could be a webserver with reference datasets, and local installs that use hail-based pipelines (eg. seqr-hail prototype) can avoid downloading large files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/840
https://github.com/hail-is/hail/issues/840:62,Deployability,install,installs,62,"There could be a webserver with reference datasets, and local installs that use hail-based pipelines (eg. seqr-hail prototype) can avoid downloading large files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/840
https://github.com/hail-is/hail/issues/840:91,Deployability,pipeline,pipelines,91,"There could be a webserver with reference datasets, and local installs that use hail-based pipelines (eg. seqr-hail prototype) can avoid downloading large files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/840
https://github.com/hail-is/hail/issues/840:131,Safety,avoid,avoid,131,"There could be a webserver with reference datasets, and local installs that use hail-based pipelines (eg. seqr-hail prototype) can avoid downloading large files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/840
https://github.com/hail-is/hail/issues/844:6,Security,access,access,6,Can't access global within filtergenotypes expression.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/844
https://github.com/hail-is/hail/pull/847:28,Security,access,accessible,28,Fixes issue #844 global not accessible from filtergenotypes and exportgenotypes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/847
https://github.com/hail-is/hail/pull/851:0,Deployability,Update,Updated,0,"Updated calculation; Fixes #764. Branch name is numbered incorrectly, issue reference here is correct.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/851
https://github.com/hail-is/hail/issues/859:49,Modifiability,rewrite,rewrite,49,Ideally:; - Add a flag to state whether to force rewrite or not,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/859
https://github.com/hail-is/hail/pull/867:8,Integrability,depend,depends,8,"This PR depends on PR #738. Merge that first. This adds `--min` and `--max` flags to the `ibd` command. The resulting TSV will only contain sample pairs with `pi_hat` above the minimum and below the maximum, inclusively of both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/867
https://github.com/hail-is/hail/pull/868:104,Deployability,update,updated,104,"in the interest of modularity, the nMissing annotation need not be attached separately by linreg. I had updated this before in #645 but strangely only the documentation update when through. here are the corresponding code changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/868
https://github.com/hail-is/hail/pull/868:169,Deployability,update,update,169,"in the interest of modularity, the nMissing annotation need not be attached separately by linreg. I had updated this before in #645 but strangely only the documentation update when through. here are the corresponding code changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/868
https://github.com/hail-is/hail/issues/871:195,Testability,test,tests,195,Non-pseudo-autosomal variants (i.e. the part of the X which does not match the Y) are very unlikely to appear but can trigger significantly different behavior from pseudo-autosomal variants. Our tests should trigger this behavior more often.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/871
https://github.com/hail-is/hail/pull/873:0,Integrability,Depend,Depends,0,Depends on #867,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/873
https://github.com/hail-is/hail/pull/875:410,Modifiability,refactor,refactored,410,- copied files from hail.is repository to www/; - added navbar; - added bootstrap and jquery to repository; - added Tutorial; - added Overview page; - split docs into Introduction and Commands Reference; - moved Getting Started to it's own page; - modified gradle build step from `createDocs` to `createWebsite` (maintained `createDocs` for backwards compatibility); - website is assembled in `build/docs/`; - refactored Node code so one node script compiles all 4 docs pages; - iPython notebook is automatically compiled with R images using `notedown`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/875
https://github.com/hail-is/hail/issues/876:259,Performance,perform,performance,259,"While implementing IBD, I added two type-classes to make implementing a fuzzy-comparable slightly easier. I'm not sure this is the best approach. Unfortunately, we didn't heavily discuss this choice because IBD was moving into master rather quickly after the performance issues were resolved. The files are: [AbsoluteFuzzyComparable](https://github.com/hail-is/hail/blob/master/src/main/scala/org/broadinstitute/hail/utils/AbsoluteFuzzyComparable.scala) and [RelativeFuzzyComparable](https://github.com/hail-is/hail/blob/master/src/main/scala/org/broadinstitute/hail/utils/RelativeFuzzyComparable.scala). Absolute means `(x - y) < ε`. Relative means `(x - y) < max(abs(x), abs(y)) * ε` (i.e. `D_==`). Any type for which these type classes has been defined may be compared. For example:. ``` scala; (x: T, y: T) => AbsoluteFuzzyComparable.absoluteEq(epsilon, x, y); ```. I gave a type-class generator for `Map[K, V]` where `V` is comparable, so arbitrarily nested maps may be compared in this way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/876
https://github.com/hail-is/hail/issues/882:387,Testability,test,test,387,"https://issues.apache.org/jira/browse/SPARK-5077. https://gist.github.com/danking/6e61be32e9a31f6dba61c0a0f0240ca2. [This `collect` call triggers the large transfer](https://github.com/hail-is/hail/blob/master/src/main/scala/org/broadinstitute/hail/methods/ImputeSexPlink.scala#L72). Perhaps the sample ids are very large in this case? Perhaps too many variants passed the MAF threshold test? In this case, Maryam is using a MAF threshold of 0.05.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/882
https://github.com/hail-is/hail/pull/899:76,Availability,failure,failure,76,Don't construct intermediate string.; Print full exception on metadata load failure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/899
https://github.com/hail-is/hail/pull/899:71,Performance,load,load,71,Don't construct intermediate string.; Print full exception on metadata load failure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/899
https://github.com/hail-is/hail/issues/902:406,Testability,log,log,406,"Hi, i've been giving hail a first go today. It looks great, thanks. I've come across a problem. The worker nodes on our cluster only have 2GB `/tmp` dir which fills up on some hail operations. Using the `-t` flag doesn't help. E.g. ```; hail --tmpdir /local read $invds splitmulti write -o $outvds; ```. Will still fill the dir `/tmp/blockmgr-<uuid>/` and crash. Is there a simple solution to this?. [hail.log.txt](https://github.com/hail-is/hail/files/511675/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902
https://github.com/hail-is/hail/issues/902:465,Testability,log,log,465,"Hi, i've been giving hail a first go today. It looks great, thanks. I've come across a problem. The worker nodes on our cluster only have 2GB `/tmp` dir which fills up on some hail operations. Using the `-t` flag doesn't help. E.g. ```; hail --tmpdir /local read $invds splitmulti write -o $outvds; ```. Will still fill the dir `/tmp/blockmgr-<uuid>/` and crash. Is there a simple solution to this?. [hail.log.txt](https://github.com/hail-is/hail/files/511675/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902
https://github.com/hail-is/hail/issues/902:374,Usability,simpl,simple,374,"Hi, i've been giving hail a first go today. It looks great, thanks. I've come across a problem. The worker nodes on our cluster only have 2GB `/tmp` dir which fills up on some hail operations. Using the `-t` flag doesn't help. E.g. ```; hail --tmpdir /local read $invds splitmulti write -o $outvds; ```. Will still fill the dir `/tmp/blockmgr-<uuid>/` and crash. Is there a simple solution to this?. [hail.log.txt](https://github.com/hail-is/hail/files/511675/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902
https://github.com/hail-is/hail/pull/906:68,Testability,test,test,68,-added symEigD and symEigR in stats on Breeze matrices; -added unit test and speed test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906
https://github.com/hail-is/hail/pull/906:83,Testability,test,test,83,-added symEigD and symEigR in stats on Breeze matrices; -added unit test and speed test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/906
https://github.com/hail-is/hail/issues/913:36,Availability,error,error,36,What might be the issue as I had an error reported while running:. hail importvcf /user/jkoskela/ibd/vcf/v30_ibd_exomes.vcf.bgz splitmulti \. > write -o /user/jkoskela/ibd/hail/v30_split_ibd.vds; > hail: info: running: importvcf /user/jkoskela/ibd/vcf/v30_ibd_exomes.vcf.bgz; > [Stage 0:====================================================>(4569 + 1) / 4570]hail: info: Coerced sorted dataset; > hail: info: running: splitmulti; > hail: info: running: write -o /user/jkoskela/ibd/hail/v30_split_ibd.vds; > [Stage 2:> (0 + 162) / 4570]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. Log can be found in:. /humgen/atgu1/fs03/jkoskela/hail.log,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/913
https://github.com/hail-is/hail/issues/913:602,Safety,abort,aborted,602,What might be the issue as I had an error reported while running:. hail importvcf /user/jkoskela/ibd/vcf/v30_ibd_exomes.vcf.bgz splitmulti \. > write -o /user/jkoskela/ibd/hail/v30_split_ibd.vds; > hail: info: running: importvcf /user/jkoskela/ibd/vcf/v30_ibd_exomes.vcf.bgz; > [Stage 0:====================================================>(4569 + 1) / 4570]hail: info: Coerced sorted dataset; > hail: info: running: splitmulti; > hail: info: running: write -o /user/jkoskela/ibd/hail/v30_split_ibd.vds; > [Stage 2:> (0 + 162) / 4570]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. Log can be found in:. /humgen/atgu1/fs03/jkoskela/hail.log,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/913
https://github.com/hail-is/hail/issues/913:611,Testability,Log,Log,611,What might be the issue as I had an error reported while running:. hail importvcf /user/jkoskela/ibd/vcf/v30_ibd_exomes.vcf.bgz splitmulti \. > write -o /user/jkoskela/ibd/hail/v30_split_ibd.vds; > hail: info: running: importvcf /user/jkoskela/ibd/vcf/v30_ibd_exomes.vcf.bgz; > [Stage 0:====================================================>(4569 + 1) / 4570]hail: info: Coerced sorted dataset; > hail: info: running: splitmulti; > hail: info: running: write -o /user/jkoskela/ibd/hail/v30_split_ibd.vds; > [Stage 2:> (0 + 162) / 4570]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. Log can be found in:. /humgen/atgu1/fs03/jkoskela/hail.log,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/913
https://github.com/hail-is/hail/issues/913:666,Testability,log,log,666,What might be the issue as I had an error reported while running:. hail importvcf /user/jkoskela/ibd/vcf/v30_ibd_exomes.vcf.bgz splitmulti \. > write -o /user/jkoskela/ibd/hail/v30_split_ibd.vds; > hail: info: running: importvcf /user/jkoskela/ibd/vcf/v30_ibd_exomes.vcf.bgz; > [Stage 0:====================================================>(4569 + 1) / 4570]hail: info: Coerced sorted dataset; > hail: info: running: splitmulti; > hail: info: running: write -o /user/jkoskela/ibd/hail/v30_split_ibd.vds; > [Stage 2:> (0 + 162) / 4570]hail: write: caught exception: org.apache.spark.SparkException: Job aborted. Log can be found in:. /humgen/atgu1/fs03/jkoskela/hail.log,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/913
https://github.com/hail-is/hail/pull/915:117,Performance,cache,cache,117,"Don't presist in repartition/coalesce.; Fixed Int overflow bug in OrderedRDD.coalesce.; Convert to GenotypeStream in cache, persist.; Fixed filteralleles annotation bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/915
https://github.com/hail-is/hail/issues/918:13,Security,expose,expose,13,Just need to expose `InbreedingCombiner` in expr language. Needed for GnomAD QC. Related issue: https://github.com/hail-is/hail/issues/501,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/918
https://github.com/hail-is/hail/pull/921:309,Integrability,interface,interface,309,"@danking, I started playing with your ASM experiment and wrote a library for lightweight bytecode generation. The primary abstractions are `FunctionBuilder` and `Code[T]`. The latter is an object that can generate bytecode to produce a value of type `T` on the top of the stack. I'm reasonably happy with the interface, see this example for factorial:. https://github.com/cseed/hail/commit/93d95982bccd16ffa531f67fa47163f3fc8cbdde#diff-e434fa9004c38142a8f6f64ffa73b48eR109. No ClassBuilder yet. Apart from that, all the major features are there. There are a bunch of missing operations (type conversions, for example) and I only have wrapper classes for `Int` and `Double`. Once we fill it out I think it will make an excellent stand-alone library. While I optimized conditional generation to be smart about converting between indicator values (0, 1) and branch targets, it still emits some unnecessary GOTOs for fall through and could be improved. There are two double comparison bytecodes (DCMPG and DCMPL) that treat NAN differently. I wasn't sure which one to use. We should probably emulate Java/Scala. I can't tell if ASM is generating short bytecodes for load from small local indices (e.g. ILOAD_2) or small constants (e.g., ICONST_3). It isn't clear if the pretty printer that comes with ASM makes a distinction. We probably need to dump to a file and run `javap`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/921
https://github.com/hail-is/hail/pull/921:634,Integrability,wrap,wrapper,634,"@danking, I started playing with your ASM experiment and wrote a library for lightweight bytecode generation. The primary abstractions are `FunctionBuilder` and `Code[T]`. The latter is an object that can generate bytecode to produce a value of type `T` on the top of the stack. I'm reasonably happy with the interface, see this example for factorial:. https://github.com/cseed/hail/commit/93d95982bccd16ffa531f67fa47163f3fc8cbdde#diff-e434fa9004c38142a8f6f64ffa73b48eR109. No ClassBuilder yet. Apart from that, all the major features are there. There are a bunch of missing operations (type conversions, for example) and I only have wrapper classes for `Int` and `Double`. Once we fill it out I think it will make an excellent stand-alone library. While I optimized conditional generation to be smart about converting between indicator values (0, 1) and branch targets, it still emits some unnecessary GOTOs for fall through and could be improved. There are two double comparison bytecodes (DCMPG and DCMPL) that treat NAN differently. I wasn't sure which one to use. We should probably emulate Java/Scala. I can't tell if ASM is generating short bytecodes for load from small local indices (e.g. ILOAD_2) or small constants (e.g., ICONST_3). It isn't clear if the pretty printer that comes with ASM makes a distinction. We probably need to dump to a file and run `javap`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/921
https://github.com/hail-is/hail/pull/921:757,Performance,optimiz,optimized,757,"@danking, I started playing with your ASM experiment and wrote a library for lightweight bytecode generation. The primary abstractions are `FunctionBuilder` and `Code[T]`. The latter is an object that can generate bytecode to produce a value of type `T` on the top of the stack. I'm reasonably happy with the interface, see this example for factorial:. https://github.com/cseed/hail/commit/93d95982bccd16ffa531f67fa47163f3fc8cbdde#diff-e434fa9004c38142a8f6f64ffa73b48eR109. No ClassBuilder yet. Apart from that, all the major features are there. There are a bunch of missing operations (type conversions, for example) and I only have wrapper classes for `Int` and `Double`. Once we fill it out I think it will make an excellent stand-alone library. While I optimized conditional generation to be smart about converting between indicator values (0, 1) and branch targets, it still emits some unnecessary GOTOs for fall through and could be improved. There are two double comparison bytecodes (DCMPG and DCMPL) that treat NAN differently. I wasn't sure which one to use. We should probably emulate Java/Scala. I can't tell if ASM is generating short bytecodes for load from small local indices (e.g. ILOAD_2) or small constants (e.g., ICONST_3). It isn't clear if the pretty printer that comes with ASM makes a distinction. We probably need to dump to a file and run `javap`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/921
https://github.com/hail-is/hail/pull/921:1162,Performance,load,load,1162,"@danking, I started playing with your ASM experiment and wrote a library for lightweight bytecode generation. The primary abstractions are `FunctionBuilder` and `Code[T]`. The latter is an object that can generate bytecode to produce a value of type `T` on the top of the stack. I'm reasonably happy with the interface, see this example for factorial:. https://github.com/cseed/hail/commit/93d95982bccd16ffa531f67fa47163f3fc8cbdde#diff-e434fa9004c38142a8f6f64ffa73b48eR109. No ClassBuilder yet. Apart from that, all the major features are there. There are a bunch of missing operations (type conversions, for example) and I only have wrapper classes for `Int` and `Double`. Once we fill it out I think it will make an excellent stand-alone library. While I optimized conditional generation to be smart about converting between indicator values (0, 1) and branch targets, it still emits some unnecessary GOTOs for fall through and could be improved. There are two double comparison bytecodes (DCMPG and DCMPL) that treat NAN differently. I wasn't sure which one to use. We should probably emulate Java/Scala. I can't tell if ASM is generating short bytecodes for load from small local indices (e.g. ILOAD_2) or small constants (e.g., ICONST_3). It isn't clear if the pretty printer that comes with ASM makes a distinction. We probably need to dump to a file and run `javap`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/921
https://github.com/hail-is/hail/pull/921:1253,Usability,clear,clear,1253,"@danking, I started playing with your ASM experiment and wrote a library for lightweight bytecode generation. The primary abstractions are `FunctionBuilder` and `Code[T]`. The latter is an object that can generate bytecode to produce a value of type `T` on the top of the stack. I'm reasonably happy with the interface, see this example for factorial:. https://github.com/cseed/hail/commit/93d95982bccd16ffa531f67fa47163f3fc8cbdde#diff-e434fa9004c38142a8f6f64ffa73b48eR109. No ClassBuilder yet. Apart from that, all the major features are there. There are a bunch of missing operations (type conversions, for example) and I only have wrapper classes for `Int` and `Double`. Once we fill it out I think it will make an excellent stand-alone library. While I optimized conditional generation to be smart about converting between indicator values (0, 1) and branch targets, it still emits some unnecessary GOTOs for fall through and could be improved. There are two double comparison bytecodes (DCMPG and DCMPL) that treat NAN differently. I wasn't sure which one to use. We should probably emulate Java/Scala. I can't tell if ASM is generating short bytecodes for load from small local indices (e.g. ILOAD_2) or small constants (e.g., ICONST_3). It isn't clear if the pretty printer that comes with ASM makes a distinction. We probably need to dump to a file and run `javap`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/921
https://github.com/hail-is/hail/pull/922:29,Testability,test,tests,29,- added to expr doc; - added tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/922
https://github.com/hail-is/hail/issues/945:49,Modifiability,rewrite,rewrite,49,"E.g., replace spaces with underscores. Use regex rewrite.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/945
https://github.com/hail-is/hail/pull/953:87,Testability,test,tests,87,- added filters to LinearRegressionCommand; - added minAC to LinearRegression; - added tests to LinearRegressionSuite; - documented in linreg.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/953
https://github.com/hail-is/hail/pull/959:19,Usability,clear,clear,19,"Added set, get and clear commands.; join works against environment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/959
https://github.com/hail-is/hail/pull/963:4,Testability,benchmark,benchmarking,4,For benchmarking.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/963
https://github.com/hail-is/hail/pull/967:19,Testability,test,test,19,- renamed Utils in test/ to TestUtils; - added TestUtilsSuite and test of vdsFromMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/967
https://github.com/hail-is/hail/pull/967:28,Testability,Test,TestUtils,28,- renamed Utils in test/ to TestUtils; - added TestUtilsSuite and test of vdsFromMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/967
https://github.com/hail-is/hail/pull/967:47,Testability,Test,TestUtilsSuite,47,- renamed Utils in test/ to TestUtils; - added TestUtilsSuite and test of vdsFromMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/967
https://github.com/hail-is/hail/pull/967:66,Testability,test,test,66,- renamed Utils in test/ to TestUtils; - added TestUtilsSuite and test of vdsFromMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/967
https://github.com/hail-is/hail/issues/978:70,Availability,error,error,70,"v.alt on a multi-allelic variant currently crashed with the following error:; java.lang.IllegalArgumentException: requirement failed: called altAllele on a non-biallelic variant. The error message could be improved (""called alt / altAllele on a ...""). Or maybe v.alt on a multi-allelic variant could return a comma-delimited string of alt alleles?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/978
https://github.com/hail-is/hail/issues/978:183,Availability,error,error,183,"v.alt on a multi-allelic variant currently crashed with the following error:; java.lang.IllegalArgumentException: requirement failed: called altAllele on a non-biallelic variant. The error message could be improved (""called alt / altAllele on a ...""). Or maybe v.alt on a multi-allelic variant could return a comma-delimited string of alt alleles?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/978
https://github.com/hail-is/hail/issues/978:189,Integrability,message,message,189,"v.alt on a multi-allelic variant currently crashed with the following error:; java.lang.IllegalArgumentException: requirement failed: called altAllele on a non-biallelic variant. The error message could be improved (""called alt / altAllele on a ...""). Or maybe v.alt on a multi-allelic variant could return a comma-delimited string of alt alleles?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/978
https://github.com/hail-is/hail/pull/979:8,Deployability,integrat,integrating,8,Not for integrating in the current state.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/979
https://github.com/hail-is/hail/pull/979:8,Integrability,integrat,integrating,8,Not for integrating in the current state.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/979
https://github.com/hail-is/hail/issues/1003:425,Availability,ERROR,ERROR,425,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:881,Availability,failure,failure,881,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:938,Availability,failure,failure,938,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:1082,Availability,error,error,1082,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:1295,Availability,error,error,1295,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:781,Deployability,deploy,deploy,781,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:1163,Deployability,deploy,deploy,1163,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:40,Modifiability,config,configured,40,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:740,Safety,abort,aborted,740,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:860,Safety,abort,aborted,860,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:55,Testability,log,log,55,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:276,Testability,log,log-file,276,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1003:301,Testability,log,log,301,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1003
https://github.com/hail-is/hail/issues/1006:64,Testability,log,logo,64,Style of <ls> elements in collapsed state are not correct. Hail logo should be centered.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1006
https://github.com/hail-is/hail/issues/1010:15,Performance,load,load,15,"Hail failed to load VCF with haploid calls, such as those on chr Y or chrX nonPar male regions. According to VCF spec v4.2, those haploid calls should be represented as 0 or 1 instead of 0/0, or 1/1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1010
https://github.com/hail-is/hail/pull/1011:181,Deployability,integrat,integrate,181,"Combined with vdsFromMatrix, this allows Alex to generate a vds and then vcf with a proscribed population structure as needed for methods dev. Later we can move this to main and/or integrate into our generative testing framework.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1011
https://github.com/hail-is/hail/pull/1011:181,Integrability,integrat,integrate,181,"Combined with vdsFromMatrix, this allows Alex to generate a vds and then vcf with a proscribed population structure as needed for methods dev. Later we can move this to main and/or integrate into our generative testing framework.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1011
https://github.com/hail-is/hail/pull/1011:211,Testability,test,testing,211,"Combined with vdsFromMatrix, this allows Alex to generate a vds and then vcf with a proscribed population structure as needed for methods dev. Later we can move this to main and/or integrate into our generative testing framework.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1011
https://github.com/hail-is/hail/pull/1012:2,Deployability,Update,Updated,2,- Updated key table case class; - All commands are hidden for now:; - `WriteKeyTable`: Write key table to parquet file; - `ReadKeyTable`: Read key table from parquet file and put in env; - `ExportKeyTable`: Write key table to tsp file; - `AddKeyTable`: Creates a new key table from transformations of a VDS. Names of commands are a work in progress...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1012
https://github.com/hail-is/hail/pull/1015:17,Testability,Test,TestUtils,17,- added these to TestUtils; - added tests in TestUtilsSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1015
https://github.com/hail-is/hail/pull/1015:36,Testability,test,tests,36,- added these to TestUtils; - added tests in TestUtilsSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1015
https://github.com/hail-is/hail/pull/1015:45,Testability,Test,TestUtilsSuite,45,- added these to TestUtils; - added tests in TestUtilsSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1015
https://github.com/hail-is/hail/issues/1017:29,Availability,error,error,29,"Hi - I receive the following error when running the `variantqc` example from the [Getting Started documentation](https://hail.is/getting_started.html):. `$ ./build/install/hail/bin/hail read ~/sample.vds splitmulti variantqc -o ~/variantqc.tsv sampleqc -o ~/sampleqc.tsv; `. `hail: fatal: variantqc: parse error: ""-o"" is not a valid option`. Leaving `variantqc` out runs without error and generates the sampleqc output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1017
https://github.com/hail-is/hail/issues/1017:306,Availability,error,error,306,"Hi - I receive the following error when running the `variantqc` example from the [Getting Started documentation](https://hail.is/getting_started.html):. `$ ./build/install/hail/bin/hail read ~/sample.vds splitmulti variantqc -o ~/variantqc.tsv sampleqc -o ~/sampleqc.tsv; `. `hail: fatal: variantqc: parse error: ""-o"" is not a valid option`. Leaving `variantqc` out runs without error and generates the sampleqc output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1017
https://github.com/hail-is/hail/issues/1017:379,Availability,error,error,379,"Hi - I receive the following error when running the `variantqc` example from the [Getting Started documentation](https://hail.is/getting_started.html):. `$ ./build/install/hail/bin/hail read ~/sample.vds splitmulti variantqc -o ~/variantqc.tsv sampleqc -o ~/sampleqc.tsv; `. `hail: fatal: variantqc: parse error: ""-o"" is not a valid option`. Leaving `variantqc` out runs without error and generates the sampleqc output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1017
https://github.com/hail-is/hail/issues/1017:164,Deployability,install,install,164,"Hi - I receive the following error when running the `variantqc` example from the [Getting Started documentation](https://hail.is/getting_started.html):. `$ ./build/install/hail/bin/hail read ~/sample.vds splitmulti variantqc -o ~/variantqc.tsv sampleqc -o ~/sampleqc.tsv; `. `hail: fatal: variantqc: parse error: ""-o"" is not a valid option`. Leaving `variantqc` out runs without error and generates the sampleqc output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1017
https://github.com/hail-is/hail/issues/1022:259,Availability,error,error,259,"To avoid classname serialization overhead, we should register all of our classes that are being serialized, with the most important being those which are RDD elements, until we are able to turn on `conf.set(""spark.kryo.registrationRequired"", ""true"")` without error. This will also catch places where we are serializing far more than we thought. Background here:; https://spark.apache.org/docs/latest/tuning.html#data-serialization. Only a small number of classes are registered by default:; https://github.com/apache/spark/blob/v1.4.0/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala#L317. More background in first answer here:; http://stackoverflow.com/questions/31394140/require-kryo-serialization-in-spark-scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1022
https://github.com/hail-is/hail/issues/1022:3,Safety,avoid,avoid,3,"To avoid classname serialization overhead, we should register all of our classes that are being serialized, with the most important being those which are RDD elements, until we are able to turn on `conf.set(""spark.kryo.registrationRequired"", ""true"")` without error. This will also catch places where we are serializing far more than we thought. Background here:; https://spark.apache.org/docs/latest/tuning.html#data-serialization. Only a small number of classes are registered by default:; https://github.com/apache/spark/blob/v1.4.0/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala#L317. More background in first answer here:; http://stackoverflow.com/questions/31394140/require-kryo-serialization-in-spark-scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1022
https://github.com/hail-is/hail/issues/1024:44,Performance,load,load,44,Fix this issue:. ```; XMLHttpRequest cannot load https://github.com/hail-is/hail/blob/master/www/navbar.html. Origin http://discuss.hail.is is not allowed by Access-Control-Allow-Origin.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1024
https://github.com/hail-is/hail/issues/1024:158,Security,Access,Access-Control-Allow-Origin,158,Fix this issue:. ```; XMLHttpRequest cannot load https://github.com/hail-is/hail/blob/master/www/navbar.html. Origin http://discuss.hail.is is not allowed by Access-Control-Allow-Origin.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1024
https://github.com/hail-is/hail/pull/1026:92,Testability,test,test,92,- based on Spark 2.0; - converts directly without passing through CoordinateMatrix; - added test to UtilsSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1026
https://github.com/hail-is/hail/pull/1028:45,Testability,log,logged,45,- number of partitions and storage level are logged for each command that requires vds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1028
https://github.com/hail-is/hail/pull/1029:186,Testability,test,test,186,"- created BaldingNicholsCommand, and registered command, currently hidden; - created BaldingNicholsModel to generate model and convert to annotated vds; - created BaldingNicholsSuite to test that seed is working properly; - started baldingnichols.md; - added parseCommaDelimitedDoubles and comma_delimited_doubles to Parser; - moved vdsFromMatrix from TestUtils to stats package, set wasSplit=true, added optional samplesIds parameter; - moved vdsFromMatrixTest from TestUtilsSuite to StatsSuite; - added VariantMetadata constructor with wasSplit option",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1029
https://github.com/hail-is/hail/pull/1029:352,Testability,Test,TestUtils,352,"- created BaldingNicholsCommand, and registered command, currently hidden; - created BaldingNicholsModel to generate model and convert to annotated vds; - created BaldingNicholsSuite to test that seed is working properly; - started baldingnichols.md; - added parseCommaDelimitedDoubles and comma_delimited_doubles to Parser; - moved vdsFromMatrix from TestUtils to stats package, set wasSplit=true, added optional samplesIds parameter; - moved vdsFromMatrixTest from TestUtilsSuite to StatsSuite; - added VariantMetadata constructor with wasSplit option",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1029
https://github.com/hail-is/hail/pull/1029:467,Testability,Test,TestUtilsSuite,467,"- created BaldingNicholsCommand, and registered command, currently hidden; - created BaldingNicholsModel to generate model and convert to annotated vds; - created BaldingNicholsSuite to test that seed is working properly; - started baldingnichols.md; - added parseCommaDelimitedDoubles and comma_delimited_doubles to Parser; - moved vdsFromMatrix from TestUtils to stats package, set wasSplit=true, added optional samplesIds parameter; - moved vdsFromMatrixTest from TestUtilsSuite to StatsSuite; - added VariantMetadata constructor with wasSplit option",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1029
https://github.com/hail-is/hail/issues/1055:24,Testability,test,tests,24,"The current TDT command tests for transmission disequilibrium for each variant across a number of trios. However, it would be helpful to get transmission information on each parent-proband trio as well, similar to how Mendelian-inconsistent variants are identified on Hail. Would this be possible?; Thanks :)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1055
https://github.com/hail-is/hail/pull/1061:26,Integrability,interface,interface,26,"Initial version of python interface. Still need to:. - document python interface (working with @jigold about the best way to do that); - add tests run through gradle. The interface mostly wraps commands. The main difference is that python has first class VDS objects, so the environment isn't necessary. Therefore, I had to restructure commands that take VDS names as arguments. It can be run like this:. ```; $ gradle shadowJar; $ PYTHONPATH=/path/to/hail/python SPARK_CLASSPATH=/path/to/hail/build/libs/hail-all-spark.jar pyspark; ```. Here's a simple example:. ```; >>> from pyhail import *; >>> hc = HailContext(sc) # create Hail context; >>> vds = hc.import_vcf('/Users/cseed/sample.vcf', n_partitions = 8); >>> vds.count(); {u'nSample': 100, u'nVariants': 346L, u'nGenotypes': 34600L}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1061
https://github.com/hail-is/hail/pull/1061:71,Integrability,interface,interface,71,"Initial version of python interface. Still need to:. - document python interface (working with @jigold about the best way to do that); - add tests run through gradle. The interface mostly wraps commands. The main difference is that python has first class VDS objects, so the environment isn't necessary. Therefore, I had to restructure commands that take VDS names as arguments. It can be run like this:. ```; $ gradle shadowJar; $ PYTHONPATH=/path/to/hail/python SPARK_CLASSPATH=/path/to/hail/build/libs/hail-all-spark.jar pyspark; ```. Here's a simple example:. ```; >>> from pyhail import *; >>> hc = HailContext(sc) # create Hail context; >>> vds = hc.import_vcf('/Users/cseed/sample.vcf', n_partitions = 8); >>> vds.count(); {u'nSample': 100, u'nVariants': 346L, u'nGenotypes': 34600L}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1061
https://github.com/hail-is/hail/pull/1061:171,Integrability,interface,interface,171,"Initial version of python interface. Still need to:. - document python interface (working with @jigold about the best way to do that); - add tests run through gradle. The interface mostly wraps commands. The main difference is that python has first class VDS objects, so the environment isn't necessary. Therefore, I had to restructure commands that take VDS names as arguments. It can be run like this:. ```; $ gradle shadowJar; $ PYTHONPATH=/path/to/hail/python SPARK_CLASSPATH=/path/to/hail/build/libs/hail-all-spark.jar pyspark; ```. Here's a simple example:. ```; >>> from pyhail import *; >>> hc = HailContext(sc) # create Hail context; >>> vds = hc.import_vcf('/Users/cseed/sample.vcf', n_partitions = 8); >>> vds.count(); {u'nSample': 100, u'nVariants': 346L, u'nGenotypes': 34600L}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1061
https://github.com/hail-is/hail/pull/1061:188,Integrability,wrap,wraps,188,"Initial version of python interface. Still need to:. - document python interface (working with @jigold about the best way to do that); - add tests run through gradle. The interface mostly wraps commands. The main difference is that python has first class VDS objects, so the environment isn't necessary. Therefore, I had to restructure commands that take VDS names as arguments. It can be run like this:. ```; $ gradle shadowJar; $ PYTHONPATH=/path/to/hail/python SPARK_CLASSPATH=/path/to/hail/build/libs/hail-all-spark.jar pyspark; ```. Here's a simple example:. ```; >>> from pyhail import *; >>> hc = HailContext(sc) # create Hail context; >>> vds = hc.import_vcf('/Users/cseed/sample.vcf', n_partitions = 8); >>> vds.count(); {u'nSample': 100, u'nVariants': 346L, u'nGenotypes': 34600L}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1061
https://github.com/hail-is/hail/pull/1061:141,Testability,test,tests,141,"Initial version of python interface. Still need to:. - document python interface (working with @jigold about the best way to do that); - add tests run through gradle. The interface mostly wraps commands. The main difference is that python has first class VDS objects, so the environment isn't necessary. Therefore, I had to restructure commands that take VDS names as arguments. It can be run like this:. ```; $ gradle shadowJar; $ PYTHONPATH=/path/to/hail/python SPARK_CLASSPATH=/path/to/hail/build/libs/hail-all-spark.jar pyspark; ```. Here's a simple example:. ```; >>> from pyhail import *; >>> hc = HailContext(sc) # create Hail context; >>> vds = hc.import_vcf('/Users/cseed/sample.vcf', n_partitions = 8); >>> vds.count(); {u'nSample': 100, u'nVariants': 346L, u'nGenotypes': 34600L}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1061
https://github.com/hail-is/hail/pull/1061:547,Usability,simpl,simple,547,"Initial version of python interface. Still need to:. - document python interface (working with @jigold about the best way to do that); - add tests run through gradle. The interface mostly wraps commands. The main difference is that python has first class VDS objects, so the environment isn't necessary. Therefore, I had to restructure commands that take VDS names as arguments. It can be run like this:. ```; $ gradle shadowJar; $ PYTHONPATH=/path/to/hail/python SPARK_CLASSPATH=/path/to/hail/build/libs/hail-all-spark.jar pyspark; ```. Here's a simple example:. ```; >>> from pyhail import *; >>> hc = HailContext(sc) # create Hail context; >>> vds = hc.import_vcf('/Users/cseed/sample.vcf', n_partitions = 8); >>> vds.count(); {u'nSample': 100, u'nVariants': 346L, u'nGenotypes': 34600L}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1061
https://github.com/hail-is/hail/pull/1062:15,Testability,test,test,15,Added Write to test to make sure annotation types are correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1062
https://github.com/hail-is/hail/pull/1064:796,Testability,log,log,796,"Timing on hardcall version of 1kg genomes (2535 samples, 96k variants in kernel, 15.3 million variants in assoc) is 6 minutes, using 344 cores on 1000 partitions (1m on kernel, 4m on assoc). In this run, I fixed delta at 1000.0 because I'm using a random phenotype that results in fit $delta$ being negligible (outside the search range, where linear regression becomes more appropriate). But the ""fitting delta"" step post eigen-decomposition is negligible (~ 1s). ```; gcloud dataproc clusters create cluster-jb --zone us-central1-f --master-machine-type n1-standard-16 --master-boot-disk-size 100 --num-workers 2 --num-preemptible-workers 20 --worker-machine-type n1-standard-16 --worker-boot-disk-size 100 --image-version 1.0 --project broad-ctsa. gcp-hail-lmm1 cluster-jb -l /home/jbloom/hail.log \; read -i gs://hail-1kg/ALL.1KG.qc.hardcalls.p1000.vds \; filtervariants expr --keep -c 'va.qc.callRate > .98 && va.qc.AF > 0.005' \; lmmreg \; -d 1000.0 \; -y sa.pheno \; -c sa.isFemale \; -k 'v.start % 100 == 0 && va.qc.AF > 0.02 && va.qc.AF < 0.98' \; showglobals \; -o gs://jbloom/lmmreg.1kg.globals.json \; exportvariants \; -c 'variant = v, pval = va.lmmreg.pval, beta = va.lmmreg.beta, sigmaG2 = va.lmmreg.sigmaG2, chi2 = va.lmmreg.chi2, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' \; -o gs://jbloom/lmmreg.1kg.tsv.bgz. merge time: 12.084s; hail: info: timing:; read: 55.305s; filtervariants expr: 162.868ms; lmmreg: 56.056s; showglobals: 693.315ms; exportvariants: 4m10.4s; total: 6m2.6s; ```. Timing on 2000 partitions, pre-filtered, 680 cores is 3m42s.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1064
https://github.com/hail-is/hail/issues/1065:189,Availability,ERROR,ERROR,189,"I have a GATK-generated VCF that has been passed through vt normalize. During the vt stage, a ""NaN"" value in the VCF was changed to ""nan"", which causes write to fail:. `2016-11-07 15:13:31 ERROR Hail:101 - hail: fatal: write: file:###.vcf.bgz: variant 5:49429187:G:C,T: INFO field InbreedingCoeff:; unable to convert nan (of class java.lang.String) to Double:; caught java.lang.NumberFormatException: For input string: ""nan""; offending line: 5 49429187 rs59402528 G C,T 2.28455e+07 VQSRTrancheSNP99.90t...`. The original ""NaN"" value is processed fine. As the VCF spec is pretty quiet on exact floating point representation in VCFs, could code to handle ""nan"" be added?. Apologies for edit spam -- clumsy fingers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1065
https://github.com/hail-is/hail/issues/1067:192,Integrability,interface,interface,192,The map* functions can probably be deleted. I don't think folds are used anymore. We grab `rdd` a lot. Is that because our API is bad or because additional API isn't necessary because the RDD interface is good?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1067
https://github.com/hail-is/hail/issues/1072:218,Availability,error,error,218,"- [ ] annotate with mypy; - [ ] add pylint to build; - [x] add basic unit tests; - [x] docs in python; - [x] tutorial in python; - [ ] run tests on dataproc; - [x] delete commands, state, no args4j; - [ ] accumulators/error reporting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1072
https://github.com/hail-is/hail/issues/1072:74,Testability,test,tests,74,"- [ ] annotate with mypy; - [ ] add pylint to build; - [x] add basic unit tests; - [x] docs in python; - [x] tutorial in python; - [ ] run tests on dataproc; - [x] delete commands, state, no args4j; - [ ] accumulators/error reporting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1072
https://github.com/hail-is/hail/issues/1072:139,Testability,test,tests,139,"- [ ] annotate with mypy; - [ ] add pylint to build; - [x] add basic unit tests; - [x] docs in python; - [x] tutorial in python; - [ ] run tests on dataproc; - [x] delete commands, state, no args4j; - [ ] accumulators/error reporting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1072
https://github.com/hail-is/hail/pull/1080:27,Usability,clear,clear,27,I think this is a bit more clear about Hail JARs and gives a working snippet of code (for version 1.6.2).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1080
https://github.com/hail-is/hail/pull/1081:50,Testability,test,tests,50,@tpoterba I think I've finally resolved it. Final tests are running now on the Cray.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1081
https://github.com/hail-is/hail/issues/1082:694,Performance,perform,perform,694,"Bit packing is not limited to biallelic datasets. In the biallelic case, we can represent the three states as:. | state | bits |; | ----- | -----|; | homRef | `0`/`0` |; | het | `0`/`1` |; | homVar | `1`/`1` |. We can play the same trick with two bits per allele:. | state | bits |; | --- | --- |; | A/A | `00`/`00` |; | A/G | `00`/`01` |; | A/C | `00`/`10` |; | A/T | `00`/`11` |; | | |; | G/G | `01`/`01` |; | G/C | `01`/`10` |; | G/T | `01`/`11` |; | | |; | C/C | `10`/`10` |; | C/T | `10`/`11` |; | | |; | T/T | `11`/`11` |; | | |; | NA | `01`/`00` or `10`/`00` or `11`/`00` |. We can't play the same trick with three bits per allele (six per genotype) because the SSE/AVX registers cannot perform the shift appropriately for non-byte-aligned data. So the next step would be four bits per allele (eight per genotype) which allows 16 total alleles. For each bit-count, the algorithm is exactly the same, but with shifts changed. We should write a little DSL for specifying these bit operations which automatically generates C code for various allele counts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1082
https://github.com/hail-is/hail/pull/1088:55,Integrability,depend,dependencies,55,"Rely on versions of math3 and lang3 pulled in by other dependencies,; e.g., Spark.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1088
https://github.com/hail-is/hail/pull/1090:48,Integrability,depend,dependency,48,Added python tests. They run from gradle with a dependency on the task test.; Fixed numerous python bugs. Added missing filter_genotypes.; Removed famsummary command.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1090
https://github.com/hail-is/hail/pull/1090:13,Testability,test,tests,13,Added python tests. They run from gradle with a dependency on the task test.; Fixed numerous python bugs. Added missing filter_genotypes.; Removed famsummary command.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1090
https://github.com/hail-is/hail/pull/1090:71,Testability,test,test,71,Added python tests. They run from gradle with a dependency on the task test.; Fixed numerous python bugs. Added missing filter_genotypes.; Removed famsummary command.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1090
https://github.com/hail-is/hail/pull/1092:161,Availability,avail,available,161,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092
https://github.com/hail-is/hail/pull/1092:808,Deployability,install,installDist,808,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092
https://github.com/hail-is/hail/pull/1092:308,Performance,perform,performance,308,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092
https://github.com/hail-is/hail/pull/1092:800,Testability,test,test,800,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092
https://github.com/hail-is/hail/pull/1092:934,Testability,test,tests,934,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092
https://github.com/hail-is/hail/pull/1092:983,Testability,test,test,983,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1092
https://github.com/hail-is/hail/issues/1096:1491,Availability,error,error,1491,"ommand(None, pargs); 380 ; 381 def index_bgen(self, path):. /Users/tpoterba/hail/python/pyhail/context.pyc in run_command(self, vds, pargs); 43 jstate = self._jstate(vds.jvds if vds != None else None); 44 result = cmd.run(jstate,; ---> 45 cmd_args); 46 return VariantDataset(self, result.vds()); 47 . /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 811 answer = self.gateway_client.send_command(command); 812 return_value = get_return_value(; --> 813 answer, self.gateway_client, self.target_id, self.name); 814 ; 815 for temp_arg in temp_args:. /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/pyspark/sql/utils.pyc in deco(*a, **kw); 43 def deco(*a, **kw):; 44 try:; ---> 45 return f(*a, **kw); 46 except py4j.protocol.Py4JJavaError as e:; 47 s = e.java_exception.toString(). /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 306 raise Py4JJavaError(; 307 ""An error occurred while calling {0}{1}{2}.\n"".; --> 308 format(target_id, ""."", name), value); 309 else:; 310 raise Py4JError(. Py4JJavaError: An error occurred while calling o92.run.; : org.broadinstitute.hail.utils.package$FatalException: arguments refer to no files; 	at org.broadinstitute.hail.utils.package$.fatal(package.scala:27); 	at org.broadinstitute.hail.driver.VCFImporter$class.globAllVcfs(ImportVCF.scala:17); 	at org.broadinstitute.hail.driver.ImportVCF$.globAllVcfs(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:83); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:258); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:263); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1096
https://github.com/hail-is/hail/issues/1096:1633,Availability,error,error,1633,"ds != None else None); 44 result = cmd.run(jstate,; ---> 45 cmd_args); 46 return VariantDataset(self, result.vds()); 47 . /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 811 answer = self.gateway_client.send_command(command); 812 return_value = get_return_value(; --> 813 answer, self.gateway_client, self.target_id, self.name); 814 ; 815 for temp_arg in temp_args:. /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/pyspark/sql/utils.pyc in deco(*a, **kw); 43 def deco(*a, **kw):; 44 try:; ---> 45 return f(*a, **kw); 46 except py4j.protocol.Py4JJavaError as e:; 47 s = e.java_exception.toString(). /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 306 raise Py4JJavaError(; 307 ""An error occurred while calling {0}{1}{2}.\n"".; --> 308 format(target_id, ""."", name), value); 309 else:; 310 raise Py4JError(. Py4JJavaError: An error occurred while calling o92.run.; : org.broadinstitute.hail.utils.package$FatalException: arguments refer to no files; 	at org.broadinstitute.hail.utils.package$.fatal(package.scala:27); 	at org.broadinstitute.hail.driver.VCFImporter$class.globAllVcfs(ImportVCF.scala:17); 	at org.broadinstitute.hail.driver.ImportVCF$.globAllVcfs(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:83); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:258); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:263); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231); 	at py4j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1096
https://github.com/hail-is/hail/issues/1096:1242,Integrability,protocol,protocol,1242,"erba/hail/python/pyhail/context.pyc in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad); 377 pargs.append('--store-gq'); 378 ; --> 379 return self.run_command(None, pargs); 380 ; 381 def index_bgen(self, path):. /Users/tpoterba/hail/python/pyhail/context.pyc in run_command(self, vds, pargs); 43 jstate = self._jstate(vds.jvds if vds != None else None); 44 result = cmd.run(jstate,; ---> 45 cmd_args); 46 return VariantDataset(self, result.vds()); 47 . /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 811 answer = self.gateway_client.send_command(command); 812 return_value = get_return_value(; --> 813 answer, self.gateway_client, self.target_id, self.name); 814 ; 815 for temp_arg in temp_args:. /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/pyspark/sql/utils.pyc in deco(*a, **kw); 43 def deco(*a, **kw):; 44 try:; ---> 45 return f(*a, **kw); 46 except py4j.protocol.Py4JJavaError as e:; 47 s = e.java_exception.toString(). /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 306 raise Py4JJavaError(; 307 ""An error occurred while calling {0}{1}{2}.\n"".; --> 308 format(target_id, ""."", name), value); 309 else:; 310 raise Py4JError(. Py4JJavaError: An error occurred while calling o92.run.; : org.broadinstitute.hail.utils.package$FatalException: arguments refer to no files; 	at org.broadinstitute.hail.utils.package$.fatal(package.scala:27); 	at org.broadinstitute.hail.driver.VCFImporter$class.globAllVcfs(ImportVCF.scala:17); 	at org.broadinstitute.hail.driver.ImportVCF$.globAllVcfs(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:83); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:258); 	at org.broadinstitute.hail.driver.Comman",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1096
https://github.com/hail-is/hail/issues/1096:1383,Integrability,protocol,protocol,1383," skip_bad_ad); 377 pargs.append('--store-gq'); 378 ; --> 379 return self.run_command(None, pargs); 380 ; 381 def index_bgen(self, path):. /Users/tpoterba/hail/python/pyhail/context.pyc in run_command(self, vds, pargs); 43 jstate = self._jstate(vds.jvds if vds != None else None); 44 result = cmd.run(jstate,; ---> 45 cmd_args); 46 return VariantDataset(self, result.vds()); 47 . /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py in __call__(self, *args); 811 answer = self.gateway_client.send_command(command); 812 return_value = get_return_value(; --> 813 answer, self.gateway_client, self.target_id, self.name); 814 ; 815 for temp_arg in temp_args:. /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/pyspark/sql/utils.pyc in deco(*a, **kw); 43 def deco(*a, **kw):; 44 try:; ---> 45 return f(*a, **kw); 46 except py4j.protocol.Py4JJavaError as e:; 47 s = e.java_exception.toString(). /Users/tpoterba/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 306 raise Py4JJavaError(; 307 ""An error occurred while calling {0}{1}{2}.\n"".; --> 308 format(target_id, ""."", name), value); 309 else:; 310 raise Py4JError(. Py4JJavaError: An error occurred while calling o92.run.; : org.broadinstitute.hail.utils.package$FatalException: arguments refer to no files; 	at org.broadinstitute.hail.utils.package$.fatal(package.scala:27); 	at org.broadinstitute.hail.driver.VCFImporter$class.globAllVcfs(ImportVCF.scala:17); 	at org.broadinstitute.hail.driver.ImportVCF$.globAllVcfs(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:83); 	at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:33); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:258); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:263); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1096
https://github.com/hail-is/hail/pull/1100:44,Modifiability,Refactor,Refactored,44,"- Incorporated navbar into Sphinx output; - Refactored CSS; - Added some formatting of Sphinx output. To-Do:; Once this branch is merged, we need to modify the Discourse copy of navbar.html",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1100
https://github.com/hail-is/hail/pull/1103:11,Integrability,interface,interface,11,"- KeyTable interface -- RDD[(Annotation, Annotation)]; - Python bindings; - Aggregator serialization fix!!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1103
https://github.com/hail-is/hail/issues/1105:140,Testability,log,log,140,"Everytime we write a vds could we (optionally, though probably default on) also append all the commands that led to that write to a `global.log`?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1105
https://github.com/hail-is/hail/issues/1107:665,Availability,error,error,665,"```; Traceback (most recent call last):; File ""<stdin>"", line 5, in <module>; File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/dataset.py"", line 77, in annotate_samples_expr; return self.hc.run_command(self, pargs); File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:765,Availability,error,error,765,"```; Traceback (most recent call last):; File ""<stdin>"", line 5, in <module>; File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/dataset.py"", line 77, in annotate_samples_expr; return self.hc.run_command(self, pargs); File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:792,Energy Efficiency,schedul,scheduler,792,"```; Traceback (most recent call last):; File ""<stdin>"", line 5, in <module>; File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/dataset.py"", line 77, in annotate_samples_expr; return self.hc.run_command(self, pargs); File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:882,Energy Efficiency,schedul,scheduler,882,"```; Traceback (most recent call last):; File ""<stdin>"", line 5, in <module>; File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/dataset.py"", line 77, in annotate_samples_expr; return self.hc.run_command(self, pargs); File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:979,Energy Efficiency,schedul,scheduler,979,"```; Traceback (most recent call last):; File ""<stdin>"", line 5, in <module>; File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/dataset.py"", line 77, in annotate_samples_expr; return self.hc.run_command(self, pargs); File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:1074,Energy Efficiency,schedul,scheduler,1074,"ile ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/dataset.py"", line 77, in annotate_samples_expr; return self.hc.run_command(self, pargs); File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:1237,Energy Efficiency,schedul,scheduler,1237,"ba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1095); at org.broadinstitute.hail.methods.Aggregators$.buildSampleAggregations(Aggregators.scala:66); at org.broadinstitute.hail.d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:1461,Energy Efficiency,reduce,reduce,1461,"n/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1095); at org.broadinstitute.hail.methods.Aggregators$.buildSampleAggregations(Aggregators.scala:66); at org.broadinstitute.hail.driver.AnnotateSamplesExpr$.run(AnnotateSamplesExpr.scala:63); at org.broadinstitute.hail.driver.AnnotateSamplesExpr$.run(AnnotateSamplesExpr.scala:11); at org.broadinstitute.hail.driver.Command.runCommand(Command.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:1741,Energy Efficiency,reduce,reduce,1741,ception: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1095); at org.broadinstitute.hail.methods.Aggregators$.buildSampleAggregations(Aggregators.scala:66); at org.broadinstitute.hail.driver.AnnotateSamplesExpr$.run(AnnotateSamplesExpr.scala:63); at org.broadinstitute.hail.driver.AnnotateSamplesExpr$.run(AnnotateSamplesExpr.scala:11); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:258); at org.broadinstitute.hail.driver.Command.run(Command.scala:263); at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:3963,Energy Efficiency,reduce,reduce,3963,ct.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379); at py4j.Gateway.invoke(Gateway.java:259); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:207); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NullPointerException; at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:348); at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:317); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(Aggregators.scala:85); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1.apply$mcVI$sp(Aggregators.scala:83); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:83); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:82); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1002); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:999); at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:4044,Energy Efficiency,reduce,reduce,4044,ct.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379); at py4j.Gateway.invoke(Gateway.java:259); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:207); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NullPointerException; at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:348); at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:317); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(Aggregators.scala:85); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1.apply$mcVI$sp(Aggregators.scala:83); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:83); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:82); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1002); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:999); at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:4107,Energy Efficiency,schedul,scheduler,4107,ct.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379); at py4j.Gateway.invoke(Gateway.java:259); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:207); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NullPointerException; at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:348); at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:317); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(Aggregators.scala:85); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1.apply$mcVI$sp(Aggregators.scala:83); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:83); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:82); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1002); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:999); at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:4182,Energy Efficiency,schedul,scheduler,4182,ct.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379); at py4j.Gateway.invoke(Gateway.java:259); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:207); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NullPointerException; at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:348); at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:317); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(Aggregators.scala:85); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1.apply$mcVI$sp(Aggregators.scala:83); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:83); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:82); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1002); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:999); at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:4272,Energy Efficiency,schedul,scheduler,4272,ct.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379); at py4j.Gateway.invoke(Gateway.java:259); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:207); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NullPointerException; at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:348); at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:317); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(Aggregators.scala:85); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1.apply$mcVI$sp(Aggregators.scala:83); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:83); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:82); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1002); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:999); at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:4369,Energy Efficiency,schedul,scheduler,4369,ct.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379); at py4j.Gateway.invoke(Gateway.java:259); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:207); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NullPointerException; at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:348); at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:317); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(Aggregators.scala:85); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1.apply$mcVI$sp(Aggregators.scala:83); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:83); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:82); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1002); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:999); at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:4464,Energy Efficiency,schedul,scheduler,4464,ct.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379); at py4j.Gateway.invoke(Gateway.java:259); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:207); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NullPointerException; at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:348); at org.broadinstitute.hail.methods.SumArrayAggregator.combOp(Aggregators.scala:317); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(Aggregators.scala:85); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6$$anonfun$apply$1.apply$mcVI$sp(Aggregators.scala:83); at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:83); at org.broadinstitute.hail.methods.Aggregators$$anonfun$6.apply(Aggregators.scala:82); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1002); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:999); at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:588,Integrability,protocol,protocol,588,"```; Traceback (most recent call last):; File ""<stdin>"", line 5, in <module>; File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/dataset.py"", line 77, in annotate_samples_expr; return self.hc.run_command(self, pargs); File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/issues/1107:638,Integrability,protocol,protocol,638,"```; Traceback (most recent call last):; File ""<stdin>"", line 5, in <module>; File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/dataset.py"", line 77, in annotate_samples_expr; return self.hc.run_command(self, pargs); File ""/mnt/lustre/tpoterba/hail-inst/python/pyhail/context.py"", line 45, in run_command; cmd_args); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__; File ""/opt/spark/spark-1.5.2/python/pyspark/sql/utils.py"", line 36, in deco; return f(*a, **kw); File ""/opt/spark/spark-1.5.2/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o417.run.; : org.apache.spark.SparkDriverExecutionException: Execution error; at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108); at org.apache.spark.rdd.RDD.withScope(RDD.scala:310); at org.apache.spark.rdd.RDD.reduce(RDD.scala:989); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1118); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1107
https://github.com/hail-is/hail/pull/1111:75,Testability,test,tests,75,"- added pnorm, qnorm, inverseChiSquaredTailOneDF to stats package; - added tests to StatsSuite; - added pnorm, qnorm, pchisq1tail, qchisq1tail to function registry; - added tests to ExprSuite; - added docs to HailExpressionLanguage.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1111
https://github.com/hail-is/hail/pull/1111:173,Testability,test,tests,173,"- added pnorm, qnorm, inverseChiSquaredTailOneDF to stats package; - added tests to StatsSuite; - added pnorm, qnorm, pchisq1tail, qchisq1tail to function registry; - added tests to ExprSuite; - added docs to HailExpressionLanguage.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1111
https://github.com/hail-is/hail/issues/1113:261,Modifiability,extend,extends,261,"I'm thinking something like `parseExprsAtTypes`:. ```scala; def parseExprsAtTypes[A](e: String, ec: EvalContext, hole: TypeHole[A]): Array[(BaseType, A, () => Option[Any])]; ```. ```scala; // function throws Exception if type is unacceptable; trait TypeHole[A] extends Iterable[Type => A] { }; sealed case class RepeatingTypeHole[A](f: Type => A) extends TypeHole[A] {; def iterator() = repeat(f).iterator; }; sealed case class ExactTypeHole(types: Type*) extends TypeHole[Unit] {; def iterator() =; types.iterator.map(x => y => if (x == y) () else fatal(s""$x should be of type $y""); }. def repeat[A](it: Iterable[A]) = Stream.continually(it.toStream).flatten; ```. Then the new LMMReg check for covariance types would be:; ```scala; val compiledExprs = parseExprsAtTypes(options.covSA, ec, RepeatingTypeHole(toDouble)); val covSA = vds.sampleIdsAndAnnotations.map { case (s, sa) =>; ec.setAll(s, sa); compiledExprs.map { case (typ, convert, query) =>; query().map(convert); }; }; ```. And AnnotationImpex would be something like:. ```scala; val fs = Parser.parseExprs(variantFields, ec, ExactTypeHole(TString, TInt, TString, TArray(TString))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1113
https://github.com/hail-is/hail/issues/1113:347,Modifiability,extend,extends,347,"I'm thinking something like `parseExprsAtTypes`:. ```scala; def parseExprsAtTypes[A](e: String, ec: EvalContext, hole: TypeHole[A]): Array[(BaseType, A, () => Option[Any])]; ```. ```scala; // function throws Exception if type is unacceptable; trait TypeHole[A] extends Iterable[Type => A] { }; sealed case class RepeatingTypeHole[A](f: Type => A) extends TypeHole[A] {; def iterator() = repeat(f).iterator; }; sealed case class ExactTypeHole(types: Type*) extends TypeHole[Unit] {; def iterator() =; types.iterator.map(x => y => if (x == y) () else fatal(s""$x should be of type $y""); }. def repeat[A](it: Iterable[A]) = Stream.continually(it.toStream).flatten; ```. Then the new LMMReg check for covariance types would be:; ```scala; val compiledExprs = parseExprsAtTypes(options.covSA, ec, RepeatingTypeHole(toDouble)); val covSA = vds.sampleIdsAndAnnotations.map { case (s, sa) =>; ec.setAll(s, sa); compiledExprs.map { case (typ, convert, query) =>; query().map(convert); }; }; ```. And AnnotationImpex would be something like:. ```scala; val fs = Parser.parseExprs(variantFields, ec, ExactTypeHole(TString, TInt, TString, TArray(TString))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1113
https://github.com/hail-is/hail/issues/1113:456,Modifiability,extend,extends,456,"I'm thinking something like `parseExprsAtTypes`:. ```scala; def parseExprsAtTypes[A](e: String, ec: EvalContext, hole: TypeHole[A]): Array[(BaseType, A, () => Option[Any])]; ```. ```scala; // function throws Exception if type is unacceptable; trait TypeHole[A] extends Iterable[Type => A] { }; sealed case class RepeatingTypeHole[A](f: Type => A) extends TypeHole[A] {; def iterator() = repeat(f).iterator; }; sealed case class ExactTypeHole(types: Type*) extends TypeHole[Unit] {; def iterator() =; types.iterator.map(x => y => if (x == y) () else fatal(s""$x should be of type $y""); }. def repeat[A](it: Iterable[A]) = Stream.continually(it.toStream).flatten; ```. Then the new LMMReg check for covariance types would be:; ```scala; val compiledExprs = parseExprsAtTypes(options.covSA, ec, RepeatingTypeHole(toDouble)); val covSA = vds.sampleIdsAndAnnotations.map { case (s, sa) =>; ec.setAll(s, sa); compiledExprs.map { case (typ, convert, query) =>; query().map(convert); }; }; ```. And AnnotationImpex would be something like:. ```scala; val fs = Parser.parseExprs(variantFields, ec, ExactTypeHole(TString, TInt, TString, TArray(TString))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1113
https://github.com/hail-is/hail/issues/1123:39,Integrability,message,message,39,"In particular, it doesn't give an info message when commands are run.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1123
https://github.com/hail-is/hail/pull/1124:223,Deployability,upgrade,upgrade,223,"I'd like to move master to Spark 2 and Scala 2.11. These changes get us as close as possible. They include:. - remove SparkExport, use reflection to get path of partition when loading from parquet; - remove SparkManager; - upgrade to Kudu 1.1.0 (Spark 2 support). The distance between this and Spark 2 is very small, see https://github.com/hail-is/hail/commit/95a588cfa72391d4303bf6891fd017ec211989db. When the master moves to Spark 2, we can maintain a spark1 branch until the on-prem machines get upgraded. Ideally, the spark1 branch could get rebased automatically as part of the CI, although I'm not quite sure how we'd handle conflicts. Alternatively, we could maintain a spark2 -> spark1 diff in the repo that gets applied as part of testing. Fixes https://github.com/hail-is/hail/issues/1117",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1124
https://github.com/hail-is/hail/pull/1124:499,Deployability,upgrade,upgraded,499,"I'd like to move master to Spark 2 and Scala 2.11. These changes get us as close as possible. They include:. - remove SparkExport, use reflection to get path of partition when loading from parquet; - remove SparkManager; - upgrade to Kudu 1.1.0 (Spark 2 support). The distance between this and Spark 2 is very small, see https://github.com/hail-is/hail/commit/95a588cfa72391d4303bf6891fd017ec211989db. When the master moves to Spark 2, we can maintain a spark1 branch until the on-prem machines get upgraded. Ideally, the spark1 branch could get rebased automatically as part of the CI, although I'm not quite sure how we'd handle conflicts. Alternatively, we could maintain a spark2 -> spark1 diff in the repo that gets applied as part of testing. Fixes https://github.com/hail-is/hail/issues/1117",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1124
https://github.com/hail-is/hail/pull/1124:176,Performance,load,loading,176,"I'd like to move master to Spark 2 and Scala 2.11. These changes get us as close as possible. They include:. - remove SparkExport, use reflection to get path of partition when loading from parquet; - remove SparkManager; - upgrade to Kudu 1.1.0 (Spark 2 support). The distance between this and Spark 2 is very small, see https://github.com/hail-is/hail/commit/95a588cfa72391d4303bf6891fd017ec211989db. When the master moves to Spark 2, we can maintain a spark1 branch until the on-prem machines get upgraded. Ideally, the spark1 branch could get rebased automatically as part of the CI, although I'm not quite sure how we'd handle conflicts. Alternatively, we could maintain a spark2 -> spark1 diff in the repo that gets applied as part of testing. Fixes https://github.com/hail-is/hail/issues/1117",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1124
https://github.com/hail-is/hail/pull/1124:740,Testability,test,testing,740,"I'd like to move master to Spark 2 and Scala 2.11. These changes get us as close as possible. They include:. - remove SparkExport, use reflection to get path of partition when loading from parquet; - remove SparkManager; - upgrade to Kudu 1.1.0 (Spark 2 support). The distance between this and Spark 2 is very small, see https://github.com/hail-is/hail/commit/95a588cfa72391d4303bf6891fd017ec211989db. When the master moves to Spark 2, we can maintain a spark1 branch until the on-prem machines get upgraded. Ideally, the spark1 branch could get rebased automatically as part of the CI, although I'm not quite sure how we'd handle conflicts. Alternatively, we could maintain a spark2 -> spark1 diff in the repo that gets applied as part of testing. Fixes https://github.com/hail-is/hail/issues/1117",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1124
https://github.com/hail-is/hail/issues/1132:183,Performance,load,load,183,"I want to annotate a field like this:; ```Gene_Conseq_MAF=(va.annot.gene + ""\n"" + va.annot.most_severe_csq + ""\nMAF:"" + str(va.lmmreg.maf))```; (so a string with \n) such that when I load in R, the top loci will be highlighted as so. However, the exported .gz file actually has new lines at each “\n""; is there a way to avoid this?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1132
https://github.com/hail-is/hail/issues/1132:320,Safety,avoid,avoid,320,"I want to annotate a field like this:; ```Gene_Conseq_MAF=(va.annot.gene + ""\n"" + va.annot.most_severe_csq + ""\nMAF:"" + str(va.lmmreg.maf))```; (so a string with \n) such that when I load in R, the top loci will be highlighted as so. However, the exported .gz file actually has new lines at each “\n""; is there a way to avoid this?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1132
https://github.com/hail-is/hail/issues/1137:38,Availability,error,errors,38,but ends up throwing odd non-specific errors down the line,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1137
https://github.com/hail-is/hail/issues/1137:45,Availability,down,down,45,but ends up throwing odd non-specific errors down the line,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1137
https://github.com/hail-is/hail/pull/1138:28,Performance,load,load,28,"This allows Spark 2 Hail to load partitioning information from VDSes; written by the Spark 1 version, but loading partitioning from previous; Spark 2 versions will now fail. Since there are far more VDSes in; uses written with Spark 1 in use, this seems like a good trade-off. There are now to serial version UIDs in the wild for Locus. I don't; see how to write code to load both of them (except maybe; loading/unloading different versions of the Locus class which seems; painful.) I would prefer a tool that converts the partitioning to; JSON instead (once support for JSON is ready). The partitioning information should be stored as JSON instead of Java; serialization, which is not a good long-term storage format.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1138
https://github.com/hail-is/hail/pull/1138:106,Performance,load,loading,106,"This allows Spark 2 Hail to load partitioning information from VDSes; written by the Spark 1 version, but loading partitioning from previous; Spark 2 versions will now fail. Since there are far more VDSes in; uses written with Spark 1 in use, this seems like a good trade-off. There are now to serial version UIDs in the wild for Locus. I don't; see how to write code to load both of them (except maybe; loading/unloading different versions of the Locus class which seems; painful.) I would prefer a tool that converts the partitioning to; JSON instead (once support for JSON is ready). The partitioning information should be stored as JSON instead of Java; serialization, which is not a good long-term storage format.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1138
https://github.com/hail-is/hail/pull/1138:371,Performance,load,load,371,"This allows Spark 2 Hail to load partitioning information from VDSes; written by the Spark 1 version, but loading partitioning from previous; Spark 2 versions will now fail. Since there are far more VDSes in; uses written with Spark 1 in use, this seems like a good trade-off. There are now to serial version UIDs in the wild for Locus. I don't; see how to write code to load both of them (except maybe; loading/unloading different versions of the Locus class which seems; painful.) I would prefer a tool that converts the partitioning to; JSON instead (once support for JSON is ready). The partitioning information should be stored as JSON instead of Java; serialization, which is not a good long-term storage format.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1138
https://github.com/hail-is/hail/pull/1138:404,Performance,load,loading,404,"This allows Spark 2 Hail to load partitioning information from VDSes; written by the Spark 1 version, but loading partitioning from previous; Spark 2 versions will now fail. Since there are far more VDSes in; uses written with Spark 1 in use, this seems like a good trade-off. There are now to serial version UIDs in the wild for Locus. I don't; see how to write code to load both of them (except maybe; loading/unloading different versions of the Locus class which seems; painful.) I would prefer a tool that converts the partitioning to; JSON instead (once support for JSON is ready). The partitioning information should be stored as JSON instead of Java; serialization, which is not a good long-term storage format.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1138
https://github.com/hail-is/hail/pull/1139:217,Performance,load,load,217,"Allow -b 0.; Added typeclasses for converting to/from JSON.; Implemented for OrderedPartitioner and Locus.; Use in VSM.{read, write}.; Remove OrderedPartitioner.ascending. It wasn't properly implemented. Still try to load serialized .vds/partitioner if partitioner.json.gz; isn't there.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1139
https://github.com/hail-is/hail/issues/1142:11,Modifiability,config,configureAndCreateSparkContext,11,"goes in ```configureAndCreateSparkContext```. ```sc.uiWebUrl.foreach { s => info(s""SparkUI started at $s"") }```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1142
https://github.com/hail-is/hail/issues/1151:11,Availability,error,error,11,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:49,Availability,ERROR,ERROR,49,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:152,Availability,ERROR,ERROR,152,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:272,Availability,failure,failure,272,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:331,Availability,failure,failure,331,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:585,Deployability,pipeline,pipeline,585,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:792,Deployability,update,updated,792,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:113,Safety,abort,aborting,113,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:251,Safety,abort,aborted,251,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:494,Testability,Log,Log,494,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:564,Testability,log,log,564,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1151:699,Testability,log,log,699,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1151
https://github.com/hail-is/hail/issues/1156:115,Availability,error,error,115,"Rather than letting Breeze throw a SingularMatrixException, we should check for dependence and give an informative error message. The most common mistakes leading to dependence are accidentally including the same covariate twice (identical columns) or encoding a categorical variable with n categories using n rather than n - 1 covariates (since the model has an intercept term, this creates linear relation. We might also consider automating this encoding).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1156
https://github.com/hail-is/hail/issues/1156:80,Integrability,depend,dependence,80,"Rather than letting Breeze throw a SingularMatrixException, we should check for dependence and give an informative error message. The most common mistakes leading to dependence are accidentally including the same covariate twice (identical columns) or encoding a categorical variable with n categories using n rather than n - 1 covariates (since the model has an intercept term, this creates linear relation. We might also consider automating this encoding).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1156
https://github.com/hail-is/hail/issues/1156:121,Integrability,message,message,121,"Rather than letting Breeze throw a SingularMatrixException, we should check for dependence and give an informative error message. The most common mistakes leading to dependence are accidentally including the same covariate twice (identical columns) or encoding a categorical variable with n categories using n rather than n - 1 covariates (since the model has an intercept term, this creates linear relation. We might also consider automating this encoding).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1156
https://github.com/hail-is/hail/issues/1156:166,Integrability,depend,dependence,166,"Rather than letting Breeze throw a SingularMatrixException, we should check for dependence and give an informative error message. The most common mistakes leading to dependence are accidentally including the same covariate twice (identical columns) or encoding a categorical variable with n categories using n rather than n - 1 covariates (since the model has an intercept term, this creates linear relation. We might also consider automating this encoding).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1156
https://github.com/hail-is/hail/issues/1156:275,Modifiability,variab,variable,275,"Rather than letting Breeze throw a SingularMatrixException, we should check for dependence and give an informative error message. The most common mistakes leading to dependence are accidentally including the same covariate twice (identical columns) or encoding a categorical variable with n categories using n rather than n - 1 covariates (since the model has an intercept term, this creates linear relation. We might also consider automating this encoding).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1156
https://github.com/hail-is/hail/pull/1157:133,Modifiability,config,configurable,133,Replaced VariantDataset ..._to_pandas with ..._keytable.; Added expand_types and flatten to KeyTable. These probably need to be more configurable but are a start.; Added KeyTable.toDF. This allows KeyTables to be easily written to Parquet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1157
https://github.com/hail-is/hail/issues/1158:76,Integrability,contract,contract,76,"- create KeyTable from DataFrame; - to make above useful, we need a way to ""contract"" native types (opposite of expand_types), that, convert Struct with the appropriate fields to Variant, etc. One alternative is to use SparkAnnotationImpex and have the user specify the Hail type. (Also means we need a way to build Hail types in python.); - annotate variants or samples with keytable; - load fam file as keytable; - remove annotevariants table, vcf, vds and annotatesamples fam, table, list, vds. They can all be implemented with annotate with keytable.; - same with filter list",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1158
https://github.com/hail-is/hail/issues/1158:388,Performance,load,load,388,"- create KeyTable from DataFrame; - to make above useful, we need a way to ""contract"" native types (opposite of expand_types), that, convert Struct with the appropriate fields to Variant, etc. One alternative is to use SparkAnnotationImpex and have the user specify the Hail type. (Also means we need a way to build Hail types in python.); - annotate variants or samples with keytable; - load fam file as keytable; - remove annotevariants table, vcf, vds and annotatesamples fam, table, list, vds. They can all be implemented with annotate with keytable.; - same with filter list",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1158
https://github.com/hail-is/hail/issues/1160:22,Testability,test,test,22,we should do a random test:; - gen a multiallelic vds; - annotate variants (Still multi) with range(v.nAlleles).map(x => if (x == 0) NA: AltAllele else va.altAlleles[x]); - split; - ensure that va.anno[va.aIndex] == va.altAllele,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1160
https://github.com/hail-is/hail/pull/1166:11,Availability,error,error,11,Fixes this error: ```except Py4JJavaError as e:; NameError: global name 'Py4JJavaError' is not defined```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1166
https://github.com/hail-is/hail/issues/1167:357,Availability,error,error,357,"Consider an annotate variants like this:. ```; va.annot.annot = if (isMissing(va.annot.annot.gene) && isDefined(va.annot_MESA.gene)) va.annot_MESA; else if (isMissing(va.annot.annot.gene) && isDefined(va.annot_FinEst.annot.gene)) va.annot_FinEst.annot; else va.annot.annot; ```. If `va.annot_MESA` and `va.annot_FinEst.annot` are not the same the following error arrises, but doesn't give much information as to what went wrong. ```; hail: fatal: annotatevariants expr: expected same-type `then' and `else' clause, got `Struct' and `Struct'; <input>:2: else if (isMissing(va.annot.annot.gene) && isDefined(va.annot_FinEst.annot.gene)) va.annot_FinEst.annot ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1167
https://github.com/hail-is/hail/issues/1179:126,Availability,error,error,126,"When creating a global map using annotateglobal exprbysample, any variant annotation after that fails by Map not serializable error",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1179
https://github.com/hail-is/hail/issues/1184:82,Availability,error,error,82,"Below is the first part of the stack trace for for a job using pyhail with a user error in the expression language. The text on line 7 isn't in the `utils.py` from pyspark but in my provided `utils.py` (though the error certainly did come from pyspark `utils.py`). ```; Traceback (most recent call last):; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/genomes_qc.py"", line 161, in <module>; .export_variants(rf_path + "".va.txt.bgz"", "","".join(out_metrics)); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 489, in export_variants; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/utils.py"", line 209, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; expression.append('va.calldata.%(pop_upper)s = gs.filter(g => %(criterion)s == ""%(pop)s"").callStats(v)' % input_dict); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o93.run.; : org.broadinstitute.hail.utils.package$FatalException: `Struct' has no field `type'; ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1184
https://github.com/hail-is/hail/issues/1184:214,Availability,error,error,214,"Below is the first part of the stack trace for for a job using pyhail with a user error in the expression language. The text on line 7 isn't in the `utils.py` from pyspark but in my provided `utils.py` (though the error certainly did come from pyspark `utils.py`). ```; Traceback (most recent call last):; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/genomes_qc.py"", line 161, in <module>; .export_variants(rf_path + "".va.txt.bgz"", "","".join(out_metrics)); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 489, in export_variants; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/utils.py"", line 209, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; expression.append('va.calldata.%(pop_upper)s = gs.filter(g => %(criterion)s == ""%(pop)s"").callStats(v)' % input_dict); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o93.run.; : org.broadinstitute.hail.utils.package$FatalException: `Struct' has no field `type'; ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1184
https://github.com/hail-is/hail/issues/1184:1107,Availability,error,error,1107,"Below is the first part of the stack trace for for a job using pyhail with a user error in the expression language. The text on line 7 isn't in the `utils.py` from pyspark but in my provided `utils.py` (though the error certainly did come from pyspark `utils.py`). ```; Traceback (most recent call last):; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/genomes_qc.py"", line 161, in <module>; .export_variants(rf_path + "".va.txt.bgz"", "","".join(out_metrics)); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 489, in export_variants; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/utils.py"", line 209, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; expression.append('va.calldata.%(pop_upper)s = gs.filter(g => %(criterion)s == ""%(pop)s"").callStats(v)' % input_dict); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o93.run.; : org.broadinstitute.hail.utils.package$FatalException: `Struct' has no field `type'; ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1184
https://github.com/hail-is/hail/issues/1184:1030,Integrability,protocol,protocol,1030,"Below is the first part of the stack trace for for a job using pyhail with a user error in the expression language. The text on line 7 isn't in the `utils.py` from pyspark but in my provided `utils.py` (though the error certainly did come from pyspark `utils.py`). ```; Traceback (most recent call last):; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/genomes_qc.py"", line 161, in <module>; .export_variants(rf_path + "".va.txt.bgz"", "","".join(out_metrics)); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 489, in export_variants; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/utils.py"", line 209, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; expression.append('va.calldata.%(pop_upper)s = gs.filter(g => %(criterion)s == ""%(pop)s"").callStats(v)' % input_dict); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o93.run.; : org.broadinstitute.hail.utils.package$FatalException: `Struct' has no field `type'; ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1184
https://github.com/hail-is/hail/issues/1184:1080,Integrability,protocol,protocol,1080,"Below is the first part of the stack trace for for a job using pyhail with a user error in the expression language. The text on line 7 isn't in the `utils.py` from pyspark but in my provided `utils.py` (though the error certainly did come from pyspark `utils.py`). ```; Traceback (most recent call last):; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/genomes_qc.py"", line 161, in <module>; .export_variants(rf_path + "".va.txt.bgz"", "","".join(out_metrics)); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 489, in export_variants; File ""/tmp/5d145552-3077-4992-8d29-3df6975c7247/utils.py"", line 209, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; expression.append('va.calldata.%(pop_upper)s = gs.filter(g => %(criterion)s == ""%(pop)s"").callStats(v)' % input_dict); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o93.run.; : org.broadinstitute.hail.utils.package$FatalException: `Struct' has no field `type'; ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1184
https://github.com/hail-is/hail/issues/1185:544,Availability,failure,failure,544,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:604,Availability,failure,failure,604,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1542,Availability,error,error,1542,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1565,Availability,error,error,1565,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1621,Availability,error,error,1621,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1137,Energy Efficiency,schedul,scheduler,1137,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1209,Energy Efficiency,schedul,scheduler,1209,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1333,Performance,concurren,concurrent,1333,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1418,Performance,concurren,concurrent,1418,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:523,Safety,abort,aborted,523,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1552,Testability,log,log,1552,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1639,Testability,log,log,1639,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1185:1698,Testability,log,log,1698,"Tried running ; `hail read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf`. got the following; ```; hail: info: running: read -i file:///mnt/lustre/bavila/denovo/myrioux3.vep.vds; [Stage 1:======================================================>(255 + 1) / 256]hail: info: running: exportvcf -o file:///mnt/lustre/bavila/denovo/myrioux3.vep.vcf; [Stage 2:=====> (2117 + 256) / 19042]hail: exportvcf: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2137 in stage 2.0 failed 4 times, most recent failure: Lost task 2137.3 in stage 2.0 (TID 3028, nid00013.urika.com): java.lang.IllegalArgumentException: Self-suppression not permitted; 	at java.lang.Throwable.addSuppressed(Throwable.java:1043); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:88); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```. Full error and log below:. [error.txt](https://github.com/hail-is/hail/files/652656/error.txt); [hail.log.txt](https://github.com/hail-is/hail/files/652665/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1185
https://github.com/hail-is/hail/issues/1186:75,Availability,error,error,75,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:136,Availability,error,error,136,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:153,Availability,error,error,153,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:1734,Availability,error,error,1734,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:1847,Availability,error,error,1847,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:1909,Availability,error,error,1909,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:167,Energy Efficiency,allocate,allocate,167,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:1383,Performance,concurren,concurrent,1383,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:1445,Performance,concurren,concurrent,1445,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:1530,Performance,concurren,concurrent,1530,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:1716,Testability,Log,Log,1716,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:1798,Testability,log,log,1798,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/issues/1186:10,Usability,simpl,simple,10,"writing a simple command such as read and write produces different type of error, but all related to memory issues. ; The most explicit error is: . ```; error='Cannot allocate memory' (errno=12); # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 7376207872 bytes for committing reserved memory.; ```. But also get this issue in other occasions:. ```; ExecutorLostFailure (executor 2265 exited caused by one of the running tasks) Reason: Container marked as failed: container_1481808189977_0001_01_002296 on host: gnomadpsychk-sw-wpt8.c.wgspd-147615.internal. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1481808189977_0001_01_002296; Exit code: 50; Stack trace: ExitCodeException exitCode=50:; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582); 	at org.apache.hadoop.util.Shell.run(Shell.java:479); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 50. Driver stacktrace:; ```. Log for this last error is here: https://storage.googleapis.com/wgspd_urv/hailNEW.log. I tried different projects and get the same error, both with pyhail and native hail. I don't see the same error when running on cray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186
https://github.com/hail-is/hail/pull/1196:2,Deployability,update,updated,2,- updated docs; - added tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1196
https://github.com/hail-is/hail/pull/1196:24,Testability,test,tests,24,- updated docs; - added tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1196
https://github.com/hail-is/hail/pull/1198:5,Safety,avoid,avoids,5,"This avoids serializing to a String before writing, which; likely doubles the memory requirements.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1198
https://github.com/hail-is/hail/issues/1201:23,Security,expose,exposed,23,Currently only `va` is exposed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1201
https://github.com/hail-is/hail/issues/1202:771,Availability,error,error,771,"The stack trace reported:. ```; [Stage 7:> (0 + 132) / 194]Traceback (most recent call last):; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/genomes_sites_vcf.py"", line 42, in <module>; sites_vds.write(tmp_vds); File ""/home/teamcity/TeamCityAgent2/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 595, in write; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/utils.py"", line 211, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o309.run.; : org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:3892,Availability,failure,failure,3892,"VDS.write(VariantSampleMatrix.scala:1073); 	at org.broadinstitute.hail.driver.Write$.run(Write.scala:35); 	at org.broadinstitute.hail.driver.Write$.run(Write.scala:6); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.Thread",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:3951,Availability,failure,failure,3951,"tute.hail.driver.Write$.run(Write.scala:35); 	at org.broadinstitute.hail.driver.Write$.run(Write.scala:6); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:10308,Deployability,update,updateAnnotation,10308,each(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:716); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:715); at org.broadinstitute.hail.expr.BinaryLambdaFun.apply(Fun.scala:131); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$apply$16.apply(FunctionRegistry.scala:231); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$1.apply(AST.scala:128); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$3$$anonfun$apply$2.apply$mcV$sp(Parser.scala:192); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5.apply(Parser.scala:208); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5.apply(Parser.scala:207); at org.broadinstitute.hail.expr.Parser$$anonfun$parseAnnotationExprs$4.apply(Parser.scala:93); at org.broadinstitute.hail.expr.Parser$$anonfun$parseAnnotationExprs$4.apply(Parser.scala:92); at org.broadinstitute.hail.driver.FilterAlleles$.org$broadinstitute$hail$driver$FilterAlleles$$updateAnnotation$1(FilterAlleles.scala:123); at org.broadinstitute.hail.driver.FilterAlleles$$anonfun$org$broadinstitute$hail$driver$FilterAlleles$$updateOrFilterRow$1$1.apply(FilterAlleles.scala:191); at org.broadinstitute.hail.driver.FilterAlleles$$anonfun$org$broadinstitute$hail$driver$FilterAlleles$$updateOrFilterRow$1$1.apply(FilterAlleles.scala:190); at scala.Option.map(Option.scala:146); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:10456,Deployability,update,updateOrFilterRow,10456,each(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:716); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:715); at org.broadinstitute.hail.expr.BinaryLambdaFun.apply(Fun.scala:131); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$apply$16.apply(FunctionRegistry.scala:231); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$1.apply(AST.scala:128); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$3$$anonfun$apply$2.apply$mcV$sp(Parser.scala:192); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5.apply(Parser.scala:208); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5.apply(Parser.scala:207); at org.broadinstitute.hail.expr.Parser$$anonfun$parseAnnotationExprs$4.apply(Parser.scala:93); at org.broadinstitute.hail.expr.Parser$$anonfun$parseAnnotationExprs$4.apply(Parser.scala:92); at org.broadinstitute.hail.driver.FilterAlleles$.org$broadinstitute$hail$driver$FilterAlleles$$updateAnnotation$1(FilterAlleles.scala:123); at org.broadinstitute.hail.driver.FilterAlleles$$anonfun$org$broadinstitute$hail$driver$FilterAlleles$$updateOrFilterRow$1$1.apply(FilterAlleles.scala:191); at org.broadinstitute.hail.driver.FilterAlleles$$anonfun$org$broadinstitute$hail$driver$FilterAlleles$$updateOrFilterRow$1$1.apply(FilterAlleles.scala:190); at scala.Option.map(Option.scala:146); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:10613,Deployability,update,updateOrFilterRow,10613,each(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:716); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:715); at org.broadinstitute.hail.expr.BinaryLambdaFun.apply(Fun.scala:131); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$apply$16.apply(FunctionRegistry.scala:231); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$1.apply(AST.scala:128); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$3$$anonfun$apply$2.apply$mcV$sp(Parser.scala:192); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5.apply(Parser.scala:208); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5.apply(Parser.scala:207); at org.broadinstitute.hail.expr.Parser$$anonfun$parseAnnotationExprs$4.apply(Parser.scala:93); at org.broadinstitute.hail.expr.Parser$$anonfun$parseAnnotationExprs$4.apply(Parser.scala:92); at org.broadinstitute.hail.driver.FilterAlleles$.org$broadinstitute$hail$driver$FilterAlleles$$updateAnnotation$1(FilterAlleles.scala:123); at org.broadinstitute.hail.driver.FilterAlleles$$anonfun$org$broadinstitute$hail$driver$FilterAlleles$$updateOrFilterRow$1$1.apply(FilterAlleles.scala:191); at org.broadinstitute.hail.driver.FilterAlleles$$anonfun$org$broadinstitute$hail$driver$FilterAlleles$$updateOrFilterRow$1$1.apply(FilterAlleles.scala:190); at scala.Option.map(Option.scala:146); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:4588,Energy Efficiency,schedul,scheduler,4588,"j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.ab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:4660,Energy Efficiency,schedul,scheduler,4660,"y4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGSch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5077,Energy Efficiency,schedul,scheduler,5077, Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5117,Energy Efficiency,schedul,scheduler,5117,xecution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5216,Energy Efficiency,schedul,scheduler,5216,; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5314,Energy Efficiency,schedul,scheduler,5314,$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5568,Energy Efficiency,schedul,scheduler,5568,g.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5649,Energy Efficiency,schedul,scheduler,5649,spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasource,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5755,Energy Efficiency,schedul,scheduler,5755,ala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:14,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5905,Energy Efficiency,schedul,scheduler,5905,ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5994,Energy Efficiency,schedul,scheduler,5994,lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.da,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:6092,Energy Efficiency,schedul,scheduler,6092,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertInt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:6188,Energy Efficiency,schedul,scheduler,6188,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:6353,Energy Efficiency,schedul,scheduler,6353,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(Re,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:7327,Energy Efficiency,schedul,scheduler,7327,	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:7399,Energy Efficiency,schedul,scheduler,7399,2); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:694,Integrability,protocol,protocol,694,"The stack trace reported:. ```; [Stage 7:> (0 + 132) / 194]Traceback (most recent call last):; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/genomes_sites_vcf.py"", line 42, in <module>; sites_vds.write(tmp_vds); File ""/home/teamcity/TeamCityAgent2/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 595, in write; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/utils.py"", line 211, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o309.run.; : org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:744,Integrability,protocol,protocol,744,"The stack trace reported:. ```; [Stage 7:> (0 + 132) / 194]Traceback (most recent call last):; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/genomes_sites_vcf.py"", line 42, in <module>; sites_vds.write(tmp_vds); File ""/home/teamcity/TeamCityAgent2/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 595, in write; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/utils.py"", line 211, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o309.run.; : org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:7899,Integrability,Wrap,WrappedArray,7899,WriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:7924,Integrability,Wrap,WrappedArray,7924,teRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.FunctionRegistry,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:8693,Integrability,Wrap,WrappedArray,8693,by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:716); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:715); at org.broadinstitute.hail.expr.BinaryLambdaFun.apply(Fun.scala:131); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$apply$16.apply(FunctionRegistry.scala:231); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$1.apply(AST.scala:128); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$3$$anonfun$apply$2.apply$mcV$sp(Parser.scala:192); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at scala.collect,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:8714,Integrability,Wrap,WrappedArray,8714,ayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:716); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$153.apply(FunctionRegistry.scala:715); at org.broadinstitute.hail.expr.BinaryLambdaFun.apply(Fun.scala:131); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$apply$16.apply(FunctionRegistry.scala:231); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$1.apply(AST.scala:128); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$3$$anonfun$apply$2.apply$mcV$sp(Parser.scala:192); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at org.broadinstitute.hail.expr.Parser$$anonfun$parseNamedExprs$5$$anonfun$apply$18.apply(Parser.scala:208); at scala.collection.IndexedSeqOpt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:4784,Performance,concurren,concurrent,4784,"lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$han",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:4869,Performance,concurren,concurrent,4869,"rted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:7523,Performance,concurren,concurrent,7523,.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLik,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:7608,Performance,concurren,concurrent,7608,org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOpti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:850,Safety,abort,aborted,850,"The stack trace reported:. ```; [Stage 7:> (0 + 132) / 194]Traceback (most recent call last):; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/genomes_sites_vcf.py"", line 42, in <module>; sites_vds.write(tmp_vds); File ""/home/teamcity/TeamCityAgent2/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 595, in write; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/utils.py"", line 211, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o309.run.; : org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:3871,Safety,abort,aborted,3871,"VDS.write(VariantSampleMatrix.scala:1073); 	at org.broadinstitute.hail.driver.Write$.run(Write.scala:35); 	at org.broadinstitute.hail.driver.Write$.run(Write.scala:6); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 7.0 failed 20 times, most recent failure: Lost task 35.19 in stage 7.0 (TID 6963, gnomad-prod-sw-m8lk.c.broad-mpg-gnomad.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.Thread",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5248,Safety,abort,abortStage,5248,.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5346,Safety,abort,abortStage,5346,1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:5591,Safety,abort,abortStage,5591,tTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ArrayIndexOutOfBoundsException. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1202:7803,Testability,log,log,7803,ion: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1202
https://github.com/hail-is/hail/issues/1204:3626,Deployability,deploy,deploy,3626,.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1204
https://github.com/hail-is/hail/issues/1204:3663,Deployability,deploy,deploy,3663,.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1204
https://github.com/hail-is/hail/issues/1204:3735,Deployability,deploy,deploy,3735,.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1204
https://github.com/hail-is/hail/issues/1204:3811,Deployability,deploy,deploy,3811,.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1204
https://github.com/hail-is/hail/issues/1204:3882,Deployability,deploy,deploy,3882,.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1204
https://github.com/hail-is/hail/issues/1204:3951,Deployability,deploy,deploy,3951,.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1204
https://github.com/hail-is/hail/issues/1204:1785,Integrability,Wrap,WrappedArray,1785,266); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:266); at org.broadinstitute.hail.expr.JSONAnnotationImpex$$anonfun$exportAnnotation$12.apply(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.JSONAnnotationImpex$$anonfun$exportAnnotation$12.apply(AnnotationImpex.scala:284); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1204
https://github.com/hail-is/hail/issues/1204:1806,Integrability,Wrap,WrappedArray,1806,llection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:266); at org.broadinstitute.hail.expr.JSONAnnotationImpex$$anonfun$exportAnnotation$12.apply(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.JSONAnnotationImpex$$anonfun$exportAnnotation$12.apply(AnnotationImpex.scala:284); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.broadinstitute.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:284); at org.broadinstitute.hail.expr.Type.toJSON(Type.scala:135); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:32); at org.broadinstitute.hail.driver.ShowGlobalAnnotations$.run(ShowGlobalAnnotations.scala:8); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1204
https://github.com/hail-is/hail/pull/1208:76,Integrability,wrap,wrapped,76,- List of str inputs for condition are modified such that each condition is wrapped in parentheses and multiple conditions are separated by `&&`. Previous was conditions were comma separated.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1208
https://github.com/hail-is/hail/issues/1213:11,Availability,error,error,11,"I get this error when using the last hail version built on Spark 1.6. ``` hail: annotatevariants vds: caught exception: java.lang.IllegalArgumentException: requirement failed: nPartitions = 2847, ranges = 2844; 	at scala.Predef$.require(Predef.scala:233); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner.<init>(OrderedPartitioner.scala:27); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner$.read(OrderedPartitioner.scala:110); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:185); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.utils.richUtils.RichHadoopConfiguration$.readObjectFile$extension(RichHadoopConfiguration.scala:205); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:9); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:61); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:9)```. however, the same commands work on the hail version ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1213
https://github.com/hail-is/hail/issues/1213:2030,Testability,log,log,2030,"vds: caught exception: java.lang.IllegalArgumentException: requirement failed: nPartitions = 2847, ranges = 2844; 	at scala.Predef$.require(Predef.scala:233); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner.<init>(OrderedPartitioner.scala:27); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner$.read(OrderedPartitioner.scala:110); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:185); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.utils.richUtils.RichHadoopConfiguration$.readObjectFile$extension(RichHadoopConfiguration.scala:205); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:9); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:61); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:9)```. however, the same commands work on the hail version built on Spark 2. ; Here the log: https://storage.googleapis.com/maryam_lipids/hailL.log. thanks",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1213
https://github.com/hail-is/hail/issues/1213:2086,Testability,log,log,2086,"vds: caught exception: java.lang.IllegalArgumentException: requirement failed: nPartitions = 2847, ranges = 2844; 	at scala.Predef$.require(Predef.scala:233); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner.<init>(OrderedPartitioner.scala:27); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner$.read(OrderedPartitioner.scala:110); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:185); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.utils.richUtils.RichHadoopConfiguration$.readObjectFile$extension(RichHadoopConfiguration.scala:205); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:9); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:61); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:9)```. however, the same commands work on the hail version built on Spark 2. ; Here the log: https://storage.googleapis.com/maryam_lipids/hailL.log. thanks",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1213
https://github.com/hail-is/hail/issues/1218:71,Availability,avail,available,71,I see the docs for the PyHail API but is there a getting started guide available yet? Also are there any plans to make a PyHail package available for installation through PyPI?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218
https://github.com/hail-is/hail/issues/1218:136,Availability,avail,available,136,I see the docs for the PyHail API but is there a getting started guide available yet? Also are there any plans to make a PyHail package available for installation through PyPI?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218
https://github.com/hail-is/hail/issues/1218:150,Deployability,install,installation,150,I see the docs for the PyHail API but is there a getting started guide available yet? Also are there any plans to make a PyHail package available for installation through PyPI?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218
https://github.com/hail-is/hail/issues/1218:65,Usability,guid,guide,65,I see the docs for the PyHail API but is there a getting started guide available yet? Also are there any plans to make a PyHail package available for installation through PyPI?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1218
https://github.com/hail-is/hail/pull/1223:754,Usability,clear,clear,754,"Convert three foreach calls to while loops. I see about a 15% reduction in variant aggregation time. I executed. ```; hail read profile225.vds \; annotatevariants expr -c 'va.foo = gs.map(g => 1).sum()' \; exportvariants -c 'va.foo' -o 'foo.tsv'; ```. Here are the comparisons between this PR, master, and some part-solutions that I tried. We always compare to this PR. In each case, the first row of numbers is the same row of timings for this PR. The second row of numbers is the timings for the alternative. Note that converting the outer loop and the last loop, but not the inner loop, seems to be slower than master. I suspect these measurements are fairly noisy, but perhaps there's something else going on in that case. Regardless, this PR is the clear winner and we know why: `while` loops are faster than `for` loops. vs master; ```; (/ (/ (+ 49.736 50.335 48.197 51.034 47.737) 5); (/ (+ 62.9 55.362 57.100 57.815 60.5) 5)). 0.84; ```. vs inner and last only; ```; (/ (/ (+ 49.736 50.335 48.197 51.034 47.737) 5); (/ (+ 52.982 63.1 57.481 56.480 51.814) 5)). 0.88; ```. vs outer and last only; ```; (/ (/ (+ 49.736 50.335 48.197 51.034 47.737) 5); (/ (+ 74.6 69.0 55.750 69.7 68.5) 5)). 0.73; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1223
https://github.com/hail-is/hail/issues/1227:94,Availability,error,error,94,Stack trace from @lfrancioli ([full trace](https://nealelab.slack.com/files/laurent/F3P268282/error.txt)). ```; Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at org.broadinstitute.hail.utils.richUtils.RichIterable$$anon$4$$anon$10.next(RichIterable.scala:71); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:54); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:45); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$4$$anonfun$apply$1.apply(AnnotateVariantsExpr.scala:51); ⋮; ```. The iterator returned by the genotype stream has an additional constraint (over the `Iterator[T]` interface) that `hasNext` must be called before every call to `next`. The failing assertion verifies that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1227
https://github.com/hail-is/hail/issues/1227:763,Integrability,interface,interface,763,Stack trace from @lfrancioli ([full trace](https://nealelab.slack.com/files/laurent/F3P268282/error.txt)). ```; Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at org.broadinstitute.hail.utils.richUtils.RichIterable$$anon$4$$anon$10.next(RichIterable.scala:71); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:54); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:45); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$4$$anonfun$apply$1.apply(AnnotateVariantsExpr.scala:51); ⋮; ```. The iterator returned by the genotype stream has an additional constraint (over the `Iterator[T]` interface) that `hasNext` must be called before every call to `next`. The failing assertion verifies that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1227
https://github.com/hail-is/hail/issues/1227:133,Testability,Assert,AssertionError,133,Stack trace from @lfrancioli ([full trace](https://nealelab.slack.com/files/laurent/F3P268282/error.txt)). ```; Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at org.broadinstitute.hail.utils.richUtils.RichIterable$$anon$4$$anon$10.next(RichIterable.scala:71); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:54); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:45); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$4$$anonfun$apply$1.apply(AnnotateVariantsExpr.scala:51); ⋮; ```. The iterator returned by the genotype stream has an additional constraint (over the `Iterator[T]` interface) that `hasNext` must be called before every call to `next`. The failing assertion verifies that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1227
https://github.com/hail-is/hail/issues/1227:149,Testability,assert,assertion,149,Stack trace from @lfrancioli ([full trace](https://nealelab.slack.com/files/laurent/F3P268282/error.txt)). ```; Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at org.broadinstitute.hail.utils.richUtils.RichIterable$$anon$4$$anon$10.next(RichIterable.scala:71); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:54); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:45); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$4$$anonfun$apply$1.apply(AnnotateVariantsExpr.scala:51); ⋮; ```. The iterator returned by the genotype stream has an additional constraint (over the `Iterator[T]` interface) that `hasNext` must be called before every call to `next`. The failing assertion verifies that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1227
https://github.com/hail-is/hail/issues/1227:184,Testability,assert,assert,184,Stack trace from @lfrancioli ([full trace](https://nealelab.slack.com/files/laurent/F3P268282/error.txt)). ```; Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at org.broadinstitute.hail.utils.richUtils.RichIterable$$anon$4$$anon$10.next(RichIterable.scala:71); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:54); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:45); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$4$$anonfun$apply$1.apply(AnnotateVariantsExpr.scala:51); ⋮; ```. The iterator returned by the genotype stream has an additional constraint (over the `Iterator[T]` interface) that `hasNext` must be called before every call to `next`. The failing assertion verifies that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1227
https://github.com/hail-is/hail/issues/1227:845,Testability,assert,assertion,845,Stack trace from @lfrancioli ([full trace](https://nealelab.slack.com/files/laurent/F3P268282/error.txt)). ```; Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at org.broadinstitute.hail.utils.richUtils.RichIterable$$anon$4$$anon$10.next(RichIterable.scala:71); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:54); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:45); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$4$$anonfun$apply$1.apply(AnnotateVariantsExpr.scala:51); ⋮; ```. The iterator returned by the genotype stream has an additional constraint (over the `Iterator[T]` interface) that `hasNext` must be called before every call to `next`. The failing assertion verifies that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1227
https://github.com/hail-is/hail/issues/1236:159,Testability,Assert,AssertionError,159,"`importvcf gs://genomics-public-data/platinum-genomes/vcf/NA12877_S1.genome.vcf` fails with:. ```; hail: fatal: write: NA12877_S1.genome.vcf: caught java.lang.AssertionError: assertion failed: expected 2 alleles in genotype, but found 1; offending line: chr2 185102796 . C . . LowGQX DP=61;MQ=60;MQ0=0 GT:AD:DP:MQ ...; ```. This might be the same as https://github.com/hail-is/hail/issues/1010",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1236
https://github.com/hail-is/hail/issues/1236:175,Testability,assert,assertion,175,"`importvcf gs://genomics-public-data/platinum-genomes/vcf/NA12877_S1.genome.vcf` fails with:. ```; hail: fatal: write: NA12877_S1.genome.vcf: caught java.lang.AssertionError: assertion failed: expected 2 alleles in genotype, but found 1; offending line: chr2 185102796 . C . . LowGQX DP=61;MQ=60;MQ0=0 GT:AD:DP:MQ ...; ```. This might be the same as https://github.com/hail-is/hail/issues/1010",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1236
https://github.com/hail-is/hail/pull/1244:38,Deployability,install,installed,38,Use curl instead of wget. wget is not installed by default on OSX.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1244
https://github.com/hail-is/hail/issues/1245:36,Deployability,continuous,continuous,36,How do we build the OSX objects for continuous deployment?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1245
https://github.com/hail-is/hail/issues/1245:47,Deployability,deploy,deployment,47,How do we build the OSX objects for continuous deployment?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1245
https://github.com/hail-is/hail/pull/1255:180,Integrability,interface,interface,180,"Added functions to make annotations py4j-convertible.; Exposed global and sample annotations in python. This is the start of a long list of changes that need to be made before our interface starts to actually look nice in python. Doing `annotate_global_expr_by_sample` followed by `show_globals` to do aggregations is horrible -- here you can just do . ```; >>> europeans = vds.query_samples('samples.filter(s => sa.pop == ""EUR"").collect()'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1255
https://github.com/hail-is/hail/pull/1255:55,Security,Expose,Exposed,55,"Added functions to make annotations py4j-convertible.; Exposed global and sample annotations in python. This is the start of a long list of changes that need to be made before our interface starts to actually look nice in python. Doing `annotate_global_expr_by_sample` followed by `show_globals` to do aggregations is horrible -- here you can just do . ```; >>> europeans = vds.query_samples('samples.filter(s => sa.pop == ""EUR"").collect()'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1255
https://github.com/hail-is/hail/pull/1259:192,Testability,Log,LogReg,192,"- removed relics of command line; - improved python options, e.g. covarites given as list of string; - factored out common ops like covariate matrix code to stats/RegressionUtils; - converted LogReg to mapAnnotations, consistent with linreg and lmmreg; - removed LinearRegressionCommand and LogisticRegressionCommand. Question: here I pass the covariates from Python to Scala as an Array of String, then merge the string on comma for the parser. We could pass it as a List of String, making jlist like jarray. In either case, I think our parsers should take the sequence, rather than have to re-split. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259
https://github.com/hail-is/hail/pull/1259:291,Testability,Log,LogisticRegressionCommand,291,"- removed relics of command line; - improved python options, e.g. covarites given as list of string; - factored out common ops like covariate matrix code to stats/RegressionUtils; - converted LogReg to mapAnnotations, consistent with linreg and lmmreg; - removed LinearRegressionCommand and LogisticRegressionCommand. Question: here I pass the covariates from Python to Scala as an Array of String, then merge the string on comma for the parser. We could pass it as a List of String, making jlist like jarray. In either case, I think our parsers should take the sequence, rather than have to re-split. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1259
https://github.com/hail-is/hail/issues/1260:6218,Availability,failure,failure,6218,"mand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:6275,Availability,failure,failure,6275,"Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:7294,Availability,Error,Error,7294,"in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), Struc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:10792,Deployability,install,installDist,10792,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:6865,Energy Efficiency,schedul,scheduler,6865,"IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayTy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:6936,Energy Efficiency,schedul,scheduler,6936,"ef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,String",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:2591,Performance,load,load,2591,"5.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37833.; 17/01/17 09:24:46 INFO NettyBlockTransferService: Server created on 129.94.72.55:37833; 17/01/17 09:24:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMasterEndpoint: Registering block manager 129.94.72.55:37833 with 15.8 GB RAM, BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 129.94.72.55, 37833); hail: info: running: read test.in.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: running: annotatevariants expr -c 'va = {}'; hail: info: running: write -o test.out.vds; [Stage 1:==> (1 + 24) / 25]hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.InsertInto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:7058,Performance,concurren,concurrent,7058,"e.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateextern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:7142,Performance,concurren,concurrent,7142,"n(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:2983,Safety,abort,aborted,2983,"ed service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37833.; 17/01/17 09:24:46 INFO NettyBlockTransferService: Server created on 129.94.72.55:37833; 17/01/17 09:24:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMasterEndpoint: Registering block manager 129.94.72.55:37833 with 15.8 GB RAM, BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 129.94.72.55, 37833); hail: info: running: read test.in.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: running: annotatevariants expr -c 'va = {}'; hail: info: running: write -o test.out.vds; [Stage 1:==> (1 + 24) / 25]hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); at org.apache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:6197,Safety,abort,aborted,6197,"mand(Command.scala:259); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:7506,Safety,unsafe,unsafe,7506,".apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8Str",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:8490,Safety,unsafe,unsafe,8490,"apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:9356,Safety,unsafe,unsafe,9356,"gType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:9707,Safety,unsafe,unsafe,9707,"), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:646,Security,Secur,SecurityManager,646,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:717,Security,Secur,SecurityManager,717,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:790,Security,Secur,SecurityManager,790,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:861,Security,Secur,SecurityManager,861,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:934,Security,Secur,SecurityManager,934,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:951,Security,Secur,SecurityManager,951,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:968,Security,authenticat,authentication,968,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:7555,Security,validat,validateexternaltype,7555,"$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:7596,Security,validat,validateexternaltype,7596,"$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:8009,Security,validat,validateexternaltype,8009,"tTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:8050,Security,validat,validateexternaltype,8050,"tTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:8539,Security,validat,validateexternaltype,8539,"(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructFiel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:8580,Security,validat,validateexternaltype,8580,"(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructFiel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:9100,Security,validat,validateexternaltype,9100,"ue], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:9405,Security,validat,validateexternaltype,9405,"invoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:9446,Security,validat,validateexternaltype,9446,"invoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:9756,Security,validat,validateexternaltype,9756,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:9797,Security,validat,validateexternaltype,9797,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:10018,Security,validat,validateexternaltype,10018,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:10059,Security,validat,validateexternaltype,10059,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:71,Testability,test,test,71,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:127,Testability,test,test,127,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:270,Testability,log,logger,270,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:413,Testability,log,logging,413,"The following command always fails at the write stage:; ```; hail read test.in.vds annotatevariants expr -c 'va = {}' write -o test.out.vds; ```. The traceback is huge, but I've copied what I think is the relevant parts:; ```; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/01/17 09:24:46 INFO SparkContext: Running Spark version 2.0.2; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls to: marpin; 17/01/17 09:24:46 INFO SecurityManager: Changing view acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: Changing modify acls groups to:; 17/01/17 09:24:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marpin); groups with view permissions: Set(); users with modify permissions: Set(marpin); groups with modify permissions: Set(); 17/01/17 09:24:46 INFO Utils: Successfully started service 'sparkDriver' on port 37801.; 17/01/17 09:24:46 INFO SparkEnv: Registering MapOutputTracker; 17/01/17 09:24:46 INFO SparkEnv: Registering BlockManagerMaster; 17/01/17 09:24:46 INFO DiskBlockManager: Created local directory at ; /tmp/hail/blockmgr-522fbeb1-5053-4884-9115-5f2af7bd912a; 17/01/17 09:24:46 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:2561,Testability,test,test,2561,"ryStore: MemoryStore started with capacity 15.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37833.; 17/01/17 09:24:46 INFO NettyBlockTransferService: Server created on 129.94.72.55:37833; 17/01/17 09:24:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMasterEndpoint: Registering block manager 129.94.72.55:37833 with 15.8 GB RAM, BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 129.94.72.55, 37833); hail: info: running: read test.in.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: running: annotatevariants expr -c 'va = {}'; hail: info: running: write -o test.out.vds; [Stage 1:==> (1 + 24) / 25]hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:2680,Testability,log,logger,2680,"46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37833.; 17/01/17 09:24:46 INFO NettyBlockTransferService: Server created on 129.94.72.55:37833; 17/01/17 09:24:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMasterEndpoint: Registering block manager 129.94.72.55:37833 with 15.8 GB RAM, BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 129.94.72.55, 37833); hail: info: running: read test.in.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: running: annotatevariants expr -c 'va = {}'; hail: info: running: write -o test.out.vds; [Stage 1:==> (1 + 24) / 25]hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:2874,Testability,test,test,2874,"9.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37833.; 17/01/17 09:24:46 INFO NettyBlockTransferService: Server created on 129.94.72.55:37833; 17/01/17 09:24:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMasterEndpoint: Registering block manager 129.94.72.55:37833 with 15.8 GB RAM, BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 129.94.72.55, 37833); hail: info: running: read test.in.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: running: annotatevariants expr -c 'va = {}'; hail: info: running: write -o test.out.vds; [Stage 1:==> (1 + 24) / 25]hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); at org.apache.spark.sql.execution.command.ExecutedCommandExe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:7637,Testability,assert,assertnotnull,7637,"$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:8091,Testability,assert,assertnotnull,8091,"tTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: org.apache.spark.sql.catalyst.expressions.GenericRow is not a valid external type for schema of boolean; named_struct(contig, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:8621,Testability,assert,assertnotnull,8621,"(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 0, contig), StringType), true), start, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 1, start), IntegerType), ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructFiel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:10100,Testability,assert,assertnotnull,10100,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:10587,Testability,test,test,10587,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:10628,Testability,test,test,10628,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:10693,Testability,test,test,10693,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/issues/1260:10714,Testability,Test,Tested,10714,",StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 2, ref), StringType), true), altAlleles, mapobjects(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object), if (isnull(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)))) null else named_struct(ref, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 0, ref), StringType), true), alt, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObjects_loopValue8, MapObjects_loopIsNull9, ObjectType(class java.lang.Object)), StructField(ref,StringType,false), StructField(alt,StringType,false)), 1, alt), StringType), true)), validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, variant), StructField(contig,StringType,false), StructField(start,IntegerType,false), StructField(ref,StringType,false), StructField(altAlleles,ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false),false)), 3, altAlleles), ArrayType(StructType(StructField(ref,StringType,false), StructField(alt,StringType,false)),false)))) AS variant#8; ```. Attached is a toy test.in.vds that reproduces the problem [test.in.vds.tar.gz](https://github.com/hail-is/hail/files/709524/test.in.vds.tar.gz). Tested on a clean ed544897f04722142b14b8e620614587c8f398a0 built with gradlew installDist.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1260
https://github.com/hail-is/hail/pull/1266:24,Testability,test,tests,24,I also disabled Spark 1 tests in CI.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1266
https://github.com/hail-is/hail/pull/1271:55,Usability,feedback,feedback,55,**This is not meant to be merged yet!**. I want to get feedback on the structure I have in place before I fill in all of the docstrings and parameter descriptions. I also need to clean up some of the code. The ``fet`` function and ``Variant`` object methods are the prototypes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1271
https://github.com/hail-is/hail/issues/1274:22,Availability,error,errors,22,"Getting the following errors when compiling on a Mac. Any suggestions?. ./gradlew shadowJar ; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/farrell/github/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/darwin; c++ -fvisibility=hidden -dynamiclib -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/darwin/libibs.dylib; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:23:no such instruction: `vmovd %xmm0, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:38:no such instruction: `vpextrq $1, %xmm0,%rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:79:no such instruction: `vpcmpeqd %xmm5, %xmm5,%xmm5'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:96:no such instruction: `vpxor %xmm1, %xmm0,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:114:no such instruction: `vpxor %xmm5, %xmm1,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:135:no such instruction: `vpand %xmm3, %xmm2,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:150:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:167:no such instruction: `vpxor %xmm5, %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:193:no such instruction: `vpor LC1(%rip), %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:212:no such instruction: `vpand %xmm1, %xmm0,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:225:no such instruction: `vpandn %xmm4, %xmm3,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:241:no such instruction: `vmovd %xmm4, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:255:no such instruction: `vpxor %xmm1, %xmm0,%xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:277:no such instruction: `v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274
https://github.com/hail-is/hail/issues/1274:10935,Availability,Error,Error,10935,"lm0m0000gn/T//ccuBKQk1.s:1675:no such instruction: `vmovaps %xmm0, (%r11,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1686:no such instruction: `vpxor -16(%rsi), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1699:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1713:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1725:no such instruction: `vmovaps %xmm0, (%r14,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1901:no such instruction: `vmovdqa LC1(%rip), %xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1916:no such instruction: `vpxor (%rdx), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1929:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1953:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1970:no such instruction: `vmovaps %xmm0, (%r8,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1981:no such instruction: `vpxor -16(%r11), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1994:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:2008:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:2020:no such instruction: `vmovaps %xmm0, (%r9,%rax)'; make: *** [lib/darwin/libibs.dylib] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.99 secs; Johns-MacBook-Pro-7:hail farrell$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274
https://github.com/hail-is/hail/issues/1274:10963,Availability,FAILURE,FAILURE,10963,"lm0m0000gn/T//ccuBKQk1.s:1675:no such instruction: `vmovaps %xmm0, (%r11,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1686:no such instruction: `vpxor -16(%rsi), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1699:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1713:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1725:no such instruction: `vmovaps %xmm0, (%r14,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1901:no such instruction: `vmovdqa LC1(%rip), %xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1916:no such instruction: `vpxor (%rdx), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1929:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1953:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1970:no such instruction: `vmovaps %xmm0, (%r8,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1981:no such instruction: `vpxor -16(%r11), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1994:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:2008:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:2020:no such instruction: `vmovaps %xmm0, (%r9,%rax)'; make: *** [lib/darwin/libibs.dylib] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.99 secs; Johns-MacBook-Pro-7:hail farrell$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274
https://github.com/hail-is/hail/issues/1274:169,Modifiability,Config,Configuring,169,"Getting the following errors when compiling on a Mac. Any suggestions?. ./gradlew shadowJar ; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/farrell/github/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/darwin; c++ -fvisibility=hidden -dynamiclib -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/darwin/libibs.dylib; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:23:no such instruction: `vmovd %xmm0, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:38:no such instruction: `vpextrq $1, %xmm0,%rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:79:no such instruction: `vpcmpeqd %xmm5, %xmm5,%xmm5'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:96:no such instruction: `vpxor %xmm1, %xmm0,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:114:no such instruction: `vpxor %xmm5, %xmm1,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:135:no such instruction: `vpand %xmm3, %xmm2,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:150:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:167:no such instruction: `vpxor %xmm5, %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:193:no such instruction: `vpor LC1(%rip), %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:212:no such instruction: `vpand %xmm1, %xmm0,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:225:no such instruction: `vpandn %xmm4, %xmm3,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:241:no such instruction: `vmovd %xmm4, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:255:no such instruction: `vpxor %xmm1, %xmm0,%xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:277:no such instruction: `v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274
https://github.com/hail-is/hail/issues/1274:11236,Testability,log,log,11236,"lm0m0000gn/T//ccuBKQk1.s:1675:no such instruction: `vmovaps %xmm0, (%r11,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1686:no such instruction: `vpxor -16(%rsi), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1699:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1713:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1725:no such instruction: `vmovaps %xmm0, (%r14,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1901:no such instruction: `vmovdqa LC1(%rip), %xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1916:no such instruction: `vpxor (%rdx), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1929:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1953:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1970:no such instruction: `vmovaps %xmm0, (%r8,%rax)'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1981:no such instruction: `vpxor -16(%r11), %xmm2,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:1994:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:2008:no such instruction: `vpor %xmm1, %xmm0,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:2020:no such instruction: `vmovaps %xmm0, (%r9,%rax)'; make: *** [lib/darwin/libibs.dylib] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.99 secs; Johns-MacBook-Pro-7:hail farrell$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1274
https://github.com/hail-is/hail/issues/1275:1005,Availability,error,error,1005,"script I am running:. ```python; rf_kt =(; hc.read(rf_path,sites_only=True); .variants_keytable(); .flatten(); .select(['va.info.MQRankSum']); ); print(rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:1111,Availability,failure,failure,1111,"ble(); .flatten(); .select(['va.info.MQRankSum']); ); print(rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:1169,Availability,failure,failure,1169,"rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:68); at is.hail.keytabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3246,Energy Efficiency,schedul,scheduler,3246,ext(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3317,Energy Efficiency,schedul,scheduler,3317,rator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:8,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3676,Energy Efficiency,schedul,scheduler,3676,ql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3716,Energy Efficiency,schedul,scheduler,3716,); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3814,Energy Efficiency,schedul,scheduler,3814,:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3911,Energy Efficiency,schedul,scheduler,3911,cala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4162,Energy Efficiency,schedul,scheduler,4162,319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4242,Energy Efficiency,schedul,scheduler,4242,cheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.sql.execution.SparkPlan.executeTake(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4347,Energy Efficiency,schedul,scheduler,4347,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347); at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4495,Energy Efficiency,schedul,scheduler,4495,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347); at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39); at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193); at org.apache.spark.sql.execution.SQLE,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4583,Energy Efficiency,schedul,scheduler,4583,617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347); at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39); at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.Dataset.wit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4680,Energy Efficiency,schedul,scheduler,4680,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347); at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39); at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546); at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4775,Energy Efficiency,schedul,scheduler,4775,.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347); at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39); at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546); at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192); at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4938,Energy Efficiency,schedul,scheduler,4938,abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347); at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39); at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546); at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192); at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199); at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1935); at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Datas,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:8899,Energy Efficiency,schedul,scheduler,8899,68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:65); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:8970,Energy Efficiency,schedul,scheduler,8970,68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:65); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:928,Integrability,protocol,protocol,928,"This is the script I am running:. ```python; rf_kt =(; hc.read(rf_path,sites_only=True); .variants_keytable(); .flatten(); .select(['va.info.MQRankSum']); ); print(rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:978,Integrability,protocol,protocol,978,"This is the script I am running:. ```python; rf_kt =(; hc.read(rf_path,sites_only=True); .variants_keytable(); .flatten(); .select(['va.info.MQRankSum']); ); print(rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3439,Performance,concurren,concurrent,3439,xt(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3523,Performance,concurren,concurrent,3523,9); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:9092,Performance,concurren,concurrent,9092,68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:65); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:9176,Performance,concurren,concurrent,9176,68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:65); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:1090,Safety,abort,aborted,1090,"ble(); .flatten(); .select(['va.info.MQRankSum']); ); print(rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3846,Safety,abort,abortStage,3846,park.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:3943,Safety,abort,abortStage,3943,he.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:4185,Safety,abort,abortStage,4185,RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1275:513,Testability,test,test,513,"This is the script I am running:. ```python; rf_kt =(; hc.read(rf_path,sites_only=True); .variants_keytable(); .flatten(); .select(['va.info.MQRankSum']); ); print(rf_kt.schema()). rf_df = rf_kt.to_dataframe(); rf_df.printSchema(); rf_df.show(); ```. And here is the output and stacktrace as it crashes when running `show()`:. ```; Struct {; `va.info.MQRankSum`: Double; }; root; |-- va.info.MQRankSum: double (nullable = true); Traceback (most recent call last):; File ""/tmp/db4d8e04-85c9-4eba-b0b3-4ade69200fd3/test.py"", line 60, in <module>; rf_df.show(); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o73.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 2520, gnomad-prod-sw-s89f.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 21; at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8$$anonfun$10.apply(KeyTable.scala:68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1275
https://github.com/hail-is/hail/issues/1283:52,Availability,down,downcode,52,"filteralleles, or more specifically, the subset and downcode processes do not particularly work on hardcalls (it finds the next most likely genotypes in the PLs, which are nonexistent). One possible workaround would be to force users to `filter_altered_genotypes=True` but this will probably require some new code. But otherwise, this should definitely throw an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1283
https://github.com/hail-is/hail/issues/1283:362,Availability,error,error,362,"filteralleles, or more specifically, the subset and downcode processes do not particularly work on hardcalls (it finds the next most likely genotypes in the PLs, which are nonexistent). One possible workaround would be to force users to `filter_altered_genotypes=True` but this will probably require some new code. But otherwise, this should definitely throw an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1283
https://github.com/hail-is/hail/issues/1284:874,Availability,error,error,874,"this the exception (attached the log):. ``` ; File ""/tmp/7f8f775e-2ec3-40ee-ad5b-3e4df5649682/annotate_and_generate_scores_cloud.py"", line 24, in <module>; .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/dataset.py"", line 3095, in vep; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/context.py"", line 81, in run_command; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/java.py"", line 5, in jarray; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 228, in __setitem__; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 211, in __set_item; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:; java.lang.IllegalArgumentException: array element type mismatch; at java.lang.reflect.Array.set(Native Method); at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:141); at py4j.commands.ArrayCommand.execute(ArrayCommand.java:94); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745) ; ```. [hail.txt](https://github.com/hail-is/hail/files/725407/hail.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1284
https://github.com/hail-is/hail/issues/1284:801,Integrability,protocol,protocol,801,"this the exception (attached the log):. ``` ; File ""/tmp/7f8f775e-2ec3-40ee-ad5b-3e4df5649682/annotate_and_generate_scores_cloud.py"", line 24, in <module>; .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/dataset.py"", line 3095, in vep; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/context.py"", line 81, in run_command; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/java.py"", line 5, in jarray; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 228, in __setitem__; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 211, in __set_item; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:; java.lang.IllegalArgumentException: array element type mismatch; at java.lang.reflect.Array.set(Native Method); at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:141); at py4j.commands.ArrayCommand.execute(ArrayCommand.java:94); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745) ; ```. [hail.txt](https://github.com/hail-is/hail/files/725407/hail.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1284
https://github.com/hail-is/hail/issues/1284:851,Integrability,protocol,protocol,851,"this the exception (attached the log):. ``` ; File ""/tmp/7f8f775e-2ec3-40ee-ad5b-3e4df5649682/annotate_and_generate_scores_cloud.py"", line 24, in <module>; .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/dataset.py"", line 3095, in vep; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/context.py"", line 81, in run_command; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/java.py"", line 5, in jarray; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 228, in __setitem__; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 211, in __set_item; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:; java.lang.IllegalArgumentException: array element type mismatch; at java.lang.reflect.Array.set(Native Method); at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:141); at py4j.commands.ArrayCommand.execute(ArrayCommand.java:94); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745) ; ```. [hail.txt](https://github.com/hail-is/hail/files/725407/hail.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1284
https://github.com/hail-is/hail/issues/1284:161,Modifiability,config,config,161,"this the exception (attached the log):. ``` ; File ""/tmp/7f8f775e-2ec3-40ee-ad5b-3e4df5649682/annotate_and_generate_scores_cloud.py"", line 24, in <module>; .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/dataset.py"", line 3095, in vep; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/context.py"", line 81, in run_command; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/java.py"", line 5, in jarray; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 228, in __setitem__; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 211, in __set_item; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:; java.lang.IllegalArgumentException: array element type mismatch; at java.lang.reflect.Array.set(Native Method); at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:141); at py4j.commands.ArrayCommand.execute(ArrayCommand.java:94); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745) ; ```. [hail.txt](https://github.com/hail-is/hail/files/725407/hail.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1284
https://github.com/hail-is/hail/issues/1284:33,Testability,log,log,33,"this the exception (attached the log):. ``` ; File ""/tmp/7f8f775e-2ec3-40ee-ad5b-3e4df5649682/annotate_and_generate_scores_cloud.py"", line 24, in <module>; .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/dataset.py"", line 3095, in vep; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/context.py"", line 81, in run_command; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/java.py"", line 5, in jarray; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 228, in __setitem__; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 211, in __set_item; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:; java.lang.IllegalArgumentException: array element type mismatch; at java.lang.reflect.Array.set(Native Method); at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:141); at py4j.commands.ArrayCommand.execute(ArrayCommand.java:94); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745) ; ```. [hail.txt](https://github.com/hail-is/hail/files/725407/hail.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1284
https://github.com/hail-is/hail/pull/1285:72,Deployability,update,update,72,Require partitioner.json.gz in VSM.read. Added hc.write_partitioning to update legacy VDSes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1285
https://github.com/hail-is/hail/issues/1286:303,Availability,failure,failures,303,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:685,Availability,failure,failures,685,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:1428,Availability,error,error,1428,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:1208,Integrability,message,messages,1208,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:1434,Integrability,message,messages,1434,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:12,Testability,test,tests,12,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:60,Testability,assert,assert,60,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:99,Testability,assert,assert,99,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:138,Testability,assert,assert,138,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:173,Testability,assert,assert,173,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:212,Testability,assert,assert,212,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:252,Testability,assert,assert,252,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:298,Testability,Test,Test,298,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:394,Testability,test,tests,394,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:442,Testability,assert,assert,442,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:480,Testability,assert,assert,480,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:518,Testability,assert,assert,518,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:553,Testability,assert,assert,553,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:591,Testability,assert,assert,591,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:630,Testability,assert,assert,630,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:680,Testability,test,test,680,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:783,Testability,Test,TestFailedException,783,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:1259,Testability,Test,TestFailedException,1259,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/issues/1286:335,Usability,simpl,simply,335,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1286
https://github.com/hail-is/hail/pull/1288:341,Usability,feedback,feedback,341,"This is mostly done, though I'll give it another pass tomorrow to see if I catch any problems and to remove some unneeded text file exports and imports. I also want to create one script at the end of tutorial that can be easily copy and pasted and runs the whole tutorial. Just wanted to post it so people could start looking at it and give feedback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1288
https://github.com/hail-is/hail/pull/1289:204,Deployability,update,updated,204,"fixed numeric aggregations behavior on empty arrays and sets; - modified min, max, mean, and median functions in FunctionRegistry; - added regression tests for empty sets and empty arrays to ExprSuite; - updated and refactored ExpressionLanguage docs to reflect changes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1289
https://github.com/hail-is/hail/pull/1289:216,Modifiability,refactor,refactored,216,"fixed numeric aggregations behavior on empty arrays and sets; - modified min, max, mean, and median functions in FunctionRegistry; - added regression tests for empty sets and empty arrays to ExprSuite; - updated and refactored ExpressionLanguage docs to reflect changes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1289
https://github.com/hail-is/hail/pull/1289:150,Testability,test,tests,150,"fixed numeric aggregations behavior on empty arrays and sets; - modified min, max, mean, and median functions in FunctionRegistry; - added regression tests for empty sets and empty arrays to ExprSuite; - updated and refactored ExpressionLanguage docs to reflect changes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1289
https://github.com/hail-is/hail/pull/1292:180,Performance,perform,performance,180,"This was identified as a cost center during my compiler investigations. The effect is not measurable on the standard benchmark [1]. More investigation is needed. As we improve the performance of the expression language, I suspect this will constitute a more significant fraction of execution time. [1]:; ```; filtergenotypes -c ' g.dp > 400 ||; (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) ||; (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) ||; (g.isHet && ( (g.ad[0] + g.ad[1]) / g.dp < 0.9 || g.ad[1] / g.dp < 0.20 || g.pl[0] < 20 ))' --keep; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1292
https://github.com/hail-is/hail/pull/1292:117,Testability,benchmark,benchmark,117,"This was identified as a cost center during my compiler investigations. The effect is not measurable on the standard benchmark [1]. More investigation is needed. As we improve the performance of the expression language, I suspect this will constitute a more significant fraction of execution time. [1]:; ```; filtergenotypes -c ' g.dp > 400 ||; (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) ||; (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) ||; (g.isHet && ( (g.ad[0] + g.ad[1]) / g.dp < 0.9 || g.ad[1] / g.dp < 0.20 || g.pl[0] < 20 ))' --keep; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1292
https://github.com/hail-is/hail/issues/1293:777,Integrability,interface,interface,777,"```; vds.annotate_samples_expr('sa = gs.map(g => g.oneHotGenotype(v)).collect()[0]').annotate_global_expr_by_sample('global = samples.map(s => sa).collect()[0]').annotate_variants_expr('va = global'); ```. ```; FatalError: NotSerializableException: is.hail.variant.Genotype$$anonfun$oneHotGenotype$1$$anon$2; Serialization stack:; 	- object not serializable (class: is.hail.variant.Genotype$$anonfun$oneHotGenotype$1$$anon$2, value: Genotype(0, 0, 0, 0, 0, 0, 0, 0, 1, 0)); 	- field (class: is.hail.driver.AnnotateVariantsExpr$$anonfun$4, name: localGlobalAnnotation$1, type: class java.lang.Object); 	- object (class is.hail.driver.AnnotateVariantsExpr$$anonfun$4, <function3>); 	- field (class: is.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1, name: f$1, type: interface scala.Function3); 	- object (class is.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1, <function2>); 	- field (class: is.hail.utils.richUtils.RichPairRDD$$anonfun$mapValuesWithKey$extension$1, name: f$1, type: interface scala.Function2); 	- object (class is.hail.utils.richUtils.RichPairRDD$$anonfun$mapValuesWithKey$extension$1, <function1>)```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1293
https://github.com/hail-is/hail/issues/1293:1008,Integrability,interface,interface,1008,"```; vds.annotate_samples_expr('sa = gs.map(g => g.oneHotGenotype(v)).collect()[0]').annotate_global_expr_by_sample('global = samples.map(s => sa).collect()[0]').annotate_variants_expr('va = global'); ```. ```; FatalError: NotSerializableException: is.hail.variant.Genotype$$anonfun$oneHotGenotype$1$$anon$2; Serialization stack:; 	- object not serializable (class: is.hail.variant.Genotype$$anonfun$oneHotGenotype$1$$anon$2, value: Genotype(0, 0, 0, 0, 0, 0, 0, 0, 1, 0)); 	- field (class: is.hail.driver.AnnotateVariantsExpr$$anonfun$4, name: localGlobalAnnotation$1, type: class java.lang.Object); 	- object (class is.hail.driver.AnnotateVariantsExpr$$anonfun$4, <function3>); 	- field (class: is.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1, name: f$1, type: interface scala.Function3); 	- object (class is.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1, <function2>); 	- field (class: is.hail.utils.richUtils.RichPairRDD$$anonfun$mapValuesWithKey$extension$1, name: f$1, type: interface scala.Function2); 	- object (class is.hail.utils.richUtils.RichPairRDD$$anonfun$mapValuesWithKey$extension$1, <function1>)```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1293
https://github.com/hail-is/hail/pull/1297:318,Testability,test,tests,318,"Includes:; - Type system in python that mirrors scala; - Annotation conversion system, and full support for all annotation objects in python; - query_variants and query_samples functions on VDS that return python objects; - implemented Variant, Genotype, etc as first class python objects and fully documented them; - tests for the above",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1297
https://github.com/hail-is/hail/issues/1307:19,Deployability,deploy,deployment,19,Maybe do automatic deployment here? With precompiled binaries?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1307
https://github.com/hail-is/hail/issues/1311:1007,Availability,error,error,1007,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/issues/1311:1145,Availability,failure,failure,1145,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/issues/1311:1203,Availability,failure,failure,1203,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/issues/1311:930,Integrability,protocol,protocol,930,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/issues/1311:980,Integrability,protocol,protocol,980,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/issues/1311:89,Modifiability,sandbox,sandbox,89,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/issues/1311:337,Performance,load,loads,337,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/issues/1311:1124,Safety,abort,aborted,1124,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/issues/1311:89,Testability,sandbox,sandbox,89,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1311
https://github.com/hail-is/hail/pull/1312:47,Testability,test,test,47,"- added skipLEB128 to byte iterator; - modifed test in GenotypeStreamSuite. Before merging, we should run a larger number of generated examples through the test. @cseed any advise?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1312
https://github.com/hail-is/hail/pull/1312:156,Testability,test,test,156,"- added skipLEB128 to byte iterator; - modifed test in GenotypeStreamSuite. Before merging, we should run a larger number of generated examples through the test. @cseed any advise?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1312
https://github.com/hail-is/hail/issues/1319:258,Testability,test,test,258,"I want to repeat the same expression for all the columns of a keytable. For example doing something like this:. ```; (fin_vds_split_anno; .make_keytable('gene=va.geneann.gene','gt = g.gt',[]); .aggregate_by_key('gene=gene','SUMID=*.gt.sum'); .export(root + 'test.tsv')); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1319
https://github.com/hail-is/hail/pull/1322:8,Testability,test,test,8,"- added test to GenotypeStreamSuite; - moved Boolean functions like isHet back to Genotype object as functions on gt: Int, for wider use with hardCallIterator. Eager for feedback on these steps and next steps.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1322
https://github.com/hail-is/hail/pull/1322:170,Usability,feedback,feedback,170,"- added test to GenotypeStreamSuite; - moved Boolean functions like isHet back to Genotype object as functions on gt: Int, for wider use with hardCallIterator. Eager for feedback on these steps and next steps.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1322
https://github.com/hail-is/hail/issues/1327:237,Availability,down,down,237,"This opens the possibility for compiler differences to fail builds for our users. The CI server should simply set CXX and CC to clang and rebuild hail. Moreover, we need to ensure the hail build system passes these variables all the way down to `libsimdpp`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327
https://github.com/hail-is/hail/issues/1327:215,Modifiability,variab,variables,215,"This opens the possibility for compiler differences to fail builds for our users. The CI server should simply set CXX and CC to clang and rebuild hail. Moreover, we need to ensure the hail build system passes these variables all the way down to `libsimdpp`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327
https://github.com/hail-is/hail/issues/1327:103,Usability,simpl,simply,103,"This opens the possibility for compiler differences to fail builds for our users. The CI server should simply set CXX and CC to clang and rebuild hail. Moreover, we need to ensure the hail build system passes these variables all the way down to `libsimdpp`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1327
https://github.com/hail-is/hail/pull/1330:105,Safety,safe,safe,105,"`inttypes.h` is definitely needed for standards compliant usage of; printf format strings. The latter is safe, but should not be necessary; on recent C++ compilers with recent stdlibs. Compiling with a C++ compiler and a somewhat old version of glibc will; not provide PRIu64 and friends unless this macro is defined before; including `inttypes.h`. For more details see [1] with associated comments; and the related bug fix [2] which landed in glibc 2.18. [1] http://stackoverflow.com/questions/8132399/how-to-printf-uint64-t; [2] https://sourceware.org/bugzilla/show_bug.cgi?id=15366",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1330
https://github.com/hail-is/hail/pull/1331:17,Availability,error,error,17,This prevents an error in _run_command where we attempt to put an int into an array of `java.lang.String`s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1331
https://github.com/hail-is/hail/pull/1340:160,Availability,error,errors,160,Based on https://clang.llvm.org/cxx_status.html and https://gcc.gnu.org/gcc-4.7/cxx0x_status.html Clang 3.3 and GCC 4.7 should be sufficient to prevent compile errors arising from not supporting C++11 features. FYI @cseed @tpoterba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1340
https://github.com/hail-is/hail/issues/1341:444,Availability,error,error-no-such-instruction-while-assembling-project-on-mac-os-x,444,It appears (see [1] and [2]) that compiling AVX2 instructions (which hail uses to calculate IBD quickly) on a Mac using some versions of MacPorts GCC doesn't work. The Hail team recommends compiling with Clang when on Mac OS X. We _do not recommend_ removing AVX2 compatibility (either by adding `-mno-avx` or removing `-march=native`) because the AVX2 instructions are vital to IBD performance. [1] http://stackoverflow.com/questions/10327939/error-no-such-instruction-while-assembling-project-on-mac-os-x; [2] https://github.com/Theano/Theano/issues/1980,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1341
https://github.com/hail-is/hail/issues/1341:383,Performance,perform,performance,383,It appears (see [1] and [2]) that compiling AVX2 instructions (which hail uses to calculate IBD quickly) on a Mac using some versions of MacPorts GCC doesn't work. The Hail team recommends compiling with Clang when on Mac OS X. We _do not recommend_ removing AVX2 compatibility (either by adding `-mno-avx` or removing `-march=native`) because the AVX2 instructions are vital to IBD performance. [1] http://stackoverflow.com/questions/10327939/error-no-such-instruction-while-assembling-project-on-mac-os-x; [2] https://github.com/Theano/Theano/issues/1980,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1341
https://github.com/hail-is/hail/pull/1342:43,Availability,error,error,43,"Remarkably, this is just a warning, not an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1342
https://github.com/hail-is/hail/issues/1349:281,Modifiability,variab,variable,281,"Also may need to add back annotate_global_expr. Either way include these examples which used to be un the FAQ:. **How do I access an annotation name with white-space in the Hail Expression Language?**. Put the annotation name in back ticks. ```; annotateglobal expr -c 'global.`my variable` = global.`lof count`'; ```. **How do I count the number of samples matching a phenotype annotation?**. ```; annotateglobal expr -c '; global.nMales = samples.count(sa.pheno.sex == ""Male""),; global.nFemales = samples.count(sa.pheno.sex == ""Female""),; global.nSamples = samples.count(true)'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1349
https://github.com/hail-is/hail/issues/1349:123,Security,access,access,123,"Also may need to add back annotate_global_expr. Either way include these examples which used to be un the FAQ:. **How do I access an annotation name with white-space in the Hail Expression Language?**. Put the annotation name in back ticks. ```; annotateglobal expr -c 'global.`my variable` = global.`lof count`'; ```. **How do I count the number of samples matching a phenotype annotation?**. ```; annotateglobal expr -c '; global.nMales = samples.count(sa.pheno.sex == ""Male""),; global.nFemales = samples.count(sa.pheno.sex == ""Female""),; global.nSamples = samples.count(true)'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1349
https://github.com/hail-is/hail/issues/1351:52,Availability,avail,available,52,"In the following hail call, the `sa` binding is not available in the filter's lambda argument. In almost all modern programming languages, bindings are lexical, descending all the way into nested code. We would like to support the same intuitive notion of binding in hail. ```; vds.annotate_samples_expr(; ""sa.mendel = gs.filter(g => va.mendel.filter(x => x.fam == sa.fam).length).count()""); ```. ### Design Suggestion. As we move towards the compiler, this should become more natural because these filters will always be inlined. We need only not reset the environment when descending into a lambda.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1351
https://github.com/hail-is/hail/issues/1351:236,Usability,intuit,intuitive,236,"In the following hail call, the `sa` binding is not available in the filter's lambda argument. In almost all modern programming languages, bindings are lexical, descending all the way into nested code. We would like to support the same intuitive notion of binding in hail. ```; vds.annotate_samples_expr(; ""sa.mendel = gs.filter(g => va.mendel.filter(x => x.fam == sa.fam).length).count()""); ```. ### Design Suggestion. As we move towards the compiler, this should become more natural because these filters will always be inlined. We need only not reset the environment when descending into a lambda.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1351
https://github.com/hail-is/hail/pull/1358:22,Safety,avoid,avoid,22,"Had to cast toLong to avoid overflow on multiplication, cast back to Int because number of partitions must be an integer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1358
https://github.com/hail-is/hail/pull/1359:220,Modifiability,variab,variable,220,"If you follow the call path of a `org.apache.hadoop.fs.LocalFileSystem.mkdirs`,; you'll find that it's actually implemented on; `org.apache.hadoop.fs.FilterFileSystem` which implements it in terms of some; internal `fs` variable, which in `LocalFileSystem`'s case is; `org.apache.hadoop.fs.RawLocalFileSystem`. This class has `mkOneDirWithMode`,; which delegates to `java.io.File`. Now we're at ground truth. This is a well; documented Java API. It returns true if the directory was created, false; otherwise. No `IOException`s. Going back up through the callstack, we find that; `IOException`s are thrown for a variety of unusual circumstances (like if; you're creating `/foo/bar/baz` and `/foo/bar` is a file that isn't a; directory), but, ultimately, if the directory *already exists* `mkdirs`; returns `false`, it does *not* throw an `IOException`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1359
https://github.com/hail-is/hail/issues/1360:348,Performance,load,loadClass,348,"Getting this over the past few days when doing, well, basically any query. Log: [hail.log.txt](https://github.com/hail-is/hail/files/755839/hail.log.txt). ```; Caused by: java.lang.ClassNotFoundException: is.hail.sparkextras.ReorderedPartitionsRDDPartition; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1360
https://github.com/hail-is/hail/issues/1360:407,Performance,load,loadClass,407,"Getting this over the past few days when doing, well, basically any query. Log: [hail.log.txt](https://github.com/hail-is/hail/files/755839/hail.log.txt). ```; Caused by: java.lang.ClassNotFoundException: is.hail.sparkextras.ReorderedPartitionsRDDPartition; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1360
https://github.com/hail-is/hail/issues/1360:75,Testability,Log,Log,75,"Getting this over the past few days when doing, well, basically any query. Log: [hail.log.txt](https://github.com/hail-is/hail/files/755839/hail.log.txt). ```; Caused by: java.lang.ClassNotFoundException: is.hail.sparkextras.ReorderedPartitionsRDDPartition; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1360
https://github.com/hail-is/hail/issues/1360:86,Testability,log,log,86,"Getting this over the past few days when doing, well, basically any query. Log: [hail.log.txt](https://github.com/hail-is/hail/files/755839/hail.log.txt). ```; Caused by: java.lang.ClassNotFoundException: is.hail.sparkextras.ReorderedPartitionsRDDPartition; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1360
https://github.com/hail-is/hail/issues/1360:145,Testability,log,log,145,"Getting this over the past few days when doing, well, basically any query. Log: [hail.log.txt](https://github.com/hail-is/hail/files/755839/hail.log.txt). ```; Caused by: java.lang.ClassNotFoundException: is.hail.sparkextras.ReorderedPartitionsRDDPartition; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1360
https://github.com/hail-is/hail/pull/1363:215,Performance,perform,performance,215,"This branch:; ```; In [2]: %timeit vds.split_multi().count(); 1 loop, best of 3: 18.2 s per loop; ```. Master:; ```; In [2]: %timeit vds.split_multi().count(); 1 loop, best of 3: 27.7 s per loop; ```. I predict the performance improvement will increase with large datasets.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1363
https://github.com/hail-is/hail/pull/1363:203,Safety,predict,predict,203,"This branch:; ```; In [2]: %timeit vds.split_multi().count(); 1 loop, best of 3: 18.2 s per loop; ```. Master:; ```; In [2]: %timeit vds.split_multi().count(); 1 loop, best of 3: 27.7 s per loop; ```. I predict the performance improvement will increase with large datasets.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1363
https://github.com/hail-is/hail/pull/1367:599,Testability,test,test,599,"- added `doctest` step to `makeHailDocs` in gradle script; - All example files are in `python/hail/docs/data/`; - All output files in code should have the path `output/`; - For multi-line code statements, only first line can have `>>>`; all other lines must start with `...`; - Modified `doctest` to **NOT** check output. This is different than how `doctest` normally works.; - added overwrite option to python HailContext and the `write` command in scala.; - added more classes to Hail namespace (`from hail import *`). For `data/example.vds`, the code used to generate it is:. ```; hc.import(""src/test/resources/sample.vcf.bgz"").downsample_variants(10).annotate_variants_expr('va.useInKinship = pcoin(0.9), va.panel_maf = 0.1, va.anno1 = 5, va.anno2 = 0, va.consequence = ""LOF"", va.gene = ""A"", va.score = 5.0').split_multi().variant_qc().sample_qc().annotate_samples_expr('sa.isCase = true, sa.pheno.isCase = pcoin(0.5), sa.pheno.isFemale = pcoin(0.5), sa.pheno.age=rnorm(65, 10), sa.cov.PC1 = rnorm(0,1), sa.pheno.height = rnorm(70, 10), sa.cov1 = rnorm(0, 1), sa.cov2 = rnorm(0,1), sa.pheno.bloodPressure= rnorm(120,20), sa.pheno.cohortName = ""cohort1""').write(""python/hail/docs/data/example.vds"", overwrite=True); ```. For `data/exampe2.vds`, the code is:. ```; hc.import(""src/test/resources/sample.vcf.bgz"").downsample_variants(5).annotate_variants_expr('va.anno1 = 5, va.toKeep1 = true, va.toKeep2 = false, va.toKeep3 = true').split_multi().write(""python/hail/docs/data/example2.vds"", overwrite=True); ```. For `data/example_lmmreg.vds`, the code is:. ```; hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInK",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1367
https://github.com/hail-is/hail/pull/1367:1282,Testability,test,test,1282,"ocs/data/`; - All output files in code should have the path `output/`; - For multi-line code statements, only first line can have `>>>`; all other lines must start with `...`; - Modified `doctest` to **NOT** check output. This is different than how `doctest` normally works.; - added overwrite option to python HailContext and the `write` command in scala.; - added more classes to Hail namespace (`from hail import *`). For `data/example.vds`, the code used to generate it is:. ```; hc.import(""src/test/resources/sample.vcf.bgz"").downsample_variants(10).annotate_variants_expr('va.useInKinship = pcoin(0.9), va.panel_maf = 0.1, va.anno1 = 5, va.anno2 = 0, va.consequence = ""LOF"", va.gene = ""A"", va.score = 5.0').split_multi().variant_qc().sample_qc().annotate_samples_expr('sa.isCase = true, sa.pheno.isCase = pcoin(0.5), sa.pheno.isFemale = pcoin(0.5), sa.pheno.age=rnorm(65, 10), sa.cov.PC1 = rnorm(0,1), sa.pheno.height = rnorm(70, 10), sa.cov1 = rnorm(0, 1), sa.cov2 = rnorm(0,1), sa.pheno.bloodPressure= rnorm(120,20), sa.pheno.cohortName = ""cohort1""').write(""python/hail/docs/data/example.vds"", overwrite=True); ```. For `data/exampe2.vds`, the code is:. ```; hc.import(""src/test/resources/sample.vcf.bgz"").downsample_variants(5).annotate_variants_expr('va.anno1 = 5, va.toKeep1 = true, va.toKeep2 = false, va.toKeep3 = true').split_multi().write(""python/hail/docs/data/example2.vds"", overwrite=True); ```. For `data/example_lmmreg.vds`, the code is:. ```; hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'); .write(""python/hail/docs/data/example_lmmreg.vds"", overwrite=True); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1367
https://github.com/hail-is/hail/pull/1367:1583,Testability,test,test,1583,"ocs/data/`; - All output files in code should have the path `output/`; - For multi-line code statements, only first line can have `>>>`; all other lines must start with `...`; - Modified `doctest` to **NOT** check output. This is different than how `doctest` normally works.; - added overwrite option to python HailContext and the `write` command in scala.; - added more classes to Hail namespace (`from hail import *`). For `data/example.vds`, the code used to generate it is:. ```; hc.import(""src/test/resources/sample.vcf.bgz"").downsample_variants(10).annotate_variants_expr('va.useInKinship = pcoin(0.9), va.panel_maf = 0.1, va.anno1 = 5, va.anno2 = 0, va.consequence = ""LOF"", va.gene = ""A"", va.score = 5.0').split_multi().variant_qc().sample_qc().annotate_samples_expr('sa.isCase = true, sa.pheno.isCase = pcoin(0.5), sa.pheno.isFemale = pcoin(0.5), sa.pheno.age=rnorm(65, 10), sa.cov.PC1 = rnorm(0,1), sa.pheno.height = rnorm(70, 10), sa.cov1 = rnorm(0, 1), sa.cov2 = rnorm(0,1), sa.pheno.bloodPressure= rnorm(120,20), sa.pheno.cohortName = ""cohort1""').write(""python/hail/docs/data/example.vds"", overwrite=True); ```. For `data/exampe2.vds`, the code is:. ```; hc.import(""src/test/resources/sample.vcf.bgz"").downsample_variants(5).annotate_variants_expr('va.anno1 = 5, va.toKeep1 = true, va.toKeep2 = false, va.toKeep3 = true').split_multi().write(""python/hail/docs/data/example2.vds"", overwrite=True); ```. For `data/example_lmmreg.vds`, the code is:. ```; hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'); .write(""python/hail/docs/data/example_lmmreg.vds"", overwrite=True); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1367
https://github.com/hail-is/hail/issues/1368:702,Availability,Error,Error,702,"Issue came up on doctest branch. Reproducible example:; ```; assoc_vds = hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'). kinship_vds = assoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1368
https://github.com/hail-is/hail/issues/1368:1989,Availability,error,error,1989,"ssoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurred while calling o155.iterableToArrayList. Trace:; py4j.Py4JException: Method iterableToArrayList([class [D]) does not exist; at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); at py4j.Gateway.invoke(Gateway.java:272); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1368
https://github.com/hail-is/hail/issues/1368:708,Integrability,message,message,708,"Issue came up on doctest branch. Reproducible example:; ```; assoc_vds = hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'). kinship_vds = assoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1368
https://github.com/hail-is/hail/issues/1368:1892,Integrability,protocol,protocol,1892,"ssoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurred while calling o155.iterableToArrayList. Trace:; py4j.Py4JException: Method iterableToArrayList([class [D]) does not exist; at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); at py4j.Gateway.invoke(Gateway.java:272); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1368
https://github.com/hail-is/hail/issues/1368:92,Testability,test,test,92,"Issue came up on doctest branch. Reproducible example:; ```; assoc_vds = hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'). kinship_vds = assoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1368
https://github.com/hail-is/hail/issues/1368:897,Testability,test,test,897,"Issue came up on doctest branch. Reproducible example:; ```; assoc_vds = hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'). kinship_vds = assoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1368
https://github.com/hail-is/hail/pull/1374:176,Availability,down,downloading,176,"- **Requires the python modules ** `nbsphinx`, `matplotlib`, `pandas`, `numpy`, and `seaborn`.; - Use property `-Dtutorial.home=/path/to/tutorial/files` with `gradle` to avoid downloading tutorial files with `wget`.; - Added new tgz file with tutorial files (reduced number of samples to 248 from 2535) https://storage.googleapis.com/hail-tutorial/Hail_Tutorial_Data-v2.tgz; - Edited tutorial to reflect smaller input file.; - Added iPython notebook to repository (this should be edited from now on); - Added tutorial to Sphinx docs.; - Changed tutorial location on website.; - Removed old tutorial infrastructure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1374
https://github.com/hail-is/hail/pull/1374:259,Energy Efficiency,reduce,reduced,259,"- **Requires the python modules ** `nbsphinx`, `matplotlib`, `pandas`, `numpy`, and `seaborn`.; - Use property `-Dtutorial.home=/path/to/tutorial/files` with `gradle` to avoid downloading tutorial files with `wget`.; - Added new tgz file with tutorial files (reduced number of samples to 248 from 2535) https://storage.googleapis.com/hail-tutorial/Hail_Tutorial_Data-v2.tgz; - Edited tutorial to reflect smaller input file.; - Added iPython notebook to repository (this should be edited from now on); - Added tutorial to Sphinx docs.; - Changed tutorial location on website.; - Removed old tutorial infrastructure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1374
https://github.com/hail-is/hail/pull/1374:170,Safety,avoid,avoid,170,"- **Requires the python modules ** `nbsphinx`, `matplotlib`, `pandas`, `numpy`, and `seaborn`.; - Use property `-Dtutorial.home=/path/to/tutorial/files` with `gradle` to avoid downloading tutorial files with `wget`.; - Added new tgz file with tutorial files (reduced number of samples to 248 from 2535) https://storage.googleapis.com/hail-tutorial/Hail_Tutorial_Data-v2.tgz; - Edited tutorial to reflect smaller input file.; - Added iPython notebook to repository (this should be edited from now on); - Added tutorial to Sphinx docs.; - Changed tutorial location on website.; - Removed old tutorial infrastructure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1374
https://github.com/hail-is/hail/pull/1375:68,Modifiability,variab,variables,68,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:126,Testability,log,logreg,126,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:236,Testability,Log,LogisticRegressionFit,236,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:297,Testability,test,test,297,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:305,Testability,log,logreg,305,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:323,Testability,test,test,323,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:337,Testability,test,test,337,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:367,Testability,test,test,367,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:390,Testability,log,logreg,390,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:512,Testability,test,test,512,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:37,Usability,simpl,simplifying,37,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/pull/1375:106,Usability,simpl,simplifying,106,"- removed nullFit function entirely, simplifying code; - null model variables are first rather than last, simplifying code; - logreg fit now only computes margins of score and firth on first iteration; - score and fisher are Options in LogisticRegressionFit (not computed by Firth); - added Firth test to logreg; - added R test of Firth test; - added Firth to EPACTS test; - added Firth to logreg documentation; - added TriSolve for solving triangular systems (LAPACK dtrtrs), used in fitFirth; - added TriSolve test",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1375
https://github.com/hail-is/hail/issues/1377:131,Availability,error,error,131,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1377
https://github.com/hail-is/hail/issues/1377:285,Availability,error,error,285,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1377
https://github.com/hail-is/hail/issues/1377:2,Deployability,install,installed,2,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1377
https://github.com/hail-is/hail/issues/1377:191,Deployability,install,install,191,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1377
https://github.com/hail-is/hail/issues/1377:97,Testability,test,test,97,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1377
https://github.com/hail-is/hail/issues/1377:230,Testability,test,test,230,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1377
https://github.com/hail-is/hail/issues/1378:67,Availability,Avail,Available,67,"```; : is.hail.utils.package$FatalException: symbol `v' not found; Available symbols:; va: Struct; table: Struct; x: Dict[Int, Struct]; <input>:1:va.split_allele = let x = table.`va.split_allele` in range(v.nAltAlleles)[1:].map(i => if(x.contains(i)) x[i].val else NA: String); ```. Note: No idea if this impacts other `annotate_variants...`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1378
https://github.com/hail-is/hail/pull/1383:88,Availability,error,errors,88,"I hope this is the last one. Instead of assert (since the catch doesn't catch assertion errors), I just use an `if`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1383
https://github.com/hail-is/hail/pull/1383:40,Testability,assert,assert,40,"I hope this is the last one. Instead of assert (since the catch doesn't catch assertion errors), I just use an `if`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1383
https://github.com/hail-is/hail/pull/1383:78,Testability,assert,assertion,78,"I hope this is the last one. Instead of assert (since the catch doesn't catch assertion errors), I just use an `if`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1383
https://github.com/hail-is/hail/issues/1394:59,Usability,guid,guide,59,Need to add the comments from #1367 and #1374 to the style-guide.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1394
https://github.com/hail-is/hail/pull/1401:170,Energy Efficiency,efficient,efficient,170,In Spark 2.0.2 [toIndexedRowMatrix](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L270) is efficient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1401
https://github.com/hail-is/hail/pull/1402:15,Testability,log,logreg,15,"- e.g., now va.logreg.wald.beta is just va.logreg.beta; - changed docs and tests accordingly",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1402
https://github.com/hail-is/hail/pull/1402:43,Testability,log,logreg,43,"- e.g., now va.logreg.wald.beta is just va.logreg.beta; - changed docs and tests accordingly",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1402
https://github.com/hail-is/hail/pull/1402:75,Testability,test,tests,75,"- e.g., now va.logreg.wald.beta is just va.logreg.beta; - changed docs and tests accordingly",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1402
https://github.com/hail-is/hail/pull/1411:453,Testability,test,tested,453,**Do not merge this until a Discuss post and Slack/Gitter announcements have been made!**. - Added a FieldType to the function registry; - Parentheses required for all method calls (ex: `toSum()`); - Fields like `v.contig` don't have parentheses; - Added docstrings to the function registry and descriptions to Types; - Added static pages for Language Constructs and Operators; - Pages for Functions and Types are built automatically and Python code is tested.; - Removed dropdown for docs from navbar.; - Added leveneHaldane PDF to repository,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1411
https://github.com/hail-is/hail/issues/1416:10,Availability,failure,failure,10,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:786,Availability,ERROR,ERROR,786,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:5,Testability,test,test,5,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:96,Testability,test,test,96,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:682,Testability,test,testHail,682,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:776,Testability,test,testHail,776,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:812,Testability,test,tests,812,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:845,Testability,test,testHail,845,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:939,Testability,test,testHail,939,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:997,Testability,test,testHail,997,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:1073,Testability,test,tests,1073,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:1123,Testability,test,testHail,1123,"ce code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:1224,Testability,test,testHail,1224,"ildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)')). vds_kinship = vds_assoc.filter_variants_expr('va.qc.AF > .05'). vds_assoc = vds_assoc.lmmreg(vds_kinship",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:1292,Testability,test,testHail,1292,"ildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)')). vds_kinship = vds_assoc.filter_variants_expr('va.qc.AF > .05'). vds_assoc = vds_assoc.lmmreg(vds_kinship",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:1415,Testability,test,testHail,1415,"ailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)')). vds_kinship = vds_assoc.filter_variants_expr('va.qc.AF > .05'). vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:1460,Testability,test,testHail,1460,"ailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)')). vds_kinship = vds_assoc.filter_variants_expr('va.qc.AF > .05'). vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:1782,Testability,test,tests,1782,"ailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)')). vds_kinship = vds_assoc.filter_variants_expr('va.qc.AF > .05'). vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1416:74,Usability,simpl,simply,74,"This test failure has occurred a few times. It's usually been resolved by simply re-running the test without any source code changes. There are at least four recent examples, all on different feature branches: . - [2017-02-21 11:10 UTC](https://ci.hail.is/viewLog.html?buildId=6489&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi#); - [2017-02-15 21:51 UTC](https://ci.hail.is/viewLog.html?buildId=5446#2135525582_186_5446_problem); - [2017-02-15 18:45 UTC](https://ci.hail.is/viewLog.html?buildId=5333#1163765489_186_5333_problem); - [2017-02-13 22:42 UTC](https://ci.hail.is/viewLog.html?buildId=4934&tab=buildResultsDiv&buildTypeId=HailSourceCode_HailCi). ```; [11:19:14][:testHail] ======================================================================; [11:19:14][:testHail] ERROR: test_dataset (hail.tests.ContextTests); [11:19:14][:testHail] ----------------------------------------------------------------------; [11:19:14][:testHail] Traceback (most recent call last):; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/tests.py"", line 233, in test_dataset; [11:19:14][:testHail] vds_assoc = vds_assoc.lmmreg(vds_kinship, 'sa.pheno', ['sa.cov1', 'sa.cov2']); [11:19:14][:testHail] File ""<decorator-gen-89>"", line 2, in lmmreg; [11:19:14][:testHail] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 93, in handle_py4j; [11:19:14][:testHail] raise FatalError(msg); [11:19:14][:testHail] FatalError: lmmreg: failed to fit delta: REML realized at delta upper search boundary e^10 = 22026.465794806718, indicating negligible genetic component of variance. Standard linear regression may be more appropriate.; ```. These are the [offending lines](https://github.com/hail-is/hail/blob/master/python/hail/tests.py#L222-L233):. ```; vds_assoc = (hc.import_vcf(test_resources + '/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr(; 'sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1416
https://github.com/hail-is/hail/issues/1419:274,Availability,ERROR,ERROR,274,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:336,Availability,ERROR,ERROR,336,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:393,Availability,ERROR,ERROR,393,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:448,Availability,ERROR,ERROR,448,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:518,Availability,ERROR,ERROR,518,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:589,Availability,ERROR,ERROR,589,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:628,Availability,ERROR,ERROR,628,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:730,Availability,ERROR,ERROR,730,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:750,Availability,ERROR,ERROR,750,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:811,Availability,ERROR,ERROR,811,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:917,Availability,ERROR,ERROR,917,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:987,Availability,ERROR,ERROR,987,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1121,Availability,ERROR,ERROR,1121,"roblem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1204,Availability,ERROR,ERROR,1204,"top and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1332,Availability,ERROR,ERROR,1332,"RROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1401,Availability,ERROR,ERROR,1401,"m.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1536,Availability,ERROR,ERROR,1536,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1595,Availability,ERROR,ERROR,1595,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1735,Availability,ERROR,ERROR,1735,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1810,Availability,ERROR,ERROR,1810,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1948,Availability,ERROR,ERROR,1948,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1983,Availability,ERROR,ERROR,1983,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:2089,Availability,ERROR,ERROR,2089,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:138,Deployability,install,install,138,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:176,Testability,test,tests,176,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:361,Testability,test,testExportKT,361,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:776,Testability,test,tests,776,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:1069,Testability,test,tests,1069," Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:2115,Testability,test,tests,2115,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/issues/1419:2408,Testability,test,tests,2408,"fo: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn't exist. However, when I use spark 2.0.2, it is able to run almost all tests successfully. Perhaps a problem with the paths in spark 2.1.0?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419
https://github.com/hail-is/hail/pull/1421:933,Performance,perform,performance,933,"@tomwhite @jkeebler Added a parquet_genotypes flag to VDS.write:. ```; import hail; hc = hail.HailContext(); vds = hc.import_vcf('sample.vcf'); vds.count(); vds.write('sample.pq.vds', overwrite=True, parquet_genotypes=True); ```. Then in the Spark shell:. ```; scala> val df = spark.read.parquet(""sample.pq.vds/rdd.parquet""); scala> df.printSchema(); root; |-- variant: struct (nullable = true); [variant and annotation fields elided]; |-- gs: array (nullable = true); | |-- element: struct (containsNull = true); | | |-- gt: integer (nullable = true); | | |-- ad: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- dp: integer (nullable = true); | | |-- gq: integer (nullable = true); | | |-- px: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- fakeRef: boolean (nullable = true); | | |-- isDosage: boolean (nullable = true); ```. I added correctness tests, but no performance testing on the Hail side yet. Note, the `px` field is the `PL` in the case of sequence data and 16-bit fixed-point dosages in the case of array data (See `Gentoype` for more details.) We know if we have dosage or not globally (`VariantMetadata.isDosage`), so I can customize the resulting schema in v2. Finally, I'm seeing `containsNull = true` here, but I set it to `containsNull = false` when I constructed the schema programatically. Spark/Parquet seem to be consistently ignoring my non-missing hints. Have you seen this before? Any idea why it is happening?. From `Genotype.schema`:. ```; def schema: DataType = StructType(Array(; StructField(""gt"", IntegerType),; StructField(""ad"", ArrayType(IntegerType, containsNull = false)),; StructField(""dp"", IntegerType),; StructField(""gq"", IntegerType),; StructField(""px"", ArrayType(IntegerType, containsNull = false)),; StructField(""fakeRef"", BooleanType, nullable = false),; StructField(""isDosage"", BooleanType, nullable = false))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1421
https://github.com/hail-is/hail/pull/1421:919,Testability,test,tests,919,"@tomwhite @jkeebler Added a parquet_genotypes flag to VDS.write:. ```; import hail; hc = hail.HailContext(); vds = hc.import_vcf('sample.vcf'); vds.count(); vds.write('sample.pq.vds', overwrite=True, parquet_genotypes=True); ```. Then in the Spark shell:. ```; scala> val df = spark.read.parquet(""sample.pq.vds/rdd.parquet""); scala> df.printSchema(); root; |-- variant: struct (nullable = true); [variant and annotation fields elided]; |-- gs: array (nullable = true); | |-- element: struct (containsNull = true); | | |-- gt: integer (nullable = true); | | |-- ad: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- dp: integer (nullable = true); | | |-- gq: integer (nullable = true); | | |-- px: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- fakeRef: boolean (nullable = true); | | |-- isDosage: boolean (nullable = true); ```. I added correctness tests, but no performance testing on the Hail side yet. Note, the `px` field is the `PL` in the case of sequence data and 16-bit fixed-point dosages in the case of array data (See `Gentoype` for more details.) We know if we have dosage or not globally (`VariantMetadata.isDosage`), so I can customize the resulting schema in v2. Finally, I'm seeing `containsNull = true` here, but I set it to `containsNull = false` when I constructed the schema programatically. Spark/Parquet seem to be consistently ignoring my non-missing hints. Have you seen this before? Any idea why it is happening?. From `Genotype.schema`:. ```; def schema: DataType = StructType(Array(; StructField(""gt"", IntegerType),; StructField(""ad"", ArrayType(IntegerType, containsNull = false)),; StructField(""dp"", IntegerType),; StructField(""gq"", IntegerType),; StructField(""px"", ArrayType(IntegerType, containsNull = false)),; StructField(""fakeRef"", BooleanType, nullable = false),; StructField(""isDosage"", BooleanType, nullable = false))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1421
https://github.com/hail-is/hail/pull/1421:945,Testability,test,testing,945,"@tomwhite @jkeebler Added a parquet_genotypes flag to VDS.write:. ```; import hail; hc = hail.HailContext(); vds = hc.import_vcf('sample.vcf'); vds.count(); vds.write('sample.pq.vds', overwrite=True, parquet_genotypes=True); ```. Then in the Spark shell:. ```; scala> val df = spark.read.parquet(""sample.pq.vds/rdd.parquet""); scala> df.printSchema(); root; |-- variant: struct (nullable = true); [variant and annotation fields elided]; |-- gs: array (nullable = true); | |-- element: struct (containsNull = true); | | |-- gt: integer (nullable = true); | | |-- ad: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- dp: integer (nullable = true); | | |-- gq: integer (nullable = true); | | |-- px: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- fakeRef: boolean (nullable = true); | | |-- isDosage: boolean (nullable = true); ```. I added correctness tests, but no performance testing on the Hail side yet. Note, the `px` field is the `PL` in the case of sequence data and 16-bit fixed-point dosages in the case of array data (See `Gentoype` for more details.) We know if we have dosage or not globally (`VariantMetadata.isDosage`), so I can customize the resulting schema in v2. Finally, I'm seeing `containsNull = true` here, but I set it to `containsNull = false` when I constructed the schema programatically. Spark/Parquet seem to be consistently ignoring my non-missing hints. Have you seen this before? Any idea why it is happening?. From `Genotype.schema`:. ```; def schema: DataType = StructType(Array(; StructField(""gt"", IntegerType),; StructField(""ad"", ArrayType(IntegerType, containsNull = false)),; StructField(""dp"", IntegerType),; StructField(""gq"", IntegerType),; StructField(""px"", ArrayType(IntegerType, containsNull = false)),; StructField(""fakeRef"", BooleanType, nullable = false),; StructField(""isDosage"", BooleanType, nullable = false))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1421
https://github.com/hail-is/hail/pull/1425:16,Testability,log,logreg,16,"This also fixes logreg issue in MutableGenotype branch, by no longer filtering on genotype stream.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1425
https://github.com/hail-is/hail/pull/1427:50,Testability,assert,assert,50,"`delta` is a randomly chosen double, we shouldn't assert its equivalence with a specific value.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1427
https://github.com/hail-is/hail/pull/1432:398,Testability,Test,Test,398,"- Added `genotype_schema` to VariantMetadata.; - Added `isGenericGenotype` flag to VariantMetadata; - VDS Write: if `!isGenericGenotype` write to parquet as usual. Else, export row with schema equal to `genotype_schema`.; - VDS Read: if `!isGenericGenotype`, import as usual. Else, import row with schema equal to `genotype_schema` and convert row to Iterable[Genotype] (null genotype for now).; - Test for read/write",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1432
https://github.com/hail-is/hail/issues/1447:102,Availability,error,error,102,"I did the following ; ```; import is.hail._; val hc = HailContext(sc); ```. When I did so, I got this error message . ```; java.util.NoSuchElementException: spark.hadoop.io.compression.codecs; at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:235); at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:235); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.SparkConf.get(SparkConf.scala:235); at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:94); at is.hail.HailContext$.apply(HailContext.scala:171); ... 50 elided; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1447
https://github.com/hail-is/hail/issues/1447:108,Integrability,message,message,108,"I did the following ; ```; import is.hail._; val hc = HailContext(sc); ```. When I did so, I got this error message . ```; java.util.NoSuchElementException: spark.hadoop.io.compression.codecs; at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:235); at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:235); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.SparkConf.get(SparkConf.scala:235); at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:94); at is.hail.HailContext$.apply(HailContext.scala:171); ... 50 elided; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1447
https://github.com/hail-is/hail/issues/1459:140,Availability,avail,available,140,"[This](https://github.com/hail-is/hail/commit/9f186be0111d241756484136a2ffa8eb1a8a1feb) commit imports the `decorator` library, which isn't available by default on google cloud dataproc machines. . One can get around it (by rolling this commit back or by installing the library), but it makes the google cloud tutorial not work by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1459
https://github.com/hail-is/hail/issues/1459:224,Deployability,rolling,rolling,224,"[This](https://github.com/hail-is/hail/commit/9f186be0111d241756484136a2ffa8eb1a8a1feb) commit imports the `decorator` library, which isn't available by default on google cloud dataproc machines. . One can get around it (by rolling this commit back or by installing the library), but it makes the google cloud tutorial not work by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1459
https://github.com/hail-is/hail/issues/1459:255,Deployability,install,installing,255,"[This](https://github.com/hail-is/hail/commit/9f186be0111d241756484136a2ffa8eb1a8a1feb) commit imports the `decorator` library, which isn't available by default on google cloud dataproc machines. . One can get around it (by rolling this commit back or by installing the library), but it makes the google cloud tutorial not work by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1459
https://github.com/hail-is/hail/pull/1473:281,Testability,benchmark,benchmark,281,"This gets us back to the ~65s total time referenced in [here](https://github.com/hail-is/hail/pull/1254#issuecomment-274951875), but [ultimately not achieved](https://github.com/hail-is/hail/pull/1254#issuecomment-284071915) by the final PR. In that time, `master`'s speed on that benchmark decreased ~10s. This PR + [the compiler PR](https://github.com/hail-is/hail/pull/1254) yield about a total 30% reduction in time on that benchmark.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1473
https://github.com/hail-is/hail/pull/1473:428,Testability,benchmark,benchmark,428,"This gets us back to the ~65s total time referenced in [here](https://github.com/hail-is/hail/pull/1254#issuecomment-274951875), but [ultimately not achieved](https://github.com/hail-is/hail/pull/1254#issuecomment-284071915) by the final PR. In that time, `master`'s speed on that benchmark decreased ~10s. This PR + [the compiler PR](https://github.com/hail-is/hail/pull/1254) yield about a total 30% reduction in time on that benchmark.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1473
https://github.com/hail-is/hail/pull/1475:25,Availability,down,down,25,Also moved runAssoc code down so it only executes when needed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1475
https://github.com/hail-is/hail/pull/1478:20,Testability,test,test,20,@tpoterba : Can you test? Use the gradle command `createDocsNoTest`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1478
https://github.com/hail-is/hail/pull/1480:67,Testability,test,testing,67,Forgot to rebase before adding the tasks for building docs without testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1480
https://github.com/hail-is/hail/pull/1491:17,Modifiability,extend,extend,17,"- Array: append, extend; - Set: add, union, difference, intersection, issubset",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1491
https://github.com/hail-is/hail/pull/1502:123,Availability,error,error,123,"Now the command:; ```; pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(); ```; gives the right error; ```; FatalError Traceback (most recent call last); <ipython-input-28-2fb5c41b2314> in <module>(); ----> 1 pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(). <decorator-gen-218> in logreg(self, test, y, covariates, root). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 105 except Py4JJavaError as e:; 106 msg = env.jutils.getMinimalMessage(e.java_exception); --> 107 raise FatalError(msg); 108 except Py4JError as e:; 109 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: `|' expected but `,' found; <input>:1:sa.scores.PC1, sa.scores.PC2; ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1502
https://github.com/hail-is/hail/pull/1502:628,Availability,error,error,628,"Now the command:; ```; pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(); ```; gives the right error; ```; FatalError Traceback (most recent call last); <ipython-input-28-2fb5c41b2314> in <module>(); ----> 1 pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(). <decorator-gen-218> in logreg(self, test, y, covariates, root). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 105 except Py4JJavaError as e:; 106 msg = env.jutils.getMinimalMessage(e.java_exception); --> 107 raise FatalError(msg); 108 except Py4JError as e:; 109 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: `|' expected but `,' found; <input>:1:sa.scores.PC1, sa.scores.PC2; ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1502
https://github.com/hail-is/hail/pull/1502:31,Testability,log,logreg,31,"Now the command:; ```; pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(); ```; gives the right error; ```; FatalError Traceback (most recent call last); <ipython-input-28-2fb5c41b2314> in <module>(); ----> 1 pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(). <decorator-gen-218> in logreg(self, test, y, covariates, root). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 105 except Py4JJavaError as e:; 106 msg = env.jutils.getMinimalMessage(e.java_exception); --> 107 raise FatalError(msg); 108 except Py4JError as e:; 109 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: `|' expected but `,' found; <input>:1:sa.scores.PC1, sa.scores.PC2; ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1502
https://github.com/hail-is/hail/pull/1502:244,Testability,log,logreg,244,"Now the command:; ```; pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(); ```; gives the right error; ```; FatalError Traceback (most recent call last); <ipython-input-28-2fb5c41b2314> in <module>(); ----> 1 pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(). <decorator-gen-218> in logreg(self, test, y, covariates, root). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 105 except Py4JJavaError as e:; 106 msg = env.jutils.getMinimalMessage(e.java_exception); --> 107 raise FatalError(msg); 108 except Py4JError as e:; 109 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: `|' expected but `,' found; <input>:1:sa.scores.PC1, sa.scores.PC2; ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1502
https://github.com/hail-is/hail/pull/1502:338,Testability,log,logreg,338,"Now the command:; ```; pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(); ```; gives the right error; ```; FatalError Traceback (most recent call last); <ipython-input-28-2fb5c41b2314> in <module>(); ----> 1 pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(). <decorator-gen-218> in logreg(self, test, y, covariates, root). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 105 except Py4JJavaError as e:; 106 msg = env.jutils.getMinimalMessage(e.java_exception); --> 107 raise FatalError(msg); 108 except Py4JError as e:; 109 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: `|' expected but `,' found; <input>:1:sa.scores.PC1, sa.scores.PC2; ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1502
https://github.com/hail-is/hail/pull/1502:351,Testability,test,test,351,"Now the command:; ```; pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(); ```; gives the right error; ```; FatalError Traceback (most recent call last); <ipython-input-28-2fb5c41b2314> in <module>(); ----> 1 pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(). <decorator-gen-218> in logreg(self, test, y, covariates, root). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 105 except Py4JJavaError as e:; 106 msg = env.jutils.getMinimalMessage(e.java_exception); --> 107 raise FatalError(msg); 108 except Py4JError as e:; 109 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: `|' expected but `,' found; <input>:1:sa.scores.PC1, sa.scores.PC2; ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1502
https://github.com/hail-is/hail/pull/1502:622,Testability,log,log,622,"Now the command:; ```; pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(); ```; gives the right error; ```; FatalError Traceback (most recent call last); <ipython-input-28-2fb5c41b2314> in <module>(); ----> 1 pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(). <decorator-gen-218> in logreg(self, test, y, covariates, root). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 105 except Py4JJavaError as e:; 106 msg = env.jutils.getMinimalMessage(e.java_exception); --> 107 raise FatalError(msg); 108 except Py4JError as e:; 109 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: `|' expected but `,' found; <input>:1:sa.scores.PC1, sa.scores.PC2; ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1502
https://github.com/hail-is/hail/issues/1505:634,Performance,perform,performance,634,"- [x] Remove TSample (it's just string); - [x] Remove count, or rework it to return a tuple; - [ ] Change concordance to use TDict; - [x] Filter_samples_list should take a list; - [x] TextTableConfig goes away; - [x] Annotate_samples_table and annotate_variants_table go away; - [x] Import_annotations_table goes away; - [x] KeyTable to VariantDataset conversion; - [ ] Precompiled binaries (!!!!!!!); - [ ] Fix log output to jupyter notebooks; - [ ] add all aggregator functions to sets and arrays (or fix aggregable scope issue); - [x] add python file-like objects so people can write to cloud file systems / HDFS; - [ ] ~~improved performance on python object conversion (or lazy evaluation at the least)~~ back compatible; - [x] annotate_samples_fam goes away; - [x] annotate VDS with interval keytable; - [x] read/write keytables to parquet; - [ ] rename logreg/ linreg / lmmreg to be more descriptive; - [x] no methods take a file; - [x] first-class object for Pedigree in python; - [x] make annotation-of-counts behavior consistent across the regression methods; - [x] make linreg / logreg / lmmreg consistent on whether they output count annotations. If anybody has other tasks, edit this post to add them here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1505
https://github.com/hail-is/hail/issues/1505:412,Testability,log,log,412,"- [x] Remove TSample (it's just string); - [x] Remove count, or rework it to return a tuple; - [ ] Change concordance to use TDict; - [x] Filter_samples_list should take a list; - [x] TextTableConfig goes away; - [x] Annotate_samples_table and annotate_variants_table go away; - [x] Import_annotations_table goes away; - [x] KeyTable to VariantDataset conversion; - [ ] Precompiled binaries (!!!!!!!); - [ ] Fix log output to jupyter notebooks; - [ ] add all aggregator functions to sets and arrays (or fix aggregable scope issue); - [x] add python file-like objects so people can write to cloud file systems / HDFS; - [ ] ~~improved performance on python object conversion (or lazy evaluation at the least)~~ back compatible; - [x] annotate_samples_fam goes away; - [x] annotate VDS with interval keytable; - [x] read/write keytables to parquet; - [ ] rename logreg/ linreg / lmmreg to be more descriptive; - [x] no methods take a file; - [x] first-class object for Pedigree in python; - [x] make annotation-of-counts behavior consistent across the regression methods; - [x] make linreg / logreg / lmmreg consistent on whether they output count annotations. If anybody has other tasks, edit this post to add them here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1505
https://github.com/hail-is/hail/issues/1505:860,Testability,log,logreg,860,"- [x] Remove TSample (it's just string); - [x] Remove count, or rework it to return a tuple; - [ ] Change concordance to use TDict; - [x] Filter_samples_list should take a list; - [x] TextTableConfig goes away; - [x] Annotate_samples_table and annotate_variants_table go away; - [x] Import_annotations_table goes away; - [x] KeyTable to VariantDataset conversion; - [ ] Precompiled binaries (!!!!!!!); - [ ] Fix log output to jupyter notebooks; - [ ] add all aggregator functions to sets and arrays (or fix aggregable scope issue); - [x] add python file-like objects so people can write to cloud file systems / HDFS; - [ ] ~~improved performance on python object conversion (or lazy evaluation at the least)~~ back compatible; - [x] annotate_samples_fam goes away; - [x] annotate VDS with interval keytable; - [x] read/write keytables to parquet; - [ ] rename logreg/ linreg / lmmreg to be more descriptive; - [x] no methods take a file; - [x] first-class object for Pedigree in python; - [x] make annotation-of-counts behavior consistent across the regression methods; - [x] make linreg / logreg / lmmreg consistent on whether they output count annotations. If anybody has other tasks, edit this post to add them here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1505
https://github.com/hail-is/hail/issues/1505:1090,Testability,log,logreg,1090,"- [x] Remove TSample (it's just string); - [x] Remove count, or rework it to return a tuple; - [ ] Change concordance to use TDict; - [x] Filter_samples_list should take a list; - [x] TextTableConfig goes away; - [x] Annotate_samples_table and annotate_variants_table go away; - [x] Import_annotations_table goes away; - [x] KeyTable to VariantDataset conversion; - [ ] Precompiled binaries (!!!!!!!); - [ ] Fix log output to jupyter notebooks; - [ ] add all aggregator functions to sets and arrays (or fix aggregable scope issue); - [x] add python file-like objects so people can write to cloud file systems / HDFS; - [ ] ~~improved performance on python object conversion (or lazy evaluation at the least)~~ back compatible; - [x] annotate_samples_fam goes away; - [x] annotate VDS with interval keytable; - [x] read/write keytables to parquet; - [ ] rename logreg/ linreg / lmmreg to be more descriptive; - [x] no methods take a file; - [x] first-class object for Pedigree in python; - [x] make annotation-of-counts behavior consistent across the regression methods; - [x] make linreg / logreg / lmmreg consistent on whether they output count annotations. If anybody has other tasks, edit this post to add them here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1505
https://github.com/hail-is/hail/issues/1511:121,Availability,error,errors,121,"Python API doesn't support lists like it claims. Should be on VariantDatasetFunctions, not VSM[T] (will cause class cast errors)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1511
https://github.com/hail-is/hail/pull/1517:384,Availability,redundant,redundant,384,"- Values of types other than `Array` and `Boolean` get output in VCF format (e.g. `.` instead of `NA` for missing values); - `NaN` values are converted to missing (`.`) when exporting VCF since VCF doesn't handle `NaN`; - Changes to handling of filters:; - `.` <=> `NA:Set[String]`; - `PASS` <=> `{}:Set[String]`; - `other` <=> `{""other""}:Set[String]""`; - Removed `va.pass` entirely (redundant with `va.filters` and needs constant synchronization)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1517
https://github.com/hail-is/hail/pull/1517:431,Integrability,synchroniz,synchronization,431,"- Values of types other than `Array` and `Boolean` get output in VCF format (e.g. `.` instead of `NA` for missing values); - `NaN` values are converted to missing (`.`) when exporting VCF since VCF doesn't handle `NaN`; - Changes to handling of filters:; - `.` <=> `NA:Set[String]`; - `PASS` <=> `{}:Set[String]`; - `other` <=> `{""other""}:Set[String]""`; - Removed `va.pass` entirely (redundant with `va.filters` and needs constant synchronization)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1517
https://github.com/hail-is/hail/pull/1517:384,Safety,redund,redundant,384,"- Values of types other than `Array` and `Boolean` get output in VCF format (e.g. `.` instead of `NA` for missing values); - `NaN` values are converted to missing (`.`) when exporting VCF since VCF doesn't handle `NaN`; - Changes to handling of filters:; - `.` <=> `NA:Set[String]`; - `PASS` <=> `{}:Set[String]`; - `other` <=> `{""other""}:Set[String]""`; - Removed `va.pass` entirely (redundant with `va.filters` and needs constant synchronization)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1517
https://github.com/hail-is/hail/issues/1518:118,Deployability,release,release-,118,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518
https://github.com/hail-is/hail/issues/1518:639,Modifiability,config,config,639,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518
https://github.com/hail-is/hail/issues/1518:948,Performance,concurren,concurrent,948,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518
https://github.com/hail-is/hail/issues/1518:998,Performance,concurren,concurrent,998,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518
https://github.com/hail-is/hail/issues/1518:1064,Performance,concurren,concurrent,1064,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518
https://github.com/hail-is/hail/issues/1518:1149,Performance,queue,queued,1149,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518
https://github.com/hail-is/hail/issues/1518:83,Security,hash,hash,83,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518
https://github.com/hail-is/hail/issues/1518:1218,Testability,log,log,1218,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1518
https://github.com/hail-is/hail/issues/1520:176,Availability,down,downloaded,176,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:912,Availability,error,error,912,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:1108,Availability,error,error,1108,"e script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:1284,Availability,error,error,1284,"-- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:1480,Availability,error,error,1480,"e; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 3072K; NUMA node0 CPU(s): 0,1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:1588,Availability,Error,Error,1588,"e; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 3072K; NUMA node0 CPU(s): 0,1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:1783,Availability,avail,avail,1783,"e; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 3072K; NUMA node0 CPU(s): 0,1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:89,Deployability,install,install,89,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:191,Deployability,install,installed,191,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:218,Integrability,depend,dependencies,218,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:375,Modifiability,Config,Configuring,375,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:2343,Performance,cache,cache,2343,"e; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 3072K; NUMA node0 CPU(s): 0,1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:2359,Performance,cache,cache,2359,"e; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 3072K; NUMA node0 CPU(s): 0,1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1520:2374,Performance,cache,cache,2374,"e; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 2; On-line CPU(s) list: 0,1; Thread(s) per core: 1; Core(s) per socket: 2; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 23; Model name: Intel(R) Core(TM)2 CPU P8600 @ 2.40GHz; Stepping: 10; CPU MHz: 800.000; CPU max MHz: 2401.0000; CPU min MHz: 800.0000; BogoMIPS: 4800.16; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 3072K; NUMA node0 CPU(s): 0,1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1520
https://github.com/hail-is/hail/issues/1522:63,Performance,Perform,Performs,63,"Tiny tiny tiny point about the API docs: query_variants says, ""Performs aggregation queries over variants and variant annotations, and returns python object(s) and type(s)."" But should end after ""object(s)"". It would've been faster for me to clone the repo, make this changes and submit a pull request. . Sorry.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1522
https://github.com/hail-is/hail/issues/1525:587,Availability,error,errors,587,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:836,Availability,error,errors,836,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:1218,Availability,error,error,1218,"y where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm = SparkContext._gateway.jvm; 258. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/java_gateway.py in launch_gateway(conf); 75 def preexec_func():; 76 signal.signal(signal.SIGINT, signal.SIG_IGN); ---> 77 proc ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:3175,Availability,error,error,3175,"al(signal.SIGINT, signal.SIG_IGN); ---> 77 proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); 78 else:; 79 # preexec_fn not supported on Windows. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags); 388 p2cread, p2cwrite,; 389 c2pread, c2pwrite,; --> 390 errread, errwrite); 391 except Exception:; 392 # Preserve original exception in case os.close raises. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in _execute_child(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite); 1022 raise; 1023 child_exception = pickle.loads(data); -> 1024 raise child_exception; 1025; 1026. OSError: [Errno 2] No such file or directory. and the second error we would get would be. ------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-6-93fa734a63bb> in <module>() ----> 1 hc_nate = HailContext() /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:4585,Availability,error,error,4585,", min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:4723,Availability,error,error,4723,"g, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.exec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:4880,Availability,error,error,4880,"7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:300,Deployability,install,installation,300,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:1156,Deployability,install,installations,1156,"users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm = SparkContext._gateway.jvm; 258. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:1194,Integrability,synchroniz,synchronized,1194,"users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm = SparkContext._gateway.jvm; 258. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:4326,Integrability,protocol,protocol,4326,"thon-input-6-93fa734a63bb> in <module>() ----> 1 hc_nate = HailContext() /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:4479,Integrability,protocol,protocol,4479,"t__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:5066,Modifiability,config,configureAndCreateSparkContext,5066,"t, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:6337,Modifiability,config,configureAndCreateSparkContext,6337,ntext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2274) at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2353) at org.apache.spark.SparkContext.<init>(SparkContext.scala:85) at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) at is.hail.HailContext$.apply(HailContext.scala:164) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:280) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:214) at java.lang.Thread.run(Thread.java:745). Thank you so much!!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:907,Performance,concurren,concurrent,907,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:3058,Performance,load,loads,3058,"7/python/pyspark/java_gateway.py in launch_gateway(conf); 75 def preexec_func():; 76 signal.signal(signal.SIGINT, signal.SIG_IGN); ---> 77 proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); 78 else:; 79 # preexec_fn not supported on Windows. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags); 388 p2cread, p2cwrite,; 389 c2pread, c2pwrite,; --> 390 errread, errwrite); 391 except Exception:; 392 # Preserve original exception in case os.close raises. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in _execute_child(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite); 1022 raise; 1023 child_exception = pickle.loads(data); -> 1024 raise child_exception; 1025; 1026. OSError: [Errno 2] No such file or directory. and the second error we would get would be. ------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-6-93fa734a63bb> in <module>() ----> 1 hc_nate = HailContext() /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_clie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:454,Testability,test,testing,454,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:1499,Testability,log,log,1499," = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm = SparkContext._gateway.jvm; 258. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/java_gateway.py in launch_gateway(conf); 75 def preexec_func():; 76 signal.signal(signal.SIGINT, signal.SIG_IGN); ---> 77 proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); 78 else:; 79 # preexec_fn not supported on Windows. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:3517,Testability,log,log,3517,"sal_newlines, startupinfo, creationflags); 388 p2cread, p2cwrite,; 389 c2pread, c2pwrite,; --> 390 errread, errwrite); 391 except Exception:; 392 # Preserve original exception in case os.close raises. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in _execute_child(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite); 1022 raise; 1023 child_exception = pickle.loads(data); -> 1024 raise child_exception; 1025; 1026. OSError: [Errno 2] No such file or directory. and the second error we would get would be. ------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-6-93fa734a63bb> in <module>() ----> 1 hc_nate = HailContext() /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, tar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:3704,Testability,log,log,3704,"raises. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in _execute_child(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite); 1022 raise; 1023 child_exception = pickle.loads(data); -> 1024 raise child_exception; 1025; 1026. OSError: [Errno 2] No such file or directory. and the second error we would get would be. ------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-6-93fa734a63bb> in <module>() ----> 1 hc_nate = HailContext() /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:5867,Testability,assert,assertNoOtherContextIsRunning,5867,"re this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2274) at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2353) at org.apache.spark.SparkContext.<init>(SparkContext.scala:85) at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) at is.hail.HailContext$.apply(HailContext.scala:164) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Ga",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:5972,Testability,assert,assertNoOtherContextIsRunning,5972,ted at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2274) at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2353) at org.apache.spark.SparkContext.<init>(SparkContext.scala:85) at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) at is.hail.HailContext$.apply(HailContext.scala:164) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:280) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/issues/1525:6111,Testability,assert,assertNoOtherContextIsRunning,6111,ontext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2274) at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2353) at org.apache.spark.SparkContext.<init>(SparkContext.scala:85) at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) at is.hail.HailContext$.apply(HailContext.scala:164) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:280) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:214) at java.lang.Thread.run(Thread.java:745). ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1525
https://github.com/hail-is/hail/pull/1531:174,Integrability,message,message,174,- Fixed VEP not annotating any allele when a `*` allele present at the site; now annotates the other alleles.; - Fixed VEP crash when returning no results; now issues a warn message.; - Changed annotation type with `csq` option from `String` to `Array[String]` (was wrong),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1531
https://github.com/hail-is/hail/pull/1532:22,Availability,error,error,22,- fixed serialization error with `oneHotGenotype` and `oneHotAlleles`; - Call type is a `java.lang.Integer`; - Missing value is `null`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1532
https://github.com/hail-is/hail/pull/1538:9,Availability,error,error,9,"Resolves error where exportVariants exported an Interval as `Interval(14:968765858,22:1565768082)` and import parser expects `14:968765858-22:1565768082`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1538
https://github.com/hail-is/hail/pull/1548:15,Testability,test,test,15,"Now the lmmreg test is deterministic. And I found a bug in lmmreg by implementing the same test in LinearMixedRegressionSuite, which I then fixed. Namely: `kinshipVds` should have been `filtKinshipVds`. I've left this in as a regression test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1548
https://github.com/hail-is/hail/pull/1548:91,Testability,test,test,91,"Now the lmmreg test is deterministic. And I found a bug in lmmreg by implementing the same test in LinearMixedRegressionSuite, which I then fixed. Namely: `kinshipVds` should have been `filtKinshipVds`. I've left this in as a regression test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1548
https://github.com/hail-is/hail/pull/1548:237,Testability,test,test,237,"Now the lmmreg test is deterministic. And I found a bug in lmmreg by implementing the same test in LinearMixedRegressionSuite, which I then fixed. Namely: `kinshipVds` should have been `filtKinshipVds`. I've left this in as a regression test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1548
https://github.com/hail-is/hail/pull/1549:1754,Testability,assert,assertion,1754,"The scala compiler generates an implementation of this type for every primitive type:. ```; dking@wmb16-359 # ll build/classes/main/is/hail/utils/*ArrayBuilder*.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3142 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcB$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3155 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcC$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3151 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcD$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3148 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcF$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3139 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcI$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3145 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcJ$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3148 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcS$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3209 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcV$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 3147 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder$mcZ$sp.class; -rw-r--r-- 1 dking CHARLES\Domain Users 9220 Mar 14 12:00 build/classes/main/is/hail/utils/ArrayBuilder.class; -rw-r--r-- 1 dking CHARLES\Domain Users 959 Mar 14 12:00 build/classes/main/is/hail/utils/ByteArrayBuilder.class; ```. Curiously, it generates an implementation for a list of units (that's `mcV`), which of course is just a natural number. Uses of `ArrayBuilder[T]` where `T` is primitive also do The Right Thing. You can take a look at `ArrayBuilderSuite` for an example. The only boxing present there is for the assertion library (which traffics in boxed values).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1549
https://github.com/hail-is/hail/pull/1550:22,Testability,test,test,22,"Now the lmmreg python test is deterministic. And I found another bug in lmmreg by implementing the same test in LinearMixedRegressionSuite, which I then fixed, namely, kinshipVds should have been filtKinshipVds. Upon rebasing, that test failed again due to interaction of IntIterator and lazyFilterWith, which I've also fixed. I've added a regression test to LinearMixedRegressionSuite, paralleling the Python test, that catches both bugs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1550
https://github.com/hail-is/hail/pull/1550:104,Testability,test,test,104,"Now the lmmreg python test is deterministic. And I found another bug in lmmreg by implementing the same test in LinearMixedRegressionSuite, which I then fixed, namely, kinshipVds should have been filtKinshipVds. Upon rebasing, that test failed again due to interaction of IntIterator and lazyFilterWith, which I've also fixed. I've added a regression test to LinearMixedRegressionSuite, paralleling the Python test, that catches both bugs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1550
https://github.com/hail-is/hail/pull/1550:232,Testability,test,test,232,"Now the lmmreg python test is deterministic. And I found another bug in lmmreg by implementing the same test in LinearMixedRegressionSuite, which I then fixed, namely, kinshipVds should have been filtKinshipVds. Upon rebasing, that test failed again due to interaction of IntIterator and lazyFilterWith, which I've also fixed. I've added a regression test to LinearMixedRegressionSuite, paralleling the Python test, that catches both bugs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1550
https://github.com/hail-is/hail/pull/1550:351,Testability,test,test,351,"Now the lmmreg python test is deterministic. And I found another bug in lmmreg by implementing the same test in LinearMixedRegressionSuite, which I then fixed, namely, kinshipVds should have been filtKinshipVds. Upon rebasing, that test failed again due to interaction of IntIterator and lazyFilterWith, which I've also fixed. I've added a regression test to LinearMixedRegressionSuite, paralleling the Python test, that catches both bugs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1550
https://github.com/hail-is/hail/pull/1550:410,Testability,test,test,410,"Now the lmmreg python test is deterministic. And I found another bug in lmmreg by implementing the same test in LinearMixedRegressionSuite, which I then fixed, namely, kinshipVds should have been filtKinshipVds. Upon rebasing, that test failed again due to interaction of IntIterator and lazyFilterWith, which I've also fixed. I've added a regression test to LinearMixedRegressionSuite, paralleling the Python test, that catches both bugs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1550
https://github.com/hail-is/hail/pull/1552:92,Availability,error,error,92,"This PR breaks ""fatal"" out into two functions, ""fatal"" and ""abort"". Fatal is for unexpected error handling, and produces a python stacktrace, but 'abort' is for handled, expected errors (like invalid method inputs) and does not generate a stack trace in python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552
https://github.com/hail-is/hail/pull/1552:179,Availability,error,errors,179,"This PR breaks ""fatal"" out into two functions, ""fatal"" and ""abort"". Fatal is for unexpected error handling, and produces a python stacktrace, but 'abort' is for handled, expected errors (like invalid method inputs) and does not generate a stack trace in python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552
https://github.com/hail-is/hail/pull/1552:60,Safety,abort,abort,60,"This PR breaks ""fatal"" out into two functions, ""fatal"" and ""abort"". Fatal is for unexpected error handling, and produces a python stacktrace, but 'abort' is for handled, expected errors (like invalid method inputs) and does not generate a stack trace in python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552
https://github.com/hail-is/hail/pull/1552:147,Safety,abort,abort,147,"This PR breaks ""fatal"" out into two functions, ""fatal"" and ""abort"". Fatal is for unexpected error handling, and produces a python stacktrace, but 'abort' is for handled, expected errors (like invalid method inputs) and does not generate a stack trace in python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552
https://github.com/hail-is/hail/issues/1556:83,Availability,error,error,83,"TextContext should attach the caught exception as the cause on the resulting fatal error. I have partial code for this and will make a PR, just leaving this here as a reminder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1556
https://github.com/hail-is/hail/pull/1557:14,Security,expose,expose,14,The functions expose Field attributes (useful for VCF header); - Annotations generated using annotate_alleles_expr now have 'Number=A'; - VEP now parses the Description when using csq=True option (required to know the fields stored in the |-delimited field),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1557
https://github.com/hail-is/hail/issues/1564:541,Testability,log,log,541,"```; dking@wmb16-359 # python; Python 2.7.12 |Anaconda 4.2.0 (x86_64)| (default, Jul 2 2016, 17:43:17) ; [GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> from hail import *; >>> hc = HailContext(local=""local[1]""); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hc.readhail: info: SparkUI: http://10.238.60.117:4040; >>> hc.write_partitioning(""/Users/dking/projects/hail-data/profile225.vds""); [Stage 4:==================================================> (58 + 4) / 65]hail: info: Coerced sorted dataset; ```. note the `+ 4`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1564
https://github.com/hail-is/hail/issues/1564:573,Testability,log,logging,573,"```; dking@wmb16-359 # python; Python 2.7.12 |Anaconda 4.2.0 (x86_64)| (default, Jul 2 2016, 17:43:17) ; [GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> from hail import *; >>> hc = HailContext(local=""local[1]""); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hc.readhail: info: SparkUI: http://10.238.60.117:4040; >>> hc.write_partitioning(""/Users/dking/projects/hail-data/profile225.vds""); [Stage 4:==================================================> (58 + 4) / 65]hail: info: Coerced sorted dataset; ```. note the `+ 4`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1564
https://github.com/hail-is/hail/pull/1566:5,Safety,safe,safe,5,"It's safe here since we're within bounds and don't use negative indexing.; ```; def unsafeValueAt(row: Int, col: Int): V = data(linearIndex(row, col)); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1566
https://github.com/hail-is/hail/pull/1566:84,Safety,unsafe,unsafeValueAt,84,"It's safe here since we're within bounds and don't use negative indexing.; ```; def unsafeValueAt(row: Int, col: Int): V = data(linearIndex(row, col)); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1566
https://github.com/hail-is/hail/issues/1567:131,Testability,log,log,131,"so using the basic usage of the vds on a small data set with a single region, . ```; result = vds.aggregate_intervals('region_test.log',; 'n_total = variants.count()','out.log'); ```; ; has an out.log file like so . ```; Contig Start End [Ljava.lang.String;@1787beef; 22	16050213 16050608 21; ```. As I understand it, the first line should have the header to the fourth column as n_total. Thought would give a heads up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1567
https://github.com/hail-is/hail/issues/1567:172,Testability,log,log,172,"so using the basic usage of the vds on a small data set with a single region, . ```; result = vds.aggregate_intervals('region_test.log',; 'n_total = variants.count()','out.log'); ```; ; has an out.log file like so . ```; Contig Start End [Ljava.lang.String;@1787beef; 22	16050213 16050608 21; ```. As I understand it, the first line should have the header to the fourth column as n_total. Thought would give a heads up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1567
https://github.com/hail-is/hail/issues/1567:197,Testability,log,log,197,"so using the basic usage of the vds on a small data set with a single region, . ```; result = vds.aggregate_intervals('region_test.log',; 'n_total = variants.count()','out.log'); ```; ; has an out.log file like so . ```; Contig Start End [Ljava.lang.String;@1787beef; 22	16050213 16050608 21; ```. As I understand it, the first line should have the header to the fourth column as n_total. Thought would give a heads up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1567
https://github.com/hail-is/hail/issues/1569:17,Testability,log,logistic,17,"After linear and logistic, the most common regression is poission (count data):; https://en.wikipedia.org/wiki/Poisson_regression. Adding it involves a minimal change to the logistic regression code. We could consider other common cases later, like probit and log-linear.; https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1569
https://github.com/hail-is/hail/issues/1569:174,Testability,log,logistic,174,"After linear and logistic, the most common regression is poission (count data):; https://en.wikipedia.org/wiki/Poisson_regression. Adding it involves a minimal change to the logistic regression code. We could consider other common cases later, like probit and log-linear.; https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1569
https://github.com/hail-is/hail/issues/1569:260,Testability,log,log-linear,260,"After linear and logistic, the most common regression is poission (count data):; https://en.wikipedia.org/wiki/Poisson_regression. Adding it involves a minimal change to the logistic regression code. We could consider other common cases later, like probit and log-linear.; https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1569
https://github.com/hail-is/hail/pull/1570:762,Testability,test,test,762,"@cseed definitely ready for some feedback. I replaced Iterable with SharedIterable in VSM, and carried it through to the point that Main builds ... which took more than 30 lines. I had to comment out VSMSubgen because I could not get this to work in SharedIterable:. `implicit def canBuildFrom[T]: CanBuildFrom[Coll, T, SharedIterable[T]] = ReusableCBF.asInstanceOf[GenericCanBuildFrom[T]]`. I left in FIXME's wherever I used toIterable to go from SharedIterable to Iterable, such as VSM functions that will need to be implemented separately in GDS and VSM (in retrospect, I suppose I could have just searched for toIterable usages). toIterable won't yet cause problems since I haven't yet moved to MutableGenotypeStreamIterator, so I could temporarily make the test work with toIterators as well, though I'd then need to go and switch them all back later. Once the Builder issue is resolved, I'm thinking the next step would be to make SharedIterable abstract and add abstract functions that are necessary to remove the toIterables and for the tests (like `expandCollect`). Then I'd have two realizations, SharedIterableGenotype for VDS which takes advantage of copy on Genotype, and UnsharedIterable (names need adjustment) for GDS which assumes no sharing. If all tests pass, I'd then switch over to MutableGenotypeStreamIterator...and in principal everything should pass.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1570
https://github.com/hail-is/hail/pull/1570:1045,Testability,test,tests,1045,"@cseed definitely ready for some feedback. I replaced Iterable with SharedIterable in VSM, and carried it through to the point that Main builds ... which took more than 30 lines. I had to comment out VSMSubgen because I could not get this to work in SharedIterable:. `implicit def canBuildFrom[T]: CanBuildFrom[Coll, T, SharedIterable[T]] = ReusableCBF.asInstanceOf[GenericCanBuildFrom[T]]`. I left in FIXME's wherever I used toIterable to go from SharedIterable to Iterable, such as VSM functions that will need to be implemented separately in GDS and VSM (in retrospect, I suppose I could have just searched for toIterable usages). toIterable won't yet cause problems since I haven't yet moved to MutableGenotypeStreamIterator, so I could temporarily make the test work with toIterators as well, though I'd then need to go and switch them all back later. Once the Builder issue is resolved, I'm thinking the next step would be to make SharedIterable abstract and add abstract functions that are necessary to remove the toIterables and for the tests (like `expandCollect`). Then I'd have two realizations, SharedIterableGenotype for VDS which takes advantage of copy on Genotype, and UnsharedIterable (names need adjustment) for GDS which assumes no sharing. If all tests pass, I'd then switch over to MutableGenotypeStreamIterator...and in principal everything should pass.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1570
https://github.com/hail-is/hail/pull/1570:1267,Testability,test,tests,1267,"@cseed definitely ready for some feedback. I replaced Iterable with SharedIterable in VSM, and carried it through to the point that Main builds ... which took more than 30 lines. I had to comment out VSMSubgen because I could not get this to work in SharedIterable:. `implicit def canBuildFrom[T]: CanBuildFrom[Coll, T, SharedIterable[T]] = ReusableCBF.asInstanceOf[GenericCanBuildFrom[T]]`. I left in FIXME's wherever I used toIterable to go from SharedIterable to Iterable, such as VSM functions that will need to be implemented separately in GDS and VSM (in retrospect, I suppose I could have just searched for toIterable usages). toIterable won't yet cause problems since I haven't yet moved to MutableGenotypeStreamIterator, so I could temporarily make the test work with toIterators as well, though I'd then need to go and switch them all back later. Once the Builder issue is resolved, I'm thinking the next step would be to make SharedIterable abstract and add abstract functions that are necessary to remove the toIterables and for the tests (like `expandCollect`). Then I'd have two realizations, SharedIterableGenotype for VDS which takes advantage of copy on Genotype, and UnsharedIterable (names need adjustment) for GDS which assumes no sharing. If all tests pass, I'd then switch over to MutableGenotypeStreamIterator...and in principal everything should pass.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1570
https://github.com/hail-is/hail/pull/1570:33,Usability,feedback,feedback,33,"@cseed definitely ready for some feedback. I replaced Iterable with SharedIterable in VSM, and carried it through to the point that Main builds ... which took more than 30 lines. I had to comment out VSMSubgen because I could not get this to work in SharedIterable:. `implicit def canBuildFrom[T]: CanBuildFrom[Coll, T, SharedIterable[T]] = ReusableCBF.asInstanceOf[GenericCanBuildFrom[T]]`. I left in FIXME's wherever I used toIterable to go from SharedIterable to Iterable, such as VSM functions that will need to be implemented separately in GDS and VSM (in retrospect, I suppose I could have just searched for toIterable usages). toIterable won't yet cause problems since I haven't yet moved to MutableGenotypeStreamIterator, so I could temporarily make the test work with toIterators as well, though I'd then need to go and switch them all back later. Once the Builder issue is resolved, I'm thinking the next step would be to make SharedIterable abstract and add abstract functions that are necessary to remove the toIterables and for the tests (like `expandCollect`). Then I'd have two realizations, SharedIterableGenotype for VDS which takes advantage of copy on Genotype, and UnsharedIterable (names need adjustment) for GDS which assumes no sharing. If all tests pass, I'd then switch over to MutableGenotypeStreamIterator...and in principal everything should pass.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1570
https://github.com/hail-is/hail/pull/1572:7,Performance,optimiz,optimized,7,…added optimized keyBy in scala / python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1572
https://github.com/hail-is/hail/pull/1576:0,Integrability,Depend,Dependent,0,Dependent on #1574,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1576
https://github.com/hail-is/hail/pull/1590:199,Testability,test,tests,199,-Created KinshipMatrix class in Python; -Created method rrm() on VDS that returns a KinshipMatrix; -Modified lmmreg to take a KinshipMatrix instead of a VDS from which to compute kinship. ; -Created tests for KinshipMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1590
https://github.com/hail-is/hail/pull/1595:84,Testability,test,tests,84,Changes:. -Added RichIndexedRowMatrix with new method .toBlockMatrixDense(); -Added tests for new method ; -Changed ComputeRRM to use new method,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1595
https://github.com/hail-is/hail/pull/1596:15,Testability,test,tests,15,python dataset tests have been restructured to test most functions on both VDS and GDS with for loop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1596
https://github.com/hail-is/hail/pull/1596:47,Testability,test,test,47,python dataset tests have been restructured to test most functions on both VDS and GDS with for loop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1596
https://github.com/hail-is/hail/issues/1601:1135,Deployability,release,release-,1135,"Using `591f7e6`, getting a NullPointer when trying to `explode` and then write to parquet. ```; Caused by: java.lang.NullPointerException; at is.hail.keytable.KeyTable$$anonfun$59.apply(KeyTable.scala:536); at is.hail.keytable.KeyTable$$anonfun$59.apply(KeyTable.scala:534); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); ```. Example:; ```; from hail import *; hc = HailContext(); output = 'gs://' # set this to some output file; a_anns = [u'va.info.AC', u'va.info.AF', u'va.info.AS_FilterStatus']. def index_into_arrays(a_based_annotations, vep_root=None):; annotations = []; if a_based_annotations:; for ann in a_based_annotations:; annotations.append('%s = %s[va.aIndex - 1]' % (ann, ann)); if vep_root:; sub_fields = ['transcript_consequences', 'intergenic_consequences', 'motif_feature_consequences', 'regulatory_feature_consequences']; annotations.extend(['%s.%s = %s.%s.filter(x => x.allele_num == va.aIndex)' % (vep_root, sub_field, vep_root, sub_field) for sub_field in sub_fields]); return annotations. kt = (hc.read('gs://gnomad-public/release-170228/gnomad.exomes.r2.0.1.sites.autosomes.vds').split_multi(); .annotate_variants_expr('va.info = select(va.info, AC, AF, AS_FilterStatus)'); .annotate_variants_expr(index_into_arrays(a_anns, vep_root='va.vep')); .annotate_variants_expr(['va.filters = va.filters.mkString(""|"")',; 'va.info.AS_FilterStatus = va.info.AS_FilterStatus.mkString(""|"")',; 'va.vep = va.vep.transcript_consequences.map(x => drop(x, domains))']); .variants_keytable()). # kt.flatten().to_dataframe().write.parquet(output) # works; kt.flatten().explode('va.vep').to_dataframe().write.parquet(output) # fails; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1601
https://github.com/hail-is/hail/issues/1601:942,Modifiability,extend,extend,942,"Using `591f7e6`, getting a NullPointer when trying to `explode` and then write to parquet. ```; Caused by: java.lang.NullPointerException; at is.hail.keytable.KeyTable$$anonfun$59.apply(KeyTable.scala:536); at is.hail.keytable.KeyTable$$anonfun$59.apply(KeyTable.scala:534); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); ```. Example:; ```; from hail import *; hc = HailContext(); output = 'gs://' # set this to some output file; a_anns = [u'va.info.AC', u'va.info.AF', u'va.info.AS_FilterStatus']. def index_into_arrays(a_based_annotations, vep_root=None):; annotations = []; if a_based_annotations:; for ann in a_based_annotations:; annotations.append('%s = %s[va.aIndex - 1]' % (ann, ann)); if vep_root:; sub_fields = ['transcript_consequences', 'intergenic_consequences', 'motif_feature_consequences', 'regulatory_feature_consequences']; annotations.extend(['%s.%s = %s.%s.filter(x => x.allele_num == va.aIndex)' % (vep_root, sub_field, vep_root, sub_field) for sub_field in sub_fields]); return annotations. kt = (hc.read('gs://gnomad-public/release-170228/gnomad.exomes.r2.0.1.sites.autosomes.vds').split_multi(); .annotate_variants_expr('va.info = select(va.info, AC, AF, AS_FilterStatus)'); .annotate_variants_expr(index_into_arrays(a_anns, vep_root='va.vep')); .annotate_variants_expr(['va.filters = va.filters.mkString(""|"")',; 'va.info.AS_FilterStatus = va.info.AS_FilterStatus.mkString(""|"")',; 'va.vep = va.vep.transcript_consequences.map(x => drop(x, domains))']); .variants_keytable()). # kt.flatten().to_dataframe().write.parquet(output) # works; kt.flatten().explode('va.vep').to_dataframe().write.parquet(output) # fails; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1601
https://github.com/hail-is/hail/issues/1610:83,Availability,error,errors,83,This would prevent people from using it naively on sample aggregations and getting errors. Can also add a mode that takes an integer nAlleles,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1610
https://github.com/hail-is/hail/pull/1613:63,Availability,error,errors,63,"If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run ./configure. The ./configure script queries the user for sparkVersion and generates a valid gradle.properties file. Afterwards, the user can execute gradle normally without any -D parameters. Users may still override the sparkVersion variable on the command line by specifying -PsparkVersion=2.1.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613
https://github.com/hail-is/hail/pull/1613:97,Modifiability,config,configure,97,"If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run ./configure. The ./configure script queries the user for sparkVersion and generates a valid gradle.properties file. Afterwards, the user can execute gradle normally without any -D parameters. Users may still override the sparkVersion variable on the command line by specifying -PsparkVersion=2.1.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613
https://github.com/hail-is/hail/pull/1613:114,Modifiability,config,configure,114,"If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run ./configure. The ./configure script queries the user for sparkVersion and generates a valid gradle.properties file. Afterwards, the user can execute gradle normally without any -D parameters. Users may still override the sparkVersion variable on the command line by specifying -PsparkVersion=2.1.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613
https://github.com/hail-is/hail/pull/1613:329,Modifiability,variab,variable,329,"If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run ./configure. The ./configure script queries the user for sparkVersion and generates a valid gradle.properties file. Afterwards, the user can execute gradle normally without any -D parameters. Users may still override the sparkVersion variable on the command line by specifying -PsparkVersion=2.1.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1613
https://github.com/hail-is/hail/issues/1616:5,Deployability,pipeline,pipelines,5,long pipelines are harder and harder to debug these days without it,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1616
https://github.com/hail-is/hail/issues/1623:586,Availability,Error,Error,586,"```python; vds.filter_variants_expr('v => va.pass').count(); ```. ```; vds.filter_variants_expr('v => va.pass').count(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-44-0380f72331b7> in <module>(); ----> 1 vds.filter_variants_expr('v => va.pass').count(). <decorator-gen-223> in filter_variants_expr(self, condition, keep). /Users/tpoterba/hail/python/hail/java.py in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: UnsupportedOperationException: null. Java stack trace:; java.lang.UnsupportedOperationException: null; 	at is.hail.expr.AST.typecheckThis(AST.scala:215); 	at is.hail.expr.AST.typecheckThis(AST.scala:213); 	at is.hail.expr.AST.typecheck(AST.scala:219); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:67); 	at is.hail.expr.Parser$.parseTypedExpr(Parser.scala:77); 	at is.hail.variant.VariantSampleMatrix.filterVariantsExpr(VariantSampleMatrix.scala:1229); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1623
https://github.com/hail-is/hail/issues/1623:726,Availability,error,error,726,"```python; vds.filter_variants_expr('v => va.pass').count(); ```. ```; vds.filter_variants_expr('v => va.pass').count(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-44-0380f72331b7> in <module>(); ----> 1 vds.filter_variants_expr('v => va.pass').count(). <decorator-gen-223> in filter_variants_expr(self, condition, keep). /Users/tpoterba/hail/python/hail/java.py in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: UnsupportedOperationException: null. Java stack trace:; java.lang.UnsupportedOperationException: null; 	at is.hail.expr.AST.typecheckThis(AST.scala:215); 	at is.hail.expr.AST.typecheckThis(AST.scala:213); 	at is.hail.expr.AST.typecheck(AST.scala:219); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:67); 	at is.hail.expr.Parser$.parseTypedExpr(Parser.scala:77); 	at is.hail.variant.VariantSampleMatrix.filterVariantsExpr(VariantSampleMatrix.scala:1229); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1623
https://github.com/hail-is/hail/issues/1623:2035,Availability,Error,Error,2035,"ts_expr('v => va.pass').count(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-44-0380f72331b7> in <module>(); ----> 1 vds.filter_variants_expr('v => va.pass').count(). <decorator-gen-223> in filter_variants_expr(self, condition, keep). /Users/tpoterba/hail/python/hail/java.py in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: UnsupportedOperationException: null. Java stack trace:; java.lang.UnsupportedOperationException: null; 	at is.hail.expr.AST.typecheckThis(AST.scala:215); 	at is.hail.expr.AST.typecheckThis(AST.scala:213); 	at is.hail.expr.AST.typecheck(AST.scala:219); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:67); 	at is.hail.expr.Parser$.parseTypedExpr(Parser.scala:77); 	at is.hail.variant.VariantSampleMatrix.filterVariantsExpr(VariantSampleMatrix.scala:1229); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-a3d64be; Error summary: UnsupportedOperationException: null```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1623
https://github.com/hail-is/hail/issues/1623:668,Integrability,protocol,protocol,668,"```python; vds.filter_variants_expr('v => va.pass').count(); ```. ```; vds.filter_variants_expr('v => va.pass').count(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-44-0380f72331b7> in <module>(); ----> 1 vds.filter_variants_expr('v => va.pass').count(). <decorator-gen-223> in filter_variants_expr(self, condition, keep). /Users/tpoterba/hail/python/hail/java.py in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: UnsupportedOperationException: null. Java stack trace:; java.lang.UnsupportedOperationException: null; 	at is.hail.expr.AST.typecheckThis(AST.scala:215); 	at is.hail.expr.AST.typecheckThis(AST.scala:213); 	at is.hail.expr.AST.typecheck(AST.scala:219); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:67); 	at is.hail.expr.Parser$.parseTypedExpr(Parser.scala:77); 	at is.hail.variant.VariantSampleMatrix.filterVariantsExpr(VariantSampleMatrix.scala:1229); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1623
https://github.com/hail-is/hail/issues/1633:215,Availability,avail,available,215,"...once KinshipMatrix is in, with RRM going there. The current export to file formats on GRM should be moved to KinshipMatrix too. And then doc on lmmreg should be updated to reflect there are more options than RRM available.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1633
https://github.com/hail-is/hail/issues/1633:164,Deployability,update,updated,164,"...once KinshipMatrix is in, with RRM going there. The current export to file formats on GRM should be moved to KinshipMatrix too. And then doc on lmmreg should be updated to reflect there are more options than RRM available.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1633
https://github.com/hail-is/hail/pull/1634:62,Testability,test,test,62,Adds readBiallelicDosage to genotype and an additional dosage test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1634
https://github.com/hail-is/hail/pull/1644:89,Integrability,interface,interface,89,* CamelCased minRep (was inconsistent globally and more importantly camelcased in python interface!); * Added `isStar` to `AltAllele` and made sure that `*` alleles aren't counted as `SNP`s anymore; * Modified minRep to ignore `*` alleles and correctly minrep in their presence; * FilterAlleles now filters sites where only a `*` allele is left by default (keepStar option allows overriding this behavior),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1644
https://github.com/hail-is/hail/pull/1654:44,Availability,error,error,44,"for `vds.linreg('sa.pheno', 'sa.cov1')` the error is now:. ```; sa.cov1 is not an array; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:518,Availability,Error,Error,518,"for `vds.linreg('sa.pheno', 'sa.cov1')` the error is now:. ```; sa.cov1 is not an array; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:658,Availability,error,error,658,"for `vds.linreg('sa.pheno', 'sa.cov1')` the error is now:. ```; sa.cov1 is not an array; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:740,Availability,Avail,Available,740,"for `vds.linreg('sa.pheno', 'sa.cov1')` the error is now:. ```; sa.cov1 is not an array; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:870,Availability,Avail,Available,870,"for `vds.linreg('sa.pheno', 'sa.cov1')` the error is now:. ```; sa.cov1 is not an array; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:947,Availability,Error,ErrorHandling,947,"for `vds.linreg('sa.pheno', 'sa.cov1')` the error is now:. ```; sa.cov1 is not an array; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:973,Availability,Error,ErrorHandling,973,"for `vds.linreg('sa.pheno', 'sa.cov1')` the error is now:. ```; sa.cov1 is not an array; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:1077,Availability,error,error,1077,"rray; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.stats.RegressionUtils$.getPhenoCovCompleteSamples(RegressionUtils.scala:36); at is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:3043,Availability,Error,Error,3043,.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.stats.RegressionUtils$.getPhenoCovCompleteSamples(RegressionUtils.scala:36); at is.hail.methods.LinearRegression$.apply(LinearRegression.scala:21); at is.hail.variant.VariantDatasetFunctions$.linreg$extension(VariantDataset.scala:848); at is.hail.variant.VariantDatasetFunctions.linreg(VariantDataset.scala:846); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-07f60b2; Error summary: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:3095,Availability,Avail,Available,3095,.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.stats.RegressionUtils$.getPhenoCovCompleteSamples(RegressionUtils.scala:36); at is.hail.methods.LinearRegression$.apply(LinearRegression.scala:21); at is.hail.variant.VariantDatasetFunctions$.linreg$extension(VariantDataset.scala:848); at is.hail.variant.VariantDatasetFunctions.linreg(VariantDataset.scala:846); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-07f60b2; Error summary: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1654:600,Integrability,protocol,protocol,600,"for `vds.linreg('sa.pheno', 'sa.cov1')` the error is now:. ```; sa.cov1 is not an array; ```. rather than. ```; FatalError Traceback (most recent call last); <ipython-input-18-89d0abef0d0a> in <module>(); ----> 1 vds1 = vds.linreg('sa.pheno', 'sa.cov1').count(). <decorator-gen-240> in linreg(self, y, covariates, root, use_dosages, min_ac, min_af). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^. Java stack trace:; is.hail.utils.HailException: symbol `a' not found; Available symbols:; s: String; sa: Struct ; <input>:1:a; ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:20); at is.hail.expr.ParserUtils$.error(Parser.scala:24); at is.hail.expr.AST.parseError(AST.scala:222); at is.hail.expr.SymRef.typecheckThis(AST.scala:648); at is.hail.expr.AST.typecheck(AST.scala:219); at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:57); at is.hail.expr.Parser$.parseExpr(Parser.scala:67); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at is.hail.stats.RegressionUtils$$anonfun$3.apply(RegressionUtils.scala:36); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1654
https://github.com/hail-is/hail/pull/1664:35,Safety,avoid,avoid,35,* Added a quick check in minRep to avoid copmuting things if ref is already minimal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1664
https://github.com/hail-is/hail/pull/1668:8,Testability,test,test,8,* Added test for deletion at multi-allelic site; * Added test for minrep *,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1668
https://github.com/hail-is/hail/pull/1668:57,Testability,test,test,57,* Added test for deletion at multi-allelic site; * Added test for minrep *,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1668
https://github.com/hail-is/hail/pull/1669:34,Usability,clear,clearer,34,The representation docs seem much clearer than the expr docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1669
https://github.com/hail-is/hail/pull/1678:0,Integrability,Depend,Depends,0,Depends on #1602,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1678
https://github.com/hail-is/hail/pull/1679:241,Testability,test,test,241,"To be clear, the bug was that if the rdd of IndexedRows backing the IndexedRowMatrix omitted some rows (as a way of indicating that they contained only 0's), that omitted row would not get written to TSV file. This has been corrected, and a test was created for it by making the test data in the Suite more interesting and adding an assertion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1679
https://github.com/hail-is/hail/pull/1679:279,Testability,test,test,279,"To be clear, the bug was that if the rdd of IndexedRows backing the IndexedRowMatrix omitted some rows (as a way of indicating that they contained only 0's), that omitted row would not get written to TSV file. This has been corrected, and a test was created for it by making the test data in the Suite more interesting and adding an assertion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1679
https://github.com/hail-is/hail/pull/1679:333,Testability,assert,assertion,333,"To be clear, the bug was that if the rdd of IndexedRows backing the IndexedRowMatrix omitted some rows (as a way of indicating that they contained only 0's), that omitted row would not get written to TSV file. This has been corrected, and a test was created for it by making the test data in the Suite more interesting and adding an assertion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1679
https://github.com/hail-is/hail/pull/1679:6,Usability,clear,clear,6,"To be clear, the bug was that if the rdd of IndexedRows backing the IndexedRowMatrix omitted some rows (as a way of indicating that they contained only 0's), that omitted row would not get written to TSV file. This has been corrected, and a test was created for it by making the test data in the Suite more interesting and adding an assertion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1679
https://github.com/hail-is/hail/pull/1681:169,Testability,test,tests,169,"-Changed the ""grm()"" method to return instances of KinshipMatrix. . -Added various export to file methods that were supported by GRM to the KinshipMatrix. -Modified GRM tests appropriately. . -Add RichIndexedRowMatrix implicit to the hail utils implicit file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1681
https://github.com/hail-is/hail/issues/1683:63,Availability,error,errors,63,"When accessing two VDS files in `gs://hail-common/`, I got two errors. 1. gs://hail-common/all_coding_plus_minus_50bp_vep.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:563,Availability,Error,ErrorHandling,563,"When accessing two VDS files in `gs://hail-common/`, I got two errors. 1. gs://hail-common/all_coding_plus_minus_50bp_vep.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:589,Availability,Error,ErrorHandling,589,"When accessing two VDS files in `gs://hail-common/`, I got two errors. 1. gs://hail-common/all_coding_plus_minus_50bp_vep.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:2211,Availability,Error,Error,2211,"table.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:2843,Availability,Error,ErrorHandling,2843,"lectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:2869,Availability,Error,ErrorHandling,2869,"(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMet",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:4491,Availability,Error,Error,4491,".json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:349,Performance,load,loading,349,"When accessing two VDS files in `gs://hail-common/`, I got two errors. 1. gs://hail-common/all_coding_plus_minus_50bp_vep.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:487,Performance,load,loading,487,"When accessing two VDS files in `gs://hail-common/`, I got two errors. 1. gs://hail-common/all_coding_plus_minus_50bp_vep.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:2274,Performance,load,loading,2274,"ion.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversable",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:2629,Performance,load,loading,2629,"ngMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:2767,Performance,load,loading,2767,"on.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```. 2. gs://hail-common/gencode_and_production_intervals.merged.hg19.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAcc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:4554,Performance,load,loading,4554,".json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/issues/1683:5,Security,access,accessing,5,"When accessing two VDS files in `gs://hail-common/`, I got two errors. 1. gs://hail-common/all_coding_plus_minus_50bp_vep.vds. ```; File ""<decorator-gen-162>"", line 2, in read; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1683
https://github.com/hail-is/hail/pull/1687:85,Availability,checkpoint,checkpoint,85,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1687
https://github.com/hail-is/hail/pull/1687:97,Availability,Checkpoint,Checkpoint,97,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1687
https://github.com/hail-is/hail/pull/1687:109,Availability,checkpoint,checkpoint,109,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1687
https://github.com/hail-is/hail/pull/1687:231,Availability,error,errors,231,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1687
https://github.com/hail-is/hail/pull/1687:0,Modifiability,Refactor,Refactor,0,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1687
https://github.com/hail-is/hail/pull/1687:127,Testability,test,tests,127,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1687
https://github.com/hail-is/hail/pull/1687:160,Testability,test,tests,160,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1687
https://github.com/hail-is/hail/pull/1687:243,Testability,test,tests,243,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1687
https://github.com/hail-is/hail/pull/1688:85,Availability,checkpoint,checkpoint,85,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1688
https://github.com/hail-is/hail/pull/1688:97,Availability,Checkpoint,Checkpoint,97,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1688
https://github.com/hail-is/hail/pull/1688:109,Availability,checkpoint,checkpoint,109,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1688
https://github.com/hail-is/hail/pull/1688:231,Availability,error,errors,231,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1688
https://github.com/hail-is/hail/pull/1688:0,Modifiability,Refactor,Refactor,0,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1688
https://github.com/hail-is/hail/pull/1688:127,Testability,test,tests,127,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1688
https://github.com/hail-is/hail/pull/1688:160,Testability,test,tests,160,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1688
https://github.com/hail-is/hail/pull/1688:243,Testability,test,tests,243,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1688
https://github.com/hail-is/hail/pull/1703:11,Testability,test,tests,11,cleaned up tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1703
https://github.com/hail-is/hail/issues/1705:205,Availability,error,error,205,"Tried to run this code:. `vds.annotate_samples_expr('sa.variant1 = gs.filter(g => v == Variant(""1:55505447:C:T"")).collect()[0].gt')`. But the variant was not in the dataset at all, so got an out of bounds error, but it looked like this:. ```; [Stage 2:======================================================>(278 + 1) / 279]// class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable scala/Function2 {. // access flags 0x1; public apply(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;; L0; ALOAD 1; CHECKCAST [Ljava/lang/Object;; LDC 0; AALOAD; INVOKEINTERFACE scala/Function0.apply ()Ljava/lang/Object;; CHECKCAST scala/collection/IndexedSeq; ASTORE 3; ALOAD 3; IFNULL L1; NEW java/lang/Integer; DUP; LDC 0; INVOKESPECIAL java/lang/Integer.<init> (I)V; ASTORE 4; ALOAD 4; IFNULL L2; ALOAD 4; INVOKEVIRTUAL java/lang/Number.intValue ()I; ISTORE 5; ALOAD 3; ILOAD 5; LDC 0; IF_ICMPGE L3; GOTO L4; L4; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq java/lang/Integer I] [scala/collection/IndexedSeq]; ILOAD 5; ALOAD 3; INVOKEINTERFACE scala/collection/IndexedSeq.size ()I; IADD; GOTO L5; L3; FRAME SAME1 scala/collection/IndexedSeq; ILOAD 5; L5; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq java/lang/Integer I] [scala/collection/IndexedSeq I]; INVOKEINTERFACE scala/collection/IndexedSeq.apply (I)Ljava/lang/Object;; GOTO L6; L2; FRAME CHOP 1; ACONST_NULL; L6; FRAME SAME1 java/lang/Object; GOTO L7; L1; FRAME CHOP 1; ACONST_NULL; L7; FRAME SAME1 java/lang/Object; CHECKCAST is/hail/variant/Genotype; ASTORE 6; ALOAD 6; IFNULL L8; ALOAD 6; INVOKEVIRTUAL is/hail/variant/Genotype.unboxedGT ()I; ISTORE 7; ILOAD 7; LDC -1; IF_ICMPEQ L9; GOTO L10; L10; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq T T is/hail/variant/Genotype I] []; NEW java/lang/Integer; DUP; I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1705
https://github.com/hail-is/hail/issues/1705:5904,Availability,Error,Error,5904,cala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at is.hail.expr.Parser$$anonfun$parseNamedExprs$3.apply(Parser.scala:229); 	at is.hail.expr.Parser$$anonfun$parseNamedExprs$3.apply(Parser.scala:228); 	at is.hail.expr.Parser$$anonfun$parseAnnotationExprs$4.apply(Parser.scala:115); 	at is.hail.expr.Parser$$anonfun$parseAnnotationExprs$4.apply(Parser.scala:114); 	at is.hail.variant.VariantSampleMatrix$$anonfun$34.apply(VariantSampleMatrix.scala:544); 	at is.hail.variant.VariantSampleMatrix$$anonfun$34.apply(VariantSampleMatrix.scala:540); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.variant.VariantSampleMatrix.annotateSamplesExpr(VariantSampleMatrix.scala:540); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-e081278; Error summary: IndexOutOfBoundsException: 0; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1705
https://github.com/hail-is/hail/issues/1705:354,Security,access,access,354,"Tried to run this code:. `vds.annotate_samples_expr('sa.variant1 = gs.filter(g => v == Variant(""1:55505447:C:T"")).collect()[0].gt')`. But the variant was not in the dataset at all, so got an out of bounds error, but it looked like this:. ```; [Stage 2:======================================================>(278 + 1) / 279]// class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable scala/Function2 {. // access flags 0x1; public apply(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;; L0; ALOAD 1; CHECKCAST [Ljava/lang/Object;; LDC 0; AALOAD; INVOKEINTERFACE scala/Function0.apply ()Ljava/lang/Object;; CHECKCAST scala/collection/IndexedSeq; ASTORE 3; ALOAD 3; IFNULL L1; NEW java/lang/Integer; DUP; LDC 0; INVOKESPECIAL java/lang/Integer.<init> (I)V; ASTORE 4; ALOAD 4; IFNULL L2; ALOAD 4; INVOKEVIRTUAL java/lang/Number.intValue ()I; ISTORE 5; ALOAD 3; ILOAD 5; LDC 0; IF_ICMPGE L3; GOTO L4; L4; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq java/lang/Integer I] [scala/collection/IndexedSeq]; ILOAD 5; ALOAD 3; INVOKEINTERFACE scala/collection/IndexedSeq.size ()I; IADD; GOTO L5; L3; FRAME SAME1 scala/collection/IndexedSeq; ILOAD 5; L5; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq java/lang/Integer I] [scala/collection/IndexedSeq I]; INVOKEINTERFACE scala/collection/IndexedSeq.apply (I)Ljava/lang/Object;; GOTO L6; L2; FRAME CHOP 1; ACONST_NULL; L6; FRAME SAME1 java/lang/Object; GOTO L7; L1; FRAME CHOP 1; ACONST_NULL; L7; FRAME SAME1 java/lang/Object; CHECKCAST is/hail/variant/Genotype; ASTORE 6; ALOAD 6; IFNULL L8; ALOAD 6; INVOKEVIRTUAL is/hail/variant/Genotype.unboxedGT ()I; ISTORE 7; ILOAD 7; LDC -1; IF_ICMPEQ L9; GOTO L10; L10; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq T T is/hail/variant/Genotype I] []; NEW java/lang/Integer; DUP; I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1705
https://github.com/hail-is/hail/issues/1705:468,Security,access,access,468,"Tried to run this code:. `vds.annotate_samples_expr('sa.variant1 = gs.filter(g => v == Variant(""1:55505447:C:T"")).collect()[0].gt')`. But the variant was not in the dataset at all, so got an out of bounds error, but it looked like this:. ```; [Stage 2:======================================================>(278 + 1) / 279]// class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable scala/Function2 {. // access flags 0x1; public apply(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;; L0; ALOAD 1; CHECKCAST [Ljava/lang/Object;; LDC 0; AALOAD; INVOKEINTERFACE scala/Function0.apply ()Ljava/lang/Object;; CHECKCAST scala/collection/IndexedSeq; ASTORE 3; ALOAD 3; IFNULL L1; NEW java/lang/Integer; DUP; LDC 0; INVOKESPECIAL java/lang/Integer.<init> (I)V; ASTORE 4; ALOAD 4; IFNULL L2; ALOAD 4; INVOKEVIRTUAL java/lang/Number.intValue ()I; ISTORE 5; ALOAD 3; ILOAD 5; LDC 0; IF_ICMPGE L3; GOTO L4; L4; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq java/lang/Integer I] [scala/collection/IndexedSeq]; ILOAD 5; ALOAD 3; INVOKEINTERFACE scala/collection/IndexedSeq.size ()I; IADD; GOTO L5; L3; FRAME SAME1 scala/collection/IndexedSeq; ILOAD 5; L5; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq java/lang/Integer I] [scala/collection/IndexedSeq I]; INVOKEINTERFACE scala/collection/IndexedSeq.apply (I)Ljava/lang/Object;; GOTO L6; L2; FRAME CHOP 1; ACONST_NULL; L6; FRAME SAME1 java/lang/Object; GOTO L7; L1; FRAME CHOP 1; ACONST_NULL; L7; FRAME SAME1 java/lang/Object; CHECKCAST is/hail/variant/Genotype; ASTORE 6; ALOAD 6; IFNULL L8; ALOAD 6; INVOKEVIRTUAL is/hail/variant/Genotype.unboxedGT ()I; ISTORE 7; ILOAD 7; LDC -1; IF_ICMPEQ L9; GOTO L10; L10; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq T T is/hail/variant/Genotype I] []; NEW java/lang/Integer; DUP; I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1705
https://github.com/hail-is/hail/issues/1705:2497,Security,access,access,2497,"_NULL; L6; FRAME SAME1 java/lang/Object; GOTO L7; L1; FRAME CHOP 1; ACONST_NULL; L7; FRAME SAME1 java/lang/Object; CHECKCAST is/hail/variant/Genotype; ASTORE 6; ALOAD 6; IFNULL L8; ALOAD 6; INVOKEVIRTUAL is/hail/variant/Genotype.unboxedGT ()I; ISTORE 7; ILOAD 7; LDC -1; IF_ICMPEQ L9; GOTO L10; L10; FRAME FULL [is/hail/codegen/generated/C0 java/lang/Object java/lang/Object scala/collection/IndexedSeq T T is/hail/variant/Genotype I] []; NEW java/lang/Integer; DUP; ILOAD 7; INVOKESPECIAL java/lang/Integer.<init> (I)V; GOTO L11; L9; FRAME SAME; ACONST_NULL; L11; FRAME SAME1 java/lang/Integer; GOTO L12; L8; FRAME CHOP 1; ACONST_NULL; L12; FRAME SAME1 java/lang/Integer; CHECKCAST java/lang/Integer; ARETURN; L13; LOCALVARIABLE local3 Ljava/lang/Object; L0 L13 3; LOCALVARIABLE local4 Ljava/lang/Object; L0 L13 4; LOCALVARIABLE local5 I L0 L13 5; LOCALVARIABLE local6 Ljava/lang/Object; L0 L13 6; LOCALVARIABLE local7 I L0 L13 7; MAXSTACK = 3; MAXLOCALS = 8. // access flags 0x1; public <init>()V; ALOAD 0; INVOKESPECIAL java/lang/Object.<init> ()V; RETURN; MAXSTACK = 1; MAXLOCALS = 1; }; Traceback (most recent call last):; File ""/tmp/990e2e0c-f333-467e-b1fd-6cfe8f5d5645/fastlmmForJon.py"", line 6, in <module>; vds = vds.annotate_samples_expr('sa.variant1 = gs.filter(g => v == Variant(""1:15211:T:G"")).collect()[0].gt'); File ""<decorator-gen-65>"", line 2, in annotate_samples_expr; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: IndexOutOfBoundsException: 0. Java stack trace:; java.lang.IndexOutOfBoundsException: 0; 	at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43); 	at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48); 	at is.hail.codegen.generated.C0.apply(Unknown Source); 	at is.hail.asm4s.Function2Builder$$anon$11.apply(FunctionBuilder.scala:441); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:72); 	at is.hail.expr.CM$$anonfun$runWithDelaye",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1705
https://github.com/hail-is/hail/pull/1708:12,Testability,test,tests,12,"I have core tests in place and would appreciate feedback on more exotic tests to add. There are similarities between `buildSampleAggregations` and `makeSampleFunctions` in Aggregators but differences throughout as well, so I'm not sure how hard to push on pulling out elements.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708
https://github.com/hail-is/hail/pull/1708:72,Testability,test,tests,72,"I have core tests in place and would appreciate feedback on more exotic tests to add. There are similarities between `buildSampleAggregations` and `makeSampleFunctions` in Aggregators but differences throughout as well, so I'm not sure how hard to push on pulling out elements.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708
https://github.com/hail-is/hail/pull/1708:48,Usability,feedback,feedback,48,"I have core tests in place and would appreciate feedback on more exotic tests to add. There are similarities between `buildSampleAggregations` and `makeSampleFunctions` in Aggregators but differences throughout as well, so I'm not sure how hard to push on pulling out elements.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1708
https://github.com/hail-is/hail/pull/1712:61,Modifiability,plugin,plugins,61,"Also, added hail.vep.extra_plugins for specifying additional plugins beyond the default ones (such as LoF_splice.pm for predicting variants' probability of splice junction disruption or creation)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1712
https://github.com/hail-is/hail/pull/1712:120,Safety,predict,predicting,120,"Also, added hail.vep.extra_plugins for specifying additional plugins beyond the default ones (such as LoF_splice.pm for predicting variants' probability of splice junction disruption or creation)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1712
https://github.com/hail-is/hail/pull/1720:20,Modifiability,refactor,refactoring,20,"I also did a bit of refactoring in lmmreg to make this change more organic. I will add a test asap, but want to simultaneously give @alexb-3 a chance to look over the math.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720
https://github.com/hail-is/hail/pull/1720:89,Testability,test,test,89,"I also did a bit of refactoring in lmmreg to make this change more organic. I will add a test asap, but want to simultaneously give @alexb-3 a chance to look over the math.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1720
https://github.com/hail-is/hail/issues/1722:211,Energy Efficiency,reduce,reduce,211,"Talked with Dan and Tim about tests taking a long time, and we concluded it would probably save a lot of test time if SparkSuite just had lazy vals for vds's representing sample1.vcf and sample2.vcf in order to reduce time spend reading them in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1722
https://github.com/hail-is/hail/issues/1722:30,Testability,test,tests,30,"Talked with Dan and Tim about tests taking a long time, and we concluded it would probably save a lot of test time if SparkSuite just had lazy vals for vds's representing sample1.vcf and sample2.vcf in order to reduce time spend reading them in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1722
https://github.com/hail-is/hail/issues/1722:105,Testability,test,test,105,"Talked with Dan and Tim about tests taking a long time, and we concluded it would probably save a lot of test time if SparkSuite just had lazy vals for vds's representing sample1.vcf and sample2.vcf in order to reduce time spend reading them in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1722
https://github.com/hail-is/hail/issues/1725:2888,Availability,error,error,2888,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725
https://github.com/hail-is/hail/issues/1725:2995,Availability,failure,failure,2995,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725
https://github.com/hail-is/hail/issues/1725:3053,Availability,failure,failure,3053,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725
https://github.com/hail-is/hail/issues/1725:2811,Integrability,protocol,protocol,2811,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725
https://github.com/hail-is/hail/issues/1725:2861,Integrability,protocol,protocol,2861,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725
https://github.com/hail-is/hail/issues/1725:2974,Safety,abort,aborted,2974,",A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String; }; [u'va.varid']; +--------+-------+--------------------+--------------------+--------------------+; |v.contig|v.start| v.ref| v.altAlleles| va.varid|; +--------+-------+--------------------+--------------------+--------------------+; | 01| 10013| A| [[A,C]]| 1:10013_A_C|; | 01| 10179| G| [[G,T]]| 1:10179_G_T|; | 01| 10259| C| [[C,A]]| 1:10259_C_A|; | 01| 10292| C| [[C,T]]| 1:10292_C_T|; | 01| 10402| G| [[G,A]]| 1:10402_G_A|; | 01| 10527| T| [[T,A]]| 1:10527_T_A|; | 01| 10611| G| [[G,A]]| 1:10611_G_A|; | 01| 10754| G| [[G,C]]| 1:10754_G_C|; | 01| 11099| T| [[T,G]]| 1:11099_T_G|; | 01| 11115| C| [[C,A]]| 1:11155_C_A|; +--------+-------+--------------------+--------------------+--------------------+; only showing top 10 rows. Struct {; v: Variant,; `va.varid`: String,; C1: Double,; C2: Double; }; [u'va.varid']; [Stage 6:====================================================>(1640 + 1) / 1641]Traceback (most recent call last):; File ""/tmp/ec5f6e42-0ea7-404d-8311-f97f7ec26ad6/kt_troubleshooting_issue_042617.py"", line 31, in <module>; kt2.to_dataframe().show(10); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 287, in show; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o448.showString.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 20 times, most recent failure: Lost task 0.19 in stage 8.0 (TID 3406, cluster-mh-sw-xn3h.c.practice.internal): java.lang.ClassCastException: java.lang.String cannot be cast to is.hail.variant.Variant; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1725
https://github.com/hail-is/hail/pull/1727:255,Integrability,interface,interface,255,"I haven't ported type checking to everything yet -- just hail context. It'll take an hour or two to write it for everything, and I want to wait until after breakingbad is merged to do that to avoid a hellish rebase (also in case you want me to change the interface).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1727
https://github.com/hail-is/hail/pull/1727:192,Safety,avoid,avoid,192,"I haven't ported type checking to everything yet -- just hail context. It'll take an hour or two to write it for everything, and I want to wait until after breakingbad is merged to do that to avoid a hellish rebase (also in case you want me to change the interface).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1727
https://github.com/hail-is/hail/issues/1728:9,Modifiability,plugin,plugin,9,Just use plugin.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1728
https://github.com/hail-is/hail/issues/1743:18,Performance,load,load,18,"Should be able to load file from export_blah types='/path/to/types' directly into TextTableConfig (or new hc.import_table) without having to explicitly load as string, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1743
https://github.com/hail-is/hail/issues/1743:152,Performance,load,load,152,"Should be able to load file from export_blah types='/path/to/types' directly into TextTableConfig (or new hc.import_table) without having to explicitly load as string, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1743
https://github.com/hail-is/hail/issues/1744:77,Testability,assert,assert,77,"here:; ```; def check(nAlleles: Int) {; val nGenotypes = triangle(nAlleles); assert(gt.forall(i => i >= 0 && i < nGenotypes)); assert(ad.forall(a => a.length == nAlleles)); assert(px.forall(a => a.length == nGenotypes)); }; ```. It's possible to just get ""AssertionFailed"" if you construct an invalid genotype with expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1744
https://github.com/hail-is/hail/issues/1744:127,Testability,assert,assert,127,"here:; ```; def check(nAlleles: Int) {; val nGenotypes = triangle(nAlleles); assert(gt.forall(i => i >= 0 && i < nGenotypes)); assert(ad.forall(a => a.length == nAlleles)); assert(px.forall(a => a.length == nGenotypes)); }; ```. It's possible to just get ""AssertionFailed"" if you construct an invalid genotype with expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1744
https://github.com/hail-is/hail/issues/1744:173,Testability,assert,assert,173,"here:; ```; def check(nAlleles: Int) {; val nGenotypes = triangle(nAlleles); assert(gt.forall(i => i >= 0 && i < nGenotypes)); assert(ad.forall(a => a.length == nAlleles)); assert(px.forall(a => a.length == nGenotypes)); }; ```. It's possible to just get ""AssertionFailed"" if you construct an invalid genotype with expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1744
https://github.com/hail-is/hail/issues/1744:256,Testability,Assert,AssertionFailed,256,"here:; ```; def check(nAlleles: Int) {; val nGenotypes = triangle(nAlleles); assert(gt.forall(i => i >= 0 && i < nGenotypes)); assert(ad.forall(a => a.length == nAlleles)); assert(px.forall(a => a.length == nGenotypes)); }; ```. It's possible to just get ""AssertionFailed"" if you construct an invalid genotype with expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1744
https://github.com/hail-is/hail/pull/1745:67,Performance,load,loaded,67,Data formats that use Characters (including old VDSes) will now be loaded as `TString`s. Resolves #1710,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1745
https://github.com/hail-is/hail/pull/1750:0,Integrability,Depend,Depends,0,Depends on #1748,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1750
https://github.com/hail-is/hail/pull/1757:0,Integrability,Depend,Depends,0,Depends on #1698,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1757
https://github.com/hail-is/hail/pull/1761:990,Availability,robust,robust,990,"- In the `combOp`, the `TakeByAggregator` was calling the `seqOp` which recomputes the sort-key. Since the evaluation context was not set correctly for the keys, the expression that evaluates the sort-key obviously produced the wrong value for the given key. - `TakeByAggregator.result` used `PriorityQueue.toArray`, which is not guaranteed to produce the elements in sorted order, according to [the PriorityQueue docs](http://www.scala-lang.org/api/current/scala/collection/mutable/PriorityQueue.html). Instead, we must `clone` the `PriorityQueue` and then `dequeueAll` the elements. I'm not certain the `clone` is necessary. @cseed, does the Aggregator interface permit multiple calls to `result`?. > Only the dequeue and dequeueAll methods will return elements in priority order (while removing elements from the heap). Standard collection methods including drop, iterator, and toString will remove or traverse the heap in whichever order seems most convenient. I also added some fairly robust tests that compare `takeBy(f, 10)` to `collect().sortBy(f)`. In particular: ; - the `takeBy` should be a prefix of `sortBy` when lensing by `f`,; - for every sort-key except last one, the elements should be the same as in `sortBy`, and; - for the last sort-key, the elements should be a subset of those in `sortBy`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1761
https://github.com/hail-is/hail/pull/1761:655,Integrability,interface,interface,655,"- In the `combOp`, the `TakeByAggregator` was calling the `seqOp` which recomputes the sort-key. Since the evaluation context was not set correctly for the keys, the expression that evaluates the sort-key obviously produced the wrong value for the given key. - `TakeByAggregator.result` used `PriorityQueue.toArray`, which is not guaranteed to produce the elements in sorted order, according to [the PriorityQueue docs](http://www.scala-lang.org/api/current/scala/collection/mutable/PriorityQueue.html). Instead, we must `clone` the `PriorityQueue` and then `dequeueAll` the elements. I'm not certain the `clone` is necessary. @cseed, does the Aggregator interface permit multiple calls to `result`?. > Only the dequeue and dequeueAll methods will return elements in priority order (while removing elements from the heap). Standard collection methods including drop, iterator, and toString will remove or traverse the heap in whichever order seems most convenient. I also added some fairly robust tests that compare `takeBy(f, 10)` to `collect().sortBy(f)`. In particular: ; - the `takeBy` should be a prefix of `sortBy` when lensing by `f`,; - for every sort-key except last one, the elements should be the same as in `sortBy`, and; - for the last sort-key, the elements should be a subset of those in `sortBy`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1761
https://github.com/hail-is/hail/pull/1761:997,Testability,test,tests,997,"- In the `combOp`, the `TakeByAggregator` was calling the `seqOp` which recomputes the sort-key. Since the evaluation context was not set correctly for the keys, the expression that evaluates the sort-key obviously produced the wrong value for the given key. - `TakeByAggregator.result` used `PriorityQueue.toArray`, which is not guaranteed to produce the elements in sorted order, according to [the PriorityQueue docs](http://www.scala-lang.org/api/current/scala/collection/mutable/PriorityQueue.html). Instead, we must `clone` the `PriorityQueue` and then `dequeueAll` the elements. I'm not certain the `clone` is necessary. @cseed, does the Aggregator interface permit multiple calls to `result`?. > Only the dequeue and dequeueAll methods will return elements in priority order (while removing elements from the heap). Standard collection methods including drop, iterator, and toString will remove or traverse the heap in whichever order seems most convenient. I also added some fairly robust tests that compare `takeBy(f, 10)` to `collect().sortBy(f)`. In particular: ; - the `takeBy` should be a prefix of `sortBy` when lensing by `f`,; - for every sort-key except last one, the elements should be the same as in `sortBy`, and; - for the last sort-key, the elements should be a subset of those in `sortBy`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1761
https://github.com/hail-is/hail/pull/1764:21,Testability,test,tests,21,I also added several tests. I'm interested in your comments on how best to write tests of expressions that manipulate aggregators. Resolves #1758. **WARNING:** This is a breaking change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1764
https://github.com/hail-is/hail/pull/1764:81,Testability,test,tests,81,I also added several tests. I'm interested in your comments on how best to write tests of expressions that manipulate aggregators. Resolves #1758. **WARNING:** This is a breaking change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1764
https://github.com/hail-is/hail/pull/1765:186,Modifiability,flexible,flexible,186,"This is the common use case for collapsed burden testing, and will eventually be the input to linreg_burden and logreg_burden. The resulting table can by pushed to Python or PySpark for flexible analysis. Unlike in the burden regression methods, this table need not have numeric values, and it doesn't filter out samples with missing phenotype or covariates (since there are none to speak of here).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1765
https://github.com/hail-is/hail/pull/1765:49,Testability,test,testing,49,"This is the common use case for collapsed burden testing, and will eventually be the input to linreg_burden and logreg_burden. The resulting table can by pushed to Python or PySpark for flexible analysis. Unlike in the burden regression methods, this table need not have numeric values, and it doesn't filter out samples with missing phenotype or covariates (since there are none to speak of here).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1765
https://github.com/hail-is/hail/issues/1766:232,Availability,error,errors,232,"The image below describes what I mean. The brown files are not tracked by git, because these are generated by the sphinx autosummary directives. However, they're still used as a base when we build the website. I've run into several errors today with broken references (from other git branches!) which were resolved by deleting all these untracked rsts and rebuilding. Ideally, gradle clean would delete all these, or we would copy this directory to a temporary staging area before starting the build. ![image](https://cloud.githubusercontent.com/assets/10562794/25728126/21712204-30fb-11e7-8938-964b68c940e6.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1766
https://github.com/hail-is/hail/pull/1769:0,Testability,test,testAll,0,testAll triggers testHail and test. test no longer triggers testHail. **WARNING:** the CI server will not run `testHail` on this branch nor will it run `testHail` after this branch is merged; we must manually edit the,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1769
https://github.com/hail-is/hail/pull/1769:17,Testability,test,testHail,17,testAll triggers testHail and test. test no longer triggers testHail. **WARNING:** the CI server will not run `testHail` on this branch nor will it run `testHail` after this branch is merged; we must manually edit the,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1769
https://github.com/hail-is/hail/pull/1769:30,Testability,test,test,30,testAll triggers testHail and test. test no longer triggers testHail. **WARNING:** the CI server will not run `testHail` on this branch nor will it run `testHail` after this branch is merged; we must manually edit the,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1769
https://github.com/hail-is/hail/pull/1769:36,Testability,test,test,36,testAll triggers testHail and test. test no longer triggers testHail. **WARNING:** the CI server will not run `testHail` on this branch nor will it run `testHail` after this branch is merged; we must manually edit the,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1769
https://github.com/hail-is/hail/pull/1769:60,Testability,test,testHail,60,testAll triggers testHail and test. test no longer triggers testHail. **WARNING:** the CI server will not run `testHail` on this branch nor will it run `testHail` after this branch is merged; we must manually edit the,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1769
https://github.com/hail-is/hail/pull/1769:111,Testability,test,testHail,111,testAll triggers testHail and test. test no longer triggers testHail. **WARNING:** the CI server will not run `testHail` on this branch nor will it run `testHail` after this branch is merged; we must manually edit the,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1769
https://github.com/hail-is/hail/pull/1769:153,Testability,test,testHail,153,testAll triggers testHail and test. test no longer triggers testHail. **WARNING:** the CI server will not run `testHail` on this branch nor will it run `testHail` after this branch is merged; we must manually edit the,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1769
https://github.com/hail-is/hail/issues/1774:361,Deployability,update,updated,361,"So many methods or parts of methods (like most variant and sample qc stats, and mean or standard deviation in genotype stream imputation or normalization) rely solely on the histogram of gt counts (like nHomRef, nMissing). We can write less code and gain efficiency by including these as a variant fields (or reserved variant annotation?) that is automatically updated whenever datasets are joined or filtered (independent of things like the va.qc.nHet) and written and read with VDS. I also think this will help users not run full qc over and over when all they want is updated missingness of want AF for a method like IBD. I'm curious what others think.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1774
https://github.com/hail-is/hail/issues/1774:571,Deployability,update,updated,571,"So many methods or parts of methods (like most variant and sample qc stats, and mean or standard deviation in genotype stream imputation or normalization) rely solely on the histogram of gt counts (like nHomRef, nMissing). We can write less code and gain efficiency by including these as a variant fields (or reserved variant annotation?) that is automatically updated whenever datasets are joined or filtered (independent of things like the va.qc.nHet) and written and read with VDS. I also think this will help users not run full qc over and over when all they want is updated missingness of want AF for a method like IBD. I'm curious what others think.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1774
https://github.com/hail-is/hail/pull/1775:45,Security,expose,expose,45,Added LDMatrix computation on VDS's. Did not expose any python api yet. Will do that in subsequent PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1775
https://github.com/hail-is/hail/pull/1778:185,Modifiability,rewrite,rewrite,185,"Mostly infrastructure. Added NewAST base class for Matrix and KeyTable ASTs with a primitive term rewriting engine. This should eventually be a base for AST, too (because we'll want to rewrite value expressions, too). I broke VariantMetadata into two parts: VSMMetadata (static types/metdata for VSM) and VSMLocalValue (part of MatrixValue that is computed/stored on master and broadcast). Added MatrixRead, FilterSamples and FilterVariants matrix AST nodes. Simple optimizer that pushes filters into read and some minor optimizations of filters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778
https://github.com/hail-is/hail/pull/1778:466,Performance,optimiz,optimizer,466,"Mostly infrastructure. Added NewAST base class for Matrix and KeyTable ASTs with a primitive term rewriting engine. This should eventually be a base for AST, too (because we'll want to rewrite value expressions, too). I broke VariantMetadata into two parts: VSMMetadata (static types/metdata for VSM) and VSMLocalValue (part of MatrixValue that is computed/stored on master and broadcast). Added MatrixRead, FilterSamples and FilterVariants matrix AST nodes. Simple optimizer that pushes filters into read and some minor optimizations of filters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778
https://github.com/hail-is/hail/pull/1778:521,Performance,optimiz,optimizations,521,"Mostly infrastructure. Added NewAST base class for Matrix and KeyTable ASTs with a primitive term rewriting engine. This should eventually be a base for AST, too (because we'll want to rewrite value expressions, too). I broke VariantMetadata into two parts: VSMMetadata (static types/metdata for VSM) and VSMLocalValue (part of MatrixValue that is computed/stored on master and broadcast). Added MatrixRead, FilterSamples and FilterVariants matrix AST nodes. Simple optimizer that pushes filters into read and some minor optimizations of filters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778
https://github.com/hail-is/hail/pull/1778:459,Usability,Simpl,Simple,459,"Mostly infrastructure. Added NewAST base class for Matrix and KeyTable ASTs with a primitive term rewriting engine. This should eventually be a base for AST, too (because we'll want to rewrite value expressions, too). I broke VariantMetadata into two parts: VSMMetadata (static types/metdata for VSM) and VSMLocalValue (part of MatrixValue that is computed/stored on master and broadcast). Added MatrixRead, FilterSamples and FilterVariants matrix AST nodes. Simple optimizer that pushes filters into read and some minor optimizations of filters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1778
https://github.com/hail-is/hail/pull/1780:88,Modifiability,variab,variable,88,"This is a prototype for just build 37. My plan is to add other genome builds, expose as variable in HailContext, and add support for reading JSON from a file not in the Java resources in subsequent PRs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1780
https://github.com/hail-is/hail/pull/1780:78,Security,expose,expose,78,"This is a prototype for just build 37. My plan is to add other genome builds, expose as variable in HailContext, and add support for reading JSON from a file not in the Java resources in subsequent PRs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1780
https://github.com/hail-is/hail/pull/1782:62,Energy Efficiency,allocate,allocated,62,These values are computed whenever an object of this class is allocated. This happens even when `gradle test --tests 'FOO'` filters out the class. I would prefer filtered tests to not run anything so that the desired tests run as quickly as possible.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1782
https://github.com/hail-is/hail/pull/1782:104,Testability,test,test,104,These values are computed whenever an object of this class is allocated. This happens even when `gradle test --tests 'FOO'` filters out the class. I would prefer filtered tests to not run anything so that the desired tests run as quickly as possible.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1782
https://github.com/hail-is/hail/pull/1782:111,Testability,test,tests,111,These values are computed whenever an object of this class is allocated. This happens even when `gradle test --tests 'FOO'` filters out the class. I would prefer filtered tests to not run anything so that the desired tests run as quickly as possible.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1782
https://github.com/hail-is/hail/pull/1782:171,Testability,test,tests,171,These values are computed whenever an object of this class is allocated. This happens even when `gradle test --tests 'FOO'` filters out the class. I would prefer filtered tests to not run anything so that the desired tests run as quickly as possible.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1782
https://github.com/hail-is/hail/pull/1782:217,Testability,test,tests,217,These values are computed whenever an object of this class is allocated. This happens even when `gradle test --tests 'FOO'` filters out the class. I would prefer filtered tests to not run anything so that the desired tests run as quickly as possible.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1782
https://github.com/hail-is/hail/pull/1789:2,Integrability,Depend,Depends,2,- Depends on #1780,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789
https://github.com/hail-is/hail/issues/1791:88,Testability,test,tests,88,"Talked to @tpoterba about this. We should list things here like requirements to run the tests, build the docs, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1791
https://github.com/hail-is/hail/pull/1796:290,Integrability,interface,interface,290,- aggregate intervals (easy to implement with key tables); - annotate samples list (easy to implement with annotate_samples_table); - annotate global list and annotate global table (easy to implement with key table and annotate_global (renamed from annotate_global_py). Also cleaned up VSM interface (removed annotateSamplesList),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1796
https://github.com/hail-is/hail/issues/1797:184,Deployability,integrat,integrated-call-set,184,"g.dosage() should be a single floating point value, not an array. the array is the genotype probabilities. http://www.internationalgenome.org/faq/what-does-genotype-dosage-mean-phase1-integrated-call-set/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1797
https://github.com/hail-is/hail/issues/1797:184,Integrability,integrat,integrated-call-set,184,"g.dosage() should be a single floating point value, not an array. the array is the genotype probabilities. http://www.internationalgenome.org/faq/what-does-genotype-dosage-mean-phase1-integrated-call-set/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1797
https://github.com/hail-is/hail/issues/1806:720,Availability,Error,Error,720,"```; from hail import *; hc = HailContext(); input_vcf = ""gs://seqr-hail/reference_data/GRCh38/1kg/ALL.GRCh38_sites.20170504.vcf.gz""; vds = hc.import_vcf(input_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:860,Availability,error,error,860,"```; from hail import *; hc = HailContext(); input_vcf = ""gs://seqr-hail/reference_data/GRCh38/1kg/ALL.GRCh38_sites.20170504.vcf.gz""; vds = hc.import_vcf(input_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:1040,Availability,failure,failure,1040,"lContext(); input_vcf = ""gs://seqr-hail/reference_data/GRCh38/1kg/ALL.GRCh38_sites.20170504.vcf.gz""; vds = hc.import_vcf(input_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:1098,Availability,failure,failure,1098,"8/1kg/ALL.GRCh38_sites.20170504.vcf.gz""; vds = hc.import_vcf(input_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:6866,Availability,Error,Error,6866,pl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-ebabd77; Error summary: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. ​; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:1149,Deployability,pipeline,pipeline-cluster-,1149,"nput_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2217,Energy Efficiency,schedul,scheduler,2217,gumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2289,Energy Efficiency,schedul,scheduler,2289,nelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSchedu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2653,Energy Efficiency,schedul,scheduler,2653,che.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2693,Energy Efficiency,schedul,scheduler,2693,scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2792,Energy Efficiency,schedul,scheduler,2792,); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2890,Energy Efficiency,schedul,scheduler,2890,he.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:3144,Energy Efficiency,schedul,scheduler,3144,.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:3225,Energy Efficiency,schedul,scheduler,3225,ultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:3331,Energy Efficiency,schedul,scheduler,3331,he.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:3481,Energy Efficiency,schedul,scheduler,3481,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtil,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:3570,Energy Efficiency,schedul,scheduler,3570,va.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.foral,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:3668,Energy Efficiency,schedul,scheduler,3668,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.H,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:3764,Energy Efficiency,schedul,scheduler,3764,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:498); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
