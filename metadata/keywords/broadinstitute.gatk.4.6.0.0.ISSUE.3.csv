id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/issues/2797:2342,Testability,test,testng,2342,r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2349,Testability,Test,TestNG,2349,eMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$Dis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2360,Testability,Test,TestNG,2360,nvoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2413,Testability,test,testing,2413,TestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2421,Testability,test,testng,2421,hod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2428,Testability,Test,TestNGTestClassProcessor,2428,:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2462,Testability,Test,TestNGTestClassProcessor,2462,.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2532,Testability,test,testing,2532,rnal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2540,Testability,test,testng,2540,stMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2547,Testability,Test,TestNGTestClassProcessor,2547,nvokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2577,Testability,Test,TestNGTestClassProcessor,2577,orker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccesso,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:2646,Testability,test,testing,2646,estMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAcces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:3499,Testability,test,testing,3499,rg.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Th,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:3514,Testability,Test,TestWorker,3514,ternal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2797:3530,Testability,Test,TestWorker,3530,esting.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2797
https://github.com/broadinstitute/gatk/issues/2798:59,Safety,detect,detect,59,"It has asserts for a number of annotations, but it doesn't detect when new un-asserted annotations are added. ; We should consider:; A: adding asserts for new annotations since the test was written; B: adding a clause to fail when new annotations are added without an assert; C: renaming the method",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2798
https://github.com/broadinstitute/gatk/issues/2798:7,Testability,assert,asserts,7,"It has asserts for a number of annotations, but it doesn't detect when new un-asserted annotations are added. ; We should consider:; A: adding asserts for new annotations since the test was written; B: adding a clause to fail when new annotations are added without an assert; C: renaming the method",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2798
https://github.com/broadinstitute/gatk/issues/2798:78,Testability,assert,asserted,78,"It has asserts for a number of annotations, but it doesn't detect when new un-asserted annotations are added. ; We should consider:; A: adding asserts for new annotations since the test was written; B: adding a clause to fail when new annotations are added without an assert; C: renaming the method",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2798
https://github.com/broadinstitute/gatk/issues/2798:143,Testability,assert,asserts,143,"It has asserts for a number of annotations, but it doesn't detect when new un-asserted annotations are added. ; We should consider:; A: adding asserts for new annotations since the test was written; B: adding a clause to fail when new annotations are added without an assert; C: renaming the method",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2798
https://github.com/broadinstitute/gatk/issues/2798:181,Testability,test,test,181,"It has asserts for a number of annotations, but it doesn't detect when new un-asserted annotations are added. ; We should consider:; A: adding asserts for new annotations since the test was written; B: adding a clause to fail when new annotations are added without an assert; C: renaming the method",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2798
https://github.com/broadinstitute/gatk/issues/2798:268,Testability,assert,assert,268,"It has asserts for a number of annotations, but it doesn't detect when new un-asserted annotations are added. ; We should consider:; A: adding asserts for new annotations since the test was written; B: adding a clause to fail when new annotations are added without an assert; C: renaming the method",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2798
https://github.com/broadinstitute/gatk/issues/2799:75,Availability,avail,available,75,The `ReferenceBases` annotation fails with an NPE if there is no reference available. It should fail with a helpful UserException instead. ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:503,Testability,test,testAllAnnotations,503,The `ReferenceBases` annotation fails with an NPE if there is no reference available. It should fail with a helpful UserException instead. ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:865,Testability,test,testng,865,The `ReferenceBases` annotation fails with an NPE if there is no reference available. It should fail with a helpful UserException instead. ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:959,Testability,test,testng,959,The `ReferenceBases` annotation fails with an NPE if there is no reference available. It should fail with a helpful UserException instead. ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1023,Testability,test,testng,1023,ation fails with an NPE if there is no reference available. It should fail with a helpful UserException instead. ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1091,Testability,test,testng,1091,d fail with a helpful UserException instead. ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.Re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1161,Testability,test,testng,1161,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1177,Testability,Test,TestMethodWorker,1177,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1212,Testability,Test,TestMethodWorker,1212,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1248,Testability,test,testng,1248,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1264,Testability,Test,TestMethodWorker,1264,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1285,Testability,Test,TestMethodWorker,1285,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1321,Testability,test,testng,1321,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1328,Testability,Test,TestRunner,1328,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1350,Testability,Test,TestRunner,1350,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1380,Testability,test,testng,1380,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1387,Testability,Test,TestRunner,1387,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1402,Testability,Test,TestRunner,1402,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1432,Testability,test,testng,1432,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1490,Testability,test,testng,1490,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1556,Testability,test,testng,1556,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1617,Testability,test,testng,1617,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1671,Testability,test,testng,1671,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1741,Testability,test,testng,1741,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1806,Testability,test,testng,1806,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1813,Testability,Test,TestNG,1813,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1842,Testability,Test,TestNG,1842,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1869,Testability,test,testng,1869,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1876,Testability,Test,TestNG,1876,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1900,Testability,Test,TestNG,1900,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1927,Testability,test,testng,1927,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1934,Testability,Test,TestNG,1934,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1951,Testability,Test,TestNG,1951,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1978,Testability,test,testng,1978,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1985,Testability,Test,TestNG,1985,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:1996,Testability,Test,TestNG,1996,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:2023,Testability,test,testng,2023,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2799:2086,Testability,test,testng,2086,g.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799
https://github.com/broadinstitute/gatk/issues/2802:73,Availability,error,error,73,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:144,Availability,Error,Error,144,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:187,Availability,ERROR,ERROR,187,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:297,Availability,ERROR,ERROR,297,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:456,Availability,ERROR,ERROR,456,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:657,Availability,ERROR,ERROR,657,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:851,Availability,ERROR,ERROR,851,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:1041,Availability,ERROR,ERROR,1041,"t` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:1227,Availability,ERROR,ERROR,1227, inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:1433,Availability,ERROR,ERROR,1433,st'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:1647,Availability,ERROR,ERROR,1647,0.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:1847,Availability,ERROR,ERROR,1847,[ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:2033,Availability,ERROR,ERROR,2033,433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildEx,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:2233,Availability,ERROR,ERROR,2233,rg.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:2427,Availability,ERROR,ERROR,2427,4 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:2633,Availability,ERROR,ERROR,2633,54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:2839,Availability,ERROR,ERROR,2839,434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:2990,Availability,ERROR,ERROR,2990,yIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:3176,Availability,ERROR,ERROR,3176,ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:3361,Availability,ERROR,ERROR,3361,ter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:3565,Availability,ERROR,ERROR,3565,rker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:3769,Availability,ERROR,ERROR,3769,Worker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:3973,Availability,ERROR,ERROR,3973,11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:4169,Availability,ERROR,ERROR,4169,34 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:4348,Availability,ERROR,ERROR,4348,4:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:4530,Availability,ERROR,ERROR,4530,ecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:4707,Availability,ERROR,ERROR,4707,.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:4870,Availability,ERROR,ERROR,4870,actTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:5036,Availability,ERROR,ERROR,5036,t org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:5201,Availability,ERROR,ERROR,5201,ents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:5376,Availability,ERROR,ERROR,5376,ldevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:5539,Availability,ERROR,ERROR,5539,gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:5702,Availability,ERROR,ERROR,5702, [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLau,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:5888,Availability,ERROR,ERROR,5888,ternal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doB,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:6074,Availability,ERROR,ERROR,6074,uildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:6225,Availability,ERROR,ERROR,6225,.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:6411,Availability,ERROR,ERROR,6411,s.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(Execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:6596,Availability,ERROR,ERROR,6596,er] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildAction,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:6773,Availability,ERROR,ERROR,6773,adle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildAc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:6944,Availability,ERROR,ERROR,6944,ter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:7110,Availability,ERROR,ERROR,7110,.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:7275,Availability,ERROR,ERROR,7275,nReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.toolin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:7458,Availability,ERROR,ERROR,7458,tionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:7631,Availability,ERROR,ERROR,7631,s.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.B,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:7814,Availability,ERROR,ERROR,7814,dExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:7997,Availability,ERROR,ERROR,7997,porter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8194,Availability,ERROR,ERROR,8194,.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8391,Availability,ERROR,ERROR,8391,tionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8594,Availability,ERROR,ERROR,8594,ionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8797,Availability,ERROR,ERROR,8797,11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8962,Availability,ERROR,ERROR,8962,xecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:9135,Availability,ERROR,ERROR,9135,e(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.la,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:9320,Availability,ERROR,ERROR,9320,cuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:9503,Availability,ERROR,ERROR,9503,tupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:9688,Availability,ERROR,ERROR,9688,rovider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:9873,Availability,ERROR,ERROR,9873,launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:10058,Availability,ERROR,ERROR,10058,er.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(Forwa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:10257,Availability,ERROR,ERROR,10257,ion.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:10442,Availability,ERROR,ERROR,10442,ion.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:10618,Availability,ERROR,ERROR,10618,dExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:10794,Availability,ERROR,ERROR,10794,setDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:10923,Availability,ERROR,ERROR,10923,nReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11100,Availability,ERROR,ERROR,11100,ExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11285,Availability,ERROR,ERROR,11285,ldevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11460,Availability,ERROR,ERROR,11460,ternal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11645,Availability,ERROR,ERROR,11645,ildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11808,Availability,ERROR,ERROR,11808,e.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11981,Availability,ERROR,ERROR,11981,r] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:12166,Availability,ERROR,ERROR,12166,rg.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.inte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:12357,Availability,ERROR,ERROR,12357,dle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:12530,Availability,ERROR,ERROR,12530,radle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.pr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:12715,Availability,ERROR,ERROR,12715,radle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:12908,Availability,ERROR,ERROR,12908,xec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13087,Availability,ERROR,ERROR,13087,monCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13273,Availability,ERROR,ERROR,13273,ablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.intern,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13446,Availability,ERROR,ERROR,13446,on.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13650,Availability,ERROR,ERROR,13650,ution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13844,Availability,ERROR,ERROR,13844,sy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:14026,Availability,ERROR,ERROR,14026,tor$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:14252,Availability,ERROR,ERROR,14252,3); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:14447,Availability,ERROR,ERROR,14447,R] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:14661,Availability,ERROR,ERROR,14661,adle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.ac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:14871,Availability,ERROR,ERROR,14871,ildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15039,Availability,ERROR,ERROR,15039,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15207,Availability,ERROR,ERROR,15207,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15304,Availability,Failure,FailureHandlingDispatch,15304,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15337,Availability,Failure,FailureHandlingDispatch,15337,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15385,Availability,ERROR,ERROR,15385,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15552,Availability,ERROR,ERROR,15552,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15712,Availability,ERROR,ERROR,15712,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15867,Availability,ERROR,ERROR,15867,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8102,Deployability,Continuous,ContinuousBuildActionExecuter,8102,[org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8140,Deployability,Continuous,ContinuousBuildActionExecuter,8140,uildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8299,Deployability,Continuous,ContinuousBuildActionExecuter,8299,.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:8337,Deployability,Continuous,ContinuousBuildActionExecuter,8337,radle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13175,Performance,concurren,concurrent,13175,adle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13361,Performance,concurren,concurrent,13361,rg.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.Fo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:271,Safety,Abort,Abort,271,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:1149,Security,Validat,ValidatingTaskExecuter,1149,Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:1180,Security,Validat,ValidatingTaskExecuter,1180,ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:4980,Security,access,access,4980,rnal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:15663,Security,access,access,15663,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13,Testability,test,tests,13,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:41,Testability,test,test,41,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:122,Testability,test,test,122,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:434,Testability,test,test,434,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11392,Testability,Log,LogAndCheckHealth,11392,ommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.Buil,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11418,Testability,Log,LogAndCheckHealth,11418,20); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11752,Testability,Log,LogToClient,11752,ClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.Bui,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:11772,Testability,Log,LogToClient,11772,:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionRepor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13580,Testability,Test,Test,13580, org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.e,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:13779,Testability,assert,assertNormalExitValue,13779,her.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:14350,Testability,test,testing,14350,ternal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:14545,Testability,test,testing,14545,ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.43,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/issues/2802:14759,Testability,test,testing,14759,$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExcept,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802
https://github.com/broadinstitute/gatk/pull/2803:91,Testability,test,testLoadFastaDictionaryWithFastaFile,91,Now throws a UserException.MalformedFile if the resulting header has no; dictionary. Added testLoadFastaDictionaryWithFastaFile to test this case. resolves #2609,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2803
https://github.com/broadinstitute/gatk/pull/2803:131,Testability,test,test,131,Now throws a UserException.MalformedFile if the resulting header has no; dictionary. Added testLoadFastaDictionaryWithFastaFile to test this case. resolves #2609,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2803
https://github.com/broadinstitute/gatk/pull/2804:84,Availability,redundant,redundant,84,removing the non-docker unit and integration test matrix entries because; they were redundant with the docker ones,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804
https://github.com/broadinstitute/gatk/pull/2804:33,Deployability,integrat,integration,33,removing the non-docker unit and integration test matrix entries because; they were redundant with the docker ones,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804
https://github.com/broadinstitute/gatk/pull/2804:33,Integrability,integrat,integration,33,removing the non-docker unit and integration test matrix entries because; they were redundant with the docker ones,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804
https://github.com/broadinstitute/gatk/pull/2804:84,Safety,redund,redundant,84,removing the non-docker unit and integration test matrix entries because; they were redundant with the docker ones,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804
https://github.com/broadinstitute/gatk/pull/2804:45,Testability,test,test,45,removing the non-docker unit and integration test matrix entries because; they were redundant with the docker ones,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804
https://github.com/broadinstitute/gatk/pull/2807:155,Testability,test,tests,155,"I've extracted this change to ReferenceConfidenceModel from the now-closed PR; https://github.com/broadinstitute/gatk-protected/pull/1022, since it causes tests; to fail spectacularly (including concordance tests against GATK3), so that we can; unblock the badly-needed merge of https://github.com/broadinstitute/gatk-protected/pull/1027. Let's review and test this change in isolation to be sure we understand it fully; before accepting it into master.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2807
https://github.com/broadinstitute/gatk/pull/2807:207,Testability,test,tests,207,"I've extracted this change to ReferenceConfidenceModel from the now-closed PR; https://github.com/broadinstitute/gatk-protected/pull/1022, since it causes tests; to fail spectacularly (including concordance tests against GATK3), so that we can; unblock the badly-needed merge of https://github.com/broadinstitute/gatk-protected/pull/1027. Let's review and test this change in isolation to be sure we understand it fully; before accepting it into master.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2807
https://github.com/broadinstitute/gatk/pull/2807:356,Testability,test,test,356,"I've extracted this change to ReferenceConfidenceModel from the now-closed PR; https://github.com/broadinstitute/gatk-protected/pull/1022, since it causes tests; to fail spectacularly (including concordance tests against GATK3), so that we can; unblock the badly-needed merge of https://github.com/broadinstitute/gatk-protected/pull/1027. Let's review and test this change in isolation to be sure we understand it fully; before accepting it into master.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2807
https://github.com/broadinstitute/gatk/issues/2808:49,Deployability,integrat,integration,49,"After the merger with gatk-protected, the docker integration tests are intermittently (but frequently) hitting the hard travis time limit of 50 minutes. We need to fix this ASAP!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2808
https://github.com/broadinstitute/gatk/issues/2808:49,Integrability,integrat,integration,49,"After the merger with gatk-protected, the docker integration tests are intermittently (but frequently) hitting the hard travis time limit of 50 minutes. We need to fix this ASAP!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2808
https://github.com/broadinstitute/gatk/issues/2808:61,Testability,test,tests,61,"After the merger with gatk-protected, the docker integration tests are intermittently (but frequently) hitting the hard travis time limit of 50 minutes. We need to fix this ASAP!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2808
https://github.com/broadinstitute/gatk/pull/2809:534,Availability,error,error,534,"Ongoing conversation from <https://github.com/broadinstitute/gatk-protected/pull/1130> can now continue in this merged repo. So far, @vdauwera @droazen and @samuelklee have agreed to delete the code that was requested to be archived in favor of using git versioning as the archive method with these stipulations from Geraldine:. - the PR and commit message specify whether there is a replacement for each of the tools. ; - should be a deprecation message so that if I try to run one of these tools in a newer version, I get a helpful error message that tells me the tool was removed and by what it was replaced if applicable. See GATK3 for how we implemented this previously. This should be done for all tools that we remove, regardless of whether they were purely internal or experimental. It's only a one line addition per tool and it can potentially save us a lot of headaches later down the road (even just internally). Currently, this PR is the original PR where I placed the to-be-archived code in an archive folder. . ### I have yet to make additional changes so as to follow the above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809
https://github.com/broadinstitute/gatk/pull/2809:886,Availability,down,down,886,"Ongoing conversation from <https://github.com/broadinstitute/gatk-protected/pull/1130> can now continue in this merged repo. So far, @vdauwera @droazen and @samuelklee have agreed to delete the code that was requested to be archived in favor of using git versioning as the archive method with these stipulations from Geraldine:. - the PR and commit message specify whether there is a replacement for each of the tools. ; - should be a deprecation message so that if I try to run one of these tools in a newer version, I get a helpful error message that tells me the tool was removed and by what it was replaced if applicable. See GATK3 for how we implemented this previously. This should be done for all tools that we remove, regardless of whether they were purely internal or experimental. It's only a one line addition per tool and it can potentially save us a lot of headaches later down the road (even just internally). Currently, this PR is the original PR where I placed the to-be-archived code in an archive folder. . ### I have yet to make additional changes so as to follow the above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809
https://github.com/broadinstitute/gatk/pull/2809:349,Integrability,message,message,349,"Ongoing conversation from <https://github.com/broadinstitute/gatk-protected/pull/1130> can now continue in this merged repo. So far, @vdauwera @droazen and @samuelklee have agreed to delete the code that was requested to be archived in favor of using git versioning as the archive method with these stipulations from Geraldine:. - the PR and commit message specify whether there is a replacement for each of the tools. ; - should be a deprecation message so that if I try to run one of these tools in a newer version, I get a helpful error message that tells me the tool was removed and by what it was replaced if applicable. See GATK3 for how we implemented this previously. This should be done for all tools that we remove, regardless of whether they were purely internal or experimental. It's only a one line addition per tool and it can potentially save us a lot of headaches later down the road (even just internally). Currently, this PR is the original PR where I placed the to-be-archived code in an archive folder. . ### I have yet to make additional changes so as to follow the above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809
https://github.com/broadinstitute/gatk/pull/2809:447,Integrability,message,message,447,"Ongoing conversation from <https://github.com/broadinstitute/gatk-protected/pull/1130> can now continue in this merged repo. So far, @vdauwera @droazen and @samuelklee have agreed to delete the code that was requested to be archived in favor of using git versioning as the archive method with these stipulations from Geraldine:. - the PR and commit message specify whether there is a replacement for each of the tools. ; - should be a deprecation message so that if I try to run one of these tools in a newer version, I get a helpful error message that tells me the tool was removed and by what it was replaced if applicable. See GATK3 for how we implemented this previously. This should be done for all tools that we remove, regardless of whether they were purely internal or experimental. It's only a one line addition per tool and it can potentially save us a lot of headaches later down the road (even just internally). Currently, this PR is the original PR where I placed the to-be-archived code in an archive folder. . ### I have yet to make additional changes so as to follow the above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809
https://github.com/broadinstitute/gatk/pull/2809:540,Integrability,message,message,540,"Ongoing conversation from <https://github.com/broadinstitute/gatk-protected/pull/1130> can now continue in this merged repo. So far, @vdauwera @droazen and @samuelklee have agreed to delete the code that was requested to be archived in favor of using git versioning as the archive method with these stipulations from Geraldine:. - the PR and commit message specify whether there is a replacement for each of the tools. ; - should be a deprecation message so that if I try to run one of these tools in a newer version, I get a helpful error message that tells me the tool was removed and by what it was replaced if applicable. See GATK3 for how we implemented this previously. This should be done for all tools that we remove, regardless of whether they were purely internal or experimental. It's only a one line addition per tool and it can potentially save us a lot of headaches later down the road (even just internally). Currently, this PR is the original PR where I placed the to-be-archived code in an archive folder. . ### I have yet to make additional changes so as to follow the above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809
https://github.com/broadinstitute/gatk/pull/2813:478,Availability,mask,masks,478,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2813:507,Deployability,integrat,integration,507,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2813:559,Deployability,integrat,integration,559,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2813:287,Integrability,message,messages,287,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2813:507,Integrability,integrat,integration,507,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2813:559,Integrability,integrat,integration,559,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2813:283,Testability,log,log,283,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2813:519,Testability,test,tests,519,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2813:571,Testability,test,test,571,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813
https://github.com/broadinstitute/gatk/pull/2816:143,Deployability,update,updated,143,"I've completely rewritten the documentation portion to be more helpful to users (and to reflect the new M2 rather than gatk3's M2), and I have updated the example commands. ---; ### Questions for @davidbenjamin ; - `--af_of_alleles_not_in_resource`: is this allele frequency used only in certain contexts, e.g. with matched normal analyses, or towards tumor sample variant alleles, etc.? I need to add to the doc details how this argument factors into calculations.; - I need a sentence or two describing the new algorithmic improvement on the new Mutect2 integration over uncertainty. ; - The WDLs do not include use of a contamination.table and so I did not include it in the commands. Is this something we want to nudge users to use, i.e. should I put in a sentence in the documentation section about the new tool CalculateContamination?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2816
https://github.com/broadinstitute/gatk/pull/2816:556,Deployability,integrat,integration,556,"I've completely rewritten the documentation portion to be more helpful to users (and to reflect the new M2 rather than gatk3's M2), and I have updated the example commands. ---; ### Questions for @davidbenjamin ; - `--af_of_alleles_not_in_resource`: is this allele frequency used only in certain contexts, e.g. with matched normal analyses, or towards tumor sample variant alleles, etc.? I need to add to the doc details how this argument factors into calculations.; - I need a sentence or two describing the new algorithmic improvement on the new Mutect2 integration over uncertainty. ; - The WDLs do not include use of a contamination.table and so I did not include it in the commands. Is this something we want to nudge users to use, i.e. should I put in a sentence in the documentation section about the new tool CalculateContamination?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2816
https://github.com/broadinstitute/gatk/pull/2816:556,Integrability,integrat,integration,556,"I've completely rewritten the documentation portion to be more helpful to users (and to reflect the new M2 rather than gatk3's M2), and I have updated the example commands. ---; ### Questions for @davidbenjamin ; - `--af_of_alleles_not_in_resource`: is this allele frequency used only in certain contexts, e.g. with matched normal analyses, or towards tumor sample variant alleles, etc.? I need to add to the doc details how this argument factors into calculations.; - I need a sentence or two describing the new algorithmic improvement on the new Mutect2 integration over uncertainty. ; - The WDLs do not include use of a contamination.table and so I did not include it in the commands. Is this something we want to nudge users to use, i.e. should I put in a sentence in the documentation section about the new tool CalculateContamination?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2816
https://github.com/broadinstitute/gatk/issues/2817:13,Deployability,integrat,integration,13,The standard integration/unit tests upload their test report -- the docker tests should as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2817
https://github.com/broadinstitute/gatk/issues/2817:13,Integrability,integrat,integration,13,The standard integration/unit tests upload their test report -- the docker tests should as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2817
https://github.com/broadinstitute/gatk/issues/2817:30,Testability,test,tests,30,The standard integration/unit tests upload their test report -- the docker tests should as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2817
https://github.com/broadinstitute/gatk/issues/2817:49,Testability,test,test,49,The standard integration/unit tests upload their test report -- the docker tests should as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2817
https://github.com/broadinstitute/gatk/issues/2817:75,Testability,test,tests,75,The standard integration/unit tests upload their test report -- the docker tests should as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2817
https://github.com/broadinstitute/gatk/issues/2818:310,Deployability,integrat,integration,310,"The current test report suggests that if we split off the tests in the `exome` package into a separate target, we would be close to a balanced 2-way split. Possibly the right way to do this is to have two new values for `TEST_TYPE` in `build.gradle`: `cnvIntegration` and `nonCNVIntegration`. Specifying just `integration` would run all the integration tests, as before. Specifying `cnvIntegration` would run everything in the `exome`, `copynumber`, and `coveragemodel` packages. Specifying `nonCNVIntegration` would run everything outside of those packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818
https://github.com/broadinstitute/gatk/issues/2818:341,Deployability,integrat,integration,341,"The current test report suggests that if we split off the tests in the `exome` package into a separate target, we would be close to a balanced 2-way split. Possibly the right way to do this is to have two new values for `TEST_TYPE` in `build.gradle`: `cnvIntegration` and `nonCNVIntegration`. Specifying just `integration` would run all the integration tests, as before. Specifying `cnvIntegration` would run everything in the `exome`, `copynumber`, and `coveragemodel` packages. Specifying `nonCNVIntegration` would run everything outside of those packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818
https://github.com/broadinstitute/gatk/issues/2818:310,Integrability,integrat,integration,310,"The current test report suggests that if we split off the tests in the `exome` package into a separate target, we would be close to a balanced 2-way split. Possibly the right way to do this is to have two new values for `TEST_TYPE` in `build.gradle`: `cnvIntegration` and `nonCNVIntegration`. Specifying just `integration` would run all the integration tests, as before. Specifying `cnvIntegration` would run everything in the `exome`, `copynumber`, and `coveragemodel` packages. Specifying `nonCNVIntegration` would run everything outside of those packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818
https://github.com/broadinstitute/gatk/issues/2818:341,Integrability,integrat,integration,341,"The current test report suggests that if we split off the tests in the `exome` package into a separate target, we would be close to a balanced 2-way split. Possibly the right way to do this is to have two new values for `TEST_TYPE` in `build.gradle`: `cnvIntegration` and `nonCNVIntegration`. Specifying just `integration` would run all the integration tests, as before. Specifying `cnvIntegration` would run everything in the `exome`, `copynumber`, and `coveragemodel` packages. Specifying `nonCNVIntegration` would run everything outside of those packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818
https://github.com/broadinstitute/gatk/issues/2818:12,Testability,test,test,12,"The current test report suggests that if we split off the tests in the `exome` package into a separate target, we would be close to a balanced 2-way split. Possibly the right way to do this is to have two new values for `TEST_TYPE` in `build.gradle`: `cnvIntegration` and `nonCNVIntegration`. Specifying just `integration` would run all the integration tests, as before. Specifying `cnvIntegration` would run everything in the `exome`, `copynumber`, and `coveragemodel` packages. Specifying `nonCNVIntegration` would run everything outside of those packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818
https://github.com/broadinstitute/gatk/issues/2818:58,Testability,test,tests,58,"The current test report suggests that if we split off the tests in the `exome` package into a separate target, we would be close to a balanced 2-way split. Possibly the right way to do this is to have two new values for `TEST_TYPE` in `build.gradle`: `cnvIntegration` and `nonCNVIntegration`. Specifying just `integration` would run all the integration tests, as before. Specifying `cnvIntegration` would run everything in the `exome`, `copynumber`, and `coveragemodel` packages. Specifying `nonCNVIntegration` would run everything outside of those packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818
https://github.com/broadinstitute/gatk/issues/2818:353,Testability,test,tests,353,"The current test report suggests that if we split off the tests in the `exome` package into a separate target, we would be close to a balanced 2-way split. Possibly the right way to do this is to have two new values for `TEST_TYPE` in `build.gradle`: `cnvIntegration` and `nonCNVIntegration`. Specifying just `integration` would run all the integration tests, as before. Specifying `cnvIntegration` would run everything in the `exome`, `copynumber`, and `coveragemodel` packages. Specifying `nonCNVIntegration` would run everything outside of those packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818
https://github.com/broadinstitute/gatk/issues/2819:85,Testability,test,tests,85,Do the equivalent of https://github.com/broadinstitute/gatk/issues/2818 for the unit tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2819
https://github.com/broadinstitute/gatk/issues/2822:36,Deployability,release,release,36,"I'm told there will be a new gcloud release any day now (https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2110) with @jean-philippe-martin 's NIO retry fixes. We should update as soon as it's out, and confirm that it resolves https://github.com/broadinstitute/gatk/issues/2749, https://github.com/broadinstitute/gatk/issues/2685, and (possibly) https://github.com/broadinstitute/gatk/issues/2686",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2822
https://github.com/broadinstitute/gatk/issues/2822:184,Deployability,update,update,184,"I'm told there will be a new gcloud release any day now (https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2110) with @jean-philippe-martin 's NIO retry fixes. We should update as soon as it's out, and confirm that it resolves https://github.com/broadinstitute/gatk/issues/2749, https://github.com/broadinstitute/gatk/issues/2685, and (possibly) https://github.com/broadinstitute/gatk/issues/2686",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2822
https://github.com/broadinstitute/gatk/issues/2823:178,Deployability,update,updated,178,@vruano commented on [Wed Jun 17 2015](https://github.com/broadinstitute/gatk-protected/issues/39). There some issues in the documentation text in package-info.java that was not updated properly after a last minute refactoring. This task is neither nor not urgent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2823
https://github.com/broadinstitute/gatk/issues/2823:215,Modifiability,refactor,refactoring,215,@vruano commented on [Wed Jun 17 2015](https://github.com/broadinstitute/gatk-protected/issues/39). There some issues in the documentation text in package-info.java that was not updated properly after a last minute refactoring. This task is neither nor not urgent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2823
https://github.com/broadinstitute/gatk/issues/2824:364,Availability,down,download,364,"@samuelklee commented on [Mon Oct 05 2015](https://github.com/broadinstitute/gatk-protected/issues/126). Some possible enhancements/improvements, in no particular order and of varying scope:. -Change SliceSampler to be able to handle multimodal univariate distributions. Should just be a matter of implementing the pseudocode in Neal 2003 http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461. -Add Metropolis-Hastings univariate sampler as alternative to SliceSampler. -Add Metropolis-Hastings/nested/etc. multivariate samplers as alternatives to GibbsSampler. This should only be tackled if a model/dataset necessitates it. -Implement hierarchical/multilevel models in an OOP way. Currently, the samplers operate on lists of global parameters and lists of lists of ""local"" parameters (i.e., segment-level or site-level parameters), which is a bit clunky. -Add convergence diagnostics (e.g., autocorrelation time). -Add ability to make trace plots and corner plots. -Implement more flexible discarding of burn-in. Currently, samples from all iterations are aggregated in memory. Depending on the maximum number of iterations we want to allow, it might be better to write samples to disk, only store samples in memory after burn-in, etc. so we don't run into memory issues. -Parallelization (again, only if a model/dataset necessitates it). ---. @LeeTL1220 commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153466956). @samuelklee Do we need this for the beta release?. ---. @samuelklee commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153471237). I'd say no to pretty much all of the points, except for whatever @davidbenjamin ends up needing to implement for the allele-fraction model (David, last time I looked at your branch there was some MH sampling going on?). Some of them will probably be relatively easy to address before beta (e.g., the first point about fixing up the Slic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824
https://github.com/broadinstitute/gatk/issues/2824:1522,Deployability,release,release,1522,"003 http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461. -Add Metropolis-Hastings univariate sampler as alternative to SliceSampler. -Add Metropolis-Hastings/nested/etc. multivariate samplers as alternatives to GibbsSampler. This should only be tackled if a model/dataset necessitates it. -Implement hierarchical/multilevel models in an OOP way. Currently, the samplers operate on lists of global parameters and lists of lists of ""local"" parameters (i.e., segment-level or site-level parameters), which is a bit clunky. -Add convergence diagnostics (e.g., autocorrelation time). -Add ability to make trace plots and corner plots. -Implement more flexible discarding of burn-in. Currently, samples from all iterations are aggregated in memory. Depending on the maximum number of iterations we want to allow, it might be better to write samples to disk, only store samples in memory after burn-in, etc. so we don't run into memory issues. -Parallelization (again, only if a model/dataset necessitates it). ---. @LeeTL1220 commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153466956). @samuelklee Do we need this for the beta release?. ---. @samuelklee commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153471237). I'd say no to pretty much all of the points, except for whatever @davidbenjamin ends up needing to implement for the allele-fraction model (David, last time I looked at your branch there was some MH sampling going on?). Some of them will probably be relatively easy to address before beta (e.g., the first point about fixing up the SliceSampler), but I think they are low priority. The only thing that we'll definitely have to decide on for beta release is how to store/plot the MCMC chains (i.e., the posterior samples). If all people want to see is posterior point estimates + credible intervals, we can just discard the chains, but this seems somewhat wasteful to me.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824
https://github.com/broadinstitute/gatk/issues/2824:2111,Deployability,release,release,2111,"003 http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461. -Add Metropolis-Hastings univariate sampler as alternative to SliceSampler. -Add Metropolis-Hastings/nested/etc. multivariate samplers as alternatives to GibbsSampler. This should only be tackled if a model/dataset necessitates it. -Implement hierarchical/multilevel models in an OOP way. Currently, the samplers operate on lists of global parameters and lists of lists of ""local"" parameters (i.e., segment-level or site-level parameters), which is a bit clunky. -Add convergence diagnostics (e.g., autocorrelation time). -Add ability to make trace plots and corner plots. -Implement more flexible discarding of burn-in. Currently, samples from all iterations are aggregated in memory. Depending on the maximum number of iterations we want to allow, it might be better to write samples to disk, only store samples in memory after burn-in, etc. so we don't run into memory issues. -Parallelization (again, only if a model/dataset necessitates it). ---. @LeeTL1220 commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153466956). @samuelklee Do we need this for the beta release?. ---. @samuelklee commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153471237). I'd say no to pretty much all of the points, except for whatever @davidbenjamin ends up needing to implement for the allele-fraction model (David, last time I looked at your branch there was some MH sampling going on?). Some of them will probably be relatively easy to address before beta (e.g., the first point about fixing up the SliceSampler), but I think they are low priority. The only thing that we'll definitely have to decide on for beta release is how to store/plot the MCMC chains (i.e., the posterior samples). If all people want to see is posterior point estimates + credible intervals, we can just discard the chains, but this seems somewhat wasteful to me.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824
https://github.com/broadinstitute/gatk/issues/2824:1088,Integrability,Depend,Depending,1088,"er and of varying scope:. -Change SliceSampler to be able to handle multimodal univariate distributions. Should just be a matter of implementing the pseudocode in Neal 2003 http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461. -Add Metropolis-Hastings univariate sampler as alternative to SliceSampler. -Add Metropolis-Hastings/nested/etc. multivariate samplers as alternatives to GibbsSampler. This should only be tackled if a model/dataset necessitates it. -Implement hierarchical/multilevel models in an OOP way. Currently, the samplers operate on lists of global parameters and lists of lists of ""local"" parameters (i.e., segment-level or site-level parameters), which is a bit clunky. -Add convergence diagnostics (e.g., autocorrelation time). -Add ability to make trace plots and corner plots. -Implement more flexible discarding of burn-in. Currently, samples from all iterations are aggregated in memory. Depending on the maximum number of iterations we want to allow, it might be better to write samples to disk, only store samples in memory after burn-in, etc. so we don't run into memory issues. -Parallelization (again, only if a model/dataset necessitates it). ---. @LeeTL1220 commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153466956). @samuelklee Do we need this for the beta release?. ---. @samuelklee commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153471237). I'd say no to pretty much all of the points, except for whatever @davidbenjamin ends up needing to implement for the allele-fraction model (David, last time I looked at your branch there was some MH sampling going on?). Some of them will probably be relatively easy to address before beta (e.g., the first point about fixing up the SliceSampler), but I think they are low priority. The only thing that we'll definitely have to decide on for beta release is how to store/plot the MCMC chains (i.e., the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824
https://github.com/broadinstitute/gatk/issues/2824:119,Modifiability,enhance,enhancements,119,"@samuelklee commented on [Mon Oct 05 2015](https://github.com/broadinstitute/gatk-protected/issues/126). Some possible enhancements/improvements, in no particular order and of varying scope:. -Change SliceSampler to be able to handle multimodal univariate distributions. Should just be a matter of implementing the pseudocode in Neal 2003 http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461. -Add Metropolis-Hastings univariate sampler as alternative to SliceSampler. -Add Metropolis-Hastings/nested/etc. multivariate samplers as alternatives to GibbsSampler. This should only be tackled if a model/dataset necessitates it. -Implement hierarchical/multilevel models in an OOP way. Currently, the samplers operate on lists of global parameters and lists of lists of ""local"" parameters (i.e., segment-level or site-level parameters), which is a bit clunky. -Add convergence diagnostics (e.g., autocorrelation time). -Add ability to make trace plots and corner plots. -Implement more flexible discarding of burn-in. Currently, samples from all iterations are aggregated in memory. Depending on the maximum number of iterations we want to allow, it might be better to write samples to disk, only store samples in memory after burn-in, etc. so we don't run into memory issues. -Parallelization (again, only if a model/dataset necessitates it). ---. @LeeTL1220 commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153466956). @samuelklee Do we need this for the beta release?. ---. @samuelklee commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153471237). I'd say no to pretty much all of the points, except for whatever @davidbenjamin ends up needing to implement for the allele-fraction model (David, last time I looked at your branch there was some MH sampling going on?). Some of them will probably be relatively easy to address before beta (e.g., the first point about fixing up the Slic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824
https://github.com/broadinstitute/gatk/issues/2824:991,Modifiability,flexible,flexible,991,"@samuelklee commented on [Mon Oct 05 2015](https://github.com/broadinstitute/gatk-protected/issues/126). Some possible enhancements/improvements, in no particular order and of varying scope:. -Change SliceSampler to be able to handle multimodal univariate distributions. Should just be a matter of implementing the pseudocode in Neal 2003 http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461. -Add Metropolis-Hastings univariate sampler as alternative to SliceSampler. -Add Metropolis-Hastings/nested/etc. multivariate samplers as alternatives to GibbsSampler. This should only be tackled if a model/dataset necessitates it. -Implement hierarchical/multilevel models in an OOP way. Currently, the samplers operate on lists of global parameters and lists of lists of ""local"" parameters (i.e., segment-level or site-level parameters), which is a bit clunky. -Add convergence diagnostics (e.g., autocorrelation time). -Add ability to make trace plots and corner plots. -Implement more flexible discarding of burn-in. Currently, samples from all iterations are aggregated in memory. Depending on the maximum number of iterations we want to allow, it might be better to write samples to disk, only store samples in memory after burn-in, etc. so we don't run into memory issues. -Parallelization (again, only if a model/dataset necessitates it). ---. @LeeTL1220 commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153466956). @samuelklee Do we need this for the beta release?. ---. @samuelklee commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153471237). I'd say no to pretty much all of the points, except for whatever @davidbenjamin ends up needing to implement for the allele-fraction model (David, last time I looked at your branch there was some MH sampling going on?). Some of them will probably be relatively easy to address before beta (e.g., the first point about fixing up the Slic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824
https://github.com/broadinstitute/gatk/issues/2826:163,Deployability,pipeline,pipeline,163,"@vruano commented on [Wed Oct 14 2015](https://github.com/broadinstitute/gatk-protected/issues/149). Plots required to choose some of the parameters use along the pipeline:; To have an idea how they look like and how they would be used you can refer to XHMM tutorial:; https://atgu.mgh.harvard.edu/xhmm/tutorial.shtml. These can be totally in R and you may choose to reuse XHMM original code make reference to the appropriate license; they are quite simple so probably it is not necessary:; - min and max average sample coverage (to filter extreme samples).; - Plot a histogram of the average sample target coverage to choose this cut-offs. ; - min and max std dev. coverage across targets per sample (to filter extreme targets).; - Plot another histogram but in this case of the std .dev target coverage.; - min and max average and std. dev target coverage (to filter extreme targets); - Basically the ""transpose of the two plots above so that we can filter extreme targets:; - Histogram of the mean coverage per target across samples; - Histogram of the std. dev coverage per target across samples.; - Principal components variance explained plot.; - Y is the variance explained by the component (~ eigen value).; - X is the component index where 0 is the first component and i is the ith component.; Consequently this graph is monotonic decreasing.; - Would be nice to get the component vs covariate plot to find out whether we are getting rid ; of known biases like GC content but this one may take a bit more time an might not be necessary for now in practice. . The first few plots could be done by a script that takes in a read counts file.; The principal components one may access the .pon file directly perhaps using a cran package to read hdf5 files. Otherwise you might need to write a simple tool to extract those variances from the .pon. ---. @samuelklee commented on [Wed Aug 17 2016](https://github.com/broadinstitute/gatk-protected/issues/149#issuecomment-240525897). The new germline ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2826
https://github.com/broadinstitute/gatk/issues/2826:1682,Security,access,access,1682,"thub.com/broadinstitute/gatk-protected/issues/149). Plots required to choose some of the parameters use along the pipeline:; To have an idea how they look like and how they would be used you can refer to XHMM tutorial:; https://atgu.mgh.harvard.edu/xhmm/tutorial.shtml. These can be totally in R and you may choose to reuse XHMM original code make reference to the appropriate license; they are quite simple so probably it is not necessary:; - min and max average sample coverage (to filter extreme samples).; - Plot a histogram of the average sample target coverage to choose this cut-offs. ; - min and max std dev. coverage across targets per sample (to filter extreme targets).; - Plot another histogram but in this case of the std .dev target coverage.; - min and max average and std. dev target coverage (to filter extreme targets); - Basically the ""transpose of the two plots above so that we can filter extreme targets:; - Histogram of the mean coverage per target across samples; - Histogram of the std. dev coverage per target across samples.; - Principal components variance explained plot.; - Y is the variance explained by the component (~ eigen value).; - X is the component index where 0 is the first component and i is the ith component.; Consequently this graph is monotonic decreasing.; - Would be nice to get the component vs covariate plot to find out whether we are getting rid ; of known biases like GC content but this one may take a bit more time an might not be necessary for now in practice. . The first few plots could be done by a script that takes in a read counts file.; The principal components one may access the .pon file directly perhaps using a cran package to read hdf5 files. Otherwise you might need to write a simple tool to extract those variances from the .pon. ---. @samuelklee commented on [Wed Aug 17 2016](https://github.com/broadinstitute/gatk-protected/issues/149#issuecomment-240525897). The new germline CNV tool should have some plotting capabilities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2826
https://github.com/broadinstitute/gatk/issues/2826:450,Usability,simpl,simple,450,"@vruano commented on [Wed Oct 14 2015](https://github.com/broadinstitute/gatk-protected/issues/149). Plots required to choose some of the parameters use along the pipeline:; To have an idea how they look like and how they would be used you can refer to XHMM tutorial:; https://atgu.mgh.harvard.edu/xhmm/tutorial.shtml. These can be totally in R and you may choose to reuse XHMM original code make reference to the appropriate license; they are quite simple so probably it is not necessary:; - min and max average sample coverage (to filter extreme samples).; - Plot a histogram of the average sample target coverage to choose this cut-offs. ; - min and max std dev. coverage across targets per sample (to filter extreme targets).; - Plot another histogram but in this case of the std .dev target coverage.; - min and max average and std. dev target coverage (to filter extreme targets); - Basically the ""transpose of the two plots above so that we can filter extreme targets:; - Histogram of the mean coverage per target across samples; - Histogram of the std. dev coverage per target across samples.; - Principal components variance explained plot.; - Y is the variance explained by the component (~ eigen value).; - X is the component index where 0 is the first component and i is the ith component.; Consequently this graph is monotonic decreasing.; - Would be nice to get the component vs covariate plot to find out whether we are getting rid ; of known biases like GC content but this one may take a bit more time an might not be necessary for now in practice. . The first few plots could be done by a script that takes in a read counts file.; The principal components one may access the .pon file directly perhaps using a cran package to read hdf5 files. Otherwise you might need to write a simple tool to extract those variances from the .pon. ---. @samuelklee commented on [Wed Aug 17 2016](https://github.com/broadinstitute/gatk-protected/issues/149#issuecomment-240525897). The new germline ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2826
https://github.com/broadinstitute/gatk/issues/2826:1797,Usability,simpl,simple,1797,"thub.com/broadinstitute/gatk-protected/issues/149). Plots required to choose some of the parameters use along the pipeline:; To have an idea how they look like and how they would be used you can refer to XHMM tutorial:; https://atgu.mgh.harvard.edu/xhmm/tutorial.shtml. These can be totally in R and you may choose to reuse XHMM original code make reference to the appropriate license; they are quite simple so probably it is not necessary:; - min and max average sample coverage (to filter extreme samples).; - Plot a histogram of the average sample target coverage to choose this cut-offs. ; - min and max std dev. coverage across targets per sample (to filter extreme targets).; - Plot another histogram but in this case of the std .dev target coverage.; - min and max average and std. dev target coverage (to filter extreme targets); - Basically the ""transpose of the two plots above so that we can filter extreme targets:; - Histogram of the mean coverage per target across samples; - Histogram of the std. dev coverage per target across samples.; - Principal components variance explained plot.; - Y is the variance explained by the component (~ eigen value).; - X is the component index where 0 is the first component and i is the ith component.; Consequently this graph is monotonic decreasing.; - Would be nice to get the component vs covariate plot to find out whether we are getting rid ; of known biases like GC content but this one may take a bit more time an might not be necessary for now in practice. . The first few plots could be done by a script that takes in a read counts file.; The principal components one may access the .pon file directly perhaps using a cran package to read hdf5 files. Otherwise you might need to write a simple tool to extract those variances from the .pon. ---. @samuelklee commented on [Wed Aug 17 2016](https://github.com/broadinstitute/gatk-protected/issues/149#issuecomment-240525897). The new germline CNV tool should have some plotting capabilities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2826
https://github.com/broadinstitute/gatk/issues/2827:464,Availability,down,down,464,"@akiezun commented on [Wed Oct 28 2015](https://github.com/broadinstitute/gatk-protected/issues/169). all parts that are a) mature enough b) shared between germline and somatic CNVs should be moved to the public gatk repo. . OK to do past alpha. ---. @akiezun commented on [Wed Nov 04 2015](https://github.com/broadinstitute/gatk-protected/issues/169#issuecomment-153783985). @LeeTL1220 @vruano can you list here the name of the subcomponents that would be pushed down to gatk public?; A lot of code would qualify I think: Targets, Segments, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2827
https://github.com/broadinstitute/gatk/issues/2828:260,Usability,simpl,simple,260,@achevali commented on [Thu Nov 12 2015](https://github.com/broadinstitute/gatk-protected/issues/185). ---. @LeeTL1220 commented on [Wed Dec 02 2015](https://github.com/broadinstitute/gatk-protected/issues/185#issuecomment-161477822). @achevali Is this just a simple doc change to make sure cairo and png are supported in a `capabilities()` call?. ---. @samuelklee commented on [Wed Aug 17 2016](https://github.com/broadinstitute/gatk-protected/issues/185#issuecomment-240517725). @achevali can we close this issue?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2828
https://github.com/broadinstitute/gatk/issues/2829:287,Modifiability,refactor,refactor,287,"@vdauwera commented on [Fri Dec 18 2015](https://github.com/broadinstitute/gatk-protected/issues/259). Back in July 2015, @vruano made a laundry list of issues suggesting possible improvements to HaplotypeCaller and related internals. They won't be done in GATK3 so I tagged them as ""HC-refactor"" when I closed them. Whoever ports HC should review those suggestions so that @vruano's wisdom is not wasted. . https://github.com/broadinstitute/gsa-unstable/issues?q=label%3AHC-refactor+is%3Aclosed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2829
https://github.com/broadinstitute/gatk/issues/2829:475,Modifiability,refactor,refactor,475,"@vdauwera commented on [Fri Dec 18 2015](https://github.com/broadinstitute/gatk-protected/issues/259). Back in July 2015, @vruano made a laundry list of issues suggesting possible improvements to HaplotypeCaller and related internals. They won't be done in GATK3 so I tagged them as ""HC-refactor"" when I closed them. Whoever ports HC should review those suggestions so that @vruano's wisdom is not wasted. . https://github.com/broadinstitute/gsa-unstable/issues?q=label%3AHC-refactor+is%3Aclosed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2829
https://github.com/broadinstitute/gatk/issues/2830:861,Modifiability,variab,variable,861,"@davidbenjamin commented on [Sat Dec 19 2015](https://github.com/broadinstitute/gatk-protected/issues/260). We use (linear) PCA to map the PoN, each datum of which is a high-dimensional vector over targets, to a low-dimensional manifold. Do we really believe that this manifold is simply a hyperplane?. Concretely, suppose the data really lives on a 4-dimensional curved manifold. Due to the curvature, we might require many more, say 20, flat dimensions to encompass a significant amount of the PoN's variance. What this means is that we lump a huge amount of noise in with the true signal. Non-linear alternatives worth investigating include kernel PCA -- nice because like all machine learning things involving the kernel trick you get to recycle almost all of your mathematical and algorithmic machinery, denoising autoencoders, and Gaussian process latent variable models. ---. @davidbenjamin commented on [Wed Dec 23 2015](https://github.com/broadinstitute/gatk-protected/issues/260#issuecomment-166804219). Linear PCA could be sufficient if the PoN samples are tightly clustered about their mean so that variance is a small perturbation that can be treated linearly. I don't think we know enough about PoNs to judge what actually occurs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2830
https://github.com/broadinstitute/gatk/issues/2830:281,Usability,simpl,simply,281,"@davidbenjamin commented on [Sat Dec 19 2015](https://github.com/broadinstitute/gatk-protected/issues/260). We use (linear) PCA to map the PoN, each datum of which is a high-dimensional vector over targets, to a low-dimensional manifold. Do we really believe that this manifold is simply a hyperplane?. Concretely, suppose the data really lives on a 4-dimensional curved manifold. Due to the curvature, we might require many more, say 20, flat dimensions to encompass a significant amount of the PoN's variance. What this means is that we lump a huge amount of noise in with the true signal. Non-linear alternatives worth investigating include kernel PCA -- nice because like all machine learning things involving the kernel trick you get to recycle almost all of your mathematical and algorithmic machinery, denoising autoencoders, and Gaussian process latent variable models. ---. @davidbenjamin commented on [Wed Dec 23 2015](https://github.com/broadinstitute/gatk-protected/issues/260#issuecomment-166804219). Linear PCA could be sufficient if the PoN samples are tightly clustered about their mean so that variance is a small perturbation that can be treated linearly. I don't think we know enough about PoNs to judge what actually occurs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2830
https://github.com/broadinstitute/gatk/issues/2830:688,Usability,learn,learning,688,"@davidbenjamin commented on [Sat Dec 19 2015](https://github.com/broadinstitute/gatk-protected/issues/260). We use (linear) PCA to map the PoN, each datum of which is a high-dimensional vector over targets, to a low-dimensional manifold. Do we really believe that this manifold is simply a hyperplane?. Concretely, suppose the data really lives on a 4-dimensional curved manifold. Due to the curvature, we might require many more, say 20, flat dimensions to encompass a significant amount of the PoN's variance. What this means is that we lump a huge amount of noise in with the true signal. Non-linear alternatives worth investigating include kernel PCA -- nice because like all machine learning things involving the kernel trick you get to recycle almost all of your mathematical and algorithmic machinery, denoising autoencoders, and Gaussian process latent variable models. ---. @davidbenjamin commented on [Wed Dec 23 2015](https://github.com/broadinstitute/gatk-protected/issues/260#issuecomment-166804219). Linear PCA could be sufficient if the PoN samples are tightly clustered about their mean so that variance is a small perturbation that can be treated linearly. I don't think we know enough about PoNs to judge what actually occurs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2830
https://github.com/broadinstitute/gatk/issues/2833:476,Deployability,release,release,476,"@LeeTL1220 commented on [Fri Jan 15 2016](https://github.com/broadinstitute/gatk-protected/issues/308). This may be as simple as adding a `ctx.close()` statement after the spark calculations are complete.; - [ ] Confirmed on our spark cluster that this is fixed... ---. @LeeTL1220 commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-175139853). Cannot close without the unit tests falling over. Putting this off for a later release. ---. @lbergelson commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-175154352). Possible solutions involve running `SparkContext.KillExecutors()` but I haven't looked into how it works exactly... ---. @samuelklee commented on [Thu May 11 2017](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300793994). @LeeTL1220 should I keep this open?. ---. @LeeTL1220 commented on [Thu May 11 2017](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300797579). I think this should be kept open, but low priority. On Thu, May 11, 2017 at 9:46 AM, samuelklee <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> should I keep this open?; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300793994>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk06fX-Z26myWvz9Shn_c5e4I0xHqks5r4xEigaJpZM4HGA9T>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2833
https://github.com/broadinstitute/gatk/issues/2833:427,Testability,test,tests,427,"@LeeTL1220 commented on [Fri Jan 15 2016](https://github.com/broadinstitute/gatk-protected/issues/308). This may be as simple as adding a `ctx.close()` statement after the spark calculations are complete.; - [ ] Confirmed on our spark cluster that this is fixed... ---. @LeeTL1220 commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-175139853). Cannot close without the unit tests falling over. Putting this off for a later release. ---. @lbergelson commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-175154352). Possible solutions involve running `SparkContext.KillExecutors()` but I haven't looked into how it works exactly... ---. @samuelklee commented on [Thu May 11 2017](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300793994). @LeeTL1220 should I keep this open?. ---. @LeeTL1220 commented on [Thu May 11 2017](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300797579). I think this should be kept open, but low priority. On Thu, May 11, 2017 at 9:46 AM, samuelklee <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> should I keep this open?; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300793994>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk06fX-Z26myWvz9Shn_c5e4I0xHqks5r4xEigaJpZM4HGA9T>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2833
https://github.com/broadinstitute/gatk/issues/2833:119,Usability,simpl,simple,119,"@LeeTL1220 commented on [Fri Jan 15 2016](https://github.com/broadinstitute/gatk-protected/issues/308). This may be as simple as adding a `ctx.close()` statement after the spark calculations are complete.; - [ ] Confirmed on our spark cluster that this is fixed... ---. @LeeTL1220 commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-175139853). Cannot close without the unit tests falling over. Putting this off for a later release. ---. @lbergelson commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-175154352). Possible solutions involve running `SparkContext.KillExecutors()` but I haven't looked into how it works exactly... ---. @samuelklee commented on [Thu May 11 2017](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300793994). @LeeTL1220 should I keep this open?. ---. @LeeTL1220 commented on [Thu May 11 2017](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300797579). I think this should be kept open, but low priority. On Thu, May 11, 2017 at 9:46 AM, samuelklee <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> should I keep this open?; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300793994>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk06fX-Z26myWvz9Shn_c5e4I0xHqks5r4xEigaJpZM4HGA9T>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2833
https://github.com/broadinstitute/gatk/issues/2834:361,Integrability,Depend,Dependent,361,@LeeTL1220 commented on [Thu Jan 21 2016](https://github.com/broadinstitute/gatk-protected/issues/316). How does the performance look?; This may also include having to troubleshoot adding a `sparkJar` artifact to `build.gradle`. ---. @LeeTL1220 commented on [Fri Jan 22 2016](https://github.com/broadinstitute/gatk-protected/issues/316#issuecomment-173938780). Dependent on issue https://github.com/broadinstitute/gatk-protected/issues/317,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2834
https://github.com/broadinstitute/gatk/issues/2834:117,Performance,perform,performance,117,@LeeTL1220 commented on [Thu Jan 21 2016](https://github.com/broadinstitute/gatk-protected/issues/316). How does the performance look?; This may also include having to troubleshoot adding a `sparkJar` artifact to `build.gradle`. ---. @LeeTL1220 commented on [Fri Jan 22 2016](https://github.com/broadinstitute/gatk-protected/issues/316#issuecomment-173938780). Dependent on issue https://github.com/broadinstitute/gatk-protected/issues/317,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2834
https://github.com/broadinstitute/gatk/issues/2835:683,Availability,error,error,683,"@LeeTL1220 commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/343). In `CreatePanelOfNormals` make anonymize a flag that defaults to `false`. (i.e. `--anonymize`) In other words, by default, we do _not_ produce an anonymized PoN. We could also use a separate tool that takes a pre-existing PoN and anonymizes it. . To anonymize a PoN:; - [ ] Determine which fields are private. At the very least: `fnt_control_matrix`, `log_normals`, and `log_normals_pinv`. _There may be others -- please investigate as part of this issue_; - [ ] Have `CreatePanelOfNormals` delete the fields as the last step.; - [ ] Make sure that `HDF5PoN` produces reasonable error messages if one of these fields is accessed in an anonymized PoN.; - [ ] Create CLI that can take existing PoN and delete the fields. ---. @LeeTL1220 commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/343#issuecomment-178022285). This is necessary since we may want to share PoNs and the PoN files cannot have any private data. ---. @LeeTL1220 commented on [Wed Mar 02 2016](https://github.com/broadinstitute/gatk-protected/issues/343#issuecomment-191420153). Moving this to later milestone, unless it becomes more urgent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2835
https://github.com/broadinstitute/gatk/issues/2835:689,Integrability,message,messages,689,"@LeeTL1220 commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/343). In `CreatePanelOfNormals` make anonymize a flag that defaults to `false`. (i.e. `--anonymize`) In other words, by default, we do _not_ produce an anonymized PoN. We could also use a separate tool that takes a pre-existing PoN and anonymizes it. . To anonymize a PoN:; - [ ] Determine which fields are private. At the very least: `fnt_control_matrix`, `log_normals`, and `log_normals_pinv`. _There may be others -- please investigate as part of this issue_; - [ ] Have `CreatePanelOfNormals` delete the fields as the last step.; - [ ] Make sure that `HDF5PoN` produces reasonable error messages if one of these fields is accessed in an anonymized PoN.; - [ ] Create CLI that can take existing PoN and delete the fields. ---. @LeeTL1220 commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/343#issuecomment-178022285). This is necessary since we may want to share PoNs and the PoN files cannot have any private data. ---. @LeeTL1220 commented on [Wed Mar 02 2016](https://github.com/broadinstitute/gatk-protected/issues/343#issuecomment-191420153). Moving this to later milestone, unless it becomes more urgent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2835
https://github.com/broadinstitute/gatk/issues/2835:724,Security,access,accessed,724,"@LeeTL1220 commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/343). In `CreatePanelOfNormals` make anonymize a flag that defaults to `false`. (i.e. `--anonymize`) In other words, by default, we do _not_ produce an anonymized PoN. We could also use a separate tool that takes a pre-existing PoN and anonymizes it. . To anonymize a PoN:; - [ ] Determine which fields are private. At the very least: `fnt_control_matrix`, `log_normals`, and `log_normals_pinv`. _There may be others -- please investigate as part of this issue_; - [ ] Have `CreatePanelOfNormals` delete the fields as the last step.; - [ ] Make sure that `HDF5PoN` produces reasonable error messages if one of these fields is accessed in an anonymized PoN.; - [ ] Create CLI that can take existing PoN and delete the fields. ---. @LeeTL1220 commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/343#issuecomment-178022285). This is necessary since we may want to share PoNs and the PoN files cannot have any private data. ---. @LeeTL1220 commented on [Wed Mar 02 2016](https://github.com/broadinstitute/gatk-protected/issues/343#issuecomment-191420153). Moving this to later milestone, unless it becomes more urgent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2835
https://github.com/broadinstitute/gatk/issues/2836:1169,Deployability,update,update,1169,"@samuelklee commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/344). Segment class that was reintroduced in the germline code requires reference to collection of Targets in constructor and also stores a call, segment mean, and number of targets. ModeledSegment (used in CNV) now extends this and adds methods to transform CR/log2CR, and in turn ACNVModeledSegment (used in ACNV) extends ModeledSegment in https://github.com/broadinstitute/gatk-protected/pull/329. However, this is awkward because ACNVModeledSegment does not store a call, segment mean, or number of targets. I think we decided in https://github.com/broadinstitute/gatk-protected/issues/57, https://github.com/broadinstitute/gatk-protected/issues/61, https://github.com/broadinstitute/gatk-protected/issues/70, https://github.com/broadinstitute/gatk-protected/issues/71, etc. that Segments should simply query the relevant collection of Targets, especially for things like number of targets (which, correct me if I'm wrong, is only needed upon output to file), and that we should use SimpleInterval to represent a segment whenever possible. This obviates the need to update internally held fields when merging segments, etc. @LeeTL1220 @vruano @davidbenjamin we should probably get together and decide how these classes should be structured before moving them over into public. I expect that some of this will also resolve once CNV's output is more along the lines of ACNV's (i.e., when it outputs posterior summaries).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836
https://github.com/broadinstitute/gatk/issues/2836:315,Modifiability,extend,extends,315,"@samuelklee commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/344). Segment class that was reintroduced in the germline code requires reference to collection of Targets in constructor and also stores a call, segment mean, and number of targets. ModeledSegment (used in CNV) now extends this and adds methods to transform CR/log2CR, and in turn ACNVModeledSegment (used in ACNV) extends ModeledSegment in https://github.com/broadinstitute/gatk-protected/pull/329. However, this is awkward because ACNVModeledSegment does not store a call, segment mean, or number of targets. I think we decided in https://github.com/broadinstitute/gatk-protected/issues/57, https://github.com/broadinstitute/gatk-protected/issues/61, https://github.com/broadinstitute/gatk-protected/issues/70, https://github.com/broadinstitute/gatk-protected/issues/71, etc. that Segments should simply query the relevant collection of Targets, especially for things like number of targets (which, correct me if I'm wrong, is only needed upon output to file), and that we should use SimpleInterval to represent a segment whenever possible. This obviates the need to update internally held fields when merging segments, etc. @LeeTL1220 @vruano @davidbenjamin we should probably get together and decide how these classes should be structured before moving them over into public. I expect that some of this will also resolve once CNV's output is more along the lines of ACNV's (i.e., when it outputs posterior summaries).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836
https://github.com/broadinstitute/gatk/issues/2836:415,Modifiability,extend,extends,415,"@samuelklee commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/344). Segment class that was reintroduced in the germline code requires reference to collection of Targets in constructor and also stores a call, segment mean, and number of targets. ModeledSegment (used in CNV) now extends this and adds methods to transform CR/log2CR, and in turn ACNVModeledSegment (used in ACNV) extends ModeledSegment in https://github.com/broadinstitute/gatk-protected/pull/329. However, this is awkward because ACNVModeledSegment does not store a call, segment mean, or number of targets. I think we decided in https://github.com/broadinstitute/gatk-protected/issues/57, https://github.com/broadinstitute/gatk-protected/issues/61, https://github.com/broadinstitute/gatk-protected/issues/70, https://github.com/broadinstitute/gatk-protected/issues/71, etc. that Segments should simply query the relevant collection of Targets, especially for things like number of targets (which, correct me if I'm wrong, is only needed upon output to file), and that we should use SimpleInterval to represent a segment whenever possible. This obviates the need to update internally held fields when merging segments, etc. @LeeTL1220 @vruano @davidbenjamin we should probably get together and decide how these classes should be structured before moving them over into public. I expect that some of this will also resolve once CNV's output is more along the lines of ACNV's (i.e., when it outputs posterior summaries).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836
https://github.com/broadinstitute/gatk/issues/2836:899,Usability,simpl,simply,899,"@samuelklee commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/344). Segment class that was reintroduced in the germline code requires reference to collection of Targets in constructor and also stores a call, segment mean, and number of targets. ModeledSegment (used in CNV) now extends this and adds methods to transform CR/log2CR, and in turn ACNVModeledSegment (used in ACNV) extends ModeledSegment in https://github.com/broadinstitute/gatk-protected/pull/329. However, this is awkward because ACNVModeledSegment does not store a call, segment mean, or number of targets. I think we decided in https://github.com/broadinstitute/gatk-protected/issues/57, https://github.com/broadinstitute/gatk-protected/issues/61, https://github.com/broadinstitute/gatk-protected/issues/70, https://github.com/broadinstitute/gatk-protected/issues/71, etc. that Segments should simply query the relevant collection of Targets, especially for things like number of targets (which, correct me if I'm wrong, is only needed upon output to file), and that we should use SimpleInterval to represent a segment whenever possible. This obviates the need to update internally held fields when merging segments, etc. @LeeTL1220 @vruano @davidbenjamin we should probably get together and decide how these classes should be structured before moving them over into public. I expect that some of this will also resolve once CNV's output is more along the lines of ACNV's (i.e., when it outputs posterior summaries).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836
https://github.com/broadinstitute/gatk/issues/2836:1086,Usability,Simpl,SimpleInterval,1086,"@samuelklee commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/344). Segment class that was reintroduced in the germline code requires reference to collection of Targets in constructor and also stores a call, segment mean, and number of targets. ModeledSegment (used in CNV) now extends this and adds methods to transform CR/log2CR, and in turn ACNVModeledSegment (used in ACNV) extends ModeledSegment in https://github.com/broadinstitute/gatk-protected/pull/329. However, this is awkward because ACNVModeledSegment does not store a call, segment mean, or number of targets. I think we decided in https://github.com/broadinstitute/gatk-protected/issues/57, https://github.com/broadinstitute/gatk-protected/issues/61, https://github.com/broadinstitute/gatk-protected/issues/70, https://github.com/broadinstitute/gatk-protected/issues/71, etc. that Segments should simply query the relevant collection of Targets, especially for things like number of targets (which, correct me if I'm wrong, is only needed upon output to file), and that we should use SimpleInterval to represent a segment whenever possible. This obviates the need to update internally held fields when merging segments, etc. @LeeTL1220 @vruano @davidbenjamin we should probably get together and decide how these classes should be structured before moving them over into public. I expect that some of this will also resolve once CNV's output is more along the lines of ACNV's (i.e., when it outputs posterior summaries).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836
https://github.com/broadinstitute/gatk/pull/2838:5,Deployability,update,updated,5,"Also updated GKL version to 0.5.2, since it adds some output with information about num threads",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2838
https://github.com/broadinstitute/gatk/issues/2840:118,Performance,perform,performance,118,@LeeTL1220 commented on [Wed Mar 02 2016](https://github.com/broadinstitute/gatk-protected/issues/396). We would like performance for WGS. ; Metrics could be: ; - sensitivity and precision; - concordance between multiple WGS replicates; - RMSE against HAPSEG results. Feel free to finalize this list...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2840
https://github.com/broadinstitute/gatk/issues/2841:138,Testability,test,test,138,"@davidbenjamin commented on [Tue Mar 22 2016](https://github.com/broadinstitute/gatk-protected/issues/413). We used to have a concordance test with XHMM, but we can't use that anymore.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2841
https://github.com/broadinstitute/gatk/issues/2842:115,Deployability,integrat,integrating,115,"@davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432). Before integrating this with probabilistic segmentation, which is more complicated for somatic than for germline, we can simply replace the current tangent normalization step with the mode of the likelihood (as a function of copy ratio) resulting from the generative coverage model. This requires issues https://github.com/broadinstitute/gatk-protected/issues/429 and https://github.com/broadinstitute/gatk-protected/issues/430 to be completed. ---. @davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-204015562). Also, this should only be attempted if we get good results from doing the equivalent in the germline code, issue https://github.com/broadinstitute/gatk-protected/issues/431. After it is done somatic and germline will share a PoN and all associated code, including tangent normalization. ---. @LeeTL1220 commented on [Mon Jun 06 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-223992660). This only needs to be done if we stick with CBS for segmentation. . At the very least, we need to implement this for checking performance against HMM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2842
https://github.com/broadinstitute/gatk/issues/2842:115,Integrability,integrat,integrating,115,"@davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432). Before integrating this with probabilistic segmentation, which is more complicated for somatic than for germline, we can simply replace the current tangent normalization step with the mode of the likelihood (as a function of copy ratio) resulting from the generative coverage model. This requires issues https://github.com/broadinstitute/gatk-protected/issues/429 and https://github.com/broadinstitute/gatk-protected/issues/430 to be completed. ---. @davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-204015562). Also, this should only be attempted if we get good results from doing the equivalent in the germline code, issue https://github.com/broadinstitute/gatk-protected/issues/431. After it is done somatic and germline will share a PoN and all associated code, including tangent normalization. ---. @LeeTL1220 commented on [Mon Jun 06 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-223992660). This only needs to be done if we stick with CBS for segmentation. . At the very least, we need to implement this for checking performance against HMM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2842
https://github.com/broadinstitute/gatk/issues/2842:1234,Performance,perform,performance,1234,"@davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432). Before integrating this with probabilistic segmentation, which is more complicated for somatic than for germline, we can simply replace the current tangent normalization step with the mode of the likelihood (as a function of copy ratio) resulting from the generative coverage model. This requires issues https://github.com/broadinstitute/gatk-protected/issues/429 and https://github.com/broadinstitute/gatk-protected/issues/430 to be completed. ---. @davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-204015562). Also, this should only be attempted if we get good results from doing the equivalent in the germline code, issue https://github.com/broadinstitute/gatk-protected/issues/431. After it is done somatic and germline will share a PoN and all associated code, including tangent normalization. ---. @LeeTL1220 commented on [Mon Jun 06 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-223992660). This only needs to be done if we stick with CBS for segmentation. . At the very least, we need to implement this for checking performance against HMM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2842
https://github.com/broadinstitute/gatk/issues/2842:229,Usability,simpl,simply,229,"@davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432). Before integrating this with probabilistic segmentation, which is more complicated for somatic than for germline, we can simply replace the current tangent normalization step with the mode of the likelihood (as a function of copy ratio) resulting from the generative coverage model. This requires issues https://github.com/broadinstitute/gatk-protected/issues/429 and https://github.com/broadinstitute/gatk-protected/issues/430 to be completed. ---. @davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-204015562). Also, this should only be attempted if we get good results from doing the equivalent in the germline code, issue https://github.com/broadinstitute/gatk-protected/issues/431. After it is done somatic and germline will share a PoN and all associated code, including tangent normalization. ---. @LeeTL1220 commented on [Mon Jun 06 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-223992660). This only needs to be done if we stick with CBS for segmentation. . At the very least, we need to implement this for checking performance against HMM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2842
https://github.com/broadinstitute/gatk/issues/2844:240,Testability,test,tests,240,@lbergelson commented on [Mon Apr 11 2016](https://github.com/broadinstitute/gatk-protected/issues/443). I recently broke the ability to create a spark/shadow jar and only discovered it when I went to create one later. We should add simple tests for the generated jar to be sure it can be created / executed.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2844
https://github.com/broadinstitute/gatk/issues/2844:233,Usability,simpl,simple,233,@lbergelson commented on [Mon Apr 11 2016](https://github.com/broadinstitute/gatk-protected/issues/443). I recently broke the ability to create a spark/shadow jar and only discovered it when I went to create one later. We should add simple tests for the generated jar to be sure it can be created / executed.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2844
https://github.com/broadinstitute/gatk/issues/2848:168,Testability,test,test,168,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2848:247,Testability,test,test,247,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2848:352,Testability,test,test,352,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2848:402,Testability,test,testa,402,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2848:436,Testability,test,testa,436,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2848:598,Testability,test,test,598,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2848:703,Testability,test,test,703,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2848:772,Testability,test,testa,772,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2848:800,Testability,test,testa,800,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463). using commit 39c988c2dc7f669306d246a2191d16af50496640. (using our test resources). ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam; ```. compare to gatk3.4.46. ```; java -jar /Users/akiezun/projects/GATK3.4-46/GenomeAnalysisTK.jar -T HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --bamOutput testa.gatk3.outHC.bam --out testa.gatk3.g.vcf; ```. see the first line in the output:. ```; 20 1 . N <NON_REF> . . END=9999901 GT:DP:GQ:MIN_DP:PL 0/0:0:0:0:0,0,0; ```. it's missing from GATK4 output. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215182886). This is probably because there is no coverage at all in the interval 1-9999901. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215183626). why is it different and is it OK?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/463#issuecomment-215184942). I'll need to have a look at `GenotypeGVCFs` to determine whether it's ok, but certainly we are not losing any information by leaving out this block (since it's backed by zero reads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2848
https://github.com/broadinstitute/gatk/issues/2849:208,Testability,test,test,208,@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/466). tag 39c988c2dc7f669306d246a2191d16af50496640; ```./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam. ```. the created bam file does not have an index created; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2849
https://github.com/broadinstitute/gatk/issues/2849:313,Testability,test,test,313,@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/466). tag 39c988c2dc7f669306d246a2191d16af50496640; ```./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam. ```. the created bam file does not have an index created; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2849
https://github.com/broadinstitute/gatk/issues/2849:363,Testability,test,testa,363,@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/466). tag 39c988c2dc7f669306d246a2191d16af50496640; ```./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam. ```. the created bam file does not have an index created; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2849
https://github.com/broadinstitute/gatk/issues/2849:397,Testability,test,testa,397,@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/466). tag 39c988c2dc7f669306d246a2191d16af50496640; ```./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.g.vcf -ERC GVCF --bamOutput testa.outHC.bam. ```. the created bam file does not have an index created; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2849
https://github.com/broadinstitute/gatk/issues/2850:3715,Deployability,update,update,3715,"ted on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219239). I'm still looking for the smoking gun where a query fails using a .crai, but I haven't found one yet; but the BAMIndex metadata is cerrtainly wrong after conversion from .crai. If we do decide to turn off .crai, we should do it in htsjdk. To make a .bai, just use GATK PrintReads to create the .cram. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220550). still super slow using .bai : 3:51 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220983). @akiezun Can you try increasing the -Xmx value to something ridiculous (like 32G) just to eliminate memory usage as a variable here?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221087). (and run on a machine with large memory like gsa6). ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221410). Let's collect problems first, then (tomorrow maybe) go over those discovered and make a list of showstoppers for alpha1. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469203). This and https://github.com/broadinstitute/gatk/issues/1787 imply that there might have been a CRAM performance regression in htsjdk recently -- we should test with a bunch of GATK revisions from before each successive htsjdk update to see if there was one that killed CRAM performance. I don't recall seeing a big BAM vs. CRAM performance difference when we first hooked up CRAM support to GATK... ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469348). @cmnbroad Could you have a look at this when you have time?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:2910,Modifiability,variab,variable,2910,"95). @akiezun I defer to @cmnbroad on whether the issues with `crai` are bad enough to warrant such an approach. . You should be able to make a `bai` on the cram by running GATK `PrintReads` on it. ---. @cmnbroad commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219239). I'm still looking for the smoking gun where a query fails using a .crai, but I haven't found one yet; but the BAMIndex metadata is cerrtainly wrong after conversion from .crai. If we do decide to turn off .crai, we should do it in htsjdk. To make a .bai, just use GATK PrintReads to create the .cram. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220550). still super slow using .bai : 3:51 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220983). @akiezun Can you try increasing the -Xmx value to something ridiculous (like 32G) just to eliminate memory usage as a variable here?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221087). (and run on a machine with large memory like gsa6). ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221410). Let's collect problems first, then (tomorrow maybe) go over those discovered and make a list of showstoppers for alpha1. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469203). This and https://github.com/broadinstitute/gatk/issues/1787 imply that there might have been a CRAM performance regression in htsjdk recently -- we should test with a bunch of GATK revisions from before each successive htsjdk update to see if there was one that killed CRAM performance. I don't recall seeing a big BAM vs. CRAM performance difference when we first hook",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:3589,Performance,perform,performance,3589,"ted on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219239). I'm still looking for the smoking gun where a query fails using a .crai, but I haven't found one yet; but the BAMIndex metadata is cerrtainly wrong after conversion from .crai. If we do decide to turn off .crai, we should do it in htsjdk. To make a .bai, just use GATK PrintReads to create the .cram. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220550). still super slow using .bai : 3:51 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220983). @akiezun Can you try increasing the -Xmx value to something ridiculous (like 32G) just to eliminate memory usage as a variable here?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221087). (and run on a machine with large memory like gsa6). ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221410). Let's collect problems first, then (tomorrow maybe) go over those discovered and make a list of showstoppers for alpha1. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469203). This and https://github.com/broadinstitute/gatk/issues/1787 imply that there might have been a CRAM performance regression in htsjdk recently -- we should test with a bunch of GATK revisions from before each successive htsjdk update to see if there was one that killed CRAM performance. I don't recall seeing a big BAM vs. CRAM performance difference when we first hooked up CRAM support to GATK... ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469348). @cmnbroad Could you have a look at this when you have time?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:3763,Performance,perform,performance,3763,"ted on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219239). I'm still looking for the smoking gun where a query fails using a .crai, but I haven't found one yet; but the BAMIndex metadata is cerrtainly wrong after conversion from .crai. If we do decide to turn off .crai, we should do it in htsjdk. To make a .bai, just use GATK PrintReads to create the .cram. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220550). still super slow using .bai : 3:51 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220983). @akiezun Can you try increasing the -Xmx value to something ridiculous (like 32G) just to eliminate memory usage as a variable here?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221087). (and run on a machine with large memory like gsa6). ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221410). Let's collect problems first, then (tomorrow maybe) go over those discovered and make a list of showstoppers for alpha1. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469203). This and https://github.com/broadinstitute/gatk/issues/1787 imply that there might have been a CRAM performance regression in htsjdk recently -- we should test with a bunch of GATK revisions from before each successive htsjdk update to see if there was one that killed CRAM performance. I don't recall seeing a big BAM vs. CRAM performance difference when we first hooked up CRAM support to GATK... ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469348). @cmnbroad Could you have a look at this when you have time?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:3817,Performance,perform,performance,3817,"ted on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219239). I'm still looking for the smoking gun where a query fails using a .crai, but I haven't found one yet; but the BAMIndex metadata is cerrtainly wrong after conversion from .crai. If we do decide to turn off .crai, we should do it in htsjdk. To make a .bai, just use GATK PrintReads to create the .cram. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220550). still super slow using .bai : 3:51 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220983). @akiezun Can you try increasing the -Xmx value to something ridiculous (like 32G) just to eliminate memory usage as a variable here?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221087). (and run on a machine with large memory like gsa6). ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221410). Let's collect problems first, then (tomorrow maybe) go over those discovered and make a list of showstoppers for alpha1. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469203). This and https://github.com/broadinstitute/gatk/issues/1787 imply that there might have been a CRAM performance regression in htsjdk recently -- we should test with a bunch of GATK revisions from before each successive htsjdk update to see if there was one that killed CRAM performance. I don't recall seeing a big BAM vs. CRAM performance difference when we first hooked up CRAM support to GATK... ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469348). @cmnbroad Could you have a look at this when you have time?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:190,Testability,test,test,190,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467). tag 39c988c2dc7f669306d246a2191d16af50496640; 1. Make a cram file like this (in our src/test/resources/large):; `samtools view -C -T human_g1k_v37.20.21.fasta -o CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`; `samtools index CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram`; 2. run HC on it and compare with BAM:. ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.cram.g.vcf -ERC GVCF --bamOutput testa.cram.outHC.bam; ```. time BAM : 0.7 minutes; time CRAM: 3.81 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215194800). What if you run without `--bamOutput`? Is it still slow?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215202513). In addition to testing without `--bamOutput`, you should also try it with a `.bai` index on the cram, as @cmnbroad has identified issues with the current `.crai` support in htsjdk. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215214737). yes, still super slow without `--bamOutput`. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215216639). Should we blow up on crai and say it's unsupported for now? (better that than produce bogus results). Also, how do i make a bai file for a cram file?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219095). @akiezun I defer to @cmnbroad on whether the issues with `crai` are bad enough to warrant such an approach. . You should be able to make a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:507,Testability,test,test,507,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467). tag 39c988c2dc7f669306d246a2191d16af50496640; 1. Make a cram file like this (in our src/test/resources/large):; `samtools view -C -T human_g1k_v37.20.21.fasta -o CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`; `samtools index CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram`; 2. run HC on it and compare with BAM:. ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.cram.g.vcf -ERC GVCF --bamOutput testa.cram.outHC.bam; ```. time BAM : 0.7 minutes; time CRAM: 3.81 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215194800). What if you run without `--bamOutput`? Is it still slow?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215202513). In addition to testing without `--bamOutput`, you should also try it with a `.bai` index on the cram, as @cmnbroad has identified issues with the current `.crai` support in htsjdk. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215214737). yes, still super slow without `--bamOutput`. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215216639). Should we blow up on crai and say it's unsupported for now? (better that than produce bogus results). Also, how do i make a bai file for a cram file?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219095). @akiezun I defer to @cmnbroad on whether the issues with `crai` are bad enough to warrant such an approach. . You should be able to make a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:613,Testability,test,test,613,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467). tag 39c988c2dc7f669306d246a2191d16af50496640; 1. Make a cram file like this (in our src/test/resources/large):; `samtools view -C -T human_g1k_v37.20.21.fasta -o CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`; `samtools index CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram`; 2. run HC on it and compare with BAM:. ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.cram.g.vcf -ERC GVCF --bamOutput testa.cram.outHC.bam; ```. time BAM : 0.7 minutes; time CRAM: 3.81 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215194800). What if you run without `--bamOutput`? Is it still slow?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215202513). In addition to testing without `--bamOutput`, you should also try it with a `.bai` index on the cram, as @cmnbroad has identified issues with the current `.crai` support in htsjdk. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215214737). yes, still super slow without `--bamOutput`. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215216639). Should we blow up on crai and say it's unsupported for now? (better that than produce bogus results). Also, how do i make a bai file for a cram file?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219095). @akiezun I defer to @cmnbroad on whether the issues with `crai` are bad enough to warrant such an approach. . You should be able to make a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:663,Testability,test,testa,663,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467). tag 39c988c2dc7f669306d246a2191d16af50496640; 1. Make a cram file like this (in our src/test/resources/large):; `samtools view -C -T human_g1k_v37.20.21.fasta -o CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`; `samtools index CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram`; 2. run HC on it and compare with BAM:. ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.cram.g.vcf -ERC GVCF --bamOutput testa.cram.outHC.bam; ```. time BAM : 0.7 minutes; time CRAM: 3.81 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215194800). What if you run without `--bamOutput`? Is it still slow?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215202513). In addition to testing without `--bamOutput`, you should also try it with a `.bai` index on the cram, as @cmnbroad has identified issues with the current `.crai` support in htsjdk. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215214737). yes, still super slow without `--bamOutput`. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215216639). Should we blow up on crai and say it's unsupported for now? (better that than produce bogus results). Also, how do i make a bai file for a cram file?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219095). @akiezun I defer to @cmnbroad on whether the issues with `crai` are bad enough to warrant such an approach. . You should be able to make a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:702,Testability,test,testa,702,"@akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467). tag 39c988c2dc7f669306d246a2191d16af50496640; 1. Make a cram file like this (in our src/test/resources/large):; `samtools view -C -T human_g1k_v37.20.21.fasta -o CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`; `samtools index CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram`; 2. run HC on it and compare with BAM:. ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.cram.g.vcf -ERC GVCF --bamOutput testa.cram.outHC.bam; ```. time BAM : 0.7 minutes; time CRAM: 3.81 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215194800). What if you run without `--bamOutput`? Is it still slow?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215202513). In addition to testing without `--bamOutput`, you should also try it with a `.bai` index on the cram, as @cmnbroad has identified issues with the current `.crai` support in htsjdk. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215214737). yes, still super slow without `--bamOutput`. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215216639). Should we blow up on crai and say it's unsupported for now? (better that than produce bogus results). Also, how do i make a bai file for a cram file?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219095). @akiezun I defer to @cmnbroad on whether the issues with `crai` are bad enough to warrant such an approach. . You should be able to make a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:1111,Testability,test,testing,1111,"16af50496640; 1. Make a cram file like this (in our src/test/resources/large):; `samtools view -C -T human_g1k_v37.20.21.fasta -o CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`; `samtools index CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram`; 2. run HC on it and compare with BAM:. ```; ./gatk-launch HaplotypeCaller -I ~/IdeaProjects/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L 20:1-11000000 -R ~/IdeaProjects/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta -O testa.cram.g.vcf -ERC GVCF --bamOutput testa.cram.outHC.bam; ```. time BAM : 0.7 minutes; time CRAM: 3.81 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215194800). What if you run without `--bamOutput`? Is it still slow?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215202513). In addition to testing without `--bamOutput`, you should also try it with a `.bai` index on the cram, as @cmnbroad has identified issues with the current `.crai` support in htsjdk. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215214737). yes, still super slow without `--bamOutput`. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215216639). Should we blow up on crai and say it's unsupported for now? (better that than produce bogus results). Also, how do i make a bai file for a cram file?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219095). @akiezun I defer to @cmnbroad on whether the issues with `crai` are bad enough to warrant such an approach. . You should be able to make a `bai` on the cram by running GATK `PrintReads` on it. ---. @cmnbroad commented on [Wed Apr 27 2016](https://github.com/broadinstitute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2850:3644,Testability,test,test,3644,"ted on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219239). I'm still looking for the smoking gun where a query fails using a .crai, but I haven't found one yet; but the BAMIndex metadata is cerrtainly wrong after conversion from .crai. If we do decide to turn off .crai, we should do it in htsjdk. To make a .bai, just use GATK PrintReads to create the .cram. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220550). still super slow using .bai : 3:51 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220983). @akiezun Can you try increasing the -Xmx value to something ridiculous (like 32G) just to eliminate memory usage as a variable here?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221087). (and run on a machine with large memory like gsa6). ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221410). Let's collect problems first, then (tomorrow maybe) go over those discovered and make a list of showstoppers for alpha1. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469203). This and https://github.com/broadinstitute/gatk/issues/1787 imply that there might have been a CRAM performance regression in htsjdk recently -- we should test with a bunch of GATK revisions from before each successive htsjdk update to see if there was one that killed CRAM performance. I don't recall seeing a big BAM vs. CRAM performance difference when we first hooked up CRAM support to GATK... ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469348). @cmnbroad Could you have a look at this when you have time?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850
https://github.com/broadinstitute/gatk/issues/2851:6505,Availability,down,down,6505,"e from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since options 3 and 4 seem to be the only options with votes, let's sit down next week and discuss in detail the pain points of these two options, and make a choice between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:194,Deployability,release,released,194,"@droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473). This capability would be useful if it turns out that the CNV tools (for example) need to be released much more frequently than the GATK as a whole. We don't want a release of the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:266,Deployability,release,release,266,"@droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473). This capability would be useful if it turns out that the CNV tools (for example) need to be released much more frequently than the GATK as a whole. We don't want a release of the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:405,Deployability,release,release,405,"@droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473). This capability would be useful if it turns out that the CNV tools (for example) need to be released much more frequently than the GATK as a whole. We don't want a release of the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:700,Deployability,release,release,700,"@droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473). This capability would be useful if it turns out that the CNV tools (for example) need to be released much more frequently than the GATK as a whole. We don't want a release of the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1146,Deployability,release,release,1146,"f the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1274,Deployability,release,release,1274,"f the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1341,Deployability,release,release,1341,"f the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1375,Deployability,release,release,1375,"f the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1601,Deployability,release,releases,1601,"ample. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by proj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1675,Deployability,release,released,1675," even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing peop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1735,Deployability,release,released,1735," even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing peop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1807,Deployability,release,releases,1807," even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing peop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:2127,Deployability,release,release,2127,"3#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:2241,Deployability,release,release,2241,"3#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:3195,Deployability,release,release,3195,"elease but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is cur",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:3446,Deployability,release,release,3446,"jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:3745,Deployability,release,release-ready,3745,"hes, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:3838,Deployability,release,release,3838,"s going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:3883,Deployability,release,release,3883,"s stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:4292,Deployability,release,release,4292,"peCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:4374,Deployability,release,release,4374," to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:4449,Deployability,release,release,4449," to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:4511,Deployability,release,release,4511," to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:4605,Deployability,release,release,4605,"e a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:4903,Deployability,release,release,4903,"y, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](ht",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:5405,Deployability,release,release,5405,"tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since opt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:5454,Deployability,release,releases,5454,"tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since opt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:5523,Deployability,release,release,5523,"tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since opt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:6032,Deployability,release,release,6032,"e from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since options 3 and 4 seem to be the only options with votes, let's sit down next week and discuss in detail the pain points of these two options, and make a choice between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:6076,Deployability,release,released,6076,"e from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since options 3 and 4 seem to be the only options with votes, let's sit down next week and discuss in detail the pain points of these two options, and make a choice between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:6332,Deployability,release,release,6332,"e from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since options 3 and 4 seem to be the only options with votes, let's sit down next week and discuss in detail the pain points of these two options, and make a choice between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:6040,Energy Efficiency,schedul,schedule,6040,"e from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since options 3 and 4 seem to be the only options with votes, let's sit down next week and discuss in detail the pain points of these two options, and make a choice between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:492,Security,expose,exposed,492,"@droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473). This capability would be useful if it turns out that the CNV tools (for example) need to be released much more frequently than the GATK as a whole. We don't want a release of the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:561,Security,expose,exposes,561,"@droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473). This capability would be useful if it turns out that the CNV tools (for example) need to be released much more frequently than the GATK as a whole. We don't want a release of the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:3486,Security,expose,exposed,3486,"jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:4077,Testability,test,testing,4077,"s stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:1953,Usability,clear,clear,1953,"3#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:4692,Usability,clear,clearer,4692,"ps on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2851:5971,Usability,simpl,simply,5971,"e from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since options 3 and 4 seem to be the only options with votes, let's sit down next week and discuss in detail the pain points of these two options, and make a choice between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851
https://github.com/broadinstitute/gatk/issues/2853:5693,Availability,avail,available,5693,"by IGV by removing or reordering columns, but I don't think this is unreasonable. (I think this is preferable to outputting additional 4-column segment files specifically for use with IGV, right?). ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275259864). Started a branch. Will have to cook up some new test data but should hopefully be relatively quick. Note that we will lose the dotted centromere indicators unless we require their locations as an additional input. ---. @sooheelee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275262070). I think IGV's default heatmap coloring is centered around 0 or 1, whichever CNV data isn't. As for the centromere locations, I'm not sure but perhaps [this format](http://software.broadinstitute.org/software/igv/Cytoband) can help define those for people who want to define them. I'd have to do some digging through UCSC Golden paths to see what is commonly available. ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275279714). According to the page linked above, users should be able to set data range and log/linear scale in IGV?. ---. @samuelklee commented on [Fri Jan 27 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275760865). @LeeTL1220 @achevali I would like to get rid of the per-segment ACNV plotting, unless there are any strong objections. @dlivitz Do you guys find this functionality useful?. ---. @LeeTL1220 commented on [Fri Jan 27 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275761180). Whatever you like. On Fri, Jan 27, 2017 at 3:04 PM, samuelklee <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> @achevali; > <https://github.com/achevali> I would like to get rid of the per-segment; > ACNV plotting, unless there are any strong obje",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2853
https://github.com/broadinstitute/gatk/issues/2853:4518,Integrability,Depend,Depending,4518,"e it would be nice to have the option for IGV compatibility, but I don't think the MAF format is supported by the engine nor do I know if there are plans to implement support. Perhaps @LeeTL1220 or @davidbenjamin can comment. ---. @sooheelee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275250701). To clarify, the CNV callset should have IGV compatibility where it displays as a heatmap (like GISTIC outputs in IGV). To this folks can overlay whatever mutation data they have, whether that be in MAF or VCF format. . ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275254706). Ah, gotcha. In that case it should already be relatively easy for users to create IGV-compatible output according to http://software.broadinstitute.org/software/igv/SegmentedData. Depending on which tool output they are trying to plot (CNV or ACNV), they may have to manually create files with the column order expected by IGV by removing or reordering columns, but I don't think this is unreasonable. (I think this is preferable to outputting additional 4-column segment files specifically for use with IGV, right?). ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275259864). Started a branch. Will have to cook up some new test data but should hopefully be relatively quick. Note that we will lose the dotted centromere indicators unless we require their locations as an additional input. ---. @sooheelee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275262070). I think IGV's default heatmap coloring is centered around 0 or 1, whichever CNV data isn't. As for the centromere locations, I'm not sure but perhaps [this format](http://software.broadinstitute.org/software/igv/Cytoband) can help define those for people who want to define them. I'd have to d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2853
https://github.com/broadinstitute/gatk/issues/2853:5037,Testability,test,test,5037,"re it displays as a heatmap (like GISTIC outputs in IGV). To this folks can overlay whatever mutation data they have, whether that be in MAF or VCF format. . ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275254706). Ah, gotcha. In that case it should already be relatively easy for users to create IGV-compatible output according to http://software.broadinstitute.org/software/igv/SegmentedData. Depending on which tool output they are trying to plot (CNV or ACNV), they may have to manually create files with the column order expected by IGV by removing or reordering columns, but I don't think this is unreasonable. (I think this is preferable to outputting additional 4-column segment files specifically for use with IGV, right?). ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275259864). Started a branch. Will have to cook up some new test data but should hopefully be relatively quick. Note that we will lose the dotted centromere indicators unless we require their locations as an additional input. ---. @sooheelee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275262070). I think IGV's default heatmap coloring is centered around 0 or 1, whichever CNV data isn't. As for the centromere locations, I'm not sure but perhaps [this format](http://software.broadinstitute.org/software/igv/Cytoband) can help define those for people who want to define them. I'd have to do some digging through UCSC Golden paths to see what is commonly available. ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275279714). According to the page linked above, users should be able to set data range and log/linear scale in IGV?. ---. @samuelklee commented on [Fri Jan 27 2017](https://github.com/broadinstitute/gatk-protected/issues/4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2853
https://github.com/broadinstitute/gatk/issues/2853:5916,Testability,log,log,5916,"ed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275259864). Started a branch. Will have to cook up some new test data but should hopefully be relatively quick. Note that we will lose the dotted centromere indicators unless we require their locations as an additional input. ---. @sooheelee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275262070). I think IGV's default heatmap coloring is centered around 0 or 1, whichever CNV data isn't. As for the centromere locations, I'm not sure but perhaps [this format](http://software.broadinstitute.org/software/igv/Cytoband) can help define those for people who want to define them. I'd have to do some digging through UCSC Golden paths to see what is commonly available. ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275279714). According to the page linked above, users should be able to set data range and log/linear scale in IGV?. ---. @samuelklee commented on [Fri Jan 27 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275760865). @LeeTL1220 @achevali I would like to get rid of the per-segment ACNV plotting, unless there are any strong objections. @dlivitz Do you guys find this functionality useful?. ---. @LeeTL1220 commented on [Fri Jan 27 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275761180). Whatever you like. On Fri, Jan 27, 2017 at 3:04 PM, samuelklee <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> @achevali; > <https://github.com/achevali> I would like to get rid of the per-segment; > ACNV plotting, unless there are any strong objections. @dlivitz; > <https://github.com/dlivitz> Do you guys find this functionality useful?; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2853
https://github.com/broadinstitute/gatk/issues/2853:947,Usability,simpl,simple,947,"://github.com/broadinstitute/gatk-protected/issues/495). Please see https://github.com/broadinstitute/gatk-protected/issues/224 for additional information and proposed solutions. We would like to have interactive plots generated for ACNV outputs. In the past, we would use IGV, but this is too inflexible. ---. @samuelklee commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-254863042). MAF-CR plots would also be nice. Copied from https://github.com/broadinstitute/gatk-protected/issues/224:. We also discussed possibly having some interactive plots in the future. I think that checking out packages like plotly (https://plot.ly/r/) would be a good start. The ultimate goal would be to build some sort of dashboard (maybe using shiny, http://shiny.rstudio.com/) that takes in seg files from CNV/ACNV/etc. and generates several plots at once. Even simple things like being able to interactively select which chromosomes/segments to plot, having the ability to zoom, or hover-highlighting segments would make the results much easier to parse and interpret. ---. @sooheelee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275237784). My recommendation is that some part of the output data be compatible with IGV at the very least. Take a look at the mutation overlay feature [on this page](http://software.broadinstitute.org/software/igv/MutationData) that allows users to overlay MAF mutations onto expression data. This could easily be mutation data over CNV heatmap data. . Any additional plotting feature would be cherries on top. Also, we have three related github issues. Perhaps consolidate your efforts so as not to duplicate them?; - https://github.com/broadinstitute/gatk-protected/issues/726; - https://github.com/broadinstitute/gatk-protected/issues/686; - https://github.com/broadinstitute/gatk-protected/issues/495. Finally, the b37-only compatibility is not acceptable in my opini",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2853
https://github.com/broadinstitute/gatk/issues/2855:233,Availability,down,downstream,233,"@samuelklee commented on [Fri May 06 2016](https://github.com/broadinstitute/gatk-protected/issues/498). Specifically, a scatter plot of ACNV segments in 2D MAF-CR space + histograms for their 1D projections. This will be useful for downstream tools. ---. @samuelklee commented on [Wed May 11 2016](https://github.com/broadinstitute/gatk-protected/issues/498#issuecomment-218528591). as an example:; ![maf-cr-purity-deciles](https://cloud.githubusercontent.com/assets/11076296/15189893/24556cd6-177b-11e6-8374-78d9a9b0e9a6.png). ---. @samuelklee commented on [Wed May 11 2016](https://github.com/broadinstitute/gatk-protected/issues/498#issuecomment-218528764). Would be great to have this per chromosome as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2855
https://github.com/broadinstitute/gatk/issues/2856:235,Performance,perform,performance,235,"@LeeTL1220 commented on [Tue May 17 2016](https://github.com/broadinstitute/gatk-protected/issues/508). Currently, the tumor only het pulldown assumes purity of < 1.0. However, cell lines do have a purity of 1, so we would expect poor performance in some regions. ---. @samuelklee commented on [Thu Aug 18 2016](https://github.com/broadinstitute/gatk-protected/issues/508#issuecomment-240759250). @LeeTL1220 @mbabadi can you close if appropriate?. ---. @mbabadi commented on [Thu Aug 18 2016](https://github.com/broadinstitute/gatk-protected/issues/508#issuecomment-240890316). We haven't addressed this issue yet; we haven't thought much about it; either. I'd say leave it open until we decide whether or not we want to do; something about it. @LeeTL1220 what do you think?. On Thu, Aug 18, 2016 at 11:26 AM, samuelklee notifications@github.com; wrote:. > @LeeTL1220 https://github.com/LeeTL1220 @mbabadi; > https://github.com/mbabadi can you close if appropriate?; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk-protected/issues/508#issuecomment-240759250,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AOmMjWPbLQa_lDeVIwY-bP7_9lmaXLRVks5qhHmRgaJpZM4IgpaN; > .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2856
https://github.com/broadinstitute/gatk/issues/2858:296,Performance,perform,performance,296,"@LeeTL1220 commented on [Mon May 23 2016](https://github.com/broadinstitute/gatk-protected/issues/524). - [ ] File issues for evaluations (and make sure each evaluation is described, including ground truth). Make sure that there is a milestone and assignee for each. Need to show some measure of performance for the WGS evaluation (GATK CNV only. Not ACNV)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858
https://github.com/broadinstitute/gatk/issues/2860:2634,Availability,down,down,2634,"posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it might be an easy win. But we should perhaps profile more carefully. However, I agree that changing our segmentation is more pressing! Note that oversegmentation (typically 1000+ segments) hurts us by both by increasing the number of MAF parameters and by increasing the number of similar-segment merge iterations required to smooth things (looks like the WGS samples hit the limit of 25 merge iterations = ~25 hrs). Turning off refitting between iterations helps, perhaps at the cost of smoothness of the final result, but you're still looking at 2+ hours for the initial and final fit. Just to note, other possibilities for cutting down the runtime include trimming down the number of hets for WGS, changing similar-segment merging so that we can locally refit only the MAF for the newly created segment, ditching MCMC, etc. @LeeTL1220 can you make the plots for your WGS runs so we can see what these hets look like?. ---. @davidbenjamin commented on [Thu Jun 09 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-225074377). Ahhhhh, I get it!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:2668,Availability,down,down,2668,"posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it might be an easy win. But we should perhaps profile more carefully. However, I agree that changing our segmentation is more pressing! Note that oversegmentation (typically 1000+ segments) hurts us by both by increasing the number of MAF parameters and by increasing the number of similar-segment merge iterations required to smooth things (looks like the WGS samples hit the limit of 25 merge iterations = ~25 hrs). Turning off refitting between iterations helps, perhaps at the cost of smoothness of the final result, but you're still looking at 2+ hours for the initial and final fit. Just to note, other possibilities for cutting down the runtime include trimming down the number of hets for WGS, changing similar-segment merging so that we can locally refit only the MAF for the newly created segment, ditching MCMC, etc. @LeeTL1220 can you make the plots for your WGS runs so we can see what these hets look like?. ---. @davidbenjamin commented on [Thu Jun 09 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-225074377). Ahhhhh, I get it!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:440,Performance,perform,performance,440,"@davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542). Calculating log gammas is an expensive part of the allele fraction model. We could speed this with negligible loss of accuracy by caching a few tens of thousands of values from 0 to 100 or 1000 and using linear interpolation. This issue can be closed by implementing such caching or by showing that it doesn't significantly improve performance. ---. @davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224329788). The JVM running on my laptop does 10 million log gammas per second, which is about three times as expensive as logarithms. The allele fraction model needs to calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:120,Testability,log,log,120,"@davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542). Calculating log gammas is an expensive part of the allele fraction model. We could speed this with negligible loss of accuracy by caching a few tens of thousands of values from 0 to 100 or 1000 and using linear interpolation. This issue can be closed by implementing such caching or by showing that it doesn't significantly improve performance. ---. @davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224329788). The JVM running on my laptop does 10 million log gammas per second, which is about three times as expensive as logarithms. The allele fraction model needs to calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:634,Testability,log,log,634,"@davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542). Calculating log gammas is an expensive part of the allele fraction model. We could speed this with negligible loss of accuracy by caching a few tens of thousands of values from 0 to 100 or 1000 and using linear interpolation. This issue can be closed by implementing such caching or by showing that it doesn't significantly improve performance. ---. @davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224329788). The JVM running on my laptop does 10 million log gammas per second, which is about three times as expensive as logarithms. The allele fraction model needs to calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:700,Testability,log,logarithms,700,"@davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542). Calculating log gammas is an expensive part of the allele fraction model. We could speed this with negligible loss of accuracy by caching a few tens of thousands of values from 0 to 100 or 1000 and using linear interpolation. This issue can be closed by implementing such caching or by showing that it doesn't significantly improve performance. ---. @davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224329788). The JVM running on my laptop does 10 million log gammas per second, which is about three times as expensive as logarithms. The allele fraction model needs to calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:759,Testability,log,log,759,"@davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542). Calculating log gammas is an expensive part of the allele fraction model. We could speed this with negligible loss of accuracy by caching a few tens of thousands of values from 0 to 100 or 1000 and using linear interpolation. This issue can be closed by implementing such caching or by showing that it doesn't significantly improve performance. ---. @davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224329788). The JVM running on my laptop does 10 million log gammas per second, which is about three times as expensive as logarithms. The allele fraction model needs to calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:814,Testability,log,log,814,"@davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542). Calculating log gammas is an expensive part of the allele fraction model. We could speed this with negligible loss of accuracy by caching a few tens of thousands of values from 0 to 100 or 1000 and using linear interpolation. This issue can be closed by implementing such caching or by showing that it doesn't significantly improve performance. ---. @davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224329788). The JVM running on my laptop does 10 million log gammas per second, which is about three times as expensive as logarithms. The allele fraction model needs to calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:1037,Testability,log,log,1037,"@davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542). Calculating log gammas is an expensive part of the allele fraction model. We could speed this with negligible loss of accuracy by caching a few tens of thousands of values from 0 to 100 or 1000 and using linear interpolation. This issue can be closed by implementing such caching or by showing that it doesn't significantly improve performance. ---. @davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224329788). The JVM running on my laptop does 10 million log gammas per second, which is about three times as expensive as logarithms. The allele fraction model needs to calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:1653,Testability,log,logs,1653," calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it might be an easy win. But we should perhaps profile more carefully. However, I agree that changing our segmentation is more pressing! Note that oversegmentation (typically 1000+ segments) hurts us by both by increasing the number of MAF parameters and by increasing the number of similar-segment merge iterations required to smooth things (looks like the WGS samples hit the limit of 25 merge iterations = ~25 hrs). Turning off refitting between iterations helps, perhaps at the cost of smoothness of the final result, but you're still looking at 2+ hours for the initial and final fit. Just to note, other possibilities for cutting down the runtime include trimming down the number of hets for WGS, changing similar-segment merging so that we ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:1897,Testability,test,tests,1897,"er probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it might be an easy win. But we should perhaps profile more carefully. However, I agree that changing our segmentation is more pressing! Note that oversegmentation (typically 1000+ segments) hurts us by both by increasing the number of MAF parameters and by increasing the number of similar-segment merge iterations required to smooth things (looks like the WGS samples hit the limit of 25 merge iterations = ~25 hrs). Turning off refitting between iterations helps, perhaps at the cost of smoothness of the final result, but you're still looking at 2+ hours for the initial and final fit. Just to note, other possibilities for cutting down the runtime include trimming down the number of hets for WGS, changing similar-segment merging so that we can locally refit only the MAF for the newly created segment, ditching MCMC, etc. @LeeTL1220 can you make the plots for your WGS runs so we can see what these hets look like?. ---. @davidbenj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2860:1949,Testability,log,log,1949,"er probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it might be an easy win. But we should perhaps profile more carefully. However, I agree that changing our segmentation is more pressing! Note that oversegmentation (typically 1000+ segments) hurts us by both by increasing the number of MAF parameters and by increasing the number of similar-segment merge iterations required to smooth things (looks like the WGS samples hit the limit of 25 merge iterations = ~25 hrs). Turning off refitting between iterations helps, perhaps at the cost of smoothness of the final result, but you're still looking at 2+ hours for the initial and final fit. Just to note, other possibilities for cutting down the runtime include trimming down the number of hets for WGS, changing similar-segment merging so that we can locally refit only the MAF for the newly created segment, ditching MCMC, etc. @LeeTL1220 can you make the plots for your WGS runs so we can see what these hets look like?. ---. @davidbenj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860
https://github.com/broadinstitute/gatk/issues/2863:302,Usability,Simpl,Simply,302,"@LeeTL1220 commented on [Mon Jun 20 2016](https://github.com/broadinstitute/gatk-protected/issues/573). With a small sample size, I may have found (using different samples) that the introduction of the index may have slowed the runtime of `GetBayesianHetCoverage`. We should do a more formal analysis? Simply look at the the runtime of old runs vs. new runs. See issue #561 . On the gsa5 Broad machine (i.e. using NFS), w/ the original index code (pre #561 ), on _capture_ samples:. ```; /dsde/working/lichtens/acnv/out_case_amaro_cll_pd250/pulldown$ egrep minute *.out; ...snip....; GCLL-0008-N-01.hets.tsv.out:[May 24, 2016 9:42:29 PM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed time: 39.78 minutes.; GCLL-0009-N-01.hets.tsv.out:[May 24, 2016 10:27:37 AM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed time: 24.24 minutes.; GCLL-0010-N-01.hets.tsv.out:[May 24, 2016 10:27:31 PM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed time: 84.80 minutes.; GCLL-0011-N-01.hets.tsv.out:[May 24, 2016 1:21:14 PM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed time: 197.76 minutes.; GCLL-0012-N-01.hets.tsv.out:[May 24, 2016 9:22:43 PM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed time: 20.00 minutes.; GCLL-0013-N-01.hets.tsv.out:[May 24, 2016 1:18:46 PM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed time: 195.01 minutes.; GCLL-0014-N-01.hets.tsv.out:[May 24, 2016 2:06:56 PM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed time: 243.19 minutes.; GCLL-0015-N-01.hets.tsv.out:[May 24, 2016 9:28:56 PM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed time: 26.22 minutes.; GCLL-0016-N-01.hets.tsv.out:[May 24, 2016 10:13:56 AM EDT] org.broadinstitute.hellbender.tools.exome.GetBayesianHetCoverage done. Elapsed ti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2863
https://github.com/broadinstitute/gatk/issues/2865:105,Deployability,Update,Update,105,@lbergelson commented on [Tue Jun 21 2016](https://github.com/broadinstitute/gatk-protected/issues/580). Update genotype gvcfs so includeNonVariantSites can be enabled. This may be easiest by implementing a different walker type to match the old LocusWalker behavoir but could also be done by accumulating sites as it goes. ---. @lbergelson commented on [Fri Apr 07 2017](https://github.com/broadinstitute/gatk-protected/issues/580#issuecomment-292661735). this is blocked by broadinstitute/gatk#2429,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2865
https://github.com/broadinstitute/gatk/issues/2868:126,Availability,error,error,126,@personalis commented on [Thu Jun 23 2016](https://github.com/broadinstitute/gatk-protected/issues/587). We got the following error when running gatk-launch FastqToSam:. java.lang.IllegalArgumentException: Self-suppression not permitted; at java.lang.Throwable.addSuppressed(Throwable.java:1043); at org.broadinstitute.hellbender.tools.picard.sam.FastqToSam.doWork(FastqToSam.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); Caused by: java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.l,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:2685,Availability,error,error,2685,".java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately Adam moved on to a different job so he's longer working on GATK. . I believe this is the same problem as https://github.com/broadinstitute/gatk/issues/2026 and has been patched in gatk public with https://github.com/broadinstitute/gatk/pull/2028. You might try using FastqToSam in the public repo, or wait and try a new version of protected that incorporates an updated gatk public (coming soon..)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:2145,Deployability,install,install,2145,"jdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:3307,Deployability,patch,patched,3307,".java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately Adam moved on to a different job so he's longer working on GATK. . I believe this is the same problem as https://github.com/broadinstitute/gatk/issues/2026 and has been patched in gatk public with https://github.com/broadinstitute/gatk/pull/2028. You might try using FastqToSam in the public repo, or wait and try a new version of protected that incorporates an updated gatk public (coming soon..)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:3500,Deployability,update,updated,3500,".java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately Adam moved on to a different job so he's longer working on GATK. . I believe this is the same problem as https://github.com/broadinstitute/gatk/issues/2026 and has been patched in gatk public with https://github.com/broadinstitute/gatk/pull/2028. You might try using FastqToSam in the public repo, or wait and try a new version of protected that incorporates an updated gatk public (coming soon..)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:1174,Integrability,wrap,wrapTempOutputStream,1174,"ntException: Self-suppression not permitted; at java.lang.Throwable.addSuppressed(Throwable.java:1043); at org.broadinstitute.hellbender.tools.picard.sam.FastqToSam.doWork(FastqToSam.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); Caused by: java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:900,Performance,Load,LoadSnappy,900,@personalis commented on [Thu Jun 23 2016](https://github.com/broadinstitute/gatk-protected/issues/587). We got the following error when running gatk-launch FastqToSam:. java.lang.IllegalArgumentException: Self-suppression not permitted; at java.lang.Throwable.addSuppressed(Throwable.java:1043); at org.broadinstitute.hellbender.tools.picard.sam.FastqToSam.doWork(FastqToSam.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); Caused by: java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.l,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:1831,Performance,Load,LoadSnappy,1831,"used by: java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:1931,Performance,load,loadClass,1931,"Loader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX tru",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:2000,Performance,load,loadClass,2000,"Loader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:2055,Performance,load,loadClass,2055,"til.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:2210,Performance,Load,LoadSnappy,2210,"mFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately Adam moved on to a different job so he's longer working on GATK. . ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:2780,Testability,test,test,2780,".java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately Adam moved on to a different job so he's longer working on GATK. . I believe this is the same problem as https://github.com/broadinstitute/gatk/issues/2026 and has been patched in gatk public with https://github.com/broadinstitute/gatk/pull/2028. You might try using FastqToSam in the public repo, or wait and try a new version of protected that incorporates an updated gatk public (coming soon..)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2868:2807,Testability,test,test,2807,".java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately Adam moved on to a different job so he's longer working on GATK. . I believe this is the same problem as https://github.com/broadinstitute/gatk/issues/2026 and has been patched in gatk public with https://github.com/broadinstitute/gatk/pull/2028. You might try using FastqToSam in the public repo, or wait and try a new version of protected that incorporates an updated gatk public (coming soon..)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868
https://github.com/broadinstitute/gatk/issues/2870:2149,Testability,test,tests,2149,"the port? It will require both gatk and gatk-protected changes. ---. @samuelklee commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-296749077). If you don't mind, @ronlevine, please go ahead---I probably won't get around to it for a bit. Not high priority though, I was just going through and cleaning up issues. ---. @ronlevine commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-296750806). Will do. ---. @ronlevine commented on [Wed Apr 26 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-297488855). Closing since https://github.com/broadinstitute/gatk/pull/2619 is merged. ---. @ronlevine commented on [Wed Apr 26 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-297566792). Reopening since need to do the protected part. ---. @samuelklee commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300044005). Also note that https://github.com/broadinstitute/gsa-unstable/pull/1451 should be included in this port. ---. @ronlevine commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300047651). The HaplotypeCaller tests that compare the output GVCFs between the current code and GATK 3.5 will not pass. This is due the block merging fix. So, I generated output files with with a version of GATK 3 that has this fix, 3.7-42-gbe6a37c. If it's premature for these tests, they can be disabled. ---. @droazen commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300049046). Please don't either disable these tests or move them to GATK 3.7 outputs just yet -- this would be disruptive to my current efforts to figure out what's behind some annotation differences in the GATK4 HaplotypeCaller. I do plan to migrate the tests to 3.7 outputs soon, but I need to study the diffs a bit first.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2870
https://github.com/broadinstitute/gatk/issues/2870:2396,Testability,test,tests,2396,"the port? It will require both gatk and gatk-protected changes. ---. @samuelklee commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-296749077). If you don't mind, @ronlevine, please go ahead---I probably won't get around to it for a bit. Not high priority though, I was just going through and cleaning up issues. ---. @ronlevine commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-296750806). Will do. ---. @ronlevine commented on [Wed Apr 26 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-297488855). Closing since https://github.com/broadinstitute/gatk/pull/2619 is merged. ---. @ronlevine commented on [Wed Apr 26 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-297566792). Reopening since need to do the protected part. ---. @samuelklee commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300044005). Also note that https://github.com/broadinstitute/gsa-unstable/pull/1451 should be included in this port. ---. @ronlevine commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300047651). The HaplotypeCaller tests that compare the output GVCFs between the current code and GATK 3.5 will not pass. This is due the block merging fix. So, I generated output files with with a version of GATK 3 that has this fix, 3.7-42-gbe6a37c. If it's premature for these tests, they can be disabled. ---. @droazen commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300049046). Please don't either disable these tests or move them to GATK 3.7 outputs just yet -- this would be disruptive to my current efforts to figure out what's behind some annotation differences in the GATK4 HaplotypeCaller. I do plan to migrate the tests to 3.7 outputs soon, but I need to study the diffs a bit first.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2870
https://github.com/broadinstitute/gatk/issues/2870:2589,Testability,test,tests,2589,"the port? It will require both gatk and gatk-protected changes. ---. @samuelklee commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-296749077). If you don't mind, @ronlevine, please go ahead---I probably won't get around to it for a bit. Not high priority though, I was just going through and cleaning up issues. ---. @ronlevine commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-296750806). Will do. ---. @ronlevine commented on [Wed Apr 26 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-297488855). Closing since https://github.com/broadinstitute/gatk/pull/2619 is merged. ---. @ronlevine commented on [Wed Apr 26 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-297566792). Reopening since need to do the protected part. ---. @samuelklee commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300044005). Also note that https://github.com/broadinstitute/gsa-unstable/pull/1451 should be included in this port. ---. @ronlevine commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300047651). The HaplotypeCaller tests that compare the output GVCFs between the current code and GATK 3.5 will not pass. This is due the block merging fix. So, I generated output files with with a version of GATK 3 that has this fix, 3.7-42-gbe6a37c. If it's premature for these tests, they can be disabled. ---. @droazen commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300049046). Please don't either disable these tests or move them to GATK 3.7 outputs just yet -- this would be disruptive to my current efforts to figure out what's behind some annotation differences in the GATK4 HaplotypeCaller. I do plan to migrate the tests to 3.7 outputs soon, but I need to study the diffs a bit first.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2870
https://github.com/broadinstitute/gatk/issues/2870:2798,Testability,test,tests,2798,"the port? It will require both gatk and gatk-protected changes. ---. @samuelklee commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-296749077). If you don't mind, @ronlevine, please go ahead---I probably won't get around to it for a bit. Not high priority though, I was just going through and cleaning up issues. ---. @ronlevine commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-296750806). Will do. ---. @ronlevine commented on [Wed Apr 26 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-297488855). Closing since https://github.com/broadinstitute/gatk/pull/2619 is merged. ---. @ronlevine commented on [Wed Apr 26 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-297566792). Reopening since need to do the protected part. ---. @samuelklee commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300044005). Also note that https://github.com/broadinstitute/gsa-unstable/pull/1451 should be included in this port. ---. @ronlevine commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300047651). The HaplotypeCaller tests that compare the output GVCFs between the current code and GATK 3.5 will not pass. This is due the block merging fix. So, I generated output files with with a version of GATK 3 that has this fix, 3.7-42-gbe6a37c. If it's premature for these tests, they can be disabled. ---. @droazen commented on [Mon May 08 2017](https://github.com/broadinstitute/gatk-protected/issues/602#issuecomment-300049046). Please don't either disable these tests or move them to GATK 3.7 outputs just yet -- this would be disruptive to my current efforts to figure out what's behind some annotation differences in the GATK4 HaplotypeCaller. I do plan to migrate the tests to 3.7 outputs soon, but I need to study the diffs a bit first.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2870
https://github.com/broadinstitute/gatk/issues/2875:258,Safety,avoid,avoid,258,@lbergelson commented on [Wed Jul 27 2016](https://github.com/broadinstitute/gatk-protected/issues/626). I introduced a new argument collection to help share arguments between HaplotypeCaller and HaplotypeCaller spark called `ShardingArgumentCollection`. To avoid causing a difficult rebase for other people I didn't change `AssemblyRegionWalker` to use it when I introduced it in (https://github.com/broadinstitute/gatk-protected/pull/616). It should be made a top level argument collection and used in there as well as in `HaplotypeCallerSpark`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2875
https://github.com/broadinstitute/gatk/issues/2877:182,Testability,test,testChromosomesOnDifferentSegments,182,"@samuelklee commented on [Tue Aug 09 2016](https://github.com/broadinstitute/gatk-protected/issues/640). This combined with an unset random seed was causing `AlleleFractionSegmenter.testChromosomesOnDifferentSegments` to fail intermittently in #633. @davidbenjamin could you take a look when you get back? This could be fixed easily by a `Math.min` in the right place, but I don't know if the fact that the memory length seems to blow up is something generic that happens when we collapse to a single state that needs to be addressed more carefully. ---. @samuelklee commented on [Wed Aug 17 2016](https://github.com/broadinstitute/gatk-protected/issues/640#issuecomment-240523360). @davidbenjamin can you take a quick look? I think it's probably a quick fix. ---. @davidbenjamin commented on [Wed Aug 17 2016](https://github.com/broadinstitute/gatk-protected/issues/640#issuecomment-240530369). Sure.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2877
https://github.com/broadinstitute/gatk/issues/2878:2170,Energy Efficiency,schedul,scheduler,2170,"institute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.WellformedReadFilter.test(WellformedReadFilter.java:31); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.lambda$getReads$e4b35a40$1(GATKSparkTool.java:196); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. ---. @samuelklee commented on [Wed Aug 10 2016](https://github.com/broadinstitute/gatk-protected/issues/642#issuecomment-239064255). @LeeTL1220 have you seen this during your WGS runs? I got it on a few TCGA BAMs, not sure if they're just bad samples?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2878
https://github.com/broadinstitute/gatk/issues/2878:2249,Energy Efficiency,schedul,scheduler,2249,"institute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.WellformedReadFilter.test(WellformedReadFilter.java:31); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.lambda$getReads$e4b35a40$1(GATKSparkTool.java:196); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. ---. @samuelklee commented on [Wed Aug 10 2016](https://github.com/broadinstitute/gatk-protected/issues/642#issuecomment-239064255). @LeeTL1220 have you seen this during your WGS runs? I got it on a few TCGA BAMs, not sure if they're just bad samples?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2878
https://github.com/broadinstitute/gatk/issues/2878:2328,Energy Efficiency,schedul,scheduler,2328,"institute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.WellformedReadFilter.test(WellformedReadFilter.java:31); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.lambda$getReads$e4b35a40$1(GATKSparkTool.java:196); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. ---. @samuelklee commented on [Wed Aug 10 2016](https://github.com/broadinstitute/gatk-protected/issues/642#issuecomment-239064255). @LeeTL1220 have you seen this during your WGS runs? I got it on a few TCGA BAMs, not sure if they're just bad samples?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2878
https://github.com/broadinstitute/gatk/issues/2878:2450,Performance,concurren,concurrent,2450,"institute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.WellformedReadFilter.test(WellformedReadFilter.java:31); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.lambda$getReads$e4b35a40$1(GATKSparkTool.java:196); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. ---. @samuelklee commented on [Wed Aug 10 2016](https://github.com/broadinstitute/gatk-protected/issues/642#issuecomment-239064255). @LeeTL1220 have you seen this during your WGS runs? I got it on a few TCGA BAMs, not sure if they're just bad samples?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2878
https://github.com/broadinstitute/gatk/issues/2878:2534,Performance,concurren,concurrent,2534,"institute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.WellformedReadFilter.test(WellformedReadFilter.java:31); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.lambda$getReads$e4b35a40$1(GATKSparkTool.java:196); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. ---. @samuelklee commented on [Wed Aug 10 2016](https://github.com/broadinstitute/gatk-protected/issues/642#issuecomment-239064255). @LeeTL1220 have you seen this during your WGS runs? I got it on a few TCGA BAMs, not sure if they're just bad samples?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2878
https://github.com/broadinstitute/gatk/issues/2878:1266,Testability,test,test,1266,c.readTags(BinaryTagCodec.java:282); at htsjdk.samtools.BAMRecord.decodeAttributes(BAMRecord.java:313); at htsjdk.samtools.BAMRecord.getAttribute(BAMRecord.java:293); at org.broadinstitute.hellbender.utils.read.SAMRecordToGATKReadAdapter.getReadGroup(SAMRecordToGATKReadAdapter.java:319); at org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary.lambda$static$c202bd10$1(ReadFilterLibrary.java:45); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.ReadFilter.lambda$and$cbfb947b$1(ReadFilter.java:30); at org.broadinstitute.hellbender.engine.filters.WellformedReadFilter.test(WellformedReadFilter.java:31); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.lambda$getReads$e4b35a40$1(GATKSparkTool.java:196); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:189); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2878
https://github.com/broadinstitute/gatk/pull/2879:5,Deployability,update,updated,5,"Also updated code and documentation to indicate how to properly run this; test. Sadly it has to be done manually because I don't know of a; reasonable way to disable default credentials. Nevertheless it's good that the test is there even for automated runs,; so we can check that loading the explicit credentials does not break; anything.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879
https://github.com/broadinstitute/gatk/pull/2879:280,Performance,load,loading,280,"Also updated code and documentation to indicate how to properly run this; test. Sadly it has to be done manually because I don't know of a; reasonable way to disable default credentials. Nevertheless it's good that the test is there even for automated runs,; so we can check that loading the explicit credentials does not break; anything.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879
https://github.com/broadinstitute/gatk/pull/2879:74,Testability,test,test,74,"Also updated code and documentation to indicate how to properly run this; test. Sadly it has to be done manually because I don't know of a; reasonable way to disable default credentials. Nevertheless it's good that the test is there even for automated runs,; so we can check that loading the explicit credentials does not break; anything.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879
https://github.com/broadinstitute/gatk/pull/2879:219,Testability,test,test,219,"Also updated code and documentation to indicate how to properly run this; test. Sadly it has to be done manually because I don't know of a; reasonable way to disable default credentials. Nevertheless it's good that the test is there even for automated runs,; so we can check that loading the explicit credentials does not break; anything.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879
https://github.com/broadinstitute/gatk/issues/2881:517,Availability,down,downstream,517,"@samuelklee commented on [Thu Aug 11 2016](https://github.com/broadinstitute/gatk-protected/issues/644). We currently have some recommendations for coverage collection that were based on recapseg, but it would be good to revisit them with GATK CNV and the new coverage model/HMM segmentation. In addition, GC bias correction has been implemented in the workflow, but it is not yet evaluated. Documentation and and scripts to generate evaluations and plots showing the effects on GATK CNV, the new coverage model, and downstream tools (e.g., ACNV) are needed.; - [ ] target padding; - [ ] keeping duplicates; - [ ] GC bias correction; - [ ] sequence repeat correction; - [ ] target filtering. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/644#issuecomment-295360224). I am planning on removing sequence repeat correction to close #645. We can reimplement and reevaluate in the future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2881
https://github.com/broadinstitute/gatk/issues/2882:3808,Availability,mask,masked,3808,"N; Y 59252232 59252800 target_189882_VAMP7 NaN; Y 59272120 59272713 target_189883_VAMP7 NaN; Y 59274302 59274871 target_189884_VAMP7 NaN; Y 59330180 59330708 target_189885_IL9R NaN; Y 59333828 59334429 target_189886_IL9R NaN; Y 59335302 59335904 target_189887_IL9R NaN; Y 59335905 59336289 target_189888_IL9R NaN; Y 59336290 59336776 target_189889_IL9R NaN; Y 59336840 59337486 target_189890_IL9R NaN; Y 59337698 59338400 target_189891_IL9R NaN; Y 59338503 59339109 target_189892_IL9R NaN; Y 59339943 59340528 target_189893_IL9R NaN; Y 59342236 59343330 target_189894_IL9R NaN. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242767764). @davidbenjamin could you please take a look? it sounds like it could be a problem with the reference missing these regions. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242774747). OK it turns out that the reference is hard masked and has ""N"" in that region. Nevertheless, we shouldn't get NaNs. In my opinion, the correct behavior is to drop targets on which GC percentage can not be defined + emit informative error messages. ---. @davidbenjamin commented on [Sun Sep 11 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-246177779). I will address this. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-250018497). @davidbenjamin also, CorrectGCBias produces NaNs when a sample has very low coverage. I think the correct behavior is this:. (1) when annotating targets, it is OK to produce NaNs on targets whose GC bias can not be determined. When correcting for GC bias, those targets must be removed altogether. (2) if the bias curve can not be determined (let's say because of low coverage), the tool should remove that sample from the collection and emit appropriate warning messages. If all samples are removed, the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2882
https://github.com/broadinstitute/gatk/issues/2882:3996,Availability,error,error,3996,"330180 59330708 target_189885_IL9R NaN; Y 59333828 59334429 target_189886_IL9R NaN; Y 59335302 59335904 target_189887_IL9R NaN; Y 59335905 59336289 target_189888_IL9R NaN; Y 59336290 59336776 target_189889_IL9R NaN; Y 59336840 59337486 target_189890_IL9R NaN; Y 59337698 59338400 target_189891_IL9R NaN; Y 59338503 59339109 target_189892_IL9R NaN; Y 59339943 59340528 target_189893_IL9R NaN; Y 59342236 59343330 target_189894_IL9R NaN. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242767764). @davidbenjamin could you please take a look? it sounds like it could be a problem with the reference missing these regions. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242774747). OK it turns out that the reference is hard masked and has ""N"" in that region. Nevertheless, we shouldn't get NaNs. In my opinion, the correct behavior is to drop targets on which GC percentage can not be defined + emit informative error messages. ---. @davidbenjamin commented on [Sun Sep 11 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-246177779). I will address this. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-250018497). @davidbenjamin also, CorrectGCBias produces NaNs when a sample has very low coverage. I think the correct behavior is this:. (1) when annotating targets, it is OK to produce NaNs on targets whose GC bias can not be determined. When correcting for GC bias, those targets must be removed altogether. (2) if the bias curve can not be determined (let's say because of low coverage), the tool should remove that sample from the collection and emit appropriate warning messages. If all samples are removed, the tool should produce a bad input error. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2882
https://github.com/broadinstitute/gatk/issues/2882:4836,Availability,error,error,4836,"02 59335904 target_189887_IL9R NaN; Y 59335905 59336289 target_189888_IL9R NaN; Y 59336290 59336776 target_189889_IL9R NaN; Y 59336840 59337486 target_189890_IL9R NaN; Y 59337698 59338400 target_189891_IL9R NaN; Y 59338503 59339109 target_189892_IL9R NaN; Y 59339943 59340528 target_189893_IL9R NaN; Y 59342236 59343330 target_189894_IL9R NaN. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242767764). @davidbenjamin could you please take a look? it sounds like it could be a problem with the reference missing these regions. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242774747). OK it turns out that the reference is hard masked and has ""N"" in that region. Nevertheless, we shouldn't get NaNs. In my opinion, the correct behavior is to drop targets on which GC percentage can not be defined + emit informative error messages. ---. @davidbenjamin commented on [Sun Sep 11 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-246177779). I will address this. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-250018497). @davidbenjamin also, CorrectGCBias produces NaNs when a sample has very low coverage. I think the correct behavior is this:. (1) when annotating targets, it is OK to produce NaNs on targets whose GC bias can not be determined. When correcting for GC bias, those targets must be removed altogether. (2) if the bias curve can not be determined (let's say because of low coverage), the tool should remove that sample from the collection and emit appropriate warning messages. If all samples are removed, the tool should produce a bad input error. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-302466279). @asmirnov239 can you review and if the issue is resolved, close?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2882
https://github.com/broadinstitute/gatk/issues/2882:4002,Integrability,message,messages,4002,"330180 59330708 target_189885_IL9R NaN; Y 59333828 59334429 target_189886_IL9R NaN; Y 59335302 59335904 target_189887_IL9R NaN; Y 59335905 59336289 target_189888_IL9R NaN; Y 59336290 59336776 target_189889_IL9R NaN; Y 59336840 59337486 target_189890_IL9R NaN; Y 59337698 59338400 target_189891_IL9R NaN; Y 59338503 59339109 target_189892_IL9R NaN; Y 59339943 59340528 target_189893_IL9R NaN; Y 59342236 59343330 target_189894_IL9R NaN. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242767764). @davidbenjamin could you please take a look? it sounds like it could be a problem with the reference missing these regions. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242774747). OK it turns out that the reference is hard masked and has ""N"" in that region. Nevertheless, we shouldn't get NaNs. In my opinion, the correct behavior is to drop targets on which GC percentage can not be defined + emit informative error messages. ---. @davidbenjamin commented on [Sun Sep 11 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-246177779). I will address this. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-250018497). @davidbenjamin also, CorrectGCBias produces NaNs when a sample has very low coverage. I think the correct behavior is this:. (1) when annotating targets, it is OK to produce NaNs on targets whose GC bias can not be determined. When correcting for GC bias, those targets must be removed altogether. (2) if the bias curve can not be determined (let's say because of low coverage), the tool should remove that sample from the collection and emit appropriate warning messages. If all samples are removed, the tool should produce a bad input error. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2882
https://github.com/broadinstitute/gatk/issues/2882:4762,Integrability,message,messages,4762,"02 59335904 target_189887_IL9R NaN; Y 59335905 59336289 target_189888_IL9R NaN; Y 59336290 59336776 target_189889_IL9R NaN; Y 59336840 59337486 target_189890_IL9R NaN; Y 59337698 59338400 target_189891_IL9R NaN; Y 59338503 59339109 target_189892_IL9R NaN; Y 59339943 59340528 target_189893_IL9R NaN; Y 59342236 59343330 target_189894_IL9R NaN. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242767764). @davidbenjamin could you please take a look? it sounds like it could be a problem with the reference missing these regions. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242774747). OK it turns out that the reference is hard masked and has ""N"" in that region. Nevertheless, we shouldn't get NaNs. In my opinion, the correct behavior is to drop targets on which GC percentage can not be defined + emit informative error messages. ---. @davidbenjamin commented on [Sun Sep 11 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-246177779). I will address this. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-250018497). @davidbenjamin also, CorrectGCBias produces NaNs when a sample has very low coverage. I think the correct behavior is this:. (1) when annotating targets, it is OK to produce NaNs on targets whose GC bias can not be determined. When correcting for GC bias, those targets must be removed altogether. (2) if the bias curve can not be determined (let's say because of low coverage), the tool should remove that sample from the collection and emit appropriate warning messages. If all samples are removed, the tool should produce a bad input error. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-302466279). @asmirnov239 can you review and if the issue is resolved, close?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2882
https://github.com/broadinstitute/gatk/issues/2883:911,Availability,error,error,911,"@lbergelson commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659). I got a segfault while running CreatePanelOfNormalsIntegrationTest. Subsequent runs were unable to reproduce it. ```; 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:1414,Availability,error,error,1414,"rc/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:2907,Availability,error,error,2907,", 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; > ; > #; > ; > # An error report file with more information is saved as:; > ; > # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; > ; > #; > ; > # If you would like to submit a bug report, please visit:; > ; > # http://bugreport.java.com/bugreport/crash.jsp; > ; > #; > ; > hs_err_pid2425.log.txt; > https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt; > ; > @yfarjoun https://github.com/yfarjoun Is this similar to the crash you; > saw a while back?; > ; > ; > You are receiving this beca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:3476,Availability,error,error,3476," size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; > ; > #; > ; > # An error report file with more information is saved as:; > ; > # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; > ; > #; > ; > # If you would like to submit a bug report, please visit:; > ; > # http://bugreport.java.com/bugreport/crash.jsp; > ; > #; > ; > hs_err_pid2425.log.txt; > https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt; > ; > @yfarjoun https://github.com/yfarjoun Is this similar to the crash you; > saw a while back?; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk-protected/issues/659, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0h0xGA8ntZ_9wd53IUeIqTIfWye0ks5qlf_YgaJpZM4JyIZS; > .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:926,Safety,detect,detected,926,"@lbergelson commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659). I got a segfault while running CreatePanelOfNormalsIntegrationTest. Subsequent runs were unable to reproduce it. ```; 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:2922,Safety,detect,detected,2922,", 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; > ; > #; > ; > # An error report file with more information is saved as:; > ; > # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; > ; > #; > ; > # If you would like to submit a bug report, please visit:; > ; > # http://bugreport.java.com/bugreport/crash.jsp; > ; > #; > ; > hs_err_pid2425.log.txt; > https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt; > ; > @yfarjoun https://github.com/yfarjoun Is this similar to the crash you; > saw a while back?; > ; > ; > You are receiving this beca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:362,Testability,Test,Test,362,"@lbergelson commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659). I got a segfault while running CreatePanelOfNormalsIntegrationTest. Subsequent runs were unable to reproduce it. ```; 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:368,Testability,Test,Test,368,"@lbergelson commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659). I got a segfault while running CreatePanelOfNormalsIntegrationTest. Subsequent runs were unable to reproduce it. ```; 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:428,Testability,test,test,428,"@lbergelson commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659). I got a segfault while running CreatePanelOfNormalsIntegrationTest. Subsequent runs were unable to reproduce it. ```; 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:1524,Testability,log,log,1524,"lOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:1664,Testability,log,log,1664,"82 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:1750,Testability,log,log,1750,"00 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WAR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:2325,Testability,Test,Test,2325,"va again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dump",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:2331,Testability,Test,Test,2331,"va again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dump",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:2391,Testability,test,test,2391,"va again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dump",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:3592,Testability,log,log,3592," size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; > ; > #; > ; > # An error report file with more information is saved as:; > ; > # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; > ; > #; > ; > # If you would like to submit a bug report, please visit:; > ; > # http://bugreport.java.com/bugreport/crash.jsp; > ; > #; > ; > hs_err_pid2425.log.txt; > https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt; > ; > @yfarjoun https://github.com/yfarjoun Is this similar to the crash you; > saw a while back?; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk-protected/issues/659, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0h0xGA8ntZ_9wd53IUeIqTIfWye0ks5qlf_YgaJpZM4JyIZS; > .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:3756,Testability,log,log,3756," size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; > ; > #; > ; > # An error report file with more information is saved as:; > ; > # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; > ; > #; > ; > # If you would like to submit a bug report, please visit:; > ; > # http://bugreport.java.com/bugreport/crash.jsp; > ; > #; > ; > hs_err_pid2425.log.txt; > https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt; > ; > @yfarjoun https://github.com/yfarjoun Is this similar to the crash you; > saw a while back?; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk-protected/issues/659, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0h0xGA8ntZ_9wd53IUeIqTIfWye0ks5qlf_YgaJpZM4JyIZS; > .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2883:3844,Testability,log,log,3844," size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ; > ```; > 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > ```; > ; > #; > ; > # A fatal error has been detected by the Java Runtime Environment:; > ; > #; > ; > # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; > ; > #; > ; > # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); > ; > # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); > ; > # Problematic frame:; > ; > # V [libjvm.dylib+0x1a9401]; > ; > #; > ; > # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; > ; > #; > ; > # An error report file with more information is saved as:; > ; > # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; > ; > #; > ; > # If you would like to submit a bug report, please visit:; > ; > # http://bugreport.java.com/bugreport/crash.jsp; > ; > #; > ; > hs_err_pid2425.log.txt; > https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt; > ; > @yfarjoun https://github.com/yfarjoun Is this similar to the crash you; > saw a while back?; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk-protected/issues/659, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0h0xGA8ntZ_9wd53IUeIqTIfWye0ks5qlf_YgaJpZM4JyIZS; > .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883
https://github.com/broadinstitute/gatk/issues/2884:320,Deployability,integrat,integrated,320,"@samuelklee commented on [Thu Sep 01 2016](https://github.com/broadinstitute/gatk-protected/issues/662). See comments in #660. Runtime is now about 30-40 minutes for an exome with default parameters, which is a little high. I think we could explore implementing a version of the AF model where the reference bias is not integrated out. This might require more iterations to converge, but on the other hand conditional likelihoods might be cheaper to calculate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2884
https://github.com/broadinstitute/gatk/issues/2884:320,Integrability,integrat,integrated,320,"@samuelklee commented on [Thu Sep 01 2016](https://github.com/broadinstitute/gatk-protected/issues/662). See comments in #660. Runtime is now about 30-40 minutes for an exome with default parameters, which is a little high. I think we could explore implementing a version of the AF model where the reference bias is not integrated out. This might require more iterations to converge, but on the other hand conditional likelihoods might be cheaper to calculate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2884
https://github.com/broadinstitute/gatk/issues/2890:276,Energy Efficiency,efficient,efficient,276,@lbergelson commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/698). `CountSet` is a small class that is only used in 1 place. Investigate if it can be replaced with a set from one of our many collection libraries. It needs to implement an efficient max and min operation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2890
https://github.com/broadinstitute/gatk/issues/2891:689,Availability,error,error,689,"@davidbenjamin commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/700). Currently, `Mutect2` hard filters a candidate somatic variant if any event occurs at the same locus in the panel of normal samples. The idea is to avoid false positive calls at inherently noisy sites. This approach is reasonable but perhaps we can improve it. Some thoughts:; - Asymptotically, as the size of the PoN goes to infinity eventually every site will have some event and we will filter out every variant. Obviously this is an unrealistic limit, but a model should always perform better with more data.; - It might be good to use the PoN to learn a probabilistic model of error at each site, similar to the tool EBCall which has been noted to perform quite well on indels.; - regardless of our model, we should consider alternatives to hard filtering, such as perhaps using the PoN to penalize a somatic quality score.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2891
https://github.com/broadinstitute/gatk/issues/2891:589,Performance,perform,perform,589,"@davidbenjamin commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/700). Currently, `Mutect2` hard filters a candidate somatic variant if any event occurs at the same locus in the panel of normal samples. The idea is to avoid false positive calls at inherently noisy sites. This approach is reasonable but perhaps we can improve it. Some thoughts:; - Asymptotically, as the size of the PoN goes to infinity eventually every site will have some event and we will filter out every variant. Obviously this is an unrealistic limit, but a model should always perform better with more data.; - It might be good to use the PoN to learn a probabilistic model of error at each site, similar to the tool EBCall which has been noted to perform quite well on indels.; - regardless of our model, we should consider alternatives to hard filtering, such as perhaps using the PoN to penalize a somatic quality score.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2891
https://github.com/broadinstitute/gatk/issues/2891:760,Performance,perform,perform,760,"@davidbenjamin commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/700). Currently, `Mutect2` hard filters a candidate somatic variant if any event occurs at the same locus in the panel of normal samples. The idea is to avoid false positive calls at inherently noisy sites. This approach is reasonable but perhaps we can improve it. Some thoughts:; - Asymptotically, as the size of the PoN goes to infinity eventually every site will have some event and we will filter out every variant. Obviously this is an unrealistic limit, but a model should always perform better with more data.; - It might be good to use the PoN to learn a probabilistic model of error at each site, similar to the tool EBCall which has been noted to perform quite well on indels.; - regardless of our model, we should consider alternatives to hard filtering, such as perhaps using the PoN to penalize a somatic quality score.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2891
https://github.com/broadinstitute/gatk/issues/2891:255,Safety,avoid,avoid,255,"@davidbenjamin commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/700). Currently, `Mutect2` hard filters a candidate somatic variant if any event occurs at the same locus in the panel of normal samples. The idea is to avoid false positive calls at inherently noisy sites. This approach is reasonable but perhaps we can improve it. Some thoughts:; - Asymptotically, as the size of the PoN goes to infinity eventually every site will have some event and we will filter out every variant. Obviously this is an unrealistic limit, but a model should always perform better with more data.; - It might be good to use the PoN to learn a probabilistic model of error at each site, similar to the tool EBCall which has been noted to perform quite well on indels.; - regardless of our model, we should consider alternatives to hard filtering, such as perhaps using the PoN to penalize a somatic quality score.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2891
https://github.com/broadinstitute/gatk/issues/2891:658,Usability,learn,learn,658,"@davidbenjamin commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/700). Currently, `Mutect2` hard filters a candidate somatic variant if any event occurs at the same locus in the panel of normal samples. The idea is to avoid false positive calls at inherently noisy sites. This approach is reasonable but perhaps we can improve it. Some thoughts:; - Asymptotically, as the size of the PoN goes to infinity eventually every site will have some event and we will filter out every variant. Obviously this is an unrealistic limit, but a model should always perform better with more data.; - It might be good to use the PoN to learn a probabilistic model of error at each site, similar to the tool EBCall which has been noted to perform quite well on indels.; - regardless of our model, we should consider alternatives to hard filtering, such as perhaps using the PoN to penalize a somatic quality score.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2891
https://github.com/broadinstitute/gatk/issues/2892:1293,Availability,reliab,reliable,1293,"ount the fact that targets on different chromosomes are ""infinitely"" separated. Agilent/Ice targets are very unevenly spaced and the distribution of target spacing has a heavy tail, making the matter worse. The regularizer could still be used with certain modifications. Let us define the ""target coverage noise"" for sample s as:. u_{st} = \sum_{\mu} W_{t \mu} z_{s \mu} + m_t. (1) [implementation] First of all, the target coverage noise must be regularized on each contig separately. It doesn't make sense to stack up all targets and take once giant FFT of u_{st}. This can be fixed in the current implementation with little effort. (2) [formal development + implementation] Within each contig, u_{st} must be mapped from target space to genomic position space e.g. via kernel density estimation. It is crucial to take into account the uncertainty in density estimation in the penalty function. For example, if the pre-image of a genomic position $x$ lies at the middle of a certain target $t$, the estimated value is much more reliable than the case where it lies between two largely separated targets. The penalty must be weighted according to the certainty of estimation. (3) [formal development + implementation] once step 1 and 2 are done, the iterative solver code must be updated accordingly. ---. @mbabadi commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246015485). @samuelklee @davidbenjamin @asmirnov239 Let's have a joint meeting at some point to discuss the problem. It is (probably) not too hard to figure out, and it will make our model really shine!. ---. @mbabadi commented on [Tue Sep 13 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246822768). I have some notes written on this which I can discuss tomorrow in the CNV meeting. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-249735634). Here's a nice demonstration @asmir",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2892
https://github.com/broadinstitute/gatk/issues/2892:1544,Deployability,update,updated,1544,"used with certain modifications. Let us define the ""target coverage noise"" for sample s as:. u_{st} = \sum_{\mu} W_{t \mu} z_{s \mu} + m_t. (1) [implementation] First of all, the target coverage noise must be regularized on each contig separately. It doesn't make sense to stack up all targets and take once giant FFT of u_{st}. This can be fixed in the current implementation with little effort. (2) [formal development + implementation] Within each contig, u_{st} must be mapped from target space to genomic position space e.g. via kernel density estimation. It is crucial to take into account the uncertainty in density estimation in the penalty function. For example, if the pre-image of a genomic position $x$ lies at the middle of a certain target $t$, the estimated value is much more reliable than the case where it lies between two largely separated targets. The penalty must be weighted according to the certainty of estimation. (3) [formal development + implementation] once step 1 and 2 are done, the iterative solver code must be updated accordingly. ---. @mbabadi commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246015485). @samuelklee @davidbenjamin @asmirnov239 Let's have a joint meeting at some point to discuss the problem. It is (probably) not too hard to figure out, and it will make our model really shine!. ---. @mbabadi commented on [Tue Sep 13 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246822768). I have some notes written on this which I can discuss tomorrow in the CNV meeting. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-249735634). Here's a nice demonstration @asmirnov239 @samuelklee @davidbenjamin. The first sample is denoised with D=2 principal components, the second with D=8 principal components. The obvious deletion events are gone. All common events will be absorbed in the PoN once one spends """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2892
https://github.com/broadinstitute/gatk/issues/2892:140,Safety,avoid,avoidance,140,"@mbabadi commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/701). The present implementation of the CNV-avoidance regularizer in the new coverage model assumes evenly spaced targets on a single contig. It does not take into account the fact that targets on different chromosomes are ""infinitely"" separated. Agilent/Ice targets are very unevenly spaced and the distribution of target spacing has a heavy tail, making the matter worse. The regularizer could still be used with certain modifications. Let us define the ""target coverage noise"" for sample s as:. u_{st} = \sum_{\mu} W_{t \mu} z_{s \mu} + m_t. (1) [implementation] First of all, the target coverage noise must be regularized on each contig separately. It doesn't make sense to stack up all targets and take once giant FFT of u_{st}. This can be fixed in the current implementation with little effort. (2) [formal development + implementation] Within each contig, u_{st} must be mapped from target space to genomic position space e.g. via kernel density estimation. It is crucial to take into account the uncertainty in density estimation in the penalty function. For example, if the pre-image of a genomic position $x$ lies at the middle of a certain target $t$, the estimated value is much more reliable than the case where it lies between two largely separated targets. The penalty must be weighted according to the certainty of estimation. (3) [formal development + implementation] once step 1 and 2 are done, the iterative solver code must be updated accordingly. ---. @mbabadi commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246015485). @samuelklee @davidbenjamin @asmirnov239 Let's have a joint meeting at some point to discuss the problem. It is (probably) not too hard to figure out, and it will make our model really shine!. ---. @mbabadi commented on [Tue Sep 13 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2892
https://github.com/broadinstitute/gatk/issues/2897:365,Integrability,depend,dependency,365,"@LeeTL1220 commented on [Wed Sep 21 2016](https://github.com/broadinstitute/gatk-protected/issues/712). https://hub.docker.com/r/broadinstitute/gatk-protected/tags/. Most are due to the base image being used. We may want to consider upgrading the ubuntu version. If we do this, hdfview will have to be removed. This should be fine, since the GATK now packages this dependency. Not sure about the effect on R. ---. @LeeTL1220 commented on [Wed Sep 21 2016](https://github.com/broadinstitute/gatk-protected/issues/712#issuecomment-248659967). Upgrading the ubuntu will not eliminate all vulnerabilities. ---. @LeeTL1220 commented on [Wed Sep 21 2016](https://github.com/broadinstitute/gatk-protected/issues/712#issuecomment-248679073). `emacs` makes this image much larger",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2897
https://github.com/broadinstitute/gatk/issues/2898:1971,Energy Efficiency,Reduce,ReduceOps,1971,rapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.calculateLog10GP(GenotypeCopyNumberTriStateSegments.java:198); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$composeVariantContext$0(GenotypeCopyNumberTriStateSegments.java:125); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.composeVariantContext(GenotypeCopyNumberTriStateSegments.java:141); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.makeCalls(GenotypeCopyNumberTriStateSegments.java:104); at org.broadinstitute.hellbender.tools.exome.germlinehmm.CopyNumberTriStateSegmentCaller.doWork(CopyNumberTriStateSegmentCaller.java:121); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185). ```. You can see all input files here:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2898:1981,Energy Efficiency,Reduce,ReduceOp,1981,rapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.calculateLog10GP(GenotypeCopyNumberTriStateSegments.java:198); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$composeVariantContext$0(GenotypeCopyNumberTriStateSegments.java:125); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.composeVariantContext(GenotypeCopyNumberTriStateSegments.java:141); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.makeCalls(GenotypeCopyNumberTriStateSegments.java:104); at org.broadinstitute.hellbender.tools.exome.germlinehmm.CopyNumberTriStateSegmentCaller.doWork(CopyNumberTriStateSegmentCaller.java:121); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185). ```. You can see all input files here:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2898:2009,Energy Efficiency,Reduce,ReduceOps,2009,ipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.calculateLog10GP(GenotypeCopyNumberTriStateSegments.java:198); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$composeVariantContext$0(GenotypeCopyNumberTriStateSegments.java:125); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.composeVariantContext(GenotypeCopyNumberTriStateSegments.java:141); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.makeCalls(GenotypeCopyNumberTriStateSegments.java:104); at org.broadinstitute.hellbender.tools.exome.germlinehmm.CopyNumberTriStateSegmentCaller.doWork(CopyNumberTriStateSegmentCaller.java:121); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185). ```. You can see all input files here:; /dsde/working/asmirnov,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2898:979,Integrability,wrap,wrapAndCopyInto,979,@asmirnov239 commented on [Thu Sep 29 2016](https://github.com/broadinstitute/gatk-protected/issues/727). Here is the stack trace:. ```; java.lang.IllegalArgumentException: the 'to' index must be between 'from' and the length of the data/position sequence; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:609); at org.broadinstitute.hellbender.utils.param.ParamUtils.inRange(ParamUtils.java:80); at org.broadinstitute.hellbender.utils.hmm.ForwardBackwardAlgorithm$Result.logProbability(ForwardBackwardAlgorithm.java:141); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$calculateLog10GP$6(GenotypeCopyNumberTriStateSegments.java:197); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.calculateLog10GP(GenotypeCopyNumberTriStateSegments.java:198); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$composeVariantContext$0(GenotypeCopyNumberTriStateSegments.java:125); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSeq,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2898:1907,Integrability,wrap,wrapAndCopyInto,1907,ipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.calculateLog10GP(GenotypeCopyNumberTriStateSegments.java:198); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$composeVariantContext$0(GenotypeCopyNumberTriStateSegments.java:125); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.composeVariantContext(GenotypeCopyNumberTriStateSegments.java:141); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.makeCalls(GenotypeCopyNumberTriStateSegments.java:104); at org.broadinstitute.hellbender.tools.exome.germlinehmm.CopyNumberTriStateSegmentCaller.doWork(CopyNumberTriStateSegmentCaller.java:121); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2898:302,Security,validat,validateArg,302,@asmirnov239 commented on [Thu Sep 29 2016](https://github.com/broadinstitute/gatk-protected/issues/727). Here is the stack trace:. ```; java.lang.IllegalArgumentException: the 'to' index must be between 'from' and the length of the data/position sequence; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:609); at org.broadinstitute.hellbender.utils.param.ParamUtils.inRange(ParamUtils.java:80); at org.broadinstitute.hellbender.utils.hmm.ForwardBackwardAlgorithm$Result.logProbability(ForwardBackwardAlgorithm.java:141); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$calculateLog10GP$6(GenotypeCopyNumberTriStateSegments.java:197); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.calculateLog10GP(GenotypeCopyNumberTriStateSegments.java:198); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$composeVariantContext$0(GenotypeCopyNumberTriStateSegments.java:125); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSeq,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2898:491,Testability,log,logProbability,491,@asmirnov239 commented on [Thu Sep 29 2016](https://github.com/broadinstitute/gatk-protected/issues/727). Here is the stack trace:. ```; java.lang.IllegalArgumentException: the 'to' index must be between 'from' and the length of the data/position sequence; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:609); at org.broadinstitute.hellbender.utils.param.ParamUtils.inRange(ParamUtils.java:80); at org.broadinstitute.hellbender.utils.hmm.ForwardBackwardAlgorithm$Result.logProbability(ForwardBackwardAlgorithm.java:141); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$calculateLog10GP$6(GenotypeCopyNumberTriStateSegments.java:197); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.calculateLog10GP(GenotypeCopyNumberTriStateSegments.java:198); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$composeVariantContext$0(GenotypeCopyNumberTriStateSegments.java:125); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSeq,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2898:3508,Testability,log,logger,3508,"StateSegments.makeCalls(GenotypeCopyNumberTriStateSegments.java:104); at org.broadinstitute.hellbender.tools.exome.germlinehmm.CopyNumberTriStateSegmentCaller.doWork(CopyNumberTriStateSegmentCaller.java:121); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185). ```. You can see all input files here:; /dsde/working/asmirnov/evaluations/Germline_CNV/TCGA/xhmm/debugging. The best fix would probably be to provide a warning to the user about inconsistency between target lists, and then subset input targets to match the those found in read count collection.; You can do that in the makeCalls() method of the GenotypeCopyNumberTriStateSegments tool here:. ```; @Override; protected void makeCalls(final CopyNumberTriStateHiddenMarkovModel model, final TargetCollection<Target> targets, final ReadCountCollection inputCounts) {; logger.info(""Composing list of segment intervals to genotype ..."");; final List<GenotypingSegment> segments = composeGenotypingSegments(segmentsFile, targets);; logger.info(String.format(""A total of %d segments to genotype found"", segments.size()));; final List<ForwardBackwardAlgorithm.Result<Double, Target, CopyNumberTriState>> fbResults = runFWBWAlgorithm(model, inputCounts);; for (final GenotypingSegment segment : segments) {; final VariantContext variant = composeVariantContext(segment, inputCounts.columnNames(), fbResults);; outputWriter.add(variant);; }; }; ```. @vruano Could you please look into this?. ---. @vruano commented on [Wed Oct 05 2016](https://github.com/broadinstitute/gatk-protected/issues/727#issuecomment-251744663). Ok, I will take care of this. For now just to mention that the workaround is to provide a tool with a target file that matches the targets in the input count file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2898:3669,Testability,log,logger,3669,"StateSegments.makeCalls(GenotypeCopyNumberTriStateSegments.java:104); at org.broadinstitute.hellbender.tools.exome.germlinehmm.CopyNumberTriStateSegmentCaller.doWork(CopyNumberTriStateSegmentCaller.java:121); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185). ```. You can see all input files here:; /dsde/working/asmirnov/evaluations/Germline_CNV/TCGA/xhmm/debugging. The best fix would probably be to provide a warning to the user about inconsistency between target lists, and then subset input targets to match the those found in read count collection.; You can do that in the makeCalls() method of the GenotypeCopyNumberTriStateSegments tool here:. ```; @Override; protected void makeCalls(final CopyNumberTriStateHiddenMarkovModel model, final TargetCollection<Target> targets, final ReadCountCollection inputCounts) {; logger.info(""Composing list of segment intervals to genotype ..."");; final List<GenotypingSegment> segments = composeGenotypingSegments(segmentsFile, targets);; logger.info(String.format(""A total of %d segments to genotype found"", segments.size()));; final List<ForwardBackwardAlgorithm.Result<Double, Target, CopyNumberTriState>> fbResults = runFWBWAlgorithm(model, inputCounts);; for (final GenotypingSegment segment : segments) {; final VariantContext variant = composeVariantContext(segment, inputCounts.columnNames(), fbResults);; outputWriter.add(variant);; }; }; ```. @vruano Could you please look into this?. ---. @vruano commented on [Wed Oct 05 2016](https://github.com/broadinstitute/gatk-protected/issues/727#issuecomment-251744663). Ok, I will take care of this. For now just to mention that the workaround is to provide a tool with a target file that matches the targets in the input count file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898
https://github.com/broadinstitute/gatk/issues/2902:277,Availability,failure,failure,277,@vruano commented on [Tue Oct 11 2016](https://github.com/broadinstitute/gatk-protected/issues/741). Recently there was mishap where the same class (but different code version) was present in gatk (public) and gatk-protected. #738. For some reason that did not caused a Travis failure but it was detected when trying to use the affected tool in an actual analysis. . Since there is a flow of code from gatk-protected to gatk (public) I guess it would be advisable to add step in Travis to verify there is no class name clashes between gatk-protected code base and the imported gatk.4 dependency. ---. @lbergelson commented on [Tue Oct 25 2016](https://github.com/broadinstitute/gatk-protected/issues/741#issuecomment-256082980). We've had this issue a few times. One thing that would help would be to move all the code in gatk-protected into a `protected` sub package so we would have a separate namespace that could never accidentally conflict with gatk-public.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2902
https://github.com/broadinstitute/gatk/issues/2902:584,Integrability,depend,dependency,584,@vruano commented on [Tue Oct 11 2016](https://github.com/broadinstitute/gatk-protected/issues/741). Recently there was mishap where the same class (but different code version) was present in gatk (public) and gatk-protected. #738. For some reason that did not caused a Travis failure but it was detected when trying to use the affected tool in an actual analysis. . Since there is a flow of code from gatk-protected to gatk (public) I guess it would be advisable to add step in Travis to verify there is no class name clashes between gatk-protected code base and the imported gatk.4 dependency. ---. @lbergelson commented on [Tue Oct 25 2016](https://github.com/broadinstitute/gatk-protected/issues/741#issuecomment-256082980). We've had this issue a few times. One thing that would help would be to move all the code in gatk-protected into a `protected` sub package so we would have a separate namespace that could never accidentally conflict with gatk-public.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2902
https://github.com/broadinstitute/gatk/issues/2902:296,Safety,detect,detected,296,@vruano commented on [Tue Oct 11 2016](https://github.com/broadinstitute/gatk-protected/issues/741). Recently there was mishap where the same class (but different code version) was present in gatk (public) and gatk-protected. #738. For some reason that did not caused a Travis failure but it was detected when trying to use the affected tool in an actual analysis. . Since there is a flow of code from gatk-protected to gatk (public) I guess it would be advisable to add step in Travis to verify there is no class name clashes between gatk-protected code base and the imported gatk.4 dependency. ---. @lbergelson commented on [Tue Oct 25 2016](https://github.com/broadinstitute/gatk-protected/issues/741#issuecomment-256082980). We've had this issue a few times. One thing that would help would be to move all the code in gatk-protected into a `protected` sub package so we would have a separate namespace that could never accidentally conflict with gatk-public.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2902
https://github.com/broadinstitute/gatk/issues/2903:155,Deployability,configurat,configuration,155,@LeeTL1220 commented on [Tue Oct 18 2016](https://github.com/broadinstitute/gatk-protected/issues/742). Should now be `CallAllelicSplits`. Rename the task configuration as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2903
https://github.com/broadinstitute/gatk/issues/2903:155,Modifiability,config,configuration,155,@LeeTL1220 commented on [Tue Oct 18 2016](https://github.com/broadinstitute/gatk-protected/issues/742). Should now be `CallAllelicSplits`. Rename the task configuration as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2903
https://github.com/broadinstitute/gatk/issues/2904:189,Deployability,release,release,189,@LeeTL1220 commented on [Tue Oct 18 2016](https://github.com/broadinstitute/gatk-protected/issues/743). Now called `CallAllelicSplits` . This should happen after (or right before) the next release.; - [ ] Update forum posts; - [ ] Add forum post explaining the output of the cnb_called files. See issue #585,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2904
https://github.com/broadinstitute/gatk/issues/2904:205,Deployability,Update,Update,205,@LeeTL1220 commented on [Tue Oct 18 2016](https://github.com/broadinstitute/gatk-protected/issues/743). Now called `CallAllelicSplits` . This should happen after (or right before) the next release.; - [ ] Update forum posts; - [ ] Add forum post explaining the output of the cnb_called files. See issue #585,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2904
https://github.com/broadinstitute/gatk/issues/2905:422,Testability,log,log,422,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/745). The PoN must:. - [x] Contain as much non-trivial information as possible:; \* Model parameters (W, \Psi, m; also, ordered norms of W); \* Sample names used to make the PoN; \* Posterior expectations (\gamma, z, d, total remove variance, total unexplained variance); \* Useful distributions (bootstrapped distribution of log likelihoods and sample-specific variances); \* Command-line used to make the PoN (add a log file to the HDF5 bundle in the end). - [x] Be human-readable ideally. Binary dumps are fast but are difficult to work with in debugging.; - [ ] Encapsulated and easily shared, perhaps via HDF5 bundles. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/745#issuecomment-302465665). This is partially addressed in PR https://github.com/broadinstitute/gatk-protected/pull/857. We must, however, decide whether or not we want to bundle models into HDF5 blobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2905
https://github.com/broadinstitute/gatk/issues/2905:514,Testability,log,log,514,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/745). The PoN must:. - [x] Contain as much non-trivial information as possible:; \* Model parameters (W, \Psi, m; also, ordered norms of W); \* Sample names used to make the PoN; \* Posterior expectations (\gamma, z, d, total remove variance, total unexplained variance); \* Useful distributions (bootstrapped distribution of log likelihoods and sample-specific variances); \* Command-line used to make the PoN (add a log file to the HDF5 bundle in the end). - [x] Be human-readable ideally. Binary dumps are fast but are difficult to work with in debugging.; - [ ] Encapsulated and easily shared, perhaps via HDF5 bundles. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/745#issuecomment-302465665). This is partially addressed in PR https://github.com/broadinstitute/gatk-protected/pull/857. We must, however, decide whether or not we want to bundle models into HDF5 blobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2905
https://github.com/broadinstitute/gatk/issues/2906:650,Energy Efficiency,power,power,650,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/746). Here are some possible culprits:. 1) Low base call quality; 2) Low mapping quality; 3) Significantly different noise covariates (i.e. sample coming from a different platform, or a bizarre library preparation/sequencing issue resulting in a different bias modality). (1) and (2) can be investigated by taking a few good and bad BAMs and studying the distribution of their base/mapping qualities. (3) can be studied in various ways. One approach is as follows:. Make PoNs with different numbers of principal components D and plot the total denoising power (TDP), defined as ||W z_s||^2, for each sample as a function D. For ""good"" samples, we expect a steady increase of TDP vs. D. For ""bad"" samples, we expect a lag; why? for small D, most benefit comes from choosing principal components that describe the bias covariates of ""good"" samples (majority). Eventually, they will be depleted and further data likelihood gains will require dedicating the next principal components to ""bad"" samples (minority). Another (complementary) approach is as follows:. Fix D to a reasonable value such that a differentiation between good and bad samples is still noticeable (remember: if D = #samples, no signal is left after denoising, so D should be still). Next, obtain the empirical distribution of z_s^\mu, i.e. the projection of each sample s over each principal component \mu. If bad samples do not regress, it shows the incompatibility of their bias covariates with the rest of the samples. ---. @mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/746#issuecomment-254800421). @davidbenjamin @samuelklee anything to add?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2906
https://github.com/broadinstitute/gatk/issues/2907:424,Energy Efficiency,adapt,adaptive,424,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/747). We use Genome STRiP TCGA/GPC2 call sets as ground truth. It is desirable to evaluate:. (1) XHMM and CODEX vs. GATK (almost done @asmirnov239); (2) GATK ROC curves as a function of bias latent space dimension D; (3) GATK ROC curves for a fixed D, and for different unexplained variance models: (isotropic, target-resolved, adaptive), w/ and wo/ sample-specific unexplained variance calling during PoN creations, and calling. That is, 3 x 2 x 2 = 12 cases. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/747#issuecomment-302465336). This was done a while ago. Keeping open for upcoming evaluations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2907
https://github.com/broadinstitute/gatk/issues/2907:424,Modifiability,adapt,adaptive,424,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/747). We use Genome STRiP TCGA/GPC2 call sets as ground truth. It is desirable to evaluate:. (1) XHMM and CODEX vs. GATK (almost done @asmirnov239); (2) GATK ROC curves as a function of bias latent space dimension D; (3) GATK ROC curves for a fixed D, and for different unexplained variance models: (isotropic, target-resolved, adaptive), w/ and wo/ sample-specific unexplained variance calling during PoN creations, and calling. That is, 3 x 2 x 2 = 12 cases. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/747#issuecomment-302465336). This was done a while ago. Keeping open for upcoming evaluations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2907
https://github.com/broadinstitute/gatk/issues/2908:639,Availability,avail,available,639,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/748). At the moment, CalculateTargetCoverage simply counts the number of overlapping reads with each target. Optionally, low quality calls are hard filtered. Here, we propose a probabilistic approach that avoids the usage of hard filters and fits well with the new probabilistic target coverage modeler. By definition, mapping quality MAPQ = -10 \log_10{mapping position is wrong} (see http://samtools.github.io/hts-specs/SAMv1.pdf, pg 5, item 5). It is defined in the range [0, 2^8-1]. The specific value 255 is reserved for when MAPQ is not available. Most MAPQs are well below 255. We consider the following process for assigning reads to each target. Pick a read ""k"" aligned to target ""t"" with a given MAPQ_k. By definition, it maps to the genomic position ""x"" with p_x = 1 - 10^{-MAPQ_k/10}, and to some other position with probability 1 - p_x. We refer to the alignment genomic position of read k as x_k, and the exome target(s) it overlaps with T_k. Let's assume we have T exome targets, and let z_{kt} be a 1-of-#T indicator variable for a read where t is a target and #T is the number of all exome targets. \pi_{kq} = P(z_{kq} = 1) =. p_k x O_{kq} if q \in T_k; (1 - p_k) / (#T - #T_k) if q \notin T_k. Here, O_{kq} is the fractional overlap of the read to an exome target q. Note that since we don't have the information about the next best alignment position, we take a flat prior. Finally, the number of reads belonging to target t, n_t, reads as:. n_t = \sum_k z_{kt}. Since there are many reads, n_t will be approximately Gaussian. It is an elementary calculation to calculate coverage mean E[n_t] and coverage variance var[n_t] in terms of \pi_{kq}. In the probabilistic target coverage model, var[n_t] will be added to the statistical noise. So, the read count collection will have two entries for each target: coverage mean, and coverage variance. ---. @mbabadi commented on [Wed Oct 19 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2908
https://github.com/broadinstitute/gatk/issues/2908:2501,Energy Efficiency,reduce,reduce,2501,"specs/SAMv1.pdf, pg 5, item 5). It is defined in the range [0, 2^8-1]. The specific value 255 is reserved for when MAPQ is not available. Most MAPQs are well below 255. We consider the following process for assigning reads to each target. Pick a read ""k"" aligned to target ""t"" with a given MAPQ_k. By definition, it maps to the genomic position ""x"" with p_x = 1 - 10^{-MAPQ_k/10}, and to some other position with probability 1 - p_x. We refer to the alignment genomic position of read k as x_k, and the exome target(s) it overlaps with T_k. Let's assume we have T exome targets, and let z_{kt} be a 1-of-#T indicator variable for a read where t is a target and #T is the number of all exome targets. \pi_{kq} = P(z_{kq} = 1) =. p_k x O_{kq} if q \in T_k; (1 - p_k) / (#T - #T_k) if q \notin T_k. Here, O_{kq} is the fractional overlap of the read to an exome target q. Note that since we don't have the information about the next best alignment position, we take a flat prior. Finally, the number of reads belonging to target t, n_t, reads as:. n_t = \sum_k z_{kt}. Since there are many reads, n_t will be approximately Gaussian. It is an elementary calculation to calculate coverage mean E[n_t] and coverage variance var[n_t] in terms of \pi_{kq}. In the probabilistic target coverage model, var[n_t] will be added to the statistical noise. So, the read count collection will have two entries for each target: coverage mean, and coverage variance. ---. @mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/748#issuecomment-254812286). @samuelklee @asmirnov239 ; The outcome of https://github.com/broadinstitute/gatk-protected/issues/746 will indicate whether or not we'll need this. Andrey already has a preliminary indication that a fair fraction (50%!) of reads have mapping quality < 20. This implies a very significant coverage variance. Feeding this information into the probabilistic target coverage modeler is guaranteed to significantly reduce FDR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2908
https://github.com/broadinstitute/gatk/issues/2908:1129,Modifiability,variab,variable,1129,"tCoverage simply counts the number of overlapping reads with each target. Optionally, low quality calls are hard filtered. Here, we propose a probabilistic approach that avoids the usage of hard filters and fits well with the new probabilistic target coverage modeler. By definition, mapping quality MAPQ = -10 \log_10{mapping position is wrong} (see http://samtools.github.io/hts-specs/SAMv1.pdf, pg 5, item 5). It is defined in the range [0, 2^8-1]. The specific value 255 is reserved for when MAPQ is not available. Most MAPQs are well below 255. We consider the following process for assigning reads to each target. Pick a read ""k"" aligned to target ""t"" with a given MAPQ_k. By definition, it maps to the genomic position ""x"" with p_x = 1 - 10^{-MAPQ_k/10}, and to some other position with probability 1 - p_x. We refer to the alignment genomic position of read k as x_k, and the exome target(s) it overlaps with T_k. Let's assume we have T exome targets, and let z_{kt} be a 1-of-#T indicator variable for a read where t is a target and #T is the number of all exome targets. \pi_{kq} = P(z_{kq} = 1) =. p_k x O_{kq} if q \in T_k; (1 - p_k) / (#T - #T_k) if q \notin T_k. Here, O_{kq} is the fractional overlap of the read to an exome target q. Note that since we don't have the information about the next best alignment position, we take a flat prior. Finally, the number of reads belonging to target t, n_t, reads as:. n_t = \sum_k z_{kt}. Since there are many reads, n_t will be approximately Gaussian. It is an elementary calculation to calculate coverage mean E[n_t] and coverage variance var[n_t] in terms of \pi_{kq}. In the probabilistic target coverage model, var[n_t] will be added to the statistical noise. So, the read count collection will have two entries for each target: coverage mean, and coverage variance. ---. @mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/748#issuecomment-254812286). @samuelklee @asmirnov239 ; The outcome of",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2908
https://github.com/broadinstitute/gatk/issues/2908:301,Safety,avoid,avoids,301,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/748). At the moment, CalculateTargetCoverage simply counts the number of overlapping reads with each target. Optionally, low quality calls are hard filtered. Here, we propose a probabilistic approach that avoids the usage of hard filters and fits well with the new probabilistic target coverage modeler. By definition, mapping quality MAPQ = -10 \log_10{mapping position is wrong} (see http://samtools.github.io/hts-specs/SAMv1.pdf, pg 5, item 5). It is defined in the range [0, 2^8-1]. The specific value 255 is reserved for when MAPQ is not available. Most MAPQs are well below 255. We consider the following process for assigning reads to each target. Pick a read ""k"" aligned to target ""t"" with a given MAPQ_k. By definition, it maps to the genomic position ""x"" with p_x = 1 - 10^{-MAPQ_k/10}, and to some other position with probability 1 - p_x. We refer to the alignment genomic position of read k as x_k, and the exome target(s) it overlaps with T_k. Let's assume we have T exome targets, and let z_{kt} be a 1-of-#T indicator variable for a read where t is a target and #T is the number of all exome targets. \pi_{kq} = P(z_{kq} = 1) =. p_k x O_{kq} if q \in T_k; (1 - p_k) / (#T - #T_k) if q \notin T_k. Here, O_{kq} is the fractional overlap of the read to an exome target q. Note that since we don't have the information about the next best alignment position, we take a flat prior. Finally, the number of reads belonging to target t, n_t, reads as:. n_t = \sum_k z_{kt}. Since there are many reads, n_t will be approximately Gaussian. It is an elementary calculation to calculate coverage mean E[n_t] and coverage variance var[n_t] in terms of \pi_{kq}. In the probabilistic target coverage model, var[n_t] will be added to the statistical noise. So, the read count collection will have two entries for each target: coverage mean, and coverage variance. ---. @mbabadi commented on [Wed Oct 19 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2908
https://github.com/broadinstitute/gatk/issues/2908:141,Usability,simpl,simply,141,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/748). At the moment, CalculateTargetCoverage simply counts the number of overlapping reads with each target. Optionally, low quality calls are hard filtered. Here, we propose a probabilistic approach that avoids the usage of hard filters and fits well with the new probabilistic target coverage modeler. By definition, mapping quality MAPQ = -10 \log_10{mapping position is wrong} (see http://samtools.github.io/hts-specs/SAMv1.pdf, pg 5, item 5). It is defined in the range [0, 2^8-1]. The specific value 255 is reserved for when MAPQ is not available. Most MAPQs are well below 255. We consider the following process for assigning reads to each target. Pick a read ""k"" aligned to target ""t"" with a given MAPQ_k. By definition, it maps to the genomic position ""x"" with p_x = 1 - 10^{-MAPQ_k/10}, and to some other position with probability 1 - p_x. We refer to the alignment genomic position of read k as x_k, and the exome target(s) it overlaps with T_k. Let's assume we have T exome targets, and let z_{kt} be a 1-of-#T indicator variable for a read where t is a target and #T is the number of all exome targets. \pi_{kq} = P(z_{kq} = 1) =. p_k x O_{kq} if q \in T_k; (1 - p_k) / (#T - #T_k) if q \notin T_k. Here, O_{kq} is the fractional overlap of the read to an exome target q. Note that since we don't have the information about the next best alignment position, we take a flat prior. Finally, the number of reads belonging to target t, n_t, reads as:. n_t = \sum_k z_{kt}. Since there are many reads, n_t will be approximately Gaussian. It is an elementary calculation to calculate coverage mean E[n_t] and coverage variance var[n_t] in terms of \pi_{kq}. In the probabilistic target coverage model, var[n_t] will be added to the statistical noise. So, the read count collection will have two entries for each target: coverage mean, and coverage variance. ---. @mbabadi commented on [Wed Oct 19 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2908
https://github.com/broadinstitute/gatk/issues/2909:585,Availability,down,down,585,"@samuelklee commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/750). Tool for inferring mixture of CNV subclones from ACNV output.; - [x] Develop resources for simulating tumor phylogenies/mixtures; - Wrote python code for simulating phylogenies and generating corresponding truth tables for CN profiles, ACNV segment files (with varying noise---i.e., CR and MAF credible-interval sizes---and purity levels), and plots.; - [x] Design basic algorithm; - Gibbs sampling MCMC of Dirichlet mixture of CNV subclones, to start. Graphical model is written down.; - [x] Implement basic algorithm; - CLI roughly implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:2019,Availability,recover,recovered,2019,"l, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next qu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:2079,Availability,error,error,2079,"l, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next qu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:1624,Deployability,pipeline,pipeline,1624," implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:2507,Deployability,release,release,2507,"bly well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but some performance optimization of the variant-profile sampling step will probably be required. - [ ] Evaluation on BAMs prepared with mixing scripts. - [ ] Writeup of model in technical white paper. - [ ] Tool to produce interactive plots. I think this is necessary to represent the uncertainty in ""solutions"" produced by the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:701,Modifiability,refactor,refactoring,701,"@samuelklee commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/750). Tool for inferring mixture of CNV subclones from ACNV output.; - [x] Develop resources for simulating tumor phylogenies/mixtures; - Wrote python code for simulating phylogenies and generating corresponding truth tables for CN profiles, ACNV segment files (with varying noise---i.e., CR and MAF credible-interval sizes---and purity levels), and plots.; - [x] Design basic algorithm; - Gibbs sampling MCMC of Dirichlet mixture of CNV subclones, to start. Graphical model is written down.; - [x] Implement basic algorithm; - CLI roughly implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:2576,Modifiability,refactor,refactoring,2576,"bly well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but some performance optimization of the variant-profile sampling step will probably be required. - [ ] Evaluation on BAMs prepared with mixing scripts. - [ ] Writeup of model in technical white paper. - [ ] Tool to produce interactive plots. I think this is necessary to represent the uncertainty in ""solutions"" produced by the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:1618,Performance,Queue,Queue,1618," implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:2134,Performance,perform,performance,2134,"s that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but so",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:3195,Performance,perform,performance,3195,"bly well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but some performance optimization of the variant-profile sampling step will probably be required. - [ ] Evaluation on BAMs prepared with mixing scripts. - [ ] Writeup of model in technical white paper. - [ ] Tool to produce interactive plots. I think this is necessary to represent the uncertainty in ""solutions"" produced by the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:3207,Performance,optimiz,optimization,3207,"bly well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but some performance optimization of the variant-profile sampling step will probably be required. - [ ] Evaluation on BAMs prepared with mixing scripts. - [ ] Writeup of model in technical white paper. - [ ] Tool to produce interactive plots. I think this is necessary to represent the uncertainty in ""solutions"" produced by the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:2019,Safety,recover,recovered,2019,"l, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next qu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:762,Testability,test,tests,762,"@samuelklee commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/750). Tool for inferring mixture of CNV subclones from ACNV output.; - [x] Develop resources for simulating tumor phylogenies/mixtures; - Wrote python code for simulating phylogenies and generating corresponding truth tables for CN profiles, ACNV segment files (with varying noise---i.e., CR and MAF credible-interval sizes---and purity levels), and plots.; - [x] Design basic algorithm; - Gibbs sampling MCMC of Dirichlet mixture of CNV subclones, to start. Graphical model is written down.; - [x] Implement basic algorithm; - CLI roughly implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:3084,Testability,test,tests,3084,"bly well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but some performance optimization of the variant-profile sampling step will probably be required. - [ ] Evaluation on BAMs prepared with mixing scripts. - [ ] Writeup of model in technical white paper. - [ ] Tool to produce interactive plots. I think this is necessary to represent the uncertainty in ""solutions"" produced by the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:1477,Usability,Simpl,Simple,1477,"irichlet mixture of CNV subclones, to start. Graphical model is written down.; - [x] Implement basic algorithm; - CLI roughly implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first releas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:1611,Usability,simpl,simple,1611," implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2909:2163,Usability,clear,clear,2163,"s that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but so",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909
https://github.com/broadinstitute/gatk/issues/2910:581,Integrability,depend,depending,581,"@asmirnov239 commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/751). Things that we discussed with @samuelklee that can be done to aid it:. -I think that all files we generate for individual case samples---""ReadCountCollection"" files for coverage profiles, ""AllelicCountCollection"" files for het pulldowns, and segment files---should contain the sample name as metadata in a header comment with a common tag (e.g., #sampleName = ...). Currently, these sample names are stored in column headers, in the fields of a SAMPLE column, or not at all, depending on the type of file. This would drastically simplify the use of the SampleNameFinder class, which would basically only contain a single method to parse this header comment and return the name. -CLIs that generate a file from an input BAM (CalculateTargetCoverage, GetHetCoverage, etc.) should take the sample name from that BAM by default. Since these are the first steps in our workflows, we could also optionally allow the user to specify a sample name different from that in the BAM. -Subsequent CLIs should then take the sample name from the header comment. -CLIs that take multiple non-BAM input files should check for consistency of the sample names as part of the argument validation step. -CLIs that output the sample name in plots should derive these from the header comment. -For files that contain data from multiple samples (e.g., the output of CombineReadCounts), we can probably leave the sample names in the column headers, but it would be nice to output the type of data stored in a header comment as well (e.g., PCOV or RAW). At some point I think we should restrict to RAW output only, see https://github.com/broadinstitute/gatk-protected/issues/615. -Entity names specified by the input file for the WDLs can be separate from the BAM sample names by default. However, if we do allow the user to optionally specify sample names as described in the first bullet point, we can set up the WDL to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2910
https://github.com/broadinstitute/gatk/issues/2910:1271,Security,validat,validation,1271,"n [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/751). Things that we discussed with @samuelklee that can be done to aid it:. -I think that all files we generate for individual case samples---""ReadCountCollection"" files for coverage profiles, ""AllelicCountCollection"" files for het pulldowns, and segment files---should contain the sample name as metadata in a header comment with a common tag (e.g., #sampleName = ...). Currently, these sample names are stored in column headers, in the fields of a SAMPLE column, or not at all, depending on the type of file. This would drastically simplify the use of the SampleNameFinder class, which would basically only contain a single method to parse this header comment and return the name. -CLIs that generate a file from an input BAM (CalculateTargetCoverage, GetHetCoverage, etc.) should take the sample name from that BAM by default. Since these are the first steps in our workflows, we could also optionally allow the user to specify a sample name different from that in the BAM. -Subsequent CLIs should then take the sample name from the header comment. -CLIs that take multiple non-BAM input files should check for consistency of the sample names as part of the argument validation step. -CLIs that output the sample name in plots should derive these from the header comment. -For files that contain data from multiple samples (e.g., the output of CombineReadCounts), we can probably leave the sample names in the column headers, but it would be nice to output the type of data stored in a header comment as well (e.g., PCOV or RAW). At some point I think we should restrict to RAW output only, see https://github.com/broadinstitute/gatk-protected/issues/615. -Entity names specified by the input file for the WDLs can be separate from the BAM sample names by default. However, if we do allow the user to optionally specify sample names as described in the first bullet point, we can set up the WDL to pass the entity names.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2910
https://github.com/broadinstitute/gatk/issues/2910:635,Usability,simpl,simplify,635,"@asmirnov239 commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/751). Things that we discussed with @samuelklee that can be done to aid it:. -I think that all files we generate for individual case samples---""ReadCountCollection"" files for coverage profiles, ""AllelicCountCollection"" files for het pulldowns, and segment files---should contain the sample name as metadata in a header comment with a common tag (e.g., #sampleName = ...). Currently, these sample names are stored in column headers, in the fields of a SAMPLE column, or not at all, depending on the type of file. This would drastically simplify the use of the SampleNameFinder class, which would basically only contain a single method to parse this header comment and return the name. -CLIs that generate a file from an input BAM (CalculateTargetCoverage, GetHetCoverage, etc.) should take the sample name from that BAM by default. Since these are the first steps in our workflows, we could also optionally allow the user to specify a sample name different from that in the BAM. -Subsequent CLIs should then take the sample name from the header comment. -CLIs that take multiple non-BAM input files should check for consistency of the sample names as part of the argument validation step. -CLIs that output the sample name in plots should derive these from the header comment. -For files that contain data from multiple samples (e.g., the output of CombineReadCounts), we can probably leave the sample names in the column headers, but it would be nice to output the type of data stored in a header comment as well (e.g., PCOV or RAW). At some point I think we should restrict to RAW output only, see https://github.com/broadinstitute/gatk-protected/issues/615. -Entity names specified by the input file for the WDLs can be separate from the BAM sample names by default. However, if we do allow the user to optionally specify sample names as described in the first bullet point, we can set up the WDL to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2910
https://github.com/broadinstitute/gatk/issues/2911:105,Deployability,Update,Update,105,@lbergelson commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/763). Update to the current version of public. This requires an update to spark 2.0 which means it needs some extra testing before merging. . @LeeTL1220 would like to run some wdl's to check that things are still working.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2911
https://github.com/broadinstitute/gatk/issues/2911:163,Deployability,update,update,163,@lbergelson commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/763). Update to the current version of public. This requires an update to spark 2.0 which means it needs some extra testing before merging. . @LeeTL1220 would like to run some wdl's to check that things are still working.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2911
https://github.com/broadinstitute/gatk/issues/2911:215,Testability,test,testing,215,@lbergelson commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/763). Update to the current version of public. This requires an update to spark 2.0 which means it needs some extra testing before merging. . @LeeTL1220 would like to run some wdl's to check that things are still working.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2911
https://github.com/broadinstitute/gatk/issues/2913:331,Integrability,depend,dependencies,331,"Our current docker image is pretty large. It has a lot of things that aren't needed for running gatk directly, but are useful if you're doing interactive work in the image. I.e. samtools, gcc, etc. We might want to publish 2 different docker images, . 1. an ultra slim one that cromwell could use that only has the gatk and direct dependencies; 2. a fat one that includes everything you would want for interactive work",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2913
https://github.com/broadinstitute/gatk/issues/2914:1237,Availability,robust,robust,1237,". As @pdexheimer pointed out: . > I think the bug here is in HaplotypeCaller. It technically generated a malformed (g)VCF by using an ambiguous allele for the reference. I don't know what the fix is, though. You can't have an ActiveRegionWalker skip over the ambiguous bases since it operates on a whole region. And a post hoc check in HC would be simple enough for SNVs, but what happens when the ambiguous site is part of a larger deletion?. Needs advice on what the behavior / solution should be by @akiezun @vruano . This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/discussion/4858/reference-bases-with-ambiguity-codes-in-dbsnp/p1) . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85093784). In general, don't know how HC behaves with ambiguous reference bases at all.... I would not be surprised if it just crashes or outputs garbage. Perhaps this should be part of a larger effort to make sure HC, Combine- and GenotypeGVCFs are robust on ambiguous calls. To start, currently GATK/Picard handles bases as uppercase single `byte' representation of the corresponding character. Since we are investing (a mostly wasting) 8 bits already, we could change into a bit mask representation that would allow for quick comparison of ambiguous and non-ambigous base call using bit-wise operations. NO_CALL = 0, A = 1, C = 2, G = 4, T/U = 8, N = 15, etc... . Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. . We can provide separate tools to re-ambiguate the output or reselect the reference allele as the population major allele, so making the user very aware of this. For example he/she should have an decision-making input as to how we are supposed to handle het calls where both a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914
https://github.com/broadinstitute/gatk/issues/2914:1469,Availability,mask,mask,1469,"erates on a whole region. And a post hoc check in HC would be simple enough for SNVs, but what happens when the ambiguous site is part of a larger deletion?. Needs advice on what the behavior / solution should be by @akiezun @vruano . This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/discussion/4858/reference-bases-with-ambiguity-codes-in-dbsnp/p1) . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85093784). In general, don't know how HC behaves with ambiguous reference bases at all.... I would not be surprised if it just crashes or outputs garbage. Perhaps this should be part of a larger effort to make sure HC, Combine- and GenotypeGVCFs are robust on ambiguous calls. To start, currently GATK/Picard handles bases as uppercase single `byte' representation of the corresponding character. Since we are investing (a mostly wasting) 8 bits already, we could change into a bit mask representation that would allow for quick comparison of ambiguous and non-ambigous base call using bit-wise operations. NO_CALL = 0, A = 1, C = 2, G = 4, T/U = 8, N = 15, etc... . Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. . We can provide separate tools to re-ambiguate the output or reselect the reference allele as the population major allele, so making the user very aware of this. For example he/she should have an decision-making input as to how we are supposed to handle het calls where both alleles are compatible with the reference ambiguous call; I don't think is totally correct to think of these as *hom*-ref calls but if that is what the user wants... Handling ambiguous calls in the reads... I presume that these have low quality and thus are ignored, and if not we should",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914
https://github.com/broadinstitute/gatk/issues/2914:3199,Availability,down,down,3199,"biguous call; I don't think is totally correct to think of these as *hom*-ref calls but if that is what the user wants... Handling ambiguous calls in the reads... I presume that these have low quality and thus are ignored, and if not we should force them to. . ---. @vdauwera commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85098560). I assume ambiguous basecalls in reads are ignored and therefore not an issue. It's really what to do with ambiguous ref bases that concerns me. Currently it seems that HC just accepts them as legitimate bases in certain conditions at least. . I'm not sure I understand this part:. > Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. That comes down to randomly assigning a ref allele at that site, doesn't it? I'm not sure I'm comfortable with that. ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85121261). Not random, A has priority, then C, then G and finally T amongst those that are compatible with the ambiguous code. For Example for N it would be A, but for B would C (as B means C/G/T(U)). ---. @vdauwera commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85123717). Well, I understand that it's alphabetical, but I mean it's not really meaningful -- it's even worse than random since the choice is biased by whatever accident of history caused A to be earlier in the alphabet than C. To be clear I don't have a better idea, but this one bugs me. ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85134672). I think this bias is nothing compare with the one introduced by the reference itself. I don",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914
https://github.com/broadinstitute/gatk/issues/2914:550,Usability,simpl,simple,550,"@vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/766). @vdauwera commented on [Fri Mar 06 2015](https://github.com/broadinstitute/gsa-unstable/issues/829). As @pdexheimer pointed out: . > I think the bug here is in HaplotypeCaller. It technically generated a malformed (g)VCF by using an ambiguous allele for the reference. I don't know what the fix is, though. You can't have an ActiveRegionWalker skip over the ambiguous bases since it operates on a whole region. And a post hoc check in HC would be simple enough for SNVs, but what happens when the ambiguous site is part of a larger deletion?. Needs advice on what the behavior / solution should be by @akiezun @vruano . This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/discussion/4858/reference-bases-with-ambiguity-codes-in-dbsnp/p1) . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85093784). In general, don't know how HC behaves with ambiguous reference bases at all.... I would not be surprised if it just crashes or outputs garbage. Perhaps this should be part of a larger effort to make sure HC, Combine- and GenotypeGVCFs are robust on ambiguous calls. To start, currently GATK/Picard handles bases as uppercase single `byte' representation of the corresponding character. Since we are investing (a mostly wasting) 8 bits already, we could change into a bit mask representation that would allow for quick comparison of ambiguous and non-ambigous base call using bit-wise operations. NO_CALL = 0, A = 1, C = 2, G = 4, T/U = 8, N = 15, etc... . Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. . We can provide separate tools to re-ambiguate the output or reselect the r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914
https://github.com/broadinstitute/gatk/issues/2914:1717,Usability,clear,clearest,1717,"atkforums.broadinstitute.org/discussion/4858/reference-bases-with-ambiguity-codes-in-dbsnp/p1) . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85093784). In general, don't know how HC behaves with ambiguous reference bases at all.... I would not be surprised if it just crashes or outputs garbage. Perhaps this should be part of a larger effort to make sure HC, Combine- and GenotypeGVCFs are robust on ambiguous calls. To start, currently GATK/Picard handles bases as uppercase single `byte' representation of the corresponding character. Since we are investing (a mostly wasting) 8 bits already, we could change into a bit mask representation that would allow for quick comparison of ambiguous and non-ambigous base call using bit-wise operations. NO_CALL = 0, A = 1, C = 2, G = 4, T/U = 8, N = 15, etc... . Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. . We can provide separate tools to re-ambiguate the output or reselect the reference allele as the population major allele, so making the user very aware of this. For example he/she should have an decision-making input as to how we are supposed to handle het calls where both alleles are compatible with the reference ambiguous call; I don't think is totally correct to think of these as *hom*-ref calls but if that is what the user wants... Handling ambiguous calls in the reads... I presume that these have low quality and thus are ignored, and if not we should force them to. . ---. @vdauwera commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85098560). I assume ambiguous basecalls in reads are ignored and therefore not an issue. It's really what to do with ambiguous ref bases that concerns m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914
https://github.com/broadinstitute/gatk/issues/2914:2980,Usability,clear,clearest,2980,"or allele, so making the user very aware of this. For example he/she should have an decision-making input as to how we are supposed to handle het calls where both alleles are compatible with the reference ambiguous call; I don't think is totally correct to think of these as *hom*-ref calls but if that is what the user wants... Handling ambiguous calls in the reads... I presume that these have low quality and thus are ignored, and if not we should force them to. . ---. @vdauwera commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85098560). I assume ambiguous basecalls in reads are ignored and therefore not an issue. It's really what to do with ambiguous ref bases that concerns me. Currently it seems that HC just accepts them as legitimate bases in certain conditions at least. . I'm not sure I understand this part:. > Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. That comes down to randomly assigning a ref allele at that site, doesn't it? I'm not sure I'm comfortable with that. ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85121261). Not random, A has priority, then C, then G and finally T amongst those that are compatible with the ambiguous code. For Example for N it would be A, but for B would C (as B means C/G/T(U)). ---. @vdauwera commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85123717). Well, I understand that it's alphabetical, but I mean it's not really meaningful -- it's even worse than random since the choice is biased by whatever accident of history caused A to be earlier in the alphabet than C. To be clear I don't have a better idea, but this one bugs me. ---. @vru",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914
https://github.com/broadinstitute/gatk/issues/2914:3973,Usability,clear,clear,3973,"ment-85098560). I assume ambiguous basecalls in reads are ignored and therefore not an issue. It's really what to do with ambiguous ref bases that concerns me. Currently it seems that HC just accepts them as legitimate bases in certain conditions at least. . I'm not sure I understand this part:. > Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. That comes down to randomly assigning a ref allele at that site, doesn't it? I'm not sure I'm comfortable with that. ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85121261). Not random, A has priority, then C, then G and finally T amongst those that are compatible with the ambiguous code. For Example for N it would be A, but for B would C (as B means C/G/T(U)). ---. @vdauwera commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85123717). Well, I understand that it's alphabetical, but I mean it's not really meaningful -- it's even worse than random since the choice is biased by whatever accident of history caused A to be earlier in the alphabet than C. To be clear I don't have a better idea, but this one bugs me. ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85134672). I think this bias is nothing compare with the one introduced by the reference itself. I don't see why it should it even be part of the VCF output but that would even bug more people. . In any case I think that is the way HC should handle ambiguity internally ... . if instead of having a separate tool to re-ambiguate we do it standard at the last step of HC that should be fine but then I don't know if we would be able to make every body happy at the same time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914
https://github.com/broadinstitute/gatk/issues/2915:211,Availability,error,error,211,"@vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/772). @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064). The PCR error model applied pre-pairHMM does not seem to always do the right thing. . This is explained in class TandemRepeatFinder JavaDoc (soon to be merged in). Moreover the code responsible to detect STR repeats seems rather inefficient doing multiple passes on the read bases for each position on the read when it seems that it must be possible to accomplish the same just doing at most one pass per possible STR length. This task is to fix the PCR artifact modeling issues evaluating whether there is at least no a drop in calling accuracy all. Also try to make the code more efficient. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431158). @yfarjoun and I just added a Palantir issue for this this morning -- should the analysis wait until you're done updating the code?. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431971). Just waiting for test to pass...; So you knew about this issue already?. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123432816). We were talking about it because the PCR-free option doesn't get used in production (on PCR-free data) and we didn't know how much difference it actually makes. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481297). Merged. ; I think that you can go ahead with the analysis and I would borrow your set up to see if the eventual code update improves things for PCR-plus. . ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481614). Sorry for the confusion, that merge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915
https://github.com/broadinstitute/gatk/issues/2915:1799,Deployability,update,update,1799,"es on the read bases for each position on the read when it seems that it must be possible to accomplish the same just doing at most one pass per possible STR length. This task is to fix the PCR artifact modeling issues evaluating whether there is at least no a drop in calling accuracy all. Also try to make the code more efficient. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431158). @yfarjoun and I just added a Palantir issue for this this morning -- should the analysis wait until you're done updating the code?. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431971). Just waiting for test to pass...; So you knew about this issue already?. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123432816). We were talking about it because the PCR-free option doesn't get used in production (on PCR-free data) and we didn't know how much difference it actually makes. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481297). Merged. ; I think that you can go ahead with the analysis and I would borrow your set up to see if the eventual code update improves things for PCR-plus. . ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481614). Sorry for the confusion, that merge doesn't solve this issue but one one related to the comp. performance of the existing code. . ---. @eitanbanks commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123482433). @ldgauthier that's a different issue. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-260465303). Moving to GATK4; decide there whether it's still applicable or not.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915
https://github.com/broadinstitute/gatk/issues/2915:785,Energy Efficiency,efficient,efficient,785,"@vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/772). @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064). The PCR error model applied pre-pairHMM does not seem to always do the right thing. . This is explained in class TandemRepeatFinder JavaDoc (soon to be merged in). Moreover the code responsible to detect STR repeats seems rather inefficient doing multiple passes on the read bases for each position on the read when it seems that it must be possible to accomplish the same just doing at most one pass per possible STR length. This task is to fix the PCR artifact modeling issues evaluating whether there is at least no a drop in calling accuracy all. Also try to make the code more efficient. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431158). @yfarjoun and I just added a Palantir issue for this this morning -- should the analysis wait until you're done updating the code?. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431971). Just waiting for test to pass...; So you knew about this issue already?. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123432816). We were talking about it because the PCR-free option doesn't get used in production (on PCR-free data) and we didn't know how much difference it actually makes. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481297). Merged. ; I think that you can go ahead with the analysis and I would borrow your set up to see if the eventual code update improves things for PCR-plus. . ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481614). Sorry for the confusion, that merge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915
https://github.com/broadinstitute/gatk/issues/2915:2060,Performance,perform,performance,2060,"es on the read bases for each position on the read when it seems that it must be possible to accomplish the same just doing at most one pass per possible STR length. This task is to fix the PCR artifact modeling issues evaluating whether there is at least no a drop in calling accuracy all. Also try to make the code more efficient. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431158). @yfarjoun and I just added a Palantir issue for this this morning -- should the analysis wait until you're done updating the code?. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431971). Just waiting for test to pass...; So you knew about this issue already?. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123432816). We were talking about it because the PCR-free option doesn't get used in production (on PCR-free data) and we didn't know how much difference it actually makes. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481297). Merged. ; I think that you can go ahead with the analysis and I would borrow your set up to see if the eventual code update improves things for PCR-plus. . ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481614). Sorry for the confusion, that merge doesn't solve this issue but one one related to the comp. performance of the existing code. . ---. @eitanbanks commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123482433). @ldgauthier that's a different issue. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-260465303). Moving to GATK4; decide there whether it's still applicable or not.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915
https://github.com/broadinstitute/gatk/issues/2915:400,Safety,detect,detect,400,"@vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/772). @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064). The PCR error model applied pre-pairHMM does not seem to always do the right thing. . This is explained in class TandemRepeatFinder JavaDoc (soon to be merged in). Moreover the code responsible to detect STR repeats seems rather inefficient doing multiple passes on the read bases for each position on the read when it seems that it must be possible to accomplish the same just doing at most one pass per possible STR length. This task is to fix the PCR artifact modeling issues evaluating whether there is at least no a drop in calling accuracy all. Also try to make the code more efficient. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431158). @yfarjoun and I just added a Palantir issue for this this morning -- should the analysis wait until you're done updating the code?. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431971). Just waiting for test to pass...; So you knew about this issue already?. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123432816). We were talking about it because the PCR-free option doesn't get used in production (on PCR-free data) and we didn't know how much difference it actually makes. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481297). Merged. ; I think that you can go ahead with the analysis and I would borrow your set up to see if the eventual code update improves things for PCR-plus. . ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481614). Sorry for the confusion, that merge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915
https://github.com/broadinstitute/gatk/issues/2915:1205,Testability,test,test,1205," The PCR error model applied pre-pairHMM does not seem to always do the right thing. . This is explained in class TandemRepeatFinder JavaDoc (soon to be merged in). Moreover the code responsible to detect STR repeats seems rather inefficient doing multiple passes on the read bases for each position on the read when it seems that it must be possible to accomplish the same just doing at most one pass per possible STR length. This task is to fix the PCR artifact modeling issues evaluating whether there is at least no a drop in calling accuracy all. Also try to make the code more efficient. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431158). @yfarjoun and I just added a Palantir issue for this this morning -- should the analysis wait until you're done updating the code?. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431971). Just waiting for test to pass...; So you knew about this issue already?. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123432816). We were talking about it because the PCR-free option doesn't get used in production (on PCR-free data) and we didn't know how much difference it actually makes. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481297). Merged. ; I think that you can go ahead with the analysis and I would borrow your set up to see if the eventual code update improves things for PCR-plus. . ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481614). Sorry for the confusion, that merge doesn't solve this issue but one one related to the comp. performance of the existing code. . ---. @eitanbanks commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915
https://github.com/broadinstitute/gatk/issues/2916:11712,Availability,recover,recover,11712," screenshot of the bamout file from 3.1. <img width=""1440"" alt=""screen shot 2016-05-31 at 4 56 20 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/15690232/9dfc6992-2750-11e6-94c4-0c055b3ad1bc.png"">; The first green SNP on the left is the one in question. ---. @chandrans commented on [Tue Jun 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-225999879). Figured out at Support meeting that the variant SNP is called when you include -allowNonUniqueKmersInRef in the command. . It seems the kmer including the SNP is quite common the region. I am going to tell the user about using the flag. However, I think David will take a look into the code to see what exactly is going on and whether it is a good idea to recommend using the flag in repeat regions. ---. @ldgauthier commented on [Wed Jun 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-226191676). Valentin has found that that arg is able to recover a lot of our missed; indels in the pseudo-diploid truth data, so it's worth investigating.; However, I believe when I tried it for MuTect2 against the LUAD data I; introduced a not insignificant number of additional variants, likely false; positives. On Tue, Jun 14, 2016 at 4:06 PM, chandrans <notifications@github.com> wrote:. > Figured out at Support meeting that the variant SNP is called when you; > include -allowNonUniqueKmersInRef in the command.; >; > It seems the kmer including the SNP is quite common the region.; >; > I am going to tell the user about using the flag. However, I think David; > will take a look into the code to see what exactly is going on and whether; > it is a good idea to recommend using the flag in repeat regions.; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-225999879>,; > or mute the thread; > <https://github.com/notifications/unsubscri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916
https://github.com/broadinstitute/gatk/issues/2916:10959,Energy Efficiency,green,green,10959," Note that the annotation for the 3.1 call says MQ=60. I'm very suspicious of the large deletion that gets introduced after the SNP in question in the 3.1 bamout. I suspect that might be related to the GQ0 ref call in 3.5. Can I get some more info on that? (e.g. end position, zoomed out screenshot, etc.). ---. @chandrans commented on [Tue May 31 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-222817988). Okay. I met the user at last week's MPG and I told him I would put in a good word for him :) ; He came back today confirming that the SNP is real by Sanger sequencing. As for your questions Laura, the deletion present in the artificial haplotypes and some of the reads does not get called in 3.1 or in 3.5. Here is a zoomed out screenshot of the bamout file from 3.1. <img width=""1440"" alt=""screen shot 2016-05-31 at 4 56 20 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/15690232/9dfc6992-2750-11e6-94c4-0c055b3ad1bc.png"">; The first green SNP on the left is the one in question. ---. @chandrans commented on [Tue Jun 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-225999879). Figured out at Support meeting that the variant SNP is called when you include -allowNonUniqueKmersInRef in the command. . It seems the kmer including the SNP is quite common the region. I am going to tell the user about using the flag. However, I think David will take a look into the code to see what exactly is going on and whether it is a good idea to recommend using the flag in repeat regions. ---. @ldgauthier commented on [Wed Jun 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-226191676). Valentin has found that that arg is able to recover a lot of our missed; indels in the pseudo-diploid truth data, so it's worth investigating.; However, I believe when I tried it for MuTect2 against the LUAD data I; introduced a not insignificant number of additional variants, likely false; positives. On",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916
https://github.com/broadinstitute/gatk/issues/2916:8495,Safety,detect,detecting,8495,"whether these supplementary alignments that get flagged secondary (with the `-M) also get MAPQ of 0 or have other nonzero MAPQs. We want our tools, including HaplotypeCaller, to differentiate supplementary alignments and secondary alignments and use supplementary alignments in variant discovery. . Secondary alignments are meant for multimappers (multiple valid mapping locations) and supplementary alignments are meant for chimeric reads (say two records for the same read where one half aligns to the left and the other half aligns to the right of a very large deletion against the reference). This means that we should run bwa mem without the `-M` option. . Ok, so I'm going to resume thinking HaplotypeCaller filters on MAPQ of 20. ---. @sooheelee commented on [Wed May 11 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-218558684). The implications of this is that (I think) for any workflow that cares about detecting indels in the size range that BWA-MEM would create supplementary alignment records for would then require that we run BWA-MEM without the `-M` option that we currently recommend. We want both types of mappings. ---. @vdauwera commented on [Wed May 11 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-218591369). For anyone confused about the difference between secondary and supplementary alignments: http://seqanswers.com/forums/showthread.php?t=40239. Currently we actually *don't* want our variant calling tools to distinguish them -- we prefer to consider them unusable. My understanding is that the size of events that lead to supplementary alignments fall into the scope of structural variation, and any reads that map across them are considered suspect. . So anyway for this user's case it sounds like the difference between versions is that 3.1 was making a call based on data that we (no longer?) consider as receivable evidence. So we should be satisfied with the call made by 3.5. @ldgauthier would you sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916
https://github.com/broadinstitute/gatk/issues/2916:11712,Safety,recover,recover,11712," screenshot of the bamout file from 3.1. <img width=""1440"" alt=""screen shot 2016-05-31 at 4 56 20 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/15690232/9dfc6992-2750-11e6-94c4-0c055b3ad1bc.png"">; The first green SNP on the left is the one in question. ---. @chandrans commented on [Tue Jun 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-225999879). Figured out at Support meeting that the variant SNP is called when you include -allowNonUniqueKmersInRef in the command. . It seems the kmer including the SNP is quite common the region. I am going to tell the user about using the flag. However, I think David will take a look into the code to see what exactly is going on and whether it is a good idea to recommend using the flag in repeat regions. ---. @ldgauthier commented on [Wed Jun 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-226191676). Valentin has found that that arg is able to recover a lot of our missed; indels in the pseudo-diploid truth data, so it's worth investigating.; However, I believe when I tried it for MuTect2 against the LUAD data I; introduced a not insignificant number of additional variants, likely false; positives. On Tue, Jun 14, 2016 at 4:06 PM, chandrans <notifications@github.com> wrote:. > Figured out at Support meeting that the variant SNP is called when you; > include -allowNonUniqueKmersInRef in the command.; >; > It seems the kmer including the SNP is quite common the region.; >; > I am going to tell the user about using the flag. However, I think David; > will take a look into the code to see what exactly is going on and whether; > it is a good idea to recommend using the flag in repeat regions.; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-225999879>,; > or mute the thread; > <https://github.com/notifications/unsubscri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916
https://github.com/broadinstitute/gatk/issues/2916:2447,Testability,test,test,2447,"s://cloud.githubusercontent.com/assets/6998669/15158582/9873a712-16be-11e6-95e1-f3f54d92b83b.png"">; If we zoom out:; <img width=""1440"" alt=""screen shot 2016-05-10 at 2 51 31 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/15158604/b49b60ba-16be-11e6-9727-c9b31b237483.png"">. Then, the bamout file looks like this:; <img width=""1440"" alt=""screen shot 2016-05-10 at 2 53 42 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/15158671/009f95b2-16bf-11e6-9242-de35c56b02f9.png"">; No active region there. However, the bamout file from 3.1 looks like this:; <img width=""1440"" alt=""screen shot 2016-05-10 at 2 56 08 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/15158769/582d17dc-16bf-11e6-8382-3b2eebe20c27.png"">; Notice the position is part of an active region, but all the reads have MQ0! Yet, somehow this is confidently called as a SNP site. Last thing, I checked the graphs from both versions and both look exactly the same. ; #### Steps to reproduce. Commands and test files below.; #### Expected behavior. The SNP should be called in version 3.5 and above.; #### Actual behavior. The SNP is not called. ---; #### [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/29874). ---. @chandrans commented on [Tue May 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-218258856). Test files here:; /humgen/gsa-scr1/schandra/jhomsy_MissingVariantInFather/homsy-missing-variant/snippet. Command for 3.5:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/b37/human_g1k_v37.fasta -I /humgen/gsa-scr1/schandra/jhomsy_MissingVariantInFather/homsy-missing-variant/snippet/1-03174-02.snippet.bam -L 5:180040878-180041478 -o Sheila.g.vcf -bamout Sheila.g.bam -ERC GVCF -graph Sheila.g.dot`. Command for 3.1:; ` java -jar /humgen/gsa-hpprojects/GATK/bin/GenomeAnalysisTK-3.1-1-g07a4bf8/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916
https://github.com/broadinstitute/gatk/issues/2916:2819,Testability,Test,Test,2819,"nt.com/assets/6998669/15158671/009f95b2-16bf-11e6-9242-de35c56b02f9.png"">; No active region there. However, the bamout file from 3.1 looks like this:; <img width=""1440"" alt=""screen shot 2016-05-10 at 2 56 08 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/15158769/582d17dc-16bf-11e6-8382-3b2eebe20c27.png"">; Notice the position is part of an active region, but all the reads have MQ0! Yet, somehow this is confidently called as a SNP site. Last thing, I checked the graphs from both versions and both look exactly the same. ; #### Steps to reproduce. Commands and test files below.; #### Expected behavior. The SNP should be called in version 3.5 and above.; #### Actual behavior. The SNP is not called. ---; #### [Original Forum Post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/29874). ---. @chandrans commented on [Tue May 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-218258856). Test files here:; /humgen/gsa-scr1/schandra/jhomsy_MissingVariantInFather/homsy-missing-variant/snippet. Command for 3.5:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/b37/human_g1k_v37.fasta -I /humgen/gsa-scr1/schandra/jhomsy_MissingVariantInFather/homsy-missing-variant/snippet/1-03174-02.snippet.bam -L 5:180040878-180041478 -o Sheila.g.vcf -bamout Sheila.g.bam -ERC GVCF -graph Sheila.g.dot`. Command for 3.1:; ` java -jar /humgen/gsa-hpprojects/GATK/bin/GenomeAnalysisTK-3.1-1-g07a4bf8/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/b37/human_g1k_v37.fasta -I /humgen/gsa-scr1/schandra/jhomsy_MissingVariantInFather/homsy-missing-variant/snippet/1-03174-02.snippet.bam -L 5:180040878-180041478 -o Sheila.3.1.g.vcf -bamout Sheila.3.1.g.bam -ERC GVCF -variant_index_type LINEAR -variant_index_parameter 128000 -graph Sheila.3.1.g.dot`. ---. @chandrans commented on [Tue May 10 2016](https://github.com/broadinstitute/gsa-unst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916
https://github.com/broadinstitute/gatk/issues/2916:7395,Usability,learn,learned,7395,"BXX:2:1107:10125:22061 147 5 180041116 *0 *; 63M = 180040938 -241; TCTGCGTGGTGTACACCTTGTCGAAGATGCTTTCAGGGGCCATCCACTTCAGGGGCAGCCGGG; :???<>??>?>>>?>????>><>=>>=>>>>>=>>>>>>>>==>>=>>=>>>>>>>>?>9<>=; HC:i:-369427419 MC:Z:101M; BD:Z:IONKLIMLIILMIILKJJILMLJJHMMNOJBIMNKHHMLLMMJLINJIMNLIINOPPONKIKK; MD:Z:101 PG:Z:MarkDuplicates.3.7 RG:Z:1; BI:Z:LQPNNLMNLKMNLLNLLLKMNLKLLMNNNKDJNMLGGLMMNOJMLNLKONMHINOORPNKIMM; NM:i:0 *MQ:i:60 * *AS:i:101 * XS:i:0; -bash-3.2$; ````. In terms of MAPQ being zero, this happens for reads with multiple valid; mappings. That doesn't mean it's a bad mapping, and in fact it can be a; great mapping as you can see for the example read above in the AS tag. ###REVISED; So MAPQ indicates global mapping and AS measures local mapping score.; If one mapping site contains a variant and the other does not, then calling variants for each mapped site is not a good idea. I don't know how supplementary reads are differentiated (MAPQ?--I can look into this), since the way I learned how to run bwa mem asks that all supplementary alignments be treated as secondary alignments (with the `-M` option). It seems important to confirm whether these supplementary alignments that get flagged secondary (with the `-M) also get MAPQ of 0 or have other nonzero MAPQs. We want our tools, including HaplotypeCaller, to differentiate supplementary alignments and secondary alignments and use supplementary alignments in variant discovery. . Secondary alignments are meant for multimappers (multiple valid mapping locations) and supplementary alignments are meant for chimeric reads (say two records for the same read where one half aligns to the left and the other half aligns to the right of a very large deletion against the reference). This means that we should run bwa mem without the `-M` option. . Ok, so I'm going to resume thinking HaplotypeCaller filters on MAPQ of 20. ---. @sooheelee commented on [Wed May 11 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916
https://github.com/broadinstitute/gatk/issues/2916:8232,Usability,resume,resume,8232,"ts for each mapped site is not a good idea. I don't know how supplementary reads are differentiated (MAPQ?--I can look into this), since the way I learned how to run bwa mem asks that all supplementary alignments be treated as secondary alignments (with the `-M` option). It seems important to confirm whether these supplementary alignments that get flagged secondary (with the `-M) also get MAPQ of 0 or have other nonzero MAPQs. We want our tools, including HaplotypeCaller, to differentiate supplementary alignments and secondary alignments and use supplementary alignments in variant discovery. . Secondary alignments are meant for multimappers (multiple valid mapping locations) and supplementary alignments are meant for chimeric reads (say two records for the same read where one half aligns to the left and the other half aligns to the right of a very large deletion against the reference). This means that we should run bwa mem without the `-M` option. . Ok, so I'm going to resume thinking HaplotypeCaller filters on MAPQ of 20. ---. @sooheelee commented on [Wed May 11 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-218558684). The implications of this is that (I think) for any workflow that cares about detecting indels in the size range that BWA-MEM would create supplementary alignment records for would then require that we run BWA-MEM without the `-M` option that we currently recommend. We want both types of mappings. ---. @vdauwera commented on [Wed May 11 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-218591369). For anyone confused about the difference between secondary and supplementary alignments: http://seqanswers.com/forums/showthread.php?t=40239. Currently we actually *don't* want our variant calling tools to distinguish them -- we prefer to consider them unusable. My understanding is that the size of events that lead to supplementary alignments fall into the scope of structural variation, and any reads",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916
https://github.com/broadinstitute/gatk/issues/2917:2138,Testability,test,test,2138,"over each position in the gVCF/NAVS, it will genotype the same insertion separately for the 2 samples (because they occur in records at different positions). ---. @vdauwera commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/857#issuecomment-122027548). May be solved by the spanning deletion fix. @eitanbanks do you still want methods to look at this? They need a concrete example. . ---. @eitanbanks commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/857#issuecomment-122310497). This is not solved by the spanning deletions fix. Do you want me to create two sample gVCFs that illustrate this problem?. ---. @ldgauthier commented on [Fri Jul 31 2015](https://github.com/broadinstitute/gsa-unstable/issues/857#issuecomment-126673116). Here's the example Eric came up with when we discussed this:; ![cam00218](https://cloud.githubusercontent.com/assets/6578548/9007026/6f580b40-375b-11e5-8d2d-0022b9c5646a.jpg); (I have no idea why Github rotated this); If there is a het-non-ref sample (like S1), its alleles can be represented differently in the gVCF than a sample with a bi-allelic variant. Then when they get genotyped together, the same allele can show up at two different positions in the combined VCF, i.e. the T insertion is listed at position 325 for S1 and 326 for S2, but it's the same variant. This is probably what happened in the ExAC example (#1072).; It would be great for someone to write a HC (gVCF mode) unit test for this with some artificial reads so we can start working on a splitting procedure. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/857#issuecomment-260457250). Does anyone still care about this? If so, should it go into the GATK4 repo? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/857#issuecomment-260639851). I care, I just don't have the bandwidth to work on it. Please move to GATK4.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2917
https://github.com/broadinstitute/gatk/issues/2918:6253,Availability,alive,alive,6253,"one iterations:. 0) Initialize allele frequencies to the mean of the Dirichlet heterozygosity prior; i.e. ~1 for ref, ~1/1000 for each alt, plus any allele counts from the resources. Genotype priors come from the multinomial distribution (one genotype is a draw of 2 alleles) of these allele frequencies.; 1) (E step) genotype posteriors are the product of genotype likelihoods with the priors from step 0). Pseudocounts are the sum of expected posterior allele counts.; 2) (M step) MLE allele frequencies are the mode of the Dirichlet parameterized by the sum of the original step 0) prior+resources pseudocounts with the E step pseudocounts from step 1). Hmmm that does sound a lot like what the code is doing now. I suppose it's defensible after all. ---. @ldgauthier commented on [Thu May 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-220347447). So it sounds to me like the action item here is to fix the Dirichlet heterozygosity prior. I like the idea of adding one count for the ref and 1/1000 for each alt (rather than, for example, 1000 for ref and one for alt) so the heterozygosity prior does something in the absence of external resource counts, but doesn't overwhelms them if they are present. @davidbenjamin Can you think of a more rigorous justification for the scaling of counts between sample genotype allele counts and the heterozygosity?. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-260474993). Is this still alive? To be continued in GATK3 or 4?. ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-260646009). You can move to GATK4. ---. @davidbenjamin commented on [Sun Nov 20 2016](https://github.com/broadinstitute/gatk-protected/issues/792#issuecomment-261761138). @vdauwera @ldgauthier the new qual score model does exactly this. IMO we should simply have this CLI use `AlleleFrequencyCalculator`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918
https://github.com/broadinstitute/gatk/issues/2918:4059,Deployability,update,update,4059,"ltiply (add in log space) this prior by the genotype likelihoods to get the posterior probabilities on genotypes. It appears to me that we have double-counted the input data, once to get its AC field and once to get its GLs. I believe the correct thing to do is use only the resources to define a prior which is then combined with the GLs to get the posterior. ---. @ldgauthier commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219778100). I will take the blame (both figuratively and the literal git blame) for PosteriorLikelihoodsUtils nomenclature problems. I either initiated them or didn't fix them when I refactored. I also intuitively prefer resources only without using the input AC, but that being said we've seen better results using both, specifically for a Finnish cohort with 100 founders. In the DSDE/ATGU meetings the use of the input AC was discussed as being analogous to a single step of EM. Would the true EM apply a different update for each sample in the callset?. ---. @davidbenjamin commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219833193). The better results using the double-counting might have something to do with the incorrect prior -- if the prior is skewing toward homozygosity, then double-counting your variant data might counteract this and rescue some variant genotypes, which will be mainly hets. The EM model that people implicitly seem to have in mind is alternating E steps on each sample to get genotype posteriors with M steps to learn the allele frequencies. So let's work out what happens if you do just one iterations:. 0) Initialize allele frequencies to the mean of the Dirichlet heterozygosity prior; i.e. ~1 for ref, ~1/1000 for each alt, plus any allele counts from the resources. Genotype priors come from the multinomial distribution (one genotype is a draw of 2 alleles) of these allele frequencies.; 1) (E step) genotype posteriors are th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918
https://github.com/broadinstitute/gatk/issues/2918:3724,Modifiability,refactor,refactored,3724,"C fields, and add the prior pseudocounts. Lines 111-144 grab the genotype likelihoods from the input. In line 149 we pass the allele counts and likelihoods to `calculatePosteriorGLs`. This method uses the allele counts (prior + resources + input AC) to define a Dirichlet distribution which then serves as the prior on genotypes. Finally, on line 203 we multiply (add in log space) this prior by the genotype likelihoods to get the posterior probabilities on genotypes. It appears to me that we have double-counted the input data, once to get its AC field and once to get its GLs. I believe the correct thing to do is use only the resources to define a prior which is then combined with the GLs to get the posterior. ---. @ldgauthier commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219778100). I will take the blame (both figuratively and the literal git blame) for PosteriorLikelihoodsUtils nomenclature problems. I either initiated them or didn't fix them when I refactored. I also intuitively prefer resources only without using the input AC, but that being said we've seen better results using both, specifically for a Finnish cohort with 100 founders. In the DSDE/ATGU meetings the use of the input AC was discussed as being analogous to a single step of EM. Would the true EM apply a different update for each sample in the callset?. ---. @davidbenjamin commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219833193). The better results using the double-counting might have something to do with the incorrect prior -- if the prior is skewing toward homozygosity, then double-counting your variant data might counteract this and rescue some variant genotypes, which will be mainly hets. The EM model that people implicitly seem to have in mind is alternating E steps on each sample to get genotype posteriors with M steps to learn the allele frequencies. So let's work out what happens if ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918
https://github.com/broadinstitute/gatk/issues/2918:5252,Modifiability,parameteriz,parameterized,5252," might have something to do with the incorrect prior -- if the prior is skewing toward homozygosity, then double-counting your variant data might counteract this and rescue some variant genotypes, which will be mainly hets. The EM model that people implicitly seem to have in mind is alternating E steps on each sample to get genotype posteriors with M steps to learn the allele frequencies. So let's work out what happens if you do just one iterations:. 0) Initialize allele frequencies to the mean of the Dirichlet heterozygosity prior; i.e. ~1 for ref, ~1/1000 for each alt, plus any allele counts from the resources. Genotype priors come from the multinomial distribution (one genotype is a draw of 2 alleles) of these allele frequencies.; 1) (E step) genotype posteriors are the product of genotype likelihoods with the priors from step 0). Pseudocounts are the sum of expected posterior allele counts.; 2) (M step) MLE allele frequencies are the mode of the Dirichlet parameterized by the sum of the original step 0) prior+resources pseudocounts with the E step pseudocounts from step 1). Hmmm that does sound a lot like what the code is doing now. I suppose it's defensible after all. ---. @ldgauthier commented on [Thu May 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-220347447). So it sounds to me like the action item here is to fix the Dirichlet heterozygosity prior. I like the idea of adding one count for the ref and 1/1000 for each alt (rather than, for example, 1000 for ref and one for alt) so the heterozygosity prior does something in the absence of external resource counts, but doesn't overwhelms them if they are present. @davidbenjamin Can you think of a more rigorous justification for the scaling of counts between sample genotype allele counts and the heterozygosity?. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-260474993). Is this still alive? To be continued in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918
https://github.com/broadinstitute/gatk/issues/2918:3075,Testability,log,log,3075,"ocounts to the ref allele and 1 prior pseudocount to each alt allele (for SNPS, at least). A couple more things I noticed in this code:; * `PosteriorLikelihoodsUtils` has a nomenclature problem. Likelihoods, priors, and posteriors are getting conflated and combined in various amusing permutations, chief among them being that a posterior likelihood is an oxymoron.; * It seems like there may be an overcounting problem in `PosteriorLikelihoodsUtils::calculatePosteriorGLs`. If I follow correctly, lines 82-100 get the total counts for each allele in the resources and in the input, via the MLEAC or AC fields, and add the prior pseudocounts. Lines 111-144 grab the genotype likelihoods from the input. In line 149 we pass the allele counts and likelihoods to `calculatePosteriorGLs`. This method uses the allele counts (prior + resources + input AC) to define a Dirichlet distribution which then serves as the prior on genotypes. Finally, on line 203 we multiply (add in log space) this prior by the genotype likelihoods to get the posterior probabilities on genotypes. It appears to me that we have double-counted the input data, once to get its AC field and once to get its GLs. I believe the correct thing to do is use only the resources to define a prior which is then combined with the GLs to get the posterior. ---. @ldgauthier commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219778100). I will take the blame (both figuratively and the literal git blame) for PosteriorLikelihoodsUtils nomenclature problems. I either initiated them or didn't fix them when I refactored. I also intuitively prefer resources only without using the input AC, but that being said we've seen better results using both, specifically for a Finnish cohort with 100 founders. In the DSDE/ATGU meetings the use of the input AC was discussed as being analogous to a single step of EM. Would the true EM apply a different update for each sample in the callset?. ---.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918
https://github.com/broadinstitute/gatk/issues/2918:3743,Usability,intuit,intuitively,3743," the allele counts and likelihoods to `calculatePosteriorGLs`. This method uses the allele counts (prior + resources + input AC) to define a Dirichlet distribution which then serves as the prior on genotypes. Finally, on line 203 we multiply (add in log space) this prior by the genotype likelihoods to get the posterior probabilities on genotypes. It appears to me that we have double-counted the input data, once to get its AC field and once to get its GLs. I believe the correct thing to do is use only the resources to define a prior which is then combined with the GLs to get the posterior. ---. @ldgauthier commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219778100). I will take the blame (both figuratively and the literal git blame) for PosteriorLikelihoodsUtils nomenclature problems. I either initiated them or didn't fix them when I refactored. I also intuitively prefer resources only without using the input AC, but that being said we've seen better results using both, specifically for a Finnish cohort with 100 founders. In the DSDE/ATGU meetings the use of the input AC was discussed as being analogous to a single step of EM. Would the true EM apply a different update for each sample in the callset?. ---. @davidbenjamin commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219833193). The better results using the double-counting might have something to do with the incorrect prior -- if the prior is skewing toward homozygosity, then double-counting your variant data might counteract this and rescue some variant genotypes, which will be mainly hets. The EM model that people implicitly seem to have in mind is alternating E steps on each sample to get genotype posteriors with M steps to learn the allele frequencies. So let's work out what happens if you do just one iterations:. 0) Initialize allele frequencies to the mean of the Dirichlet heterozygosity prior; i.e. ~1 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918
https://github.com/broadinstitute/gatk/issues/2918:4640,Usability,learn,learn,4640,"ratively and the literal git blame) for PosteriorLikelihoodsUtils nomenclature problems. I either initiated them or didn't fix them when I refactored. I also intuitively prefer resources only without using the input AC, but that being said we've seen better results using both, specifically for a Finnish cohort with 100 founders. In the DSDE/ATGU meetings the use of the input AC was discussed as being analogous to a single step of EM. Would the true EM apply a different update for each sample in the callset?. ---. @davidbenjamin commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219833193). The better results using the double-counting might have something to do with the incorrect prior -- if the prior is skewing toward homozygosity, then double-counting your variant data might counteract this and rescue some variant genotypes, which will be mainly hets. The EM model that people implicitly seem to have in mind is alternating E steps on each sample to get genotype posteriors with M steps to learn the allele frequencies. So let's work out what happens if you do just one iterations:. 0) Initialize allele frequencies to the mean of the Dirichlet heterozygosity prior; i.e. ~1 for ref, ~1/1000 for each alt, plus any allele counts from the resources. Genotype priors come from the multinomial distribution (one genotype is a draw of 2 alleles) of these allele frequencies.; 1) (E step) genotype posteriors are the product of genotype likelihoods with the priors from step 0). Pseudocounts are the sum of expected posterior allele counts.; 2) (M step) MLE allele frequencies are the mode of the Dirichlet parameterized by the sum of the original step 0) prior+resources pseudocounts with the E step pseudocounts from step 1). Hmmm that does sound a lot like what the code is doing now. I suppose it's defensible after all. ---. @ldgauthier commented on [Thu May 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918
https://github.com/broadinstitute/gatk/issues/2918:6663,Usability,simpl,simply,6663,"one iterations:. 0) Initialize allele frequencies to the mean of the Dirichlet heterozygosity prior; i.e. ~1 for ref, ~1/1000 for each alt, plus any allele counts from the resources. Genotype priors come from the multinomial distribution (one genotype is a draw of 2 alleles) of these allele frequencies.; 1) (E step) genotype posteriors are the product of genotype likelihoods with the priors from step 0). Pseudocounts are the sum of expected posterior allele counts.; 2) (M step) MLE allele frequencies are the mode of the Dirichlet parameterized by the sum of the original step 0) prior+resources pseudocounts with the E step pseudocounts from step 1). Hmmm that does sound a lot like what the code is doing now. I suppose it's defensible after all. ---. @ldgauthier commented on [Thu May 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-220347447). So it sounds to me like the action item here is to fix the Dirichlet heterozygosity prior. I like the idea of adding one count for the ref and 1/1000 for each alt (rather than, for example, 1000 for ref and one for alt) so the heterozygosity prior does something in the absence of external resource counts, but doesn't overwhelms them if they are present. @davidbenjamin Can you think of a more rigorous justification for the scaling of counts between sample genotype allele counts and the heterozygosity?. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-260474993). Is this still alive? To be continued in GATK3 or 4?. ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-260646009). You can move to GATK4. ---. @davidbenjamin commented on [Sun Nov 20 2016](https://github.com/broadinstitute/gatk-protected/issues/792#issuecomment-261761138). @vdauwera @ldgauthier the new qual score model does exactly this. IMO we should simply have this CLI use `AlleleFrequencyCalculator`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918
https://github.com/broadinstitute/gatk/issues/2919:1579,Deployability,pipeline,pipeline,1579,"ttps://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286449442). People care about being able to filter out artifacts that are due to tumor DNA present in the normal sample. How that is to be done is entirely up to you. . ---. @davidbenjamin commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286450985). @LeeTL1220 @vdauwera Is it our responsibility? Do you see it as being part of the Mutect wdl? And what artifacts does tumor-in-normal create? Naively I would expect it to do the opposite i.e. decrease sensitivity by flagging true somatic variants as germline events. If this is important I would consider putting in our next quarterly goals. The time investment would be similar to the new contamination tool, about 3 weeks. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286453446). I would like us to consider this part of the somatic short variants discovery pipeline (which is probably what you mean by Mutect wdl). I'm not sure how urgently we need it, probably ""not very urgent in the scheme of things, but should be done eventually"". I assume @LeeTL1220 has a better idea of what we need when on the somatic side of things. . And you're right that this creates a loss of sensitivity, not false positives. I'm being distracted by a tiny human. . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286457270). We should do it. Generally, I agree with Geraldine's statement about; urgency, though. We can meet to discuss prioritization relative to other; auxiliary tools. On Tue, Mar 14, 2017 at 11:15 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > I would like us to consider this part of the somatic short variants; > discovery pipeline (which is probably what you mean by Mutect wdl). I'm not; > sure how urgently we need it, probably ""not very urgent in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2919
https://github.com/broadinstitute/gatk/issues/2919:2441,Deployability,pipeline,pipeline,2441,"r to the new contamination tool, about 3 weeks. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286453446). I would like us to consider this part of the somatic short variants discovery pipeline (which is probably what you mean by Mutect wdl). I'm not sure how urgently we need it, probably ""not very urgent in the scheme of things, but should be done eventually"". I assume @LeeTL1220 has a better idea of what we need when on the somatic side of things. . And you're right that this creates a loss of sensitivity, not false positives. I'm being distracted by a tiny human. . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286457270). We should do it. Generally, I agree with Geraldine's statement about; urgency, though. We can meet to discuss prioritization relative to other; auxiliary tools. On Tue, Mar 14, 2017 at 11:15 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > I would like us to consider this part of the somatic short variants; > discovery pipeline (which is probably what you mean by Mutect wdl). I'm not; > sure how urgently we need it, probably ""not very urgent in the scheme of; > things, but should be done eventually"". I assume @LeeTL1220; > <https://github.com/LeeTL1220> has a better idea of what we need when on; > the somatic side of things.; >; > And you're right that this creates a loss of sensitivity, not false; > positives. I'm being distracted by a tiny human.; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286453446>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk6kK5A_Mhh9S3s4xX_EEZQlwtKvGks5rlq8hgaJpZM4K3d-Q>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2919
https://github.com/broadinstitute/gatk/issues/2920:765,Integrability,wrap,wrapAndCopyInto,765,"@samuelklee commented on [Sun Nov 27 2016](https://github.com/broadinstitute/gatk-protected/issues/802). Possibly due to bad input, but we should throw a better exception regardless:. java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.exome.gcbias.CorrectGCBias.lambda$gcContentsOfTargets$1(CorrectGCBias.java:91); 	at java.util.stream.MatchOps$1MatchSink.accept(MatchOps.java:90); 	at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:230); 	at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:196); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.allMatch(ReferencePipeline.java:454); 	at org.broadinstitute.hellbender.tools.exome.gcbias.CorrectGCBias.gcContentsOfTargets(CorrectGCBias.java:91); 	at org.broadinstitute.hellbender.tools.exome.gcbias.CorrectGCBias.doWork(CorrectGCBias.java:65); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); 	at org.broadinstitute.hellbender.Main.main(Main.java:92). ---. @samuelklee commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gatk-protected/issues/802#issuecomment-263328345). Looks like the issue was that the coverage files contained additional targets (on the sex chromoso",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2920
https://github.com/broadinstitute/gatk/issues/2922:199,Availability,error,error,199,"@eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806). An issue encountered with gatk-protected ""SparkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:228,Availability,error,error,228,"@eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806). An issue encountered with gatk-protected ""SparkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:988,Availability,error,error,988,"1 2016](https://github.com/broadinstitute/gatk-protected/issues/806). An issue encountered with gatk-protected ""SparkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:3459,Availability,error,error,3459,"ol(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	... 5 more. To bring about success I added 4 lines of code : . ```; ln -vs ${ref_fasta} ; ; ln -vs ${ref_fasta_fai} ;; ln -vs ${ref_fasta_dict} ;; FASTA_NAME=`basename ${ref_fasta} `; . ```. Then, instead of --reference ${ref_fasta} in calling gatk-protected, I put --reference $FASTA_NAME and the ""null"" exception went away and the program run successfully. ---. @eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-264264203). per @droazen : @achevali @LeeTL1220 . ---. @LeeTL1220 commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265342816). @eddiebroad Before this gets assigned, what version of gatk-protected are you using?; Assuming that this is a version we built (despite the name ""eddie.jar""): @achevali , can you figure out how you are reporting the error. @droazen are you sure this is not in the engine?. ---. @eddiebroad commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265464665). @LeeTL1220 . The original JAVA JAR where I first observed the ""null"" message I presume was based off commit 3a2bb0d. At the time the project was initiated I think it was the latest commit. The original JAR where the ""null"" message was observed was gatk-protected-all-3a2bb0d-SNAPSHOT-spark_standalone.jar . Because of the ""3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:205,Integrability,message,message,205,"@eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806). An issue encountered with gatk-protected ""SparkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:241,Integrability,message,message,241,"@eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806). An issue encountered with gatk-protected ""SparkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:994,Integrability,message,message,994,"1 2016](https://github.com/broadinstitute/gatk-protected/issues/806). An issue encountered with gatk-protected ""SparkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:1141,Integrability,message,message,1141,"parkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:409); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:3718,Integrability,message,message,3718,"fasta_dict} ;; FASTA_NAME=`basename ${ref_fasta} `; . ```. Then, instead of --reference ${ref_fasta} in calling gatk-protected, I put --reference $FASTA_NAME and the ""null"" exception went away and the program run successfully. ---. @eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-264264203). per @droazen : @achevali @LeeTL1220 . ---. @LeeTL1220 commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265342816). @eddiebroad Before this gets assigned, what version of gatk-protected are you using?; Assuming that this is a version we built (despite the name ""eddie.jar""): @achevali , can you figure out how you are reporting the error. @droazen are you sure this is not in the engine?. ---. @eddiebroad commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265464665). @LeeTL1220 . The original JAVA JAR where I first observed the ""null"" message I presume was based off commit 3a2bb0d. At the time the project was initiated I think it was the latest commit. The original JAR where the ""null"" message was observed was gatk-protected-all-3a2bb0d-SNAPSHOT-spark_standalone.jar . Because of the ""3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I can see ""EAS"" my initials and see c40e75b which appears to be a more recent commit compared to 3a2bb0d. ```; EAS in main!!!!; EAS to call instanceMain second....; EAS to call instanceMain first....; 17:28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:3872,Integrability,message,message,3872,"on went away and the program run successfully. ---. @eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-264264203). per @droazen : @achevali @LeeTL1220 . ---. @LeeTL1220 commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265342816). @eddiebroad Before this gets assigned, what version of gatk-protected are you using?; Assuming that this is a version we built (despite the name ""eddie.jar""): @achevali , can you figure out how you are reporting the error. @droazen are you sure this is not in the engine?. ---. @eddiebroad commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265464665). @LeeTL1220 . The original JAVA JAR where I first observed the ""null"" message I presume was based off commit 3a2bb0d. At the time the project was initiated I think it was the latest commit. The original JAR where the ""null"" message was observed was gatk-protected-all-3a2bb0d-SNAPSHOT-spark_standalone.jar . Because of the ""3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I can see ""EAS"" my initials and see c40e75b which appears to be a more recent commit compared to 3a2bb0d. ```; EAS in main!!!!; EAS to call instanceMain second....; EAS to call instanceMain first....; 17:28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs in instanceMain in clp.java ; 17:28:40.396 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/cromwell_root/fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar!/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:4788,Performance,load,load,4788,"e latest commit. The original JAR where the ""null"" message was observed was gatk-protected-all-3a2bb0d-SNAPSHOT-spark_standalone.jar . Because of the ""3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I can see ""EAS"" my initials and see c40e75b which appears to be a more recent commit compared to 3a2bb0d. ```; EAS in main!!!!; EAS to call instanceMain second....; EAS to call instanceMain first....; 17:28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs in instanceMain in clp.java ; 17:28:40.396 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/cromwell_root/fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar!/com/intel/gkl/native/libIntelGKL.so; 17:28:40.498 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [December 1, 2016 5:28:40 PM UTC] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts --binsize 5000 --outputFile this.entity_id.coverage.tsv --reference Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --keepXYMT false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [December 1, 2016 5:28:40 PM UTC] Executing as root@71bfa07f6996 on Linux 3.16.0-0.bpo.4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:4983,Performance,load,loaded,4983,"3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I can see ""EAS"" my initials and see c40e75b which appears to be a more recent commit compared to 3a2bb0d. ```; EAS in main!!!!; EAS to call instanceMain second....; EAS to call instanceMain first....; 17:28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs in instanceMain in clp.java ; 17:28:40.396 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/cromwell_root/fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar!/com/intel/gkl/native/libIntelGKL.so; 17:28:40.498 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [December 1, 2016 5:28:40 PM UTC] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts --binsize 5000 --outputFile this.entity_id.coverage.tsv --reference Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --keepXYMT false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [December 1, 2016 5:28:40 PM UTC] Executing as root@71bfa07f6996 on Linux 3.16.0-0.bpo.4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Version:c40e75b-SNAPSHOT; 17:28:40.501 INFO SparkGenomeReadCounts - Defaults.BUFFER_SIZE : 131072; ```. ---. @eddiebroad commented on [Wed Dec 07 2016](https",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:612,Security,access,access,612,"@eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806). An issue encountered with gatk-protected ""SparkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:5232,Security,access,access,5232,"king from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I can see ""EAS"" my initials and see c40e75b which appears to be a more recent commit compared to 3a2bb0d. ```; EAS in main!!!!; EAS to call instanceMain second....; EAS to call instanceMain first....; 17:28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs in instanceMain in clp.java ; 17:28:40.396 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/cromwell_root/fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar!/com/intel/gkl/native/libIntelGKL.so; 17:28:40.498 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [December 1, 2016 5:28:40 PM UTC] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts --binsize 5000 --outputFile this.entity_id.coverage.tsv --reference Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --keepXYMT false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [December 1, 2016 5:28:40 PM UTC] Executing as root@71bfa07f6996 on Linux 3.16.0-0.bpo.4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Version:c40e75b-SNAPSHOT; 17:28:40.501 INFO SparkGenomeReadCounts - Defaults.BUFFER_SIZE : 131072; ```. ---. @eddiebroad commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265470147). I want to mention in my case, all the reference files were present (fasta, fai, dict) BUT the dict was in a different directory and NOT in the same directory as the other two ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:4422,Testability,LOG,LOG,4422,"oazen are you sure this is not in the engine?. ---. @eddiebroad commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265464665). @LeeTL1220 . The original JAVA JAR where I first observed the ""null"" message I presume was based off commit 3a2bb0d. At the time the project was initiated I think it was the latest commit. The original JAR where the ""null"" message was observed was gatk-protected-all-3a2bb0d-SNAPSHOT-spark_standalone.jar . Because of the ""3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I can see ""EAS"" my initials and see c40e75b which appears to be a more recent commit compared to 3a2bb0d. ```; EAS in main!!!!; EAS to call instanceMain second....; EAS to call instanceMain first....; 17:28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs in instanceMain in clp.java ; 17:28:40.396 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/cromwell_root/fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar!/com/intel/gkl/native/libIntelGKL.so; 17:28:40.498 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [December 1, 2016 5:28:40 PM UTC] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts --binsize 5000 --outputFile this.entity_id.coverage.tsv --reference Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --keepXYMT false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2922:6563,Usability,clear,cleared,6563,":28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs in instanceMain in clp.java ; 17:28:40.396 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/cromwell_root/fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar!/com/intel/gkl/native/libIntelGKL.so; 17:28:40.498 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [December 1, 2016 5:28:40 PM UTC] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts --binsize 5000 --outputFile this.entity_id.coverage.tsv --reference Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --keepXYMT false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [December 1, 2016 5:28:40 PM UTC] Executing as root@71bfa07f6996 on Linux 3.16.0-0.bpo.4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Version:c40e75b-SNAPSHOT; 17:28:40.501 INFO SparkGenomeReadCounts - Defaults.BUFFER_SIZE : 131072; ```. ---. @eddiebroad commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265470147). I want to mention in my case, all the reference files were present (fasta, fai, dict) BUT the dict was in a different directory and NOT in the same directory as the other two files (which were in the same directory) and that's why the dict was not found and that's why the soft link creation fixed the issue. ---. @droazen commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265491329). @LeeTL1220 It's very possibly in the engine -- if the tool itself gets cleared of blame, feel free to bounce this one back to gatk public.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922
https://github.com/broadinstitute/gatk/issues/2923:319,Usability,Guid,Guide,319,"@eddiebroad commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/817). As a standard practice, whenever I write WDL, I have as my first command ""set -x"".; The command causes bash to print the commands it actually runs to stderr.; See Table 2-1 here; http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_03.html. @LeeTL1220 mentioned that this practice be good in the WDLs of gatk-protected so, per him, I write this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2923
https://github.com/broadinstitute/gatk/issues/2928:131,Security,expose,exposed,131,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/842). - [ ] carefully document the exposed parameters of gCNV, mark the tricky ones as advanced and document use case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2928
https://github.com/broadinstitute/gatk/issues/2929:1991,Integrability,interface,interface,1991,"ts and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and require identical results. This is the easiest approach. This was my approach during the development. First, I wrote every function without in-place operations, ran the code, optimized the function with in-place ops, ran the code again, assert. If we can automate this sort of thing, it is the easiest way out. For the time being, I annotate all functions that can potentially mutate the ICG with @QueriesICG to finally decide how we'd like to proceed. I also made a TODO for writing suc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:2713,Performance,optimiz,optimized,2713,"erent shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and require identical results. This is the easiest approach. This was my approach during the development. First, I wrote every function without in-place operations, ran the code, optimized the function with in-place ops, ran the code again, assert. If we can automate this sort of thing, it is the easiest way out. For the time being, I annotate all functions that can potentially mutate the ICG with @QueriesICG to finally decide how we'd like to proceed. I also made a TODO for writing such unit tests. Actually, the brute-force approach is very easy to implement: we just need to change the behavior of ImmutableComputableGraph.get via a global constant or a system property to return as deepcopy. We run the tool with this switch enabled/disabled and we require identical results. (I just did that and I got identical results -- at least we don't have to worry for now, though, this doesn't obviate better engineering)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:1596,Security,firewall,firewall,1596,"statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:2013,Security,access,access,2013,"ts and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and require identical results. This is the easiest approach. This was my approach during the development. First, I wrote every function without in-place operations, ran the code, optimized the function with in-place ops, ran the code again, assert. If we can automate this sort of thing, it is the easiest way out. For the time being, I annotate all functions that can potentially mutate the ICG with @QueriesICG to finally decide how we'd like to proceed. I also made a TODO for writing suc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:119,Testability,test,test,119,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:346,Testability,test,test,346,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:637,Testability,test,tests,637,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:686,Testability,test,tests,686,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:735,Testability,test,tests,735,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:834,Testability,test,tests,834,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:892,Testability,test,tests,892,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:958,Testability,test,tests,958,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:1022,Testability,test,tests,1022,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:1063,Testability,test,tests,1063,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/843). - [ ] sinusoidal test data in `FourierLinearOperatorNDArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:1127,Testability,test,tests,1127,"DArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also qu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:1184,Testability,test,test,1184,"DArrayUnitTest`; - [ ] case-sample calling on rearranged targets in `GermlineCNVCallerIntegrationTest`; - [ ] concordance on parameter estimation in `GermlineCNVCallerIntegrationTest`; - [ ] test Spark results match local results in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on max likelihood copy ratio as well in `GermlineCNVCallerIntegrationTest`; - [ ] report statistics on local copy ratio posteriors as well in `GermlineCNVCallerIntegrationTest`; - [ ] unit tests for `ComputableGraphStructure`; - [ ] unit tests for `ImmutableComputableGraph`; - [ ] unit tests for ensuring that ICG nodes are treated as immutable and not modified by mistake; - [ ] unit tests for `CoverageModelEMWorkspaceMathUtils`; - [ ] unit tests for `CoverageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also qu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:1913,Testability,test,tests,1913,"ageModelParameters` (reading/writing); - [ ] unit tests for `CoverageModelSparkUtils`; - [ ] the issue with Spark tests and custom serializers (gCNV Spark tests are currently disabled). **Discussion about gCNV ICG unit tests (May 1st, 2017):**; It is possible to automate the test for ComputableNodeFunctions. One initializes the parents to random values, calls the function, and checks whether it has had any side effects on the parents. One must make a random-data-provider-of-some-sort for each parent node because the parent INDArrays have different shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and require identical results. This is the easiest approach. This was my approach during the development. First, I wrote every function without in-place operations, ran the code, optimized the function with in-place ops, ran the code again, assert. If we can automate this sort of thing, it is the easiest way out. For the time being, I annotate all functions that can poten",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:2454,Testability,log,log,2454,"erent shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and require identical results. This is the easiest approach. This was my approach during the development. First, I wrote every function without in-place operations, ran the code, optimized the function with in-place ops, ran the code again, assert. If we can automate this sort of thing, it is the easiest way out. For the time being, I annotate all functions that can potentially mutate the ICG with @QueriesICG to finally decide how we'd like to proceed. I also made a TODO for writing such unit tests. Actually, the brute-force approach is very easy to implement: we just need to change the behavior of ImmutableComputableGraph.get via a global constant or a system property to return as deepcopy. We run the tool with this switch enabled/disabled and we require identical results. (I just did that and I got identical results -- at least we don't have to worry for now, though, this doesn't obviate better engineering)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:2503,Testability,log,log,2503,"erent shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and require identical results. This is the easiest approach. This was my approach during the development. First, I wrote every function without in-place operations, ran the code, optimized the function with in-place ops, ran the code again, assert. If we can automate this sort of thing, it is the easiest way out. For the time being, I annotate all functions that can potentially mutate the ICG with @QueriesICG to finally decide how we'd like to proceed. I also made a TODO for writing such unit tests. Actually, the brute-force approach is very easy to implement: we just need to change the behavior of ImmutableComputableGraph.get via a global constant or a system property to return as deepcopy. We run the tool with this switch enabled/disabled and we require identical results. (I just did that and I got identical results -- at least we don't have to worry for now, though, this doesn't obviate better engineering)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:2775,Testability,assert,assert,2775,"erent shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and require identical results. This is the easiest approach. This was my approach during the development. First, I wrote every function without in-place operations, ran the code, optimized the function with in-place ops, ran the code again, assert. If we can automate this sort of thing, it is the easiest way out. For the time being, I annotate all functions that can potentially mutate the ICG with @QueriesICG to finally decide how we'd like to proceed. I also made a TODO for writing such unit tests. Actually, the brute-force approach is very easy to implement: we just need to change the behavior of ImmutableComputableGraph.get via a global constant or a system property to return as deepcopy. We run the tool with this switch enabled/disabled and we require identical results. (I just did that and I got identical results -- at least we don't have to worry for now, though, this doesn't obviate better engineering)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2929:3032,Testability,test,tests,3032,"erent shapes. For other functions in CoverageModelEMComputeBlock and CoverageModelEMWorkspace that query ICG nodes -- one needs to create a firewall. One can elevate all such functions to classes that essentially behave functionally, (ICGNodeProvider, List<NodeKey>, extra arguments) -> output, as opposed to writing vanilla member functions such as CoverageModelEMComputeBlock.getBiasLatentPosteriorDataUnregularized, etc. Then we can write automated unit tests for these classes. Another approach is to write a thin ImmutableNDArray interface that blocks access to all mutators and returns instances of ImmutableNDArray when a matrix view is extracted (e.g. via INDArray.get(...)). This is also quite non-trivial and requires intimate familiarity with Nd4j codebase. Perhaps one could write an immutable DataBuffer for Nd4j. Finally, there might be a brute-force approach: substitute all in-plane operations such as muli and addi with mul and add, and in-place transformations such as Transforms.log(INDArray, boolean duplicate) with Transforms.log(INDArray, true), run gCNV, and require identical results. This is the easiest approach. This was my approach during the development. First, I wrote every function without in-place operations, ran the code, optimized the function with in-place ops, ran the code again, assert. If we can automate this sort of thing, it is the easiest way out. For the time being, I annotate all functions that can potentially mutate the ICG with @QueriesICG to finally decide how we'd like to proceed. I also made a TODO for writing such unit tests. Actually, the brute-force approach is very easy to implement: we just need to change the behavior of ImmutableComputableGraph.get via a global constant or a system property to return as deepcopy. We run the tool with this switch enabled/disabled and we require identical results. (I just did that and I got identical results -- at least we don't have to worry for now, though, this doesn't obviate better engineering)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929
https://github.com/broadinstitute/gatk/issues/2930:1390,Availability,redundant,redundant,1390,"antWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mapping with BWA in memory to *those* might be very helpful. So, we need to: 1) get our hands on a decoy genome that will play nicely with BWA, and 2) talk to the SV team. ---. @ldgauthier commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296675311). To be pedantic, the mapping quality also considers how well the read aligns; to its",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:1578,Availability,error,errors,1578," program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mapping with BWA in memory to *those* might be very helpful. So, we need to: 1) get our hands on a decoy genome that will play nicely with BWA, and 2) talk to the SV team. ---. @ldgauthier commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296675311). To be pedantic, the mapping quality also considers how well the read aligns; to its best mapping. In places where a sample has a lot of nearby SNPs; compared to the reference the mapping qualities of the reads are low; compared to reads t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:3327,Availability,redundant,redundant,3327,"e mapping quality also considers how well the read aligns; to its best mapping. In places where a sample has a lot of nearby SNPs; compared to the reference the mapping qualities of the reads are low; compared to reads that contain fewer SNPs. I've been mulling over the; conflation of these two aspects of mapping quality for a while because it; biases our VQSR results, but maybe the new filtering models will be able to; figure it out. The b37 reference with decoy contigs is here:; /humgen/1kg/reference/human_g1k_v37_decoy.fasta.I believe that the; reference issue that required the decoy in the b37 1000G work was resolved; in the hg38 reference. This is an excellent topic to discuss with Heng; during his office hours when he gets back from China in a few weeks, but I; expect the SV team will also be helpful in the meantime. On Sun, Apr 23, 2017 at 11:14 PM, David Benjamin <notifications@github.com>; wrote:. > So. . . given that our pipeline aligns with BWA, it might seem like this; > is just a redundant and laborious rehashing of the mapping quality score.; >; > *However*, the mapping quality only considers multi-mapping within the; > reference, and therefore doesn't account for mapping errors due to; > incompleteness of the reference. That is, reads from genomic regions that; > are not part of the reference (because they're hard to assemble, like; > centromeres etc) might map well to a unique regions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:3524,Availability,error,errors,3524,"apping qualities of the reads are low; compared to reads that contain fewer SNPs. I've been mulling over the; conflation of these two aspects of mapping quality for a while because it; biases our VQSR results, but maybe the new filtering models will be able to; figure it out. The b37 reference with decoy contigs is here:; /humgen/1kg/reference/human_g1k_v37_decoy.fasta.I believe that the; reference issue that required the decoy in the b37 1000G work was resolved; in the hg38 reference. This is an excellent topic to discuss with Heng; during his office hours when he gets back from China in a few weeks, but I; expect the SV team will also be helpful in the meantime. On Sun, Apr 23, 2017 at 11:14 PM, David Benjamin <notifications@github.com>; wrote:. > So. . . given that our pipeline aligns with BWA, it might seem like this; > is just a redundant and laborious rehashing of the mapping quality score.; >; > *However*, the mapping quality only considers multi-mapping within the; > reference, and therefore doesn't account for mapping errors due to; > incompleteness of the reference. That is, reads from genomic regions that; > are not part of the reference (because they're hard to assemble, like; > centromeres etc) might map well to a unique regions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCJQob4WqdwDN0R8jvbNGT1l0vSCks5rzBOmga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:5634,Availability,error,error,5634,"egions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCJQob4WqdwDN0R8jvbNGT1l0vSCks5rzBOmgaJpZM4Lb8pz>; > .; >. ---. @davidbenjamin commented on [Wed May 03 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-298946022). Copying comments from closed issue #993. Instead of running an aligner in memory, let's first try preprocessing an alignability (to the ref + decoy) resource file. Then we can simply query this file at each called variant. > ENCODE used a kmer size of 36 bp, which is seriously obsolete and will tend to underestimate alignability. However, the GEM program (paper here: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030377 and binary here: http://algorithms.cnag.cat/wiki/The_GEM_library#Documentation and blog post on how to run it here: http://blog.kokocinski.net/index.php/sequence-mappability-alignability?blog=2) was used by ENCODE to produce this track and we can easily produce it ourselves with any kmer size and any mismatch threshold. > Furthermore, once we make this track we can store this track in memory eg as a `HashedListTargetCollection` and therefore we can query it for every read to get an annotation for the number of uniquely mappable reads (up to some error tolerance). > One more thing: we can also query based on the start position of each read's mate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:5640,Availability,toler,tolerance,5640,"egions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCJQob4WqdwDN0R8jvbNGT1l0vSCks5rzBOmgaJpZM4Lb8pz>; > .; >. ---. @davidbenjamin commented on [Wed May 03 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-298946022). Copying comments from closed issue #993. Instead of running an aligner in memory, let's first try preprocessing an alignability (to the ref + decoy) resource file. Then we can simply query this file at each called variant. > ENCODE used a kmer size of 36 bp, which is seriously obsolete and will tend to underestimate alignability. However, the GEM program (paper here: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030377 and binary here: http://algorithms.cnag.cat/wiki/The_GEM_library#Documentation and blog post on how to run it here: http://blog.kokocinski.net/index.php/sequence-mappability-alignability?blog=2) was used by ENCODE to produce this track and we can easily produce it ourselves with any kmer size and any mismatch threshold. > Furthermore, once we make this track we can store this track in memory eg as a `HashedListTargetCollection` and therefore we can query it for every read to get an annotation for the number of uniquely mappable reads (up to some error tolerance). > One more thing: we can also query based on the start position of each read's mate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:1330,Deployability,pipeline,pipeline,1330,"antWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mapping with BWA in memory to *those* might be very helpful. So, we need to: 1) get our hands on a decoy genome that will play nicely with BWA, and 2) talk to the SV team. ---. @ldgauthier commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296675311). To be pedantic, the mapping quality also considers how well the read aligns; to its",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:3264,Deployability,pipeline,pipeline,3264,"e mapping quality also considers how well the read aligns; to its best mapping. In places where a sample has a lot of nearby SNPs; compared to the reference the mapping qualities of the reads are low; compared to reads that contain fewer SNPs. I've been mulling over the; conflation of these two aspects of mapping quality for a while because it; biases our VQSR results, but maybe the new filtering models will be able to; figure it out. The b37 reference with decoy contigs is here:; /humgen/1kg/reference/human_g1k_v37_decoy.fasta.I believe that the; reference issue that required the decoy in the b37 1000G work was resolved; in the hg38 reference. This is an excellent topic to discuss with Heng; during his office hours when he gets back from China in a few weeks, but I; expect the SV team will also be helpful in the meantime. On Sun, Apr 23, 2017 at 11:14 PM, David Benjamin <notifications@github.com>; wrote:. > So. . . given that our pipeline aligns with BWA, it might seem like this; > is just a redundant and laborious rehashing of the mapping quality score.; >; > *However*, the mapping quality only considers multi-mapping within the; > reference, and therefore doesn't account for mapping errors due to; > incompleteness of the reference. That is, reads from genomic regions that; > are not part of the reference (because they're hard to assemble, like; > centromeres etc) might map well to a unique regions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:108,Safety,Detect,Detect,108,"@davidbenjamin commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/844). Detect false positives due to mapping as follows: for every purported variant, align all alt reads, with an aligner that outputs multiple candidate alignments with scores, and toss out reads that map well to a different locus. The simplest version of this is 1) write a `VariantWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mappi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:1390,Safety,redund,redundant,1390,"antWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mapping with BWA in memory to *those* might be very helpful. So, we need to: 1) get our hands on a decoy genome that will play nicely with BWA, and 2) talk to the SV team. ---. @ldgauthier commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296675311). To be pedantic, the mapping quality also considers how well the read aligns; to its",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:3327,Safety,redund,redundant,3327,"e mapping quality also considers how well the read aligns; to its best mapping. In places where a sample has a lot of nearby SNPs; compared to the reference the mapping qualities of the reads are low; compared to reads that contain fewer SNPs. I've been mulling over the; conflation of these two aspects of mapping quality for a while because it; biases our VQSR results, but maybe the new filtering models will be able to; figure it out. The b37 reference with decoy contigs is here:; /humgen/1kg/reference/human_g1k_v37_decoy.fasta.I believe that the; reference issue that required the decoy in the b37 1000G work was resolved; in the hg38 reference. This is an excellent topic to discuss with Heng; during his office hours when he gets back from China in a few weeks, but I; expect the SV team will also be helpful in the meantime. On Sun, Apr 23, 2017 at 11:14 PM, David Benjamin <notifications@github.com>; wrote:. > So. . . given that our pipeline aligns with BWA, it might seem like this; > is just a redundant and laborious rehashing of the mapping quality score.; >; > *However*, the mapping quality only considers multi-mapping within the; > reference, and therefore doesn't account for mapping errors due to; > incompleteness of the reference. That is, reads from genomic regions that; > are not part of the reference (because they're hard to assemble, like; > centromeres etc) might map well to a unique regions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:5486,Security,Hash,HashedListTargetCollection,5486,"egions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCJQob4WqdwDN0R8jvbNGT1l0vSCks5rzBOmgaJpZM4Lb8pz>; > .; >. ---. @davidbenjamin commented on [Wed May 03 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-298946022). Copying comments from closed issue #993. Instead of running an aligner in memory, let's first try preprocessing an alignability (to the ref + decoy) resource file. Then we can simply query this file at each called variant. > ENCODE used a kmer size of 36 bp, which is seriously obsolete and will tend to underestimate alignability. However, the GEM program (paper here: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030377 and binary here: http://algorithms.cnag.cat/wiki/The_GEM_library#Documentation and blog post on how to run it here: http://blog.kokocinski.net/index.php/sequence-mappability-alignability?blog=2) was used by ENCODE to produce this track and we can easily produce it ourselves with any kmer size and any mismatch threshold. > Furthermore, once we make this track we can store this track in memory eg as a `HashedListTargetCollection` and therefore we can query it for every read to get an annotation for the number of uniquely mappable reads (up to some error tolerance). > One more thing: we can also query based on the start position of each read's mate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:339,Usability,simpl,simplest,339,"@davidbenjamin commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/844). Detect false positives due to mapping as follows: for every purported variant, align all alt reads, with an aligner that outputs multiple candidate alignments with scores, and toss out reads that map well to a different locus. The simplest version of this is 1) write a `VariantWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mappi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:655,Usability,simpl,simple,655,"@davidbenjamin commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/844). Detect false positives due to mapping as follows: for every purported variant, align all alt reads, with an aligner that outputs multiple candidate alignments with scores, and toss out reads that map well to a different locus. The simplest version of this is 1) write a `VariantWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mappi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2930:4815,Usability,simpl,simply,4815,"egions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCJQob4WqdwDN0R8jvbNGT1l0vSCks5rzBOmgaJpZM4Lb8pz>; > .; >. ---. @davidbenjamin commented on [Wed May 03 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-298946022). Copying comments from closed issue #993. Instead of running an aligner in memory, let's first try preprocessing an alignability (to the ref + decoy) resource file. Then we can simply query this file at each called variant. > ENCODE used a kmer size of 36 bp, which is seriously obsolete and will tend to underestimate alignability. However, the GEM program (paper here: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030377 and binary here: http://algorithms.cnag.cat/wiki/The_GEM_library#Documentation and blog post on how to run it here: http://blog.kokocinski.net/index.php/sequence-mappability-alignability?blog=2) was used by ENCODE to produce this track and we can easily produce it ourselves with any kmer size and any mismatch threshold. > Furthermore, once we make this track we can store this track in memory eg as a `HashedListTargetCollection` and therefore we can query it for every read to get an annotation for the number of uniquely mappable reads (up to some error tolerance). > One more thing: we can also query based on the start position of each read's mate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930
https://github.com/broadinstitute/gatk/issues/2931:127,Integrability,Synchroniz,SynchronizedUnivariateSolver,127,@mbabadi commented on [Thu Jan 12 2017](https://github.com/broadinstitute/gatk-protected/issues/853). - [ ] thread capping in `SynchronizedUnivariateSolver`; - [ ] faster calculation of prior copy number contribution to log likelihood in `IntegerCopyNumberExpectationsCalculator`; - [ ] optimize `CoverageModelEMWorkspace.replaceMaskedEntries` using Nd4j native ops,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2931
https://github.com/broadinstitute/gatk/issues/2931:287,Performance,optimiz,optimize,287,@mbabadi commented on [Thu Jan 12 2017](https://github.com/broadinstitute/gatk-protected/issues/853). - [ ] thread capping in `SynchronizedUnivariateSolver`; - [ ] faster calculation of prior copy number contribution to log likelihood in `IntegerCopyNumberExpectationsCalculator`; - [ ] optimize `CoverageModelEMWorkspace.replaceMaskedEntries` using Nd4j native ops,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2931
https://github.com/broadinstitute/gatk/issues/2931:220,Testability,log,log,220,@mbabadi commented on [Thu Jan 12 2017](https://github.com/broadinstitute/gatk-protected/issues/853). - [ ] thread capping in `SynchronizedUnivariateSolver`; - [ ] faster calculation of prior copy number contribution to log likelihood in `IntegerCopyNumberExpectationsCalculator`; - [ ] optimize `CoverageModelEMWorkspace.replaceMaskedEntries` using Nd4j native ops,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2931
https://github.com/broadinstitute/gatk/issues/2934:1505,Energy Efficiency,power,power,1505," them. This meant it didn't report somatic mutations that involved loss of heterozygosity or new alleles at variant germline sites. Mutect 2 should report these sites as variant. . @davidangb Not sure if this already happens, but it seems like a good thing to fix if it doesn't. ---. @vdauwera commented on [Mon Jan 23 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-274585906). I support this feature request. ---. @davidbenjamin commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-275116409). @lbergelson LoH is a great idea that we don't do already; we *do* handle new alleles at germline variant sites now. ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-294329953). Actually, I'm having second thoughts. LoH is a copy-number event that occurs in chunks, so we could end up emitting (and spending CPU time on) a huge number of additional sites. Also, @samuelklee is there any reason not to leave the LoH-finding to aCNV?. ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295324137). Yeah, LoH is aCNV's job. Mutect would do the same thing at much greater expense and with less power. ---. @lbergelson commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295335240). @davidbenjamin, aCNV won't detect point mutations leading to LOH at specific sites. It's an admittedly rare case, and maybe not clinically relevant since reversion to the reference is probably not disease causing, but it means missing real somatic variants. . ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295336177). @lbergelson Ah, I see your point. Then it becomes an interesting trade-off of time vs sensitivity. I suppose we could make it optional. I'll re-open.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2934
https://github.com/broadinstitute/gatk/issues/2934:1672,Safety,detect,detect,1672," them. This meant it didn't report somatic mutations that involved loss of heterozygosity or new alleles at variant germline sites. Mutect 2 should report these sites as variant. . @davidangb Not sure if this already happens, but it seems like a good thing to fix if it doesn't. ---. @vdauwera commented on [Mon Jan 23 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-274585906). I support this feature request. ---. @davidbenjamin commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-275116409). @lbergelson LoH is a great idea that we don't do already; we *do* handle new alleles at germline variant sites now. ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-294329953). Actually, I'm having second thoughts. LoH is a copy-number event that occurs in chunks, so we could end up emitting (and spending CPU time on) a huge number of additional sites. Also, @samuelklee is there any reason not to leave the LoH-finding to aCNV?. ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295324137). Yeah, LoH is aCNV's job. Mutect would do the same thing at much greater expense and with less power. ---. @lbergelson commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295335240). @davidbenjamin, aCNV won't detect point mutations leading to LOH at specific sites. It's an admittedly rare case, and maybe not clinically relevant since reversion to the reference is probably not disease causing, but it means missing real somatic variants. . ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295336177). @lbergelson Ah, I see your point. Then it becomes an interesting trade-off of time vs sensitivity. I suppose we could make it optional. I'll re-open.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2934
https://github.com/broadinstitute/gatk/issues/2936:715,Energy Efficiency,reduce,reduce,715,"@davidbenjamin commented on [Mon Jan 30 2017](https://github.com/broadinstitute/gatk-protected/issues/886). My original infinite HMM joint segmentation was nifty on paper but is 1) horribly slow and 2) falls into spurious local minima. One way to improve upon both of these is to learn an initial set of hidden states via Chinese Restaurant Process clustering of the raw allelic count and coverage data, without regard to segmentation and the HMM structure. Because this doesn't constrain neighboring sites to (usually) have the same state, it will yield a liberal set of initial states, which is fine because they can always be pruned. Probably, we can run a single pass of the HMM to prune most states. This will reduce the number of iterations enormously and obviate expensive max-likelihoods learning learning of the hidden state values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2936
https://github.com/broadinstitute/gatk/issues/2936:280,Usability,learn,learn,280,"@davidbenjamin commented on [Mon Jan 30 2017](https://github.com/broadinstitute/gatk-protected/issues/886). My original infinite HMM joint segmentation was nifty on paper but is 1) horribly slow and 2) falls into spurious local minima. One way to improve upon both of these is to learn an initial set of hidden states via Chinese Restaurant Process clustering of the raw allelic count and coverage data, without regard to segmentation and the HMM structure. Because this doesn't constrain neighboring sites to (usually) have the same state, it will yield a liberal set of initial states, which is fine because they can always be pruned. Probably, we can run a single pass of the HMM to prune most states. This will reduce the number of iterations enormously and obviate expensive max-likelihoods learning learning of the hidden state values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2936
https://github.com/broadinstitute/gatk/issues/2936:796,Usability,learn,learning,796,"@davidbenjamin commented on [Mon Jan 30 2017](https://github.com/broadinstitute/gatk-protected/issues/886). My original infinite HMM joint segmentation was nifty on paper but is 1) horribly slow and 2) falls into spurious local minima. One way to improve upon both of these is to learn an initial set of hidden states via Chinese Restaurant Process clustering of the raw allelic count and coverage data, without regard to segmentation and the HMM structure. Because this doesn't constrain neighboring sites to (usually) have the same state, it will yield a liberal set of initial states, which is fine because they can always be pruned. Probably, we can run a single pass of the HMM to prune most states. This will reduce the number of iterations enormously and obviate expensive max-likelihoods learning learning of the hidden state values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2936
https://github.com/broadinstitute/gatk/issues/2936:805,Usability,learn,learning,805,"@davidbenjamin commented on [Mon Jan 30 2017](https://github.com/broadinstitute/gatk-protected/issues/886). My original infinite HMM joint segmentation was nifty on paper but is 1) horribly slow and 2) falls into spurious local minima. One way to improve upon both of these is to learn an initial set of hidden states via Chinese Restaurant Process clustering of the raw allelic count and coverage data, without regard to segmentation and the HMM structure. Because this doesn't constrain neighboring sites to (usually) have the same state, it will yield a liberal set of initial states, which is fine because they can always be pruned. Probably, we can run a single pass of the HMM to prune most states. This will reduce the number of iterations enormously and obviate expensive max-likelihoods learning learning of the hidden state values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2936
https://github.com/broadinstitute/gatk/issues/2938:512,Availability,Down,Downsample,512,"@lindenb commented on [Mon Feb 06 2017](https://github.com/broadinstitute/gatk-protected/issues/891). Hi GATK team,. In my VCF I've got a variant that was called but it's only the consequence of a set of soft-clipped reads (it's a Haloplex assay, that's why I've got the 'bars' / high depth below). ![jeter](https://cloud.githubusercontent.com/assets/33838/22643385/885096d0-ec5e-11e6-91ae-7331affafc36.png). I was playing with the GATK 3.7 API to find the soft clipped reads overlapping my variation. ```java. @Downsample(by= DownsampleType.BY_SAMPLE, toCoverage=1000000); public class MyWalker ; 	extends RodWalker<Integer, Integer> implements TreeReducible<Integer> {; (...); public Integer map(RefMetaDataTracker tracker, ReferenceContext ref, AlignmentContext context) {; {; (...); final Genotype g=variantContext.getGenotype(i);; 		final ReadBackedPileup sampleReadBackedPileup =rbp.getPileupForSample(g.getSampleName());; 		 final List<GATKSAMRecord> recs= sampleReadBackedPileup.getReads();. (...); }. (...); }; ```. unfortunately **, I cannot find any GATKSAMRecord containing the soft clipping segment**. So my question is: does GATK cannot find my reads because it only considers **start/end** but not **unclippedStart/unclippedEnd** ? Is there any parameter to get those clipped reads ?. Many thanks in advance,. Pierre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2938
https://github.com/broadinstitute/gatk/issues/2938:527,Availability,Down,DownsampleType,527,"@lindenb commented on [Mon Feb 06 2017](https://github.com/broadinstitute/gatk-protected/issues/891). Hi GATK team,. In my VCF I've got a variant that was called but it's only the consequence of a set of soft-clipped reads (it's a Haloplex assay, that's why I've got the 'bars' / high depth below). ![jeter](https://cloud.githubusercontent.com/assets/33838/22643385/885096d0-ec5e-11e6-91ae-7331affafc36.png). I was playing with the GATK 3.7 API to find the soft clipped reads overlapping my variation. ```java. @Downsample(by= DownsampleType.BY_SAMPLE, toCoverage=1000000); public class MyWalker ; 	extends RodWalker<Integer, Integer> implements TreeReducible<Integer> {; (...); public Integer map(RefMetaDataTracker tracker, ReferenceContext ref, AlignmentContext context) {; {; (...); final Genotype g=variantContext.getGenotype(i);; 		final ReadBackedPileup sampleReadBackedPileup =rbp.getPileupForSample(g.getSampleName());; 		 final List<GATKSAMRecord> recs= sampleReadBackedPileup.getReads();. (...); }. (...); }; ```. unfortunately **, I cannot find any GATKSAMRecord containing the soft clipping segment**. So my question is: does GATK cannot find my reads because it only considers **start/end** but not **unclippedStart/unclippedEnd** ? Is there any parameter to get those clipped reads ?. Many thanks in advance,. Pierre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2938
https://github.com/broadinstitute/gatk/issues/2938:599,Modifiability,extend,extends,599,"@lindenb commented on [Mon Feb 06 2017](https://github.com/broadinstitute/gatk-protected/issues/891). Hi GATK team,. In my VCF I've got a variant that was called but it's only the consequence of a set of soft-clipped reads (it's a Haloplex assay, that's why I've got the 'bars' / high depth below). ![jeter](https://cloud.githubusercontent.com/assets/33838/22643385/885096d0-ec5e-11e6-91ae-7331affafc36.png). I was playing with the GATK 3.7 API to find the soft clipped reads overlapping my variation. ```java. @Downsample(by= DownsampleType.BY_SAMPLE, toCoverage=1000000); public class MyWalker ; 	extends RodWalker<Integer, Integer> implements TreeReducible<Integer> {; (...); public Integer map(RefMetaDataTracker tracker, ReferenceContext ref, AlignmentContext context) {; {; (...); final Genotype g=variantContext.getGenotype(i);; 		final ReadBackedPileup sampleReadBackedPileup =rbp.getPileupForSample(g.getSampleName());; 		 final List<GATKSAMRecord> recs= sampleReadBackedPileup.getReads();. (...); }. (...); }; ```. unfortunately **, I cannot find any GATKSAMRecord containing the soft clipping segment**. So my question is: does GATK cannot find my reads because it only considers **start/end** but not **unclippedStart/unclippedEnd** ? Is there any parameter to get those clipped reads ?. Many thanks in advance,. Pierre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2938
https://github.com/broadinstitute/gatk/issues/2939:235,Modifiability,Extend,Extend,235,@samuelklee commented on [Mon Feb 06 2017](https://github.com/broadinstitute/gatk-protected/issues/892). ---. @droazen commented on [Tue Feb 14 2017](https://github.com/broadinstitute/gatk-protected/issues/892#issuecomment-279793685). Extend `GATKTool` to get the standard `-L`/`-XL` functionality for free.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2939
https://github.com/broadinstitute/gatk/issues/2941:2596,Availability,down,down,2596,"aults.CREATE_MD5 : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CUSTOM_READER_FACTORY : ; 12:11:34.899 INFO PlotACNVResults - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:11:34.899 INFO PlotACNVResults - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.REFERENCE_FASTA : null; 12:11:34.899 INFO PlotACNVResults - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_CRAM_REF_DOWNLOAD : false; 12:11:34.900 INFO PlotACNVResults - Deflater IntelDeflater; 12:11:34.900 INFO PlotACNVResults - Initializing engine; 12:11:34.900 INFO PlotACNVResults - Done initializing engine; 12:11:35.009 INFO PlotACNVResults - Shutting down engine; [February 15, 2017 12:11:35 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1502085120; java.lang.IllegalArgumentException: There must be at least one contig above the threshold length in the sequence dictionary.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:673); 	at org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults.doWork(PlotACNVResults.java:120); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:225,Performance,load,load,225,"@achevali commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/902). It should state that the wrong file was given instead of trying to use it. ```12:11:34.836 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/dsde/working/aaronc/testing/LUAD/HMM_eval/gatk-protected/build/libs/gatk-protected-all-c17c8ed-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 12:11:34.894 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 15, 2017 12:11:34 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults --hets /dsde/working/aaronc/testing/LUAD/wgs/pulldown/TCGA-55-6972-01A-11D-1945-08.hets.tsv --tangentNormalized /dsde/working/aaronc/testing/LUAD/wgs/tumor_pcov/TCGA-55-6972-01A-11D-1945-08-gc-corrected.tn.tsv --segments out/wes/TCGA-55-6972-01A-11D-1945-08.seg --sequenceDictionaryFile /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --outputPrefix HMM_eval.TCGA-55-6972-01A-11D-1945-08. --output out/wgs/ --minimumContigLength 1000000 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 15, 2017 12:11:34 PM EST] Executing as aaronc@gsa5.broadinstitute.org on Linux 2.6.32-642.11.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; 12:11:34.899 INFO PlotACNVResults - Defaults.BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.COMPRESSION_LEVEL : 5; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_INDEX : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_MD5 : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CUSTOM_READER_FACTORY : ; 12:11:34.899 INFO PlotACNVResults - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:11:34.899 INFO PlotACNVResults - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.REFERENCE_FASTA : null; 12:11:34.899 INFO PlotACNVResults - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:481,Performance,load,loaded,481,"@achevali commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/902). It should state that the wrong file was given instead of trying to use it. ```12:11:34.836 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/dsde/working/aaronc/testing/LUAD/HMM_eval/gatk-protected/build/libs/gatk-protected-all-c17c8ed-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 12:11:34.894 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 15, 2017 12:11:34 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults --hets /dsde/working/aaronc/testing/LUAD/wgs/pulldown/TCGA-55-6972-01A-11D-1945-08.hets.tsv --tangentNormalized /dsde/working/aaronc/testing/LUAD/wgs/tumor_pcov/TCGA-55-6972-01A-11D-1945-08-gc-corrected.tn.tsv --segments out/wes/TCGA-55-6972-01A-11D-1945-08.seg --sequenceDictionaryFile /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --outputPrefix HMM_eval.TCGA-55-6972-01A-11D-1945-08. --output out/wgs/ --minimumContigLength 1000000 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 15, 2017 12:11:34 PM EST] Executing as aaronc@gsa5.broadinstitute.org on Linux 2.6.32-642.11.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; 12:11:34.899 INFO PlotACNVResults - Defaults.BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.COMPRESSION_LEVEL : 5; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_INDEX : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_MD5 : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CUSTOM_READER_FACTORY : ; 12:11:34.899 INFO PlotACNVResults - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:11:34.899 INFO PlotACNVResults - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.REFERENCE_FASTA : null; 12:11:34.899 INFO PlotACNVResults - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:3856,Performance,load,loadFastaDictionary,3856,"oadinstitute.hellbender.utils.Utils.validateArg(Utils.java:673); 	at org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults.doWork(PlotACNVResults.java:120); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is indeed a valid .dict file?. @droazen, can you comment? The use case is that we are using a dictionary file to select regions and specify chromosome lengths for plotting. This dictionary file may not necessarily correspond to the reference fasta file used to generate the data being plotted (as it may have chromosomes that the user does not want to plot removed, for example), so we don't want to allow the user to pass the fasta file. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295349726). @droazen can you chime in when you get a chance?. ---. @droazen commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295455256). @samuelklee `ReferenceUtils.loadFastaDictionary()` is fine to use for loading `.dict` files, even if a companion fasta file is not present. It is surprising that it doesn't throw if it is given a non-`.dict",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:3880,Performance,load,load,3880,"oadinstitute.hellbender.utils.Utils.validateArg(Utils.java:673); 	at org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults.doWork(PlotACNVResults.java:120); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is indeed a valid .dict file?. @droazen, can you comment? The use case is that we are using a dictionary file to select regions and specify chromosome lengths for plotting. This dictionary file may not necessarily correspond to the reference fasta file used to generate the data being plotted (as it may have chromosomes that the user does not want to plot removed, for example), so we don't want to allow the user to pass the fasta file. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295349726). @droazen can you chime in when you get a chance?. ---. @droazen commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295455256). @samuelklee `ReferenceUtils.loadFastaDictionary()` is fine to use for loading `.dict` files, even if a companion fasta file is not present. It is surprising that it doesn't throw if it is given a non-`.dict",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:3951,Performance,load,loading,3951,"oadinstitute.hellbender.utils.Utils.validateArg(Utils.java:673); 	at org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults.doWork(PlotACNVResults.java:120); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is indeed a valid .dict file?. @droazen, can you comment? The use case is that we are using a dictionary file to select regions and specify chromosome lengths for plotting. This dictionary file may not necessarily correspond to the reference fasta file used to generate the data being plotted (as it may have chromosomes that the user does not want to plot removed, for example), so we don't want to allow the user to pass the fasta file. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295349726). @droazen can you chime in when you get a chance?. ---. @droazen commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295455256). @samuelklee `ReferenceUtils.loadFastaDictionary()` is fine to use for loading `.dict` files, even if a companion fasta file is not present. It is surprising that it doesn't throw if it is given a non-`.dict",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:4739,Performance,load,loadFastaDictionary,4739,"at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is indeed a valid .dict file?. @droazen, can you comment? The use case is that we are using a dictionary file to select regions and specify chromosome lengths for plotting. This dictionary file may not necessarily correspond to the reference fasta file used to generate the data being plotted (as it may have chromosomes that the user does not want to plot removed, for example), so we don't want to allow the user to pass the fasta file. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295349726). @droazen can you chime in when you get a chance?. ---. @droazen commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295455256). @samuelklee `ReferenceUtils.loadFastaDictionary()` is fine to use for loading `.dict` files, even if a companion fasta file is not present. It is surprising that it doesn't throw if it is given a non-`.dict` input -- from a reading of the code, I suspect that it instead returns an empty sequence dictionary in that case. You could either have the caller check the return value of `ReferenceUtils.loadFastaDictionary()` to see if the returned dictionary is empty and throw if it is, or you could modify `ReferenceUtils.loadFastaDictionary()` itself to throw a `UserException` if the header returned from its `SAMTextHeaderCodec` contains no sequence dictionary. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295467825). I think the responsibility is on the method. Filed https://github.com/broadinstitute/gatk/issues/2609.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:4781,Performance,load,loading,4781,"at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is indeed a valid .dict file?. @droazen, can you comment? The use case is that we are using a dictionary file to select regions and specify chromosome lengths for plotting. This dictionary file may not necessarily correspond to the reference fasta file used to generate the data being plotted (as it may have chromosomes that the user does not want to plot removed, for example), so we don't want to allow the user to pass the fasta file. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295349726). @droazen can you chime in when you get a chance?. ---. @droazen commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295455256). @samuelklee `ReferenceUtils.loadFastaDictionary()` is fine to use for loading `.dict` files, even if a companion fasta file is not present. It is surprising that it doesn't throw if it is given a non-`.dict` input -- from a reading of the code, I suspect that it instead returns an empty sequence dictionary in that case. You could either have the caller check the return value of `ReferenceUtils.loadFastaDictionary()` to see if the returned dictionary is empty and throw if it is, or you could modify `ReferenceUtils.loadFastaDictionary()` itself to throw a `UserException` if the header returned from its `SAMTextHeaderCodec` contains no sequence dictionary. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295467825). I think the responsibility is on the method. Filed https://github.com/broadinstitute/gatk/issues/2609.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:5108,Performance,load,loadFastaDictionary,5108,"at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is indeed a valid .dict file?. @droazen, can you comment? The use case is that we are using a dictionary file to select regions and specify chromosome lengths for plotting. This dictionary file may not necessarily correspond to the reference fasta file used to generate the data being plotted (as it may have chromosomes that the user does not want to plot removed, for example), so we don't want to allow the user to pass the fasta file. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295349726). @droazen can you chime in when you get a chance?. ---. @droazen commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295455256). @samuelklee `ReferenceUtils.loadFastaDictionary()` is fine to use for loading `.dict` files, even if a companion fasta file is not present. It is surprising that it doesn't throw if it is given a non-`.dict` input -- from a reading of the code, I suspect that it instead returns an empty sequence dictionary in that case. You could either have the caller check the return value of `ReferenceUtils.loadFastaDictionary()` to see if the returned dictionary is empty and throw if it is, or you could modify `ReferenceUtils.loadFastaDictionary()` itself to throw a `UserException` if the header returned from its `SAMTextHeaderCodec` contains no sequence dictionary. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295467825). I think the responsibility is on the method. Filed https://github.com/broadinstitute/gatk/issues/2609.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:5230,Performance,load,loadFastaDictionary,5230,"at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is indeed a valid .dict file?. @droazen, can you comment? The use case is that we are using a dictionary file to select regions and specify chromosome lengths for plotting. This dictionary file may not necessarily correspond to the reference fasta file used to generate the data being plotted (as it may have chromosomes that the user does not want to plot removed, for example), so we don't want to allow the user to pass the fasta file. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295349726). @droazen can you chime in when you get a chance?. ---. @droazen commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295455256). @samuelklee `ReferenceUtils.loadFastaDictionary()` is fine to use for loading `.dict` files, even if a companion fasta file is not present. It is surprising that it doesn't throw if it is given a non-`.dict` input -- from a reading of the code, I suspect that it instead returns an empty sequence dictionary in that case. You could either have the caller check the return value of `ReferenceUtils.loadFastaDictionary()` to see if the returned dictionary is empty and throw if it is, or you could modify `ReferenceUtils.loadFastaDictionary()` itself to throw a `UserException` if the header returned from its `SAMTextHeaderCodec` contains no sequence dictionary. ---. @samuelklee commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-295467825). I think the responsibility is on the method. Filed https://github.com/broadinstitute/gatk/issues/2609.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:2953,Security,validat,validateArg,2953,"12:11:34.899 INFO PlotACNVResults - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_CRAM_REF_DOWNLOAD : false; 12:11:34.900 INFO PlotACNVResults - Deflater IntelDeflater; 12:11:34.900 INFO PlotACNVResults - Initializing engine; 12:11:34.900 INFO PlotACNVResults - Done initializing engine; 12:11:35.009 INFO PlotACNVResults - Shutting down engine; [February 15, 2017 12:11:35 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1502085120; java.lang.IllegalArgumentException: There must be at least one contig above the threshold length in the sequence dictionary.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:673); 	at org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults.doWork(PlotACNVResults.java:120); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:286,Testability,test,testing,286,"@achevali commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/902). It should state that the wrong file was given instead of trying to use it. ```12:11:34.836 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/dsde/working/aaronc/testing/LUAD/HMM_eval/gatk-protected/build/libs/gatk-protected-all-c17c8ed-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 12:11:34.894 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 15, 2017 12:11:34 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults --hets /dsde/working/aaronc/testing/LUAD/wgs/pulldown/TCGA-55-6972-01A-11D-1945-08.hets.tsv --tangentNormalized /dsde/working/aaronc/testing/LUAD/wgs/tumor_pcov/TCGA-55-6972-01A-11D-1945-08-gc-corrected.tn.tsv --segments out/wes/TCGA-55-6972-01A-11D-1945-08.seg --sequenceDictionaryFile /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --outputPrefix HMM_eval.TCGA-55-6972-01A-11D-1945-08. --output out/wgs/ --minimumContigLength 1000000 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 15, 2017 12:11:34 PM EST] Executing as aaronc@gsa5.broadinstitute.org on Linux 2.6.32-642.11.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; 12:11:34.899 INFO PlotACNVResults - Defaults.BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.COMPRESSION_LEVEL : 5; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_INDEX : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_MD5 : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CUSTOM_READER_FACTORY : ; 12:11:34.899 INFO PlotACNVResults - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:11:34.899 INFO PlotACNVResults - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.REFERENCE_FASTA : null; 12:11:34.899 INFO PlotACNVResults - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:636,Testability,test,testing,636,"@achevali commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/902). It should state that the wrong file was given instead of trying to use it. ```12:11:34.836 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/dsde/working/aaronc/testing/LUAD/HMM_eval/gatk-protected/build/libs/gatk-protected-all-c17c8ed-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 12:11:34.894 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 15, 2017 12:11:34 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults --hets /dsde/working/aaronc/testing/LUAD/wgs/pulldown/TCGA-55-6972-01A-11D-1945-08.hets.tsv --tangentNormalized /dsde/working/aaronc/testing/LUAD/wgs/tumor_pcov/TCGA-55-6972-01A-11D-1945-08-gc-corrected.tn.tsv --segments out/wes/TCGA-55-6972-01A-11D-1945-08.seg --sequenceDictionaryFile /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --outputPrefix HMM_eval.TCGA-55-6972-01A-11D-1945-08. --output out/wgs/ --minimumContigLength 1000000 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 15, 2017 12:11:34 PM EST] Executing as aaronc@gsa5.broadinstitute.org on Linux 2.6.32-642.11.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; 12:11:34.899 INFO PlotACNVResults - Defaults.BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.COMPRESSION_LEVEL : 5; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_INDEX : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_MD5 : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CUSTOM_READER_FACTORY : ; 12:11:34.899 INFO PlotACNVResults - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:11:34.899 INFO PlotACNVResults - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.REFERENCE_FASTA : null; 12:11:34.899 INFO PlotACNVResults - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2941:741,Testability,test,testing,741,"@achevali commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/902). It should state that the wrong file was given instead of trying to use it. ```12:11:34.836 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/dsde/working/aaronc/testing/LUAD/HMM_eval/gatk-protected/build/libs/gatk-protected-all-c17c8ed-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 12:11:34.894 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 15, 2017 12:11:34 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults --hets /dsde/working/aaronc/testing/LUAD/wgs/pulldown/TCGA-55-6972-01A-11D-1945-08.hets.tsv --tangentNormalized /dsde/working/aaronc/testing/LUAD/wgs/tumor_pcov/TCGA-55-6972-01A-11D-1945-08-gc-corrected.tn.tsv --segments out/wes/TCGA-55-6972-01A-11D-1945-08.seg --sequenceDictionaryFile /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --outputPrefix HMM_eval.TCGA-55-6972-01A-11D-1945-08. --output out/wgs/ --minimumContigLength 1000000 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 15, 2017 12:11:34 PM EST] Executing as aaronc@gsa5.broadinstitute.org on Linux 2.6.32-642.11.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; 12:11:34.899 INFO PlotACNVResults - Defaults.BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.COMPRESSION_LEVEL : 5; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_INDEX : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_MD5 : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CUSTOM_READER_FACTORY : ; 12:11:34.899 INFO PlotACNVResults - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:11:34.899 INFO PlotACNVResults - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.REFERENCE_FASTA : null; 12:11:34.899 INFO PlotACNVResults - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941
https://github.com/broadinstitute/gatk/issues/2942:172,Deployability,pipeline,pipelines,172,"@davidbenjamin commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903). Since most of the work is in setting up the necessary tools and pipelines to evaluate, I will lump the actual act of evaluating on specific data into this single ticket. We need to:. * CRSP specificity: apply Takuto's `mutect2-replicate-validation.wdl` on the CRSP NA12878 replicates.; * CRSP sensitivity: apply the (currently in-progress) hapmap sensitivity pipeline to CRSP data.; * cfDNA: run cfDNA samples and matched solid tumor samples (which we already have from Viktor) and run the concordance tool.; * FFPE: run FFPE and matched non-FFPE samples and run the concordance tool.; * tumor-only: run some TCGA samples with and without their matched normal and run the concordance tool. ---. @davidbenjamin commented on [Wed Mar 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-286795503). Update: CRSP sensitivity and specificity have been run several times, cfDNA is currently running for the first time. FFPE and tumor-only will use the same wdl as cfDNA, so we'll run those once cfDNA finishes. ---. @davidbenjamin commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-287675783). Update: everything done except FFPE and tumor-only. FFPE will use the same wdl as cfDNA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2942
https://github.com/broadinstitute/gatk/issues/2942:467,Deployability,pipeline,pipeline,467,"@davidbenjamin commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903). Since most of the work is in setting up the necessary tools and pipelines to evaluate, I will lump the actual act of evaluating on specific data into this single ticket. We need to:. * CRSP specificity: apply Takuto's `mutect2-replicate-validation.wdl` on the CRSP NA12878 replicates.; * CRSP sensitivity: apply the (currently in-progress) hapmap sensitivity pipeline to CRSP data.; * cfDNA: run cfDNA samples and matched solid tumor samples (which we already have from Viktor) and run the concordance tool.; * FFPE: run FFPE and matched non-FFPE samples and run the concordance tool.; * tumor-only: run some TCGA samples with and without their matched normal and run the concordance tool. ---. @davidbenjamin commented on [Wed Mar 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-286795503). Update: CRSP sensitivity and specificity have been run several times, cfDNA is currently running for the first time. FFPE and tumor-only will use the same wdl as cfDNA, so we'll run those once cfDNA finishes. ---. @davidbenjamin commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-287675783). Update: everything done except FFPE and tumor-only. FFPE will use the same wdl as cfDNA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2942
https://github.com/broadinstitute/gatk/issues/2942:934,Deployability,Update,Update,934,"@davidbenjamin commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903). Since most of the work is in setting up the necessary tools and pipelines to evaluate, I will lump the actual act of evaluating on specific data into this single ticket. We need to:. * CRSP specificity: apply Takuto's `mutect2-replicate-validation.wdl` on the CRSP NA12878 replicates.; * CRSP sensitivity: apply the (currently in-progress) hapmap sensitivity pipeline to CRSP data.; * cfDNA: run cfDNA samples and matched solid tumor samples (which we already have from Viktor) and run the concordance tool.; * FFPE: run FFPE and matched non-FFPE samples and run the concordance tool.; * tumor-only: run some TCGA samples with and without their matched normal and run the concordance tool. ---. @davidbenjamin commented on [Wed Mar 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-286795503). Update: CRSP sensitivity and specificity have been run several times, cfDNA is currently running for the first time. FFPE and tumor-only will use the same wdl as cfDNA, so we'll run those once cfDNA finishes. ---. @davidbenjamin commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-287675783). Update: everything done except FFPE and tumor-only. FFPE will use the same wdl as cfDNA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2942
https://github.com/broadinstitute/gatk/issues/2942:1279,Deployability,Update,Update,1279,"@davidbenjamin commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903). Since most of the work is in setting up the necessary tools and pipelines to evaluate, I will lump the actual act of evaluating on specific data into this single ticket. We need to:. * CRSP specificity: apply Takuto's `mutect2-replicate-validation.wdl` on the CRSP NA12878 replicates.; * CRSP sensitivity: apply the (currently in-progress) hapmap sensitivity pipeline to CRSP data.; * cfDNA: run cfDNA samples and matched solid tumor samples (which we already have from Viktor) and run the concordance tool.; * FFPE: run FFPE and matched non-FFPE samples and run the concordance tool.; * tumor-only: run some TCGA samples with and without their matched normal and run the concordance tool. ---. @davidbenjamin commented on [Wed Mar 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-286795503). Update: CRSP sensitivity and specificity have been run several times, cfDNA is currently running for the first time. FFPE and tumor-only will use the same wdl as cfDNA, so we'll run those once cfDNA finishes. ---. @davidbenjamin commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-287675783). Update: everything done except FFPE and tumor-only. FFPE will use the same wdl as cfDNA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2942
https://github.com/broadinstitute/gatk/issues/2942:345,Security,validat,validation,345,"@davidbenjamin commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903). Since most of the work is in setting up the necessary tools and pipelines to evaluate, I will lump the actual act of evaluating on specific data into this single ticket. We need to:. * CRSP specificity: apply Takuto's `mutect2-replicate-validation.wdl` on the CRSP NA12878 replicates.; * CRSP sensitivity: apply the (currently in-progress) hapmap sensitivity pipeline to CRSP data.; * cfDNA: run cfDNA samples and matched solid tumor samples (which we already have from Viktor) and run the concordance tool.; * FFPE: run FFPE and matched non-FFPE samples and run the concordance tool.; * tumor-only: run some TCGA samples with and without their matched normal and run the concordance tool. ---. @davidbenjamin commented on [Wed Mar 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-286795503). Update: CRSP sensitivity and specificity have been run several times, cfDNA is currently running for the first time. FFPE and tumor-only will use the same wdl as cfDNA, so we'll run those once cfDNA finishes. ---. @davidbenjamin commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-287675783). Update: everything done except FFPE and tumor-only. FFPE will use the same wdl as cfDNA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2942
https://github.com/broadinstitute/gatk/issues/2944:220,Availability,error,errors,220,"@meganshand commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/907). Using a tiny bam file that I typically use for testing while running the CNV wdl on the cloud, I got the following errors (the tiny file is here: `gs://broad-dsde-methods/takuto/test_files/small_NA12878_hg19.bam`):. 1. The output tsv from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:431,Availability,error,error,431,"@meganshand commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/907). Using a tiny bam file that I typically use for testing while running the CNV wdl on the cloud, I got the following errors (the tiny file is here: `gs://broad-dsde-methods/takuto/test_files/small_NA12878_hg19.bam`):. 1. The output tsv from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:2578,Availability,Error,Error,2578,"ource('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:2704,Modifiability,inherit,inherits,2704,"root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:583,Performance,Perform,PerformSegmentation,583,"@meganshand commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/907). Using a tiny bam file that I typically use for testing while running the CNV wdl on the cloud, I got the following errors (the tiny file is here: `gs://broad-dsde-methods/takuto/test_files/small_NA12878_hg19.bam`):. 1. The output tsv from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:1326,Performance,Perform,PerformSegmentation,1326," from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:3008,Performance,Perform,PerformSegmentation,3008,"30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:3046,Performance,Perform,PerformSegmentation,3046,"30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:3123,Performance,Perform,PerformSegmentation,3123,"30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:3150,Performance,Perform,PerformSegmentation,3150,"30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:152,Testability,test,testing,152,"@meganshand commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/907). Using a tiny bam file that I typically use for testing while running the CNV wdl on the cloud, I got the following errors (the tiny file is here: `gs://broad-dsde-methods/takuto/test_files/small_NA12878_hg19.bam`):. 1. The output tsv from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:932,Usability,undo,undoSplits,932,"@meganshand commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/907). Using a tiny bam file that I typically use for testing while running the CNV wdl on the cloud, I got the following errors (the tiny file is here: `gs://broad-dsde-methods/takuto/test_files/small_NA12878_hg19.bam`):. 1. The output tsv from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:950,Usability,undo,undoPrune,950,"@meganshand commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/907). Using a tiny bam file that I typically use for testing while running the CNV wdl on the cloud, I got the following errors (the tiny file is here: `gs://broad-dsde-methods/takuto/test_files/small_NA12878_hg19.bam`):. 1. The output tsv from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:967,Usability,undo,undoSD,967,"/github.com/broadinstitute/gatk-protected/issues/907). Using a tiny bam file that I typically use for testing while running the CNV wdl on the cloud, I got the following errors (the tiny file is here: `gs://broad-dsde-methods/takuto/test_files/small_NA12878_hg19.bam`):. 1. The output tsv from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --und",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:2013,Usability,undo,undosplits,2013,"UIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:2031,Usability,undo,undoprune,2031,"UIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:2048,Usability,undo,undoSD,2048,"ary 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:2488,Usability,undo,undosplits,2488,"h 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:2513,Usability,undo,undoprune,2513,"ipt -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2944:2537,Usability,undo,undoSD,2537,"cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.ins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944
https://github.com/broadinstitute/gatk/issues/2945:1539,Modifiability,refactor,refactored,1539," code, though I don't know how much you cleaned up the GATK4 version. We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. In cases where the haplotype has multiple SNPs and the phasing is poor, this could artificially inflate the likelihoods. Although we've seen that the graph traversal frequently breaks phasing then generating haplotypes anyway, so maybe I overestimate our current likelihood accuracy. Anyway, take my advice with a grain of salt. It's just some musings from a bored and somewhat sleep-deprived mom with a sleeping baby on her lap. ---. @davidbenjamin commented on [Fri Mar 03 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284024760). > I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945
https://github.com/broadinstitute/gatk/issues/2945:756,Performance,optimiz,optimization,756,"@davidbenjamin commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/909). It might be sufficient, especially for SNP calling, to run `PairHMM` over s small number of bases, say 20 or so, surrounding a variant. This might make sense for `HaplotypeCaller` as well. . . ---. @ldgauthier commented on [Fri Feb 24 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-282432230). I think @yfarjoun and I discussed this a long time ago. In theory it would speed up the really simple cases. I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. In cases where the haplotype has multiple SNPs and the phasing is poor, this could artificially inflate the likelihoods. Although we've seen that the graph traversal frequently breaks phasing then generating haplotypes anyway, so maybe I overestimate our current likelihood accuracy. Anyway, take my advice with a grain of salt. It's just some musings from a bored and somewhat sleep-deprived mom with a sleeping baby on her lap. ---. @davidbenjamin commented on [Fri Mar 03 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284024760). > I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you hav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945
https://github.com/broadinstitute/gatk/issues/2945:1820,Performance,optimiz,optimization,1820," cases where the haplotype has multiple SNPs and the phasing is poor, this could artificially inflate the likelihoods. Although we've seen that the graph traversal frequently breaks phasing then generating haplotypes anyway, so maybe I overestimate our current likelihood accuracy. Anyway, take my advice with a grain of salt. It's just some musings from a bored and somewhat sleep-deprived mom with a sleeping baby on her lap. ---. @davidbenjamin commented on [Fri Mar 03 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284024760). > I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intelligently *after* building the whole deBruijn graph. It could well be that my optimism is ill-founded. Nonetheless, having a quick-and-dirty mode would be very useful for the following purpose",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945
https://github.com/broadinstitute/gatk/issues/2945:3348,Performance,optimiz,optimize,3348," I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intelligently *after* building the whole deBruijn graph. It could well be that my optimism is ill-founded. Nonetheless, having a quick-and-dirty mode would be very useful for the following purposes where you need to run M2 a lot and don't need perfection:. * making an M2 panel of normals; * making true positives + false positives training data sets for artifact classifiers; * testing changes. ---. @ldgauthier commented on [Mon Mar 06 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284502170). ""Quick and dirty"" would be useful for testing changes, but the PoN and training sets shouldn't be recreated very often so there's less savings. I hate to leverage the fact that we break phasing to optimize things because I dream of a future where HaplotypeCaller actually calls haplotypes (as you've already added an issue for).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945
https://github.com/broadinstitute/gatk/issues/2945:1649,Testability,log,logic,1649," code, though I don't know how much you cleaned up the GATK4 version. We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. In cases where the haplotype has multiple SNPs and the phasing is poor, this could artificially inflate the likelihoods. Although we've seen that the graph traversal frequently breaks phasing then generating haplotypes anyway, so maybe I overestimate our current likelihood accuracy. Anyway, take my advice with a grain of salt. It's just some musings from a bored and somewhat sleep-deprived mom with a sleeping baby on her lap. ---. @davidbenjamin commented on [Fri Mar 03 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284024760). > I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945
https://github.com/broadinstitute/gatk/issues/2945:3001,Testability,test,testing,3001," I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intelligently *after* building the whole deBruijn graph. It could well be that my optimism is ill-founded. Nonetheless, having a quick-and-dirty mode would be very useful for the following purposes where you need to run M2 a lot and don't need perfection:. * making an M2 panel of normals; * making true positives + false positives training data sets for artifact classifiers; * testing changes. ---. @ldgauthier commented on [Mon Mar 06 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284502170). ""Quick and dirty"" would be useful for testing changes, but the PoN and training sets shouldn't be recreated very often so there's less savings. I hate to leverage the fact that we break phasing to optimize things because I dream of a future where HaplotypeCaller actually calls haplotypes (as you've already added an issue for).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945
https://github.com/broadinstitute/gatk/issues/2945:3189,Testability,test,testing,3189," I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intelligently *after* building the whole deBruijn graph. It could well be that my optimism is ill-founded. Nonetheless, having a quick-and-dirty mode would be very useful for the following purposes where you need to run M2 a lot and don't need perfection:. * making an M2 panel of normals; * making true positives + false positives training data sets for artifact classifiers; * testing changes. ---. @ldgauthier commented on [Mon Mar 06 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284502170). ""Quick and dirty"" would be useful for testing changes, but the PoN and training sets shouldn't be recreated very often so there's less savings. I hate to leverage the fact that we break phasing to optimize things because I dream of a future where HaplotypeCaller actually calls haplotypes (as you've already added an issue for).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945
https://github.com/broadinstitute/gatk/issues/2945:529,Usability,simpl,simple,529,"@davidbenjamin commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/909). It might be sufficient, especially for SNP calling, to run `PairHMM` over s small number of bases, say 20 or so, surrounding a variant. This might make sense for `HaplotypeCaller` as well. . . ---. @ldgauthier commented on [Fri Feb 24 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-282432230). I think @yfarjoun and I discussed this a long time ago. In theory it would speed up the really simple cases. I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. In cases where the haplotype has multiple SNPs and the phasing is poor, this could artificially inflate the likelihoods. Although we've seen that the graph traversal frequently breaks phasing then generating haplotypes anyway, so maybe I overestimate our current likelihood accuracy. Anyway, take my advice with a grain of salt. It's just some musings from a bored and somewhat sleep-deprived mom with a sleeping baby on her lap. ---. @davidbenjamin commented on [Fri Mar 03 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284024760). > I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you hav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945
https://github.com/broadinstitute/gatk/issues/2946:664,Availability,error,error,664,"@sooheelee commented on [Fri Feb 17 2017](https://github.com/broadinstitute/gatk-protected/issues/911). A fix was implemented for HaplotypeCaller but not ported to GenotypeGVCFs nor CombineGVCFs nor CombineVariants. Although user is using v3.7-0-gcfedb67, my understanding is that these types of fixes will only be worked on in GATK4. - Previous discussion of HaplotypeCaller fix; https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318. ---; ### Test data submitted by user can be found at; /humgen/gsa-scr1/pub/incoming/bugrep_jgeibel_1.tar.gz. This includes chicken reference files. . ---; ### The command the user uses to generate the error is very long because we have many vcfs:; ```; Program Args: -T GenotypeGVCFs -R /usr/users/geibel/chicken/chickenrefgen/galGal5_Dec2015/galGal5.fa --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72631_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72632_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72633_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72634_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72635_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72636_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72637_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72638_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72639_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72640_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72641_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72642_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_seq",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:9899,Availability,error,error,9899,en/pool_sequence_nov2016/data/gVCF/pl_GV_0036_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_CW_0037_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_DL_0038_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_KS_0039_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_OF_0040_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_WR_0041_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_RI_0042_chr26.raw.g.vcf -nt 10 --max_genotype_count 1024 -L chr26 --dbsnp /usr/users/geibel/chicken/chickenrefgen/ENSEMBL_20170106/Gallus_gallus.updated.vcf -o /usr/users/geibel/chicken/pool_sequence_nov2016/data/rawVCF/IndandPool_chr26.raw.vcf ; ```. The user actually includes a shell script in the test data bundle called `JointGenotyping_chr26.sh`. ---; ### The error shows:; ```; ##### ERROR --; ##### ERROR stack trace ; java.lang.IllegalArgumentException: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; 	at org.broadinstitute.gatk.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:319); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:461); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:164); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:9924,Availability,ERROR,ERROR,9924,en/pool_sequence_nov2016/data/gVCF/pl_GV_0036_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_CW_0037_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_DL_0038_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_KS_0039_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_OF_0040_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_WR_0041_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_RI_0042_chr26.raw.g.vcf -nt 10 --max_genotype_count 1024 -L chr26 --dbsnp /usr/users/geibel/chicken/chickenrefgen/ENSEMBL_20170106/Gallus_gallus.updated.vcf -o /usr/users/geibel/chicken/pool_sequence_nov2016/data/rawVCF/IndandPool_chr26.raw.vcf ; ```. The user actually includes a shell script in the test data bundle called `JointGenotyping_chr26.sh`. ---; ### The error shows:; ```; ##### ERROR --; ##### ERROR stack trace ; java.lang.IllegalArgumentException: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; 	at org.broadinstitute.gatk.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:319); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:461); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:164); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:9940,Availability,ERROR,ERROR,9940,en/pool_sequence_nov2016/data/gVCF/pl_GV_0036_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_CW_0037_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_DL_0038_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_KS_0039_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_OF_0040_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_WR_0041_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_RI_0042_chr26.raw.g.vcf -nt 10 --max_genotype_count 1024 -L chr26 --dbsnp /usr/users/geibel/chicken/chickenrefgen/ENSEMBL_20170106/Gallus_gallus.updated.vcf -o /usr/users/geibel/chicken/pool_sequence_nov2016/data/rawVCF/IndandPool_chr26.raw.vcf ; ```. The user actually includes a shell script in the test data bundle called `JointGenotyping_chr26.sh`. ---; ### The error shows:; ```; ##### ERROR --; ##### ERROR stack trace ; java.lang.IllegalArgumentException: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; 	at org.broadinstitute.gatk.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:319); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:461); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:164); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:11838,Availability,ERROR,ERROR,11838,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:11942,Availability,ERROR,ERROR,11942,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:11963,Availability,ERROR,ERROR,11963,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12015,Availability,ERROR,ERROR,12015,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12028,Availability,ERROR,ERROR,12028,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12134,Availability,ERROR,ERROR,12134,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12164,Availability,error,error,12164,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12223,Availability,ERROR,ERROR,12223,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12308,Availability,ERROR,ERROR,12308,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12387,Availability,ERROR,ERROR,12387,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12400,Availability,ERROR,ERROR,12400,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12507,Availability,ERROR,ERROR,12507,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:9678,Deployability,update,updated,9678,033_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_YO_0034_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_ZC_0035_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_GV_0036_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_CW_0037_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_DL_0038_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_KS_0039_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_OF_0040_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_WR_0041_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_RI_0042_chr26.raw.g.vcf -nt 10 --max_genotype_count 1024 -L chr26 --dbsnp /usr/users/geibel/chicken/chickenrefgen/ENSEMBL_20170106/Gallus_gallus.updated.vcf -o /usr/users/geibel/chicken/pool_sequence_nov2016/data/rawVCF/IndandPool_chr26.raw.vcf ; ```. The user actually includes a shell script in the test data bundle called `JointGenotyping_chr26.sh`. ---; ### The error shows:; ```; ##### ERROR --; ##### ERROR stack trace ; java.lang.IllegalArgumentException: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; 	at org.broadinstitute.gatk.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:319); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:461); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:164); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12170,Integrability,message,message,12170,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12406,Integrability,MESSAGE,MESSAGE,12406,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:11571,Performance,concurren,concurrent,11571,".walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ----------------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:11633,Performance,concurren,concurrent,11633,"); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:11718,Performance,concurren,concurrent,11718,"VCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:472,Testability,Test,Test,472,"@sooheelee commented on [Fri Feb 17 2017](https://github.com/broadinstitute/gatk-protected/issues/911). A fix was implemented for HaplotypeCaller but not ported to GenotypeGVCFs nor CombineGVCFs nor CombineVariants. Although user is using v3.7-0-gcfedb67, my understanding is that these types of fixes will only be worked on in GATK4. - Previous discussion of HaplotypeCaller fix; https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318. ---; ### Test data submitted by user can be found at; /humgen/gsa-scr1/pub/incoming/bugrep_jgeibel_1.tar.gz. This includes chicken reference files. . ---; ### The command the user uses to generate the error is very long because we have many vcfs:; ```; Program Args: -T GenotypeGVCFs -R /usr/users/geibel/chicken/chickenrefgen/galGal5_Dec2015/galGal5.fa --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72631_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72632_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72633_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72634_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72635_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72636_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72637_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72638_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72639_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72640_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72641_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/i_WL_72642_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_seq",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:9834,Testability,test,test,9834,/pool_sequence_nov2016/data/gVCF/pl_ZC_0035_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_GV_0036_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_CW_0037_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_DL_0038_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_KS_0039_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_OF_0040_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_WR_0041_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_RI_0042_chr26.raw.g.vcf -nt 10 --max_genotype_count 1024 -L chr26 --dbsnp /usr/users/geibel/chicken/chickenrefgen/ENSEMBL_20170106/Gallus_gallus.updated.vcf -o /usr/users/geibel/chicken/pool_sequence_nov2016/data/rawVCF/IndandPool_chr26.raw.vcf ; ```. The user actually includes a shell script in the test data bundle called `JointGenotyping_chr26.sh`. ---; ### The error shows:; ```; ##### ERROR --; ##### ERROR stack trace ; java.lang.IllegalArgumentException: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; 	at org.broadinstitute.gatk.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:319); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:461); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:164); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2946:12086,Usability,guid,guide,12086,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946
https://github.com/broadinstitute/gatk/issues/2947:144,Deployability,pipeline,pipeline,144,"@mbabadi commented on [Fri Feb 17 2017](https://github.com/broadinstitute/gatk-protected/issues/914). The current coverage tool in the GATK CNV pipeline, i.e. `CalculateTargetCoverage` has important caveats that make the ab-initio modeling of coverage data and subsequent CNV analysis inaccurate. We need to write a principled tool for calculating read depth histograms from targetted sequencing data. Given that a major source of coverage variance in targetted sequencing stems from the variance in bait efficiencies, the most reasonable read-depth calculation scheme is to associate **inserts to baits** rather than **single reads to targets**. No more arbitrary interval padding (which was a hacky way to get away not thinking about inserts). There is a subtle problem, though: inserts often overlap with more than one bait. In such cases, we need to have a model for estimating the probability that the insert is captured by either of the overlapping baits. The modeling can be done in the following semi-empirical fashion (thanks @yfarjoun), which needs to be done only once for each capture technology (Agilent, ICE):; - We locate isolated baits (i.e. those that are separated from one another by a few standard deviations of the average insert size); - We take a number of BAMs and calculate the empirical distribution of inserts around the isolated baits; - We fit a simple parametric distribution to the obtained empirical distributions, parameterized by bait length and insert length; we probably don't need to go all-in here, though the reference context of the bait is also likely to be an important covariate. Once these distributions are known, we can easily calculate the membership share of each bait in ambiguous cases and give each bait the appropriate share. Bonus:; -------. The empirical distribution of inserts around baits also allows us to associate a more reasonable GC content to each bait. Since GC bias is a property of the fragments that are pulled by the baits, a reasona",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947
https://github.com/broadinstitute/gatk/issues/2947:3585,Deployability,pipeline,pipeline,3585,"t-280806711). I agree with the idea. the devil, as always is in the details. - I think that the model you are interested in is parametrized with; insert-length and position (of bait) in insert, not bait-length and insert; length. Bait length is going to be a constant, or almost so, I suspect; (please check); - I suspect that of these two covariates, position from the (nearest) end; will be the most informative and will probably decay to a constant with a; decay parameter of the order of the bait length.; - the effect of GC bias will effect the overall efficacy of the bait since; it will effect the entire insert when amplified, and so it will introduce; noise that is orthogonal to the question you are after. I would design the; measurement to be as unaffected by this noise as possible. your later model; will find this constant of course. happy to talk about this more!. Yossi. On Fri, Feb 17, 2017 at 6:57 PM, Mehrtash Babadi <notifications@github.com>; wrote:. > The current coverage tool in the GATK CNV pipeline, i.e.; > CalculateTargetCoverage has important caveats that render the ab-initio; > modeling of coverage and subsequent CNV analysis inaccurate. The purpose of; > this issue is to write a new tool for dealing with targetted sequencing; > data. Since a major source of coverage variance in targetted sequencing is; > different capture efficiencies, the most reasonable read-depth calculation; > scheme is to associate *inserts to baits* rather than *single reads to; > targets*.; >; > There is a subtle problem, though: inserts often overlap with more than; > one bait. In such cases, we need to have a model for estimating the; > probability that the insert is captured by either of the overlapping baits.; > The modeling can be done in the following semi-empirical fashion (thanks; > @yfarjoun <https://github.com/yfarjoun>), which needs to be done only; > once each capture technology (Agilent, ICE):; >; > - We locate isolated baits (i.e. those that are separated from one",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947
https://github.com/broadinstitute/gatk/issues/2947:6394,Deployability,configurat,configuration,6394," Since GC bias is a; > property of the fragments that are pulled by the baits, a reasonable; > measure of ""GC content"" of each bait has to be calculated from the expected; > value of the GC content of the fragments that the bait pulls (not the GC; > content of the baits or targets), and this can be easily calculated from; > the previously obtained empirical distributions.; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/914>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qEpyk5wss6qvl653UQo-BAiQWfIks5rdjPNgaJpZM4ME4kq>; > .; >. ---. @mbabadi commented on [Sat Feb 18 2017](https://github.com/broadinstitute/gatk-protected/issues/914#issuecomment-280820307). @yfarjoun, thanks for your comments. On the first two points, I agree. Let me clarify: I was going to use the bait-length and insert-length as _hyperparameters_ of the pdf, where the pdf itself gives the probability of having an insert in a certain configuration relative to the bait. I think the parametrization you proposed, i.e. the distance between nearest ends of insert and bait, is very reasonable since the PDF is going to be reflection-symmetric once averaged over all baits; and you're right, the bait length is constant (77bp for ICE) so we can drop it from the analysis. If the fragment capture efficiency is insensitive to the relative position of the bait sequence in the fragment, we expect the pdf to be approximately uniform (save for boundary effects at the scale of bait length), with the 0.5 x (insert length - bait length) setting the upper bound of the distribution. However, some dependency on the position of the bait is expected: e.g. if the bait sequence is on the dangling end of a fragment, it is less likely to stay bound than if it is in the middle of the fragment. I'm curious to see what comes out (who knows -- maybe another 6-bp periodicity!).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947
https://github.com/broadinstitute/gatk/issues/2947:7048,Integrability,depend,dependency,7048,"/github.com/broadinstitute/gatk-protected/issues/914#issuecomment-280820307). @yfarjoun, thanks for your comments. On the first two points, I agree. Let me clarify: I was going to use the bait-length and insert-length as _hyperparameters_ of the pdf, where the pdf itself gives the probability of having an insert in a certain configuration relative to the bait. I think the parametrization you proposed, i.e. the distance between nearest ends of insert and bait, is very reasonable since the PDF is going to be reflection-symmetric once averaged over all baits; and you're right, the bait length is constant (77bp for ICE) so we can drop it from the analysis. If the fragment capture efficiency is insensitive to the relative position of the bait sequence in the fragment, we expect the pdf to be approximately uniform (save for boundary effects at the scale of bait length), with the 0.5 x (insert length - bait length) setting the upper bound of the distribution. However, some dependency on the position of the bait is expected: e.g. if the bait sequence is on the dangling end of a fragment, it is less likely to stay bound than if it is in the middle of the fragment. I'm curious to see what comes out (who knows -- maybe another 6-bp periodicity!). Regarding GC bias -- I agree with what you said; I guess I had a different point, though. To find the GC curve, one needs to regress the _coverage depth_ wrt. the _GC content_ in some way. Now, all we have is a collection of baits, along with the number of inserts pulled by each bait (let's call it the bait pileup). So, we have to designate a measure of _GC content_ to each bait pileup in order to regress the pileup size against it. A reasonable choice is to use the expected value of the GC content of the pulled fragments for each bait:. effective GC of a bait = \sum_{all inserts} GC(insert) x Pr(insert) x Pr(insert is pulled by bait). Alternatively, we can calculate the average GC content of each bait pileup, for each sample, as we ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947
https://github.com/broadinstitute/gatk/issues/2947:1447,Modifiability,parameteriz,parameterized,1447,"encing data. Given that a major source of coverage variance in targetted sequencing stems from the variance in bait efficiencies, the most reasonable read-depth calculation scheme is to associate **inserts to baits** rather than **single reads to targets**. No more arbitrary interval padding (which was a hacky way to get away not thinking about inserts). There is a subtle problem, though: inserts often overlap with more than one bait. In such cases, we need to have a model for estimating the probability that the insert is captured by either of the overlapping baits. The modeling can be done in the following semi-empirical fashion (thanks @yfarjoun), which needs to be done only once for each capture technology (Agilent, ICE):; - We locate isolated baits (i.e. those that are separated from one another by a few standard deviations of the average insert size); - We take a number of BAMs and calculate the empirical distribution of inserts around the isolated baits; - We fit a simple parametric distribution to the obtained empirical distributions, parameterized by bait length and insert length; we probably don't need to go all-in here, though the reference context of the bait is also likely to be an important covariate. Once these distributions are known, we can easily calculate the membership share of each bait in ambiguous cases and give each bait the appropriate share. Bonus:; -------. The empirical distribution of inserts around baits also allows us to associate a more reasonable GC content to each bait. Since GC bias is a property of the fragments that are pulled by the baits, a reasonable measure of ""GC content"" of each bait has to be calculated from the expected value of the GC content of the fragments that the bait pulls (not the GC content of the baits or targets), and this can be easily calculated from the previously obtained empirical distributions. ---. @mbabadi commented on [Fri Feb 17 2017](https://github.com/broadinstitute/gatk-protected/issues/914#issuecomm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947
https://github.com/broadinstitute/gatk/issues/2947:6394,Modifiability,config,configuration,6394," Since GC bias is a; > property of the fragments that are pulled by the baits, a reasonable; > measure of ""GC content"" of each bait has to be calculated from the expected; > value of the GC content of the fragments that the bait pulls (not the GC; > content of the baits or targets), and this can be easily calculated from; > the previously obtained empirical distributions.; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/914>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qEpyk5wss6qvl653UQo-BAiQWfIks5rdjPNgaJpZM4ME4kq>; > .; >. ---. @mbabadi commented on [Sat Feb 18 2017](https://github.com/broadinstitute/gatk-protected/issues/914#issuecomment-280820307). @yfarjoun, thanks for your comments. On the first two points, I agree. Let me clarify: I was going to use the bait-length and insert-length as _hyperparameters_ of the pdf, where the pdf itself gives the probability of having an insert in a certain configuration relative to the bait. I think the parametrization you proposed, i.e. the distance between nearest ends of insert and bait, is very reasonable since the PDF is going to be reflection-symmetric once averaged over all baits; and you're right, the bait length is constant (77bp for ICE) so we can drop it from the analysis. If the fragment capture efficiency is insensitive to the relative position of the bait sequence in the fragment, we expect the pdf to be approximately uniform (save for boundary effects at the scale of bait length), with the 0.5 x (insert length - bait length) setting the upper bound of the distribution. However, some dependency on the position of the bait is expected: e.g. if the bait sequence is on the dangling end of a fragment, it is less likely to stay bound than if it is in the middle of the fragment. I'm curious to see what comes out (who knows -- maybe another 6-bp periodicity!).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947
https://github.com/broadinstitute/gatk/issues/2947:1375,Usability,simpl,simple,1375,"encing data. Given that a major source of coverage variance in targetted sequencing stems from the variance in bait efficiencies, the most reasonable read-depth calculation scheme is to associate **inserts to baits** rather than **single reads to targets**. No more arbitrary interval padding (which was a hacky way to get away not thinking about inserts). There is a subtle problem, though: inserts often overlap with more than one bait. In such cases, we need to have a model for estimating the probability that the insert is captured by either of the overlapping baits. The modeling can be done in the following semi-empirical fashion (thanks @yfarjoun), which needs to be done only once for each capture technology (Agilent, ICE):; - We locate isolated baits (i.e. those that are separated from one another by a few standard deviations of the average insert size); - We take a number of BAMs and calculate the empirical distribution of inserts around the isolated baits; - We fit a simple parametric distribution to the obtained empirical distributions, parameterized by bait length and insert length; we probably don't need to go all-in here, though the reference context of the bait is also likely to be an important covariate. Once these distributions are known, we can easily calculate the membership share of each bait in ambiguous cases and give each bait the appropriate share. Bonus:; -------. The empirical distribution of inserts around baits also allows us to associate a more reasonable GC content to each bait. Since GC bias is a property of the fragments that are pulled by the baits, a reasonable measure of ""GC content"" of each bait has to be calculated from the expected value of the GC content of the fragments that the bait pulls (not the GC content of the baits or targets), and this can be easily calculated from the previously obtained empirical distributions. ---. @mbabadi commented on [Fri Feb 17 2017](https://github.com/broadinstitute/gatk-protected/issues/914#issuecomm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947
https://github.com/broadinstitute/gatk/issues/2947:4765,Usability,simpl,simple,4765,"write a new tool for dealing with targetted sequencing; > data. Since a major source of coverage variance in targetted sequencing is; > different capture efficiencies, the most reasonable read-depth calculation; > scheme is to associate *inserts to baits* rather than *single reads to; > targets*.; >; > There is a subtle problem, though: inserts often overlap with more than; > one bait. In such cases, we need to have a model for estimating the; > probability that the insert is captured by either of the overlapping baits.; > The modeling can be done in the following semi-empirical fashion (thanks; > @yfarjoun <https://github.com/yfarjoun>), which needs to be done only; > once each capture technology (Agilent, ICE):; >; > - We locate isolated baits (i.e. those that are separated from one; > another by a few standard deviations of the average insert size); > - We take a number of BAMs and calculate the empirical distribution of; > inserts around the isolated baits; > - We build a simple parametric model for the obtained empirical; > distributions, parametrized by bait length and insert length; we probably; > don't need to go all-in here, though the reference context of the bait is; > also likely to be an important covariate.; >; > Once these distributions are known, we can easily calculate the membership; > share of each bait in ambiguous cases and give each bait the appropriate; > share.; > Bonus:; >; > The empirical distribution of inserts around baits also allows us to; > associate a more reasonable GC content to each bait. Since GC bias is a; > property of the fragments that are pulled by the baits, a reasonable; > measure of ""GC content"" of each bait has to be calculated from the expected; > value of the GC content of the fragments that the bait pulls (not the GC; > content of the baits or targets), and this can be easily calculated from; > the previously obtained empirical distributions.; >; > ; > You are receiving this because you were mentioned.; > Reply to this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947
https://github.com/broadinstitute/gatk/issues/2948:189,Testability,test,testing,189,"@LeeTL1220 commented on [Tue Feb 21 2017](https://github.com/broadinstitute/gatk-protected/issues/915). This will be obviated by #884 anyway, but in case that takes a while to deliver. The testing was slowed by https://github.com/broadinstitute/cromwell/issues/2011",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2948
https://github.com/broadinstitute/gatk/issues/2949:2337,Deployability,pipeline,pipeline,2337,"4). If you organize the inputs into blocks and keep all such knobs together at the end it's not too bad. A lot of our users will need to be able to tweak those settings -- and the others can ignore them. . See here for an example of how we do it: https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.inputs.json. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607287). So we are a slimmer version of what @vdauwera has. @takutosato I agree with your frustrations, but then we have to hardcode to the worst case, which will be quite expensive (in the cloud), underutilized (in all backends), and have trouble dispatching (in SGE). . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607541). @vdauwera I will happily accept comments on our json templates. . https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607739). @davidbenjamin @takutosato The more I think about it, the more important I think this issue is. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286613100). Yeah we need to parameterize the heck out of all our WDLs. If anything, the example I linked to is not parameterized nearly as much as I'd like (it's derived from the prod pipeline so we're a bit constrained). . It's not that much clutter if you make those parameters task-level and organize the JSONs clearly. And it makes it waaaay easier for people to adjust what they need without touching the WDL itself. This becomes even more important once you move the WDL into a platform like FireCloud, where changing the WDL is a huge pain, whereas tweaking parameters (via a method config) is trivial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2949
https://github.com/broadinstitute/gatk/issues/2949:2181,Modifiability,parameteriz,parameterize,2181,"4). If you organize the inputs into blocks and keep all such knobs together at the end it's not too bad. A lot of our users will need to be able to tweak those settings -- and the others can ignore them. . See here for an example of how we do it: https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.inputs.json. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607287). So we are a slimmer version of what @vdauwera has. @takutosato I agree with your frustrations, but then we have to hardcode to the worst case, which will be quite expensive (in the cloud), underutilized (in all backends), and have trouble dispatching (in SGE). . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607541). @vdauwera I will happily accept comments on our json templates. . https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607739). @davidbenjamin @takutosato The more I think about it, the more important I think this issue is. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286613100). Yeah we need to parameterize the heck out of all our WDLs. If anything, the example I linked to is not parameterized nearly as much as I'd like (it's derived from the prod pipeline so we're a bit constrained). . It's not that much clutter if you make those parameters task-level and organize the JSONs clearly. And it makes it waaaay easier for people to adjust what they need without touching the WDL itself. This becomes even more important once you move the WDL into a platform like FireCloud, where changing the WDL is a huge pain, whereas tweaking parameters (via a method config) is trivial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2949
https://github.com/broadinstitute/gatk/issues/2949:2268,Modifiability,parameteriz,parameterized,2268,"4). If you organize the inputs into blocks and keep all such knobs together at the end it's not too bad. A lot of our users will need to be able to tweak those settings -- and the others can ignore them. . See here for an example of how we do it: https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.inputs.json. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607287). So we are a slimmer version of what @vdauwera has. @takutosato I agree with your frustrations, but then we have to hardcode to the worst case, which will be quite expensive (in the cloud), underutilized (in all backends), and have trouble dispatching (in SGE). . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607541). @vdauwera I will happily accept comments on our json templates. . https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607739). @davidbenjamin @takutosato The more I think about it, the more important I think this issue is. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286613100). Yeah we need to parameterize the heck out of all our WDLs. If anything, the example I linked to is not parameterized nearly as much as I'd like (it's derived from the prod pipeline so we're a bit constrained). . It's not that much clutter if you make those parameters task-level and organize the JSONs clearly. And it makes it waaaay easier for people to adjust what they need without touching the WDL itself. This becomes even more important once you move the WDL into a platform like FireCloud, where changing the WDL is a huge pain, whereas tweaking parameters (via a method config) is trivial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2949
https://github.com/broadinstitute/gatk/issues/2949:2743,Modifiability,config,config,2743,"4). If you organize the inputs into blocks and keep all such knobs together at the end it's not too bad. A lot of our users will need to be able to tweak those settings -- and the others can ignore them. . See here for an example of how we do it: https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.inputs.json. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607287). So we are a slimmer version of what @vdauwera has. @takutosato I agree with your frustrations, but then we have to hardcode to the worst case, which will be quite expensive (in the cloud), underutilized (in all backends), and have trouble dispatching (in SGE). . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607541). @vdauwera I will happily accept comments on our json templates. . https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607739). @davidbenjamin @takutosato The more I think about it, the more important I think this issue is. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286613100). Yeah we need to parameterize the heck out of all our WDLs. If anything, the example I linked to is not parameterized nearly as much as I'd like (it's derived from the prod pipeline so we're a bit constrained). . It's not that much clutter if you make those parameters task-level and organize the JSONs clearly. And it makes it waaaay easier for people to adjust what they need without touching the WDL itself. This becomes even more important once you move the WDL into a platform like FireCloud, where changing the WDL is a huge pain, whereas tweaking parameters (via a method config) is trivial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2949
https://github.com/broadinstitute/gatk/issues/2949:2467,Usability,clear,clearly,2467,"4). If you organize the inputs into blocks and keep all such knobs together at the end it's not too bad. A lot of our users will need to be able to tweak those settings -- and the others can ignore them. . See here for an example of how we do it: https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.inputs.json. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607287). So we are a slimmer version of what @vdauwera has. @takutosato I agree with your frustrations, but then we have to hardcode to the worst case, which will be quite expensive (in the cloud), underutilized (in all backends), and have trouble dispatching (in SGE). . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607541). @vdauwera I will happily accept comments on our json templates. . https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607739). @davidbenjamin @takutosato The more I think about it, the more important I think this issue is. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286613100). Yeah we need to parameterize the heck out of all our WDLs. If anything, the example I linked to is not parameterized nearly as much as I'd like (it's derived from the prod pipeline so we're a bit constrained). . It's not that much clutter if you make those parameters task-level and organize the JSONs clearly. And it makes it waaaay easier for people to adjust what they need without touching the WDL itself. This becomes even more important once you move the WDL into a platform like FireCloud, where changing the WDL is a huge pain, whereas tweaking parameters (via a method config) is trivial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2949
https://github.com/broadinstitute/gatk/issues/2952:213,Availability,down,download,213,"@LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/937). By grabbing the gatk-protected docker image (or whichever is being used for M2), this task commits to a ~2GB download. However, the task does basic bash commands, which could easily be performed using one of the ``ubuntu:14.04`` images or maybe even one smaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2952
https://github.com/broadinstitute/gatk/issues/2952:289,Performance,perform,performed,289,"@LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/937). By grabbing the gatk-protected docker image (or whichever is being used for M2), this task commits to a ~2GB download. However, the task does basic bash commands, which could easily be performed using one of the ``ubuntu:14.04`` images or maybe even one smaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2952
https://github.com/broadinstitute/gatk/issues/2954:4673,Deployability,configurat,configurations,4673,d on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535122). Also increase the kmer size to 35 does the job (-kmerSize 35) I guess that that prevents non-ref paths merge back into the reference between events thus resulting in less complex graphs. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535498). The user can be inform of using these work arounds (change the max number of haplotypes or kmersize) but those are not good solutions in general as he would pay a CPU and sensitivity penalty in other places. . ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265551340). I can see how the current best-path selection algorithm may fail to produce a good coverage of events across the active region depending on the weights on edges .... for some configurations the algorithm may dedicate too much time in exploring alternatives in one section of the graph because these are nearly equaly likely disregarding other possibilities other section just because they can only result in a relatively larger drop in the likelihood of the path. . I quick but elegant solution would be to simulate passes across the graph... first iterations would produce a quickly growing set of haplotypes but eventually repeated sampling would not produce new haplotypes. if after 100 subsequent simulations there is no new discovery or we have reached a limit (128?) we would stop there. This simulation approach could be implemented only in situations the graphs are too complex for an analytical solution. We can determine the maximum number of paths in a graph with a quick deep first traversal to decide whether to use the analytical-exact or the simulation-proximate approach. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-287813366). To be done in GATK4.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954
https://github.com/broadinstitute/gatk/issues/2954:3123,Energy Efficiency,reduce,reduce,3123,"jects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/hg19/ucsc.hg19.fasta -I /humgen/gsa-scr1/schandra/trevorconley_HaplotypeCallerNotCallingVariant/PersImmune_GATK/case/01-MDS-03A_CD14_interval.bam -o Sheila.HaplotypeCaller.g.vcf -bamout SheilaHaplotypeCaller.bam -L chr6:57397901-57398501 -ERC GVCF`. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265534394). It seems that this missed variant is a assembly problem. The region has quite a few events resulting in very complex graphs with may possible haplotypes. . If one adds the -debug -l DEBUG options you can see in the logs that the number of haplotypes for kmer size 10 hits the default maximum of 128. If one increases the limit to 512 ```(-maxNumHaplotypesInPopulation 512)``` then the variant comes up in the output. . When such a limit is hit, we reduce the number of haplotypes for further analysis using an algorithm that calculates a pseudo likelihood based on the reads that support different paths in the graph. . One would hope that this algorithm would pick up at least one of the haplotypes that contain that variant amongst the top 128 haplotypes for any kmer size, however in this instance this is not the case. So we need to verify that that algorithm is doing what is supposed to do and if so, whether we can change the way such a pseudo likelihood is calculated in order to make a better selection for this case and in general. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535122). Also increase the kmer size to 35 does the job (-kmerSize 35) I guess that that prevents non-ref paths merge back into the reference between events thus resulting in less complex graphs. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535498). The user can be inform of using t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954
https://github.com/broadinstitute/gatk/issues/2954:4625,Integrability,depend,depending,4625,"so, whether we can change the way such a pseudo likelihood is calculated in order to make a better selection for this case and in general. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535122). Also increase the kmer size to 35 does the job (-kmerSize 35) I guess that that prevents non-ref paths merge back into the reference between events thus resulting in less complex graphs. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535498). The user can be inform of using these work arounds (change the max number of haplotypes or kmersize) but those are not good solutions in general as he would pay a CPU and sensitivity penalty in other places. . ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265551340). I can see how the current best-path selection algorithm may fail to produce a good coverage of events across the active region depending on the weights on edges .... for some configurations the algorithm may dedicate too much time in exploring alternatives in one section of the graph because these are nearly equaly likely disregarding other possibilities other section just because they can only result in a relatively larger drop in the likelihood of the path. . I quick but elegant solution would be to simulate passes across the graph... first iterations would produce a quickly growing set of haplotypes but eventually repeated sampling would not produce new haplotypes. if after 100 subsequent simulations there is no new discovery or we have reached a limit (128?) we would stop there. This simulation approach could be implemented only in situations the graphs are too complex for an analytical solution. We can determine the maximum number of paths in a graph with a quick deep first traversal to decide whether to use the analytical-exact or the simulation-proximate app",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954
https://github.com/broadinstitute/gatk/issues/2954:4673,Modifiability,config,configurations,4673,d on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535122). Also increase the kmer size to 35 does the job (-kmerSize 35) I guess that that prevents non-ref paths merge back into the reference between events thus resulting in less complex graphs. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535498). The user can be inform of using these work arounds (change the max number of haplotypes or kmersize) but those are not good solutions in general as he would pay a CPU and sensitivity penalty in other places. . ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265551340). I can see how the current best-path selection algorithm may fail to produce a good coverage of events across the active region depending on the weights on edges .... for some configurations the algorithm may dedicate too much time in exploring alternatives in one section of the graph because these are nearly equaly likely disregarding other possibilities other section just because they can only result in a relatively larger drop in the likelihood of the path. . I quick but elegant solution would be to simulate passes across the graph... first iterations would produce a quickly growing set of haplotypes but eventually repeated sampling would not produce new haplotypes. if after 100 subsequent simulations there is no new discovery or we have reached a limit (128?) we would stop there. This simulation approach could be implemented only in situations the graphs are too complex for an analytical solution. We can determine the maximum number of paths in a graph with a quick deep first traversal to decide whether to use the analytical-exact or the simulation-proximate approach. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-287813366). To be done in GATK4.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954
https://github.com/broadinstitute/gatk/issues/2954:1596,Testability,Test,Test,1596,"adPosRankSum=-0.547	GT:AD:DP:GQ:PL:SB	0/1:87,46,0:133:99:1568,0,3286,1837,3433,5269:49,38,27,19`; but, the control has this:; `chr6	57398201	.	T	<NON_REF>	.	.	END=57398201	GT:DP:GQ:MIN_DP:PL	0/0:313:0:313:0,0,0`; Notice the PLs for the control are all 0. However, the bamout file has many reads that have high quality alternate alleles. I don't know why the tool is not making the call. . Bamout files (top is control and bottom is case):; <img width=""1440"" alt=""screen shot 2016-12-05 at 3 16 18 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/20900849/d8bdf974-bafd-11e6-967f-e589373c3f97.png"">. #### Steps to reproduce; Files and commands in thread below. ----. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/34280#Comment_34280. ---. @chandrans commented on [Mon Dec 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-264964927). Test files here:; `/humgen/gsa-scr1/schandra/trevorconley_HaplotypeCallerNotCallingVariant/PersImmune_GATK/`. Command for control (GVCF):; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/hg19/ucsc.hg19.fasta -I /humgen/gsa-scr1/schandra/trevorconley_HaplotypeCallerNotCallingVariant/PersImmune_GATK/control/01-MDS-03A_aCD3_interval.bam -L chr6:57397901-57398501 -o Sheila.HaplotypeCaller.control.g.vcf -ERC GVCF -bamout Sheila.HaplotypeCaller.control.bam`. Command for case (GVCF):; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/hg19/ucsc.hg19.fasta -I /humgen/gsa-scr1/schandra/trevorconley_HaplotypeCallerNotCallingVariant/PersImmune_GATK/case/01-MDS-03A_CD14_interval.bam -o Sheila.HaplotypeCaller.g.vcf -bamout SheilaHaplotypeCaller.bam -L chr6:57397901-57398501 -ERC GVCF`. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954
https://github.com/broadinstitute/gatk/issues/2954:2890,Testability,log,logs,2890,"a-scr1/schandra/trevorconley_HaplotypeCallerNotCallingVariant/PersImmune_GATK/control/01-MDS-03A_aCD3_interval.bam -L chr6:57397901-57398501 -o Sheila.HaplotypeCaller.control.g.vcf -ERC GVCF -bamout Sheila.HaplotypeCaller.control.bam`. Command for case (GVCF):; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/hg19/ucsc.hg19.fasta -I /humgen/gsa-scr1/schandra/trevorconley_HaplotypeCallerNotCallingVariant/PersImmune_GATK/case/01-MDS-03A_CD14_interval.bam -o Sheila.HaplotypeCaller.g.vcf -bamout SheilaHaplotypeCaller.bam -L chr6:57397901-57398501 -ERC GVCF`. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265534394). It seems that this missed variant is a assembly problem. The region has quite a few events resulting in very complex graphs with may possible haplotypes. . If one adds the -debug -l DEBUG options you can see in the logs that the number of haplotypes for kmer size 10 hits the default maximum of 128. If one increases the limit to 512 ```(-maxNumHaplotypesInPopulation 512)``` then the variant comes up in the output. . When such a limit is hit, we reduce the number of haplotypes for further analysis using an algorithm that calculates a pseudo likelihood based on the reads that support different paths in the graph. . One would hope that this algorithm would pick up at least one of the haplotypes that contain that variant amongst the top 128 haplotypes for any kmer size, however in this instance this is not the case. So we need to verify that that algorithm is doing what is supposed to do and if so, whether we can change the way such a pseudo likelihood is calculated in order to make a better selection for this case and in general. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535122). Also increase the kmer size to 35 does the job (-kmerSize",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954
https://github.com/broadinstitute/gatk/issues/2955:1074,Availability,error,error-when-running-genotypegvcfs,1074,"broadinstitute/gatk-protected/issues/942). @vdauwera commented on [Sat Mar 07 2015](https://github.com/broadinstitute/gsa-unstable/issues/855). #### Issue description / case details. Users who are dealing with large ploidies (typically for pooled experiments) run into limitations where the combination of the ploidy and numbers of alleles in the high teens and above produces too many possible genotypes for GenotypeGVCFs to handle under its current architecture. . For example, in the case reported here, the ploidy is 19 and the number of alternate alleles is 21, so GenotypeGVCFs cannot handle the large number of possible genotypes that result from all the possible combinations. A reasonable way to deal with this would be to cull the possible combinations dynamically at runtime to eliminate the most unlikely combinations up front. ; #### Test data. Has been provided by the user ; #### [Original forum post](http://gatkforums.broadinstitute.org/discussion/4954/combination-of-ploidy-and-number-of-alleles-error-when-running-genotypegvcfs/p1). ---. @vruano commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-77993425). The error message explain the reason well ... a possibility to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:1242,Availability,error,error,1242,"aling with large ploidies (typically for pooled experiments) run into limitations where the combination of the ploidy and numbers of alleles in the high teens and above produces too many possible genotypes for GenotypeGVCFs to handle under its current architecture. . For example, in the case reported here, the ploidy is 19 and the number of alternate alleles is 21, so GenotypeGVCFs cannot handle the large number of possible genotypes that result from all the possible combinations. A reasonable way to deal with this would be to cull the possible combinations dynamically at runtime to eliminate the most unlikely combinations up front. ; #### Test data. Has been provided by the user ; #### [Original forum post](http://gatkforums.broadinstitute.org/discussion/4954/combination-of-ploidy-and-number-of-alleles-error-when-running-genotypegvcfs/p1). ---. @vruano commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-77993425). The error message explain the reason well ... a possibility to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:3940,Availability,ERROR,ERROR,3940,"ld happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 14 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-102235192). Recording test case while sanitizing: . The files are located here: . ```; gsa1:/humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles; ```. The command I ran:. ```; java -jar /humgen/gsa-hpprojects/GATK/private_unstable_builds/GenomeAnalysisTK_latest_unstable.jar \; -T GenotypeGVCFs \; -R /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/45S_Jacobsen_rearranged.fa \; -V /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/Input_ploidy.list \; -o Sheila.GenotypeGVCFs.vcf; ```. Which produces:. ```; ##### ERROR MESSAGE: the combination of ploidy (19) and number of alleles (21) results in a very large number of genotypes (> 2147483647). You need to limit ploidy or the number of alternative alleles to analyze this locus; ```. ---. @chandrans commented on [Wed Jan 20 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-173357206). @davidbenjamin Hi David. Have you had a chance to look at this?. ---. @davidbenjamin commented on [Sat Jan 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-174245406). @chandrans My other bug turned into a very long undertaking, but seems to be nearing completion. It might even be done already, pending Laura's confirmation that the output vcfs are what we want. Then I will move on to this one. ---. @chandrans commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-175192516). Ah wonderful. Thanks David. ---. @davidbenjamin commented on [Tue Apr 12 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:6990,Availability,mask,mask,6990,"e lists one by one until enough allele's are emptied, so that the number of genotypes does not surpasses a maximum based on the largest ploidy amongst the input samples. Of course, one would need to create some temporary data-structure to make the operation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267297). Those haplotype scores have not been throughly analyzed but we are already using them to discard haplotypes beyond the maximum allowed per graph kmer size so I don't see the harm in using the for further reduction. . Certainly is a step forward from just throwing an exception back to the user. However, we should output a Warning every time we need to do such a reduction just to keep track. ---. @sooheelee commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217443170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of further interest to their aims. For example, either for more exact mapping then variant calling or separate variant calling. I say this because a quick glance at the literature suggests this is potentially a highly variable region that may be present in multiple copies depending on species. It's a ribosomal DNA locus, that is, a site from which rRNA is transcribed. In mammals (humans & mice) it looks like it is a tandemly repeated locus residing on several chromosomes:. <img width=""424"" alt=""screenshot 2016-05-06 09 37 12"" src=""https://cloud.githubusercontent.com/assets/11543866/15074654/264e199a-136e-11e6-852e-431d8690f2aa.png"">. Some random references:; - [Concerted copy number variation balances ribosomal; DNA dosage in human and mouse genomes](http://www.pnas.org/content/112/8/2485.full.pdf); - [Haplotype Detection from Next-Generation Sequencing in High-Ploidy-Level Species: 45S rDNA Gene Copies in the Hexaploid Spartina maritima.](http:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:8999,Availability,avail,available,8999,"he Hexaploid Spartina maritima.](http://www.ncbi.nlm.nih.gov/pubmed/26530424); - [Non-Random Distribution of 5S rDNA Sites and Its Association with 45S rDNA in Plant Chromosomes.](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0035139). ---. @vruano commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217501618). I think you can use -XL to exclude intervals. . ---. @SHuang-Broad commented on [Thu Oct 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-256727048). Should the same fix for HC be ported to GenotypeGVCFs? @vdauwera @vruano ?. ---. @vdauwera commented on [Fri Oct 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-257037491). That sounds like a good idea, @SHuang-Broad . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260456840). @SHuang-Broad @vruano Is this (making the HC allele culling available to GenotypeGVCFs too) still on your radar(s)?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260687763). @vdauwera yes it is on mine. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:9595,Availability,ping,ping,9595,"orted to GenotypeGVCFs? @vdauwera @vruano ?. ---. @vdauwera commented on [Fri Oct 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-257037491). That sounds like a good idea, @SHuang-Broad . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260456840). @SHuang-Broad @vruano Is this (making the HC allele culling available to GenotypeGVCFs too) still on your radar(s)?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260687763). @vdauwera yes it is on mine. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:10001,Availability,avail,available,10001,"ssuecomment-260687763). @vdauwera yes it is on mine. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolve all these, and that will definitely happen in GATK 4. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-281073466). Ah, interesting, thanks Steve. Do you have any sense of when you might be able to look further into this? This is not to pressure you, just to estimate the roadmap/timeline. An order of magnitude (weeks, months, more) would be fine. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-287838044). I'm going to move this issu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:10544,Availability,down,down,10544,"ithub.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolve all these, and that will definitely happen in GATK 4. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-281073466). Ah, interesting, thanks Steve. Do you have any sense of when you might be able to look further into this? This is not to pressure you, just to estimate the roadmap/timeline. An order of magnitude (weeks, months, more) would be fine. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-287838044). I'm going to move this issue to GATK; feel free to close it there if it is redundant with existing issues describing the same problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:11214,Availability,redundant,redundant,11214,"ithub.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolve all these, and that will definitely happen in GATK 4. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-281073466). Ah, interesting, thanks Steve. Do you have any sense of when you might be able to look further into this? This is not to pressure you, just to estimate the roadmap/timeline. An order of magnitude (weeks, months, more) would be fine. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-287838044). I'm going to move this issue to GATK; feel free to close it there if it is redundant with existing issues describing the same problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:2405,Deployability,update,updated,2405,"ty to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column and MLEAC/F format field) cannot be updated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:1347,Energy Efficiency,reduce,reduce,1347," ploidy and numbers of alleles in the high teens and above produces too many possible genotypes for GenotypeGVCFs to handle under its current architecture. . For example, in the case reported here, the ploidy is 19 and the number of alternate alleles is 21, so GenotypeGVCFs cannot handle the large number of possible genotypes that result from all the possible combinations. A reasonable way to deal with this would be to cull the possible combinations dynamically at runtime to eliminate the most unlikely combinations up front. ; #### Test data. Has been provided by the user ; #### [Original forum post](http://gatkforums.broadinstitute.org/discussion/4954/combination-of-ploidy-and-number-of-alleles-error-when-running-genotypegvcfs/p1). ---. @vruano commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-77993425). The error message explain the reason well ... a possibility to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:5416,Energy Efficiency,reduce,reduce,5416,"roadinstitute/gsa-unstable/issues/855#issuecomment-174245406). @chandrans My other bug turned into a very long undertaking, but seems to be nearing completion. It might even be done already, pending Laura's confirmation that the output vcfs are what we want. Then I will move on to this one. ---. @chandrans commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-175192516). Ah wonderful. Thanks David. ---. @davidbenjamin commented on [Tue Apr 12 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-209080316). As discussed in support meeting today, this issue is high-effort, low-reward for now but will be much easier to revisit (if it's even necessary at all) once the exact AF calculation is replaced with something less baroque. ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217265362). It seems to me that a quick and sound solution is to reduce the number of alternative alleles at this point in HCGenotypeEngine code:. ```; HaplotypeCallerGenotypingEngine.java line 260:; final Map<Allele, List<Haplotype>> alleleMapper = createAlleleMapper(mergeMap, eventMapper);; ```. Haplotypes have associated score, as a pseudo-likelihood,that can be use to sort then and selectively remove those alternative alleles that are only supported by the less likely haplotypes. ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217266027). So the idea is simple, keep removing haplotypes from that map value lists one by one until enough allele's are emptied, so that the number of genotypes does not surpasses a maximum based on the largest ploidy amongst the input samples. Of course, one would need to create some temporary data-structure to make the operation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:6296,Energy Efficiency,efficient,efficient,6296,"nted on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217265362). It seems to me that a quick and sound solution is to reduce the number of alternative alleles at this point in HCGenotypeEngine code:. ```; HaplotypeCallerGenotypingEngine.java line 260:; final Map<Allele, List<Haplotype>> alleleMapper = createAlleleMapper(mergeMap, eventMapper);; ```. Haplotypes have associated score, as a pseudo-likelihood,that can be use to sort then and selectively remove those alternative alleles that are only supported by the less likely haplotypes. ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217266027). So the idea is simple, keep removing haplotypes from that map value lists one by one until enough allele's are emptied, so that the number of genotypes does not surpasses a maximum based on the largest ploidy amongst the input samples. Of course, one would need to create some temporary data-structure to make the operation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267297). Those haplotype scores have not been throughly analyzed but we are already using them to discard haplotypes beyond the maximum allowed per graph kmer size so I don't see the harm in using the for further reduction. . Certainly is a step forward from just throwing an exception back to the user. However, we should output a Warning every time we need to do such a reduction just to keep track. ---. @sooheelee commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217443170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of further interest to their aims. For example, either for more exact mapping then variant calling or separate variant calling. I say this because a quick glance at the literature",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:1248,Integrability,message,message,1248,"aling with large ploidies (typically for pooled experiments) run into limitations where the combination of the ploidy and numbers of alleles in the high teens and above produces too many possible genotypes for GenotypeGVCFs to handle under its current architecture. . For example, in the case reported here, the ploidy is 19 and the number of alternate alleles is 21, so GenotypeGVCFs cannot handle the large number of possible genotypes that result from all the possible combinations. A reasonable way to deal with this would be to cull the possible combinations dynamically at runtime to eliminate the most unlikely combinations up front. ; #### Test data. Has been provided by the user ; #### [Original forum post](http://gatkforums.broadinstitute.org/discussion/4954/combination-of-ploidy-and-number-of-alleles-error-when-running-genotypegvcfs/p1). ---. @vruano commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-77993425). The error message explain the reason well ... a possibility to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:2334,Integrability,depend,depending,2334,"ty to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column and MLEAC/F format field) cannot be updated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:3946,Integrability,MESSAGE,MESSAGE,3946,"ld happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 14 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-102235192). Recording test case while sanitizing: . The files are located here: . ```; gsa1:/humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles; ```. The command I ran:. ```; java -jar /humgen/gsa-hpprojects/GATK/private_unstable_builds/GenomeAnalysisTK_latest_unstable.jar \; -T GenotypeGVCFs \; -R /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/45S_Jacobsen_rearranged.fa \; -V /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/Input_ploidy.list \; -o Sheila.GenotypeGVCFs.vcf; ```. Which produces:. ```; ##### ERROR MESSAGE: the combination of ploidy (19) and number of alleles (21) results in a very large number of genotypes (> 2147483647). You need to limit ploidy or the number of alternative alleles to analyze this locus; ```. ---. @chandrans commented on [Wed Jan 20 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-173357206). @davidbenjamin Hi David. Have you had a chance to look at this?. ---. @davidbenjamin commented on [Sat Jan 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-174245406). @chandrans My other bug turned into a very long undertaking, but seems to be nearing completion. It might even be done already, pending Laura's confirmation that the output vcfs are what we want. Then I will move on to this one. ---. @chandrans commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-175192516). Ah wonderful. Thanks David. ---. @davidbenjamin commented on [Tue Apr 12 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:7348,Integrability,depend,depending,7348,"ation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267297). Those haplotype scores have not been throughly analyzed but we are already using them to discard haplotypes beyond the maximum allowed per graph kmer size so I don't see the harm in using the for further reduction. . Certainly is a step forward from just throwing an exception back to the user. However, we should output a Warning every time we need to do such a reduction just to keep track. ---. @sooheelee commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217443170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of further interest to their aims. For example, either for more exact mapping then variant calling or separate variant calling. I say this because a quick glance at the literature suggests this is potentially a highly variable region that may be present in multiple copies depending on species. It's a ribosomal DNA locus, that is, a site from which rRNA is transcribed. In mammals (humans & mice) it looks like it is a tandemly repeated locus residing on several chromosomes:. <img width=""424"" alt=""screenshot 2016-05-06 09 37 12"" src=""https://cloud.githubusercontent.com/assets/11543866/15074654/264e199a-136e-11e6-852e-431d8690f2aa.png"">. Some random references:; - [Concerted copy number variation balances ribosomal; DNA dosage in human and mouse genomes](http://www.pnas.org/content/112/8/2485.full.pdf); - [Haplotype Detection from Next-Generation Sequencing in High-Ploidy-Level Species: 45S rDNA Gene Copies in the Hexaploid Spartina maritima.](http://www.ncbi.nlm.nih.gov/pubmed/26530424); - [Non-Random Distribution of 5S rDNA Sites and Its Association with 45S rDNA in Plant Chromosomes.](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0035139). ---. @vruano commented on [Fri May ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:2187,Modifiability,polymorphi,polymorphic,2187,"ty to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column and MLEAC/F format field) cannot be updated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:7293,Modifiability,variab,variable,7293,"ation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267297). Those haplotype scores have not been throughly analyzed but we are already using them to discard haplotypes beyond the maximum allowed per graph kmer size so I don't see the harm in using the for further reduction. . Certainly is a step forward from just throwing an exception back to the user. However, we should output a Warning every time we need to do such a reduction just to keep track. ---. @sooheelee commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217443170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of further interest to their aims. For example, either for more exact mapping then variant calling or separate variant calling. I say this because a quick glance at the literature suggests this is potentially a highly variable region that may be present in multiple copies depending on species. It's a ribosomal DNA locus, that is, a site from which rRNA is transcribed. In mammals (humans & mice) it looks like it is a tandemly repeated locus residing on several chromosomes:. <img width=""424"" alt=""screenshot 2016-05-06 09 37 12"" src=""https://cloud.githubusercontent.com/assets/11543866/15074654/264e199a-136e-11e6-852e-431d8690f2aa.png"">. Some random references:; - [Concerted copy number variation balances ribosomal; DNA dosage in human and mouse genomes](http://www.pnas.org/content/112/8/2485.full.pdf); - [Haplotype Detection from Next-Generation Sequencing in High-Ploidy-Level Species: 45S rDNA Gene Copies in the Hexaploid Spartina maritima.](http://www.ncbi.nlm.nih.gov/pubmed/26530424); - [Non-Random Distribution of 5S rDNA Sites and Its Association with 45S rDNA in Plant Chromosomes.](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0035139). ---. @vruano commented on [Fri May ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:7899,Safety,Detect,Detection,7899,"170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of further interest to their aims. For example, either for more exact mapping then variant calling or separate variant calling. I say this because a quick glance at the literature suggests this is potentially a highly variable region that may be present in multiple copies depending on species. It's a ribosomal DNA locus, that is, a site from which rRNA is transcribed. In mammals (humans & mice) it looks like it is a tandemly repeated locus residing on several chromosomes:. <img width=""424"" alt=""screenshot 2016-05-06 09 37 12"" src=""https://cloud.githubusercontent.com/assets/11543866/15074654/264e199a-136e-11e6-852e-431d8690f2aa.png"">. Some random references:; - [Concerted copy number variation balances ribosomal; DNA dosage in human and mouse genomes](http://www.pnas.org/content/112/8/2485.full.pdf); - [Haplotype Detection from Next-Generation Sequencing in High-Ploidy-Level Species: 45S rDNA Gene Copies in the Hexaploid Spartina maritima.](http://www.ncbi.nlm.nih.gov/pubmed/26530424); - [Non-Random Distribution of 5S rDNA Sites and Its Association with 45S rDNA in Plant Chromosomes.](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0035139). ---. @vruano commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217501618). I think you can use -XL to exclude intervals. . ---. @SHuang-Broad commented on [Thu Oct 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-256727048). Should the same fix for HC be ported to GenotypeGVCFs? @vdauwera @vruano ?. ---. @vdauwera commented on [Fri Oct 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-257037491). That sounds like a good idea, @SHuang-Broad . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260456840). @SHuang-Broad @",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:11214,Safety,redund,redundant,11214,"ithub.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolve all these, and that will definitely happen in GATK 4. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-281073466). Ah, interesting, thanks Steve. Do you have any sense of when you might be able to look further into this? This is not to pressure you, just to estimate the roadmap/timeline. An order of magnitude (weeks, months, more) would be fine. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-287838044). I'm going to move this issue to GATK; feel free to close it there if it is redundant with existing issues describing the same problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:3413,Security,sanitiz,sanitizing,3413,"pdated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 14 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-102235192). Recording test case while sanitizing: . The files are located here: . ```; gsa1:/humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles; ```. The command I ran:. ```; java -jar /humgen/gsa-hpprojects/GATK/private_unstable_builds/GenomeAnalysisTK_latest_unstable.jar \; -T GenotypeGVCFs \; -R /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/45S_Jacobsen_rearranged.fa \; -V /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/Input_ploidy.list \; -o Sheila.GenotypeGVCFs.vcf; ```. Which produces:. ```; ##### ERROR MESSAGE: the combination of ploidy (19) and number of alleles (21) results in a very large number of genotypes (> 2147483647). You need to limit ploidy or the number of alternative alleles to analyze this locus; ```. ---. @chandrans commented on [Wed Jan 20 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-173357206). @davidbenjamin Hi David. Have you had a chance to look at this?. ---. @davidbenjamin commented on [Sat Jan 23 201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:907,Testability,Test,Test,907,"@vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gatk-protected/issues/942). @vdauwera commented on [Sat Mar 07 2015](https://github.com/broadinstitute/gsa-unstable/issues/855). #### Issue description / case details. Users who are dealing with large ploidies (typically for pooled experiments) run into limitations where the combination of the ploidy and numbers of alleles in the high teens and above produces too many possible genotypes for GenotypeGVCFs to handle under its current architecture. . For example, in the case reported here, the ploidy is 19 and the number of alternate alleles is 21, so GenotypeGVCFs cannot handle the large number of possible genotypes that result from all the possible combinations. A reasonable way to deal with this would be to cull the possible combinations dynamically at runtime to eliminate the most unlikely combinations up front. ; #### Test data. Has been provided by the user ; #### [Original forum post](http://gatkforums.broadinstitute.org/discussion/4954/combination-of-ploidy-and-number-of-alleles-error-when-running-genotypegvcfs/p1). ---. @vruano commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-77993425). The error message explain the reason well ... a possibility to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:3397,Testability,test,test,3397,"pdated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 14 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-102235192). Recording test case while sanitizing: . The files are located here: . ```; gsa1:/humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles; ```. The command I ran:. ```; java -jar /humgen/gsa-hpprojects/GATK/private_unstable_builds/GenomeAnalysisTK_latest_unstable.jar \; -T GenotypeGVCFs \; -R /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/45S_Jacobsen_rearranged.fa \; -V /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/Input_ploidy.list \; -o Sheila.GenotypeGVCFs.vcf; ```. Which produces:. ```; ##### ERROR MESSAGE: the combination of ploidy (19) and number of alleles (21) results in a very large number of genotypes (> 2147483647). You need to limit ploidy or the number of alternative alleles to analyze this locus; ```. ---. @chandrans commented on [Wed Jan 20 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-173357206). @davidbenjamin Hi David. Have you had a chance to look at this?. ---. @davidbenjamin commented on [Sat Jan 23 201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:3168,Usability,simpl,simple,3168,"em to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column and MLEAC/F format field) cannot be updated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 14 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-102235192). Recording test case while sanitizing: . The files are located here: . ```; gsa1:/humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles; ```. The command I ran:. ```; java -jar /humgen/gsa-hpprojects/GATK/private_unstable_builds/GenomeAnalysisTK_latest_unstable.jar \; -T GenotypeGVCFs \; -R /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/45S_Jacobsen_rearranged.fa \; -V /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/Input_ploidy.list \; -o Sheila.GenotypeGVCFs.vcf; ```. Which produces:. ```; ##### ERROR MESSAGE: the combination of ploidy (19) and number of alleles (21) results in a very large number of genotypes (> 2147483647). You need to limit ploidy or the number of alter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:5982,Usability,simpl,simple,5982,"eward for now but will be much easier to revisit (if it's even necessary at all) once the exact AF calculation is replaced with something less baroque. ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217265362). It seems to me that a quick and sound solution is to reduce the number of alternative alleles at this point in HCGenotypeEngine code:. ```; HaplotypeCallerGenotypingEngine.java line 260:; final Map<Allele, List<Haplotype>> alleleMapper = createAlleleMapper(mergeMap, eventMapper);; ```. Haplotypes have associated score, as a pseudo-likelihood,that can be use to sort then and selectively remove those alternative alleles that are only supported by the less likely haplotypes. ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217266027). So the idea is simple, keep removing haplotypes from that map value lists one by one until enough allele's are emptied, so that the number of genotypes does not surpasses a maximum based on the largest ploidy amongst the input samples. Of course, one would need to create some temporary data-structure to make the operation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267297). Those haplotype scores have not been throughly analyzed but we are already using them to discard haplotypes beyond the maximum allowed per graph kmer size so I don't see the harm in using the for further reduction. . Certainly is a step forward from just throwing an exception back to the user. However, we should output a Warning every time we need to do such a reduction just to keep track. ---. @sooheelee commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217443170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of furthe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2955:9879,Usability,simpl,simple,9879,"tute/gsa-unstable/issues/855#issuecomment-260456840). @SHuang-Broad @vruano Is this (making the HC allele culling available to GenotypeGVCFs too) still on your radar(s)?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260687763). @vdauwera yes it is on mine. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolve all these, and that will definitely happen in GATK 4. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-281073466). Ah, interesting, thanks Steve. Do you have any sense of when you might be able to look further into this? This ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955
https://github.com/broadinstitute/gatk/issues/2956:972,Energy Efficiency,reduce,reduce,972,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/946). @vruano commented on [Sat Dec 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1537). ### Tool(s) involved; GenotypeGVCFs CombineGVCFs. ### Description. With large ploidy an exception may be thrown in GenotypeGVCFs/CombineGVCFs when the number of alleles is rather large (after combining several VCFs). . We already have a safe-guard mechanism that works well with diploids. When there is 50+ alt. alleles we stop emitting PLs. . There is already a branch that contains a solution for high ploidy. . https://github.com/broadinstitute/gsa-unstable/tree/vrr_max_alt_alleles_generate_pls. This consist in exposing this maximum constant as a user argument so that users can lower this maximum threshold as they need to avoid an exception. This task is about make this standard in master. Notice however that this is still just a hack; a better solution would reduce the list of alt. alleles as needed to handle it as needed. . ---. @vdauwera commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1537#issuecomment-288229143). @vruano Do you plan to pursue this in gsa-unstable or can this be migrated to GATK4?. ---. @vruano commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1537#issuecomment-288281209). No plans, just move it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2956
https://github.com/broadinstitute/gatk/issues/2956:440,Safety,safe,safe-guard,440,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/946). @vruano commented on [Sat Dec 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1537). ### Tool(s) involved; GenotypeGVCFs CombineGVCFs. ### Description. With large ploidy an exception may be thrown in GenotypeGVCFs/CombineGVCFs when the number of alleles is rather large (after combining several VCFs). . We already have a safe-guard mechanism that works well with diploids. When there is 50+ alt. alleles we stop emitting PLs. . There is already a branch that contains a solution for high ploidy. . https://github.com/broadinstitute/gsa-unstable/tree/vrr_max_alt_alleles_generate_pls. This consist in exposing this maximum constant as a user argument so that users can lower this maximum threshold as they need to avoid an exception. This task is about make this standard in master. Notice however that this is still just a hack; a better solution would reduce the list of alt. alleles as needed to handle it as needed. . ---. @vdauwera commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1537#issuecomment-288229143). @vruano Do you plan to pursue this in gsa-unstable or can this be migrated to GATK4?. ---. @vruano commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1537#issuecomment-288281209). No plans, just move it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2956
https://github.com/broadinstitute/gatk/issues/2956:832,Safety,avoid,avoid,832,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/946). @vruano commented on [Sat Dec 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1537). ### Tool(s) involved; GenotypeGVCFs CombineGVCFs. ### Description. With large ploidy an exception may be thrown in GenotypeGVCFs/CombineGVCFs when the number of alleles is rather large (after combining several VCFs). . We already have a safe-guard mechanism that works well with diploids. When there is 50+ alt. alleles we stop emitting PLs. . There is already a branch that contains a solution for high ploidy. . https://github.com/broadinstitute/gsa-unstable/tree/vrr_max_alt_alleles_generate_pls. This consist in exposing this maximum constant as a user argument so that users can lower this maximum threshold as they need to avoid an exception. This task is about make this standard in master. Notice however that this is still just a hack; a better solution would reduce the list of alt. alleles as needed to handle it as needed. . ---. @vdauwera commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1537#issuecomment-288229143). @vruano Do you plan to pursue this in gsa-unstable or can this be migrated to GATK4?. ---. @vruano commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1537#issuecomment-288281209). No plans, just move it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2956
https://github.com/broadinstitute/gatk/issues/2958:2541,Availability,down,downstream,2541,"es may indicate which ones are a better choice amongst the ""loosers"" we throw that info away.; ### Proposed solution. Simply do a quick and dirty AF estimation and choose the alleles with the larger frequencies. This estimate should use all the genotype likelihoods rather than just the top genotype giving a nominal score for all the alleles that would allow us to sort them all and make a better and less arbitrary selection. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260486156). I believe this was done by @vruano and @SHuang-Broad already -- right guys? Can we close this? . ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260684553). My understanding of the current state is that there are several possible places with alt allele reduction in HC, in order:; 1. The fix I put in, to prevent the calculator from becoming too slow or blow up, so downstream steps won't even include these alleles in their likelihood calculations;; 2. The fix Valentine put in, which happens after the read likelihoods are calculated (and optionally down-sampled). The relevant code is in `HaplotypCallerGenotypingEngine.java` around lines 267-279. This is the state in GATK3, in GATK4 the second possibility is not ported yet. Regarding alt allele reduction in AF calculator, has [this](https://github.com/broadinstitute/gatk/pull/1918) been ported back to GATK3?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260688221). By ""possible place"" I mean they don't always remove alt alleles, just when certain conditions are met, and are independent. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260781482). I don't think https://github.com/broadinstitute/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad com",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:2727,Availability,down,down-sampled,2727,"irty AF estimation and choose the alleles with the larger frequencies. This estimate should use all the genotype likelihoods rather than just the top genotype giving a nominal score for all the alleles that would allow us to sort them all and make a better and less arbitrary selection. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260486156). I believe this was done by @vruano and @SHuang-Broad already -- right guys? Can we close this? . ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260684553). My understanding of the current state is that there are several possible places with alt allele reduction in HC, in order:; 1. The fix I put in, to prevent the calculator from becoming too slow or blow up, so downstream steps won't even include these alleles in their likelihood calculations;; 2. The fix Valentine put in, which happens after the read likelihoods are calculated (and optionally down-sampled). The relevant code is in `HaplotypCallerGenotypingEngine.java` around lines 267-279. This is the state in GATK3, in GATK4 the second possibility is not ported yet. Regarding alt allele reduction in AF calculator, has [this](https://github.com/broadinstitute/gatk/pull/1918) been ported back to GATK3?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260688221). By ""possible place"" I mean they don't always remove alt alleles, just when certain conditions are met, and are independent. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260781482). I don't think https://github.com/broadinstitute/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad commented on [Mon Dec 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-268016064). Looking more closely, isn't th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:4510,Availability,fault,faulty,4510,"e/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad commented on [Mon Dec 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-268016064). Looking more closely, isn't this done already in #1377 ? @vruano ?. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287822737). @SHuang-Broad @vruano Status update on this?. ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287916452). @SHuang-Broad this is not fixed by #1377 as this makes reference to the selection executed by the AFCalculator... it might be that @davidbenjamin now AF calculator addressed the issue, but is also possible that he avoid it it entirely and just focused in the new QUAL calculation. . ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287921195). Looking at the code I made reference in GATK3, it seem that it is still faulty... I guess we need to take a look on whether in GATK4 has been fixed and then back-ported if people are interested. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287952396). Alright, thanks for the update. At this point we don't care too much about fixing it in GATK3; we're all about moving forward with GATK4. Do you want me to move this issue to GATK or do you already have an issue for this there?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288598906). The new qual score doesn't subset alleles at all because it doesn't need to. `AlleleSubsettingUtils` handles this upstream of the new qual. We're waiting on the HaplotypeCaller tie-out to eliminate the old qual from GATK 4, however. ---. @vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomme",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:3882,Deployability,update,update,3882,"ossibility is not ported yet. Regarding alt allele reduction in AF calculator, has [this](https://github.com/broadinstitute/gatk/pull/1918) been ported back to GATK3?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260688221). By ""possible place"" I mean they don't always remove alt alleles, just when certain conditions are met, and are independent. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260781482). I don't think https://github.com/broadinstitute/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad commented on [Mon Dec 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-268016064). Looking more closely, isn't this done already in #1377 ? @vruano ?. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287822737). @SHuang-Broad @vruano Status update on this?. ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287916452). @SHuang-Broad this is not fixed by #1377 as this makes reference to the selection executed by the AFCalculator... it might be that @davidbenjamin now AF calculator addressed the issue, but is also possible that he avoid it it entirely and just focused in the new QUAL calculation. . ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287921195). Looking at the code I made reference in GATK3, it seem that it is still faulty... I guess we need to take a look on whether in GATK4 has been fixed and then back-ported if people are interested. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287952396). Alright, thanks for the update. At this point we don't care too much about fixing it in GATK3; we're all about m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:4787,Deployability,update,update,4787,"ithub.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287822737). @SHuang-Broad @vruano Status update on this?. ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287916452). @SHuang-Broad this is not fixed by #1377 as this makes reference to the selection executed by the AFCalculator... it might be that @davidbenjamin now AF calculator addressed the issue, but is also possible that he avoid it it entirely and just focused in the new QUAL calculation. . ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287921195). Looking at the code I made reference in GATK3, it seem that it is still faulty... I guess we need to take a look on whether in GATK4 has been fixed and then back-ported if people are interested. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287952396). Alright, thanks for the update. At this point we don't care too much about fixing it in GATK3; we're all about moving forward with GATK4. Do you want me to move this issue to GATK or do you already have an issue for this there?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288598906). The new qual score doesn't subset alleles at all because it doesn't need to. `AlleleSubsettingUtils` handles this upstream of the new qual. We're waiting on the HaplotypeCaller tie-out to eliminate the old qual from GATK 4, however. ---. @vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288599388). Ah, do I understand correctly that if the new qual checks out and the old one can be eliminated, this issue no longer applies?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288601472). Well, it's possible th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:505,Energy Efficiency,reduce,reduce,505,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950). @vruano commented on [Wed May 18 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376). ## Bug Report; ### Affected tool(s). All tools that use AFCalculators (HC, GenotypeGVCFs etc).; ### Affected version(s); - master.; ### Description. When a alternative allele number reduction is needed (e.g. when the number of alt.alleles is larger than --maxAltAlleles). ExactAFCalculator descendants reduce the number of alt alleles using the reduceScope method. Different implementation of the exact-af-calculator may differ slightly on this but for the most part all of them apply the following algorithm:. For each sample, get its PLs, get the best genotype based on those. Then for the alleles included in that genotype increase their ""best allele score"" by the GQ of the genotype. Then we chose those alleles that have the highest scores. I guess often this is ok when we are dealing with many samples and the ""good"" alleles are present in the top genotypes with high confidence in a few samples and the ""bad"" alleles are not. However one can see how this criterion fails when either we are working with just a few samples (e.g. 1 sample in HC GVCF mode) or with low coverage data. . For example with a single sample only the alleles in the best genotype may have a score different than 0. All the rest have the same probability of been picked up if maxAltAlleles gives us room for more. Despite that the likelihoods of other genotypes may indicate which ones are a better choice amongst the ""loosers"" we throw that info away.; ### Proposed solution. Simply do a quick and dirty AF estimation and choose the alleles with the larger frequencies. This estimate should use all the genotype likelihoods rather than just the top genotype giving a nominal score for all the alleles that would allow us to sort them all and make a better and less arbitrary selection. ---. @vdauwera commented on [M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:548,Energy Efficiency,reduce,reduceScope,548,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950). @vruano commented on [Wed May 18 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376). ## Bug Report; ### Affected tool(s). All tools that use AFCalculators (HC, GenotypeGVCFs etc).; ### Affected version(s); - master.; ### Description. When a alternative allele number reduction is needed (e.g. when the number of alt.alleles is larger than --maxAltAlleles). ExactAFCalculator descendants reduce the number of alt alleles using the reduceScope method. Different implementation of the exact-af-calculator may differ slightly on this but for the most part all of them apply the following algorithm:. For each sample, get its PLs, get the best genotype based on those. Then for the alleles included in that genotype increase their ""best allele score"" by the GQ of the genotype. Then we chose those alleles that have the highest scores. I guess often this is ok when we are dealing with many samples and the ""good"" alleles are present in the top genotypes with high confidence in a few samples and the ""bad"" alleles are not. However one can see how this criterion fails when either we are working with just a few samples (e.g. 1 sample in HC GVCF mode) or with low coverage data. . For example with a single sample only the alleles in the best genotype may have a score different than 0. All the rest have the same probability of been picked up if maxAltAlleles gives us room for more. Despite that the likelihoods of other genotypes may indicate which ones are a better choice amongst the ""loosers"" we throw that info away.; ### Proposed solution. Simply do a quick and dirty AF estimation and choose the alleles with the larger frequencies. This estimate should use all the genotype likelihoods rather than just the top genotype giving a nominal score for all the alleles that would allow us to sort them all and make a better and less arbitrary selection. ---. @vdauwera commented on [M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:4241,Safety,avoid,avoid,4241,"emove alt alleles, just when certain conditions are met, and are independent. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260781482). I don't think https://github.com/broadinstitute/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad commented on [Mon Dec 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-268016064). Looking more closely, isn't this done already in #1377 ? @vruano ?. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287822737). @SHuang-Broad @vruano Status update on this?. ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287916452). @SHuang-Broad this is not fixed by #1377 as this makes reference to the selection executed by the AFCalculator... it might be that @davidbenjamin now AF calculator addressed the issue, but is also possible that he avoid it it entirely and just focused in the new QUAL calculation. . ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287921195). Looking at the code I made reference in GATK3, it seem that it is still faulty... I guess we need to take a look on whether in GATK4 has been fixed and then back-ported if people are interested. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287952396). Alright, thanks for the update. At this point we don't care too much about fixing it in GATK3; we're all about moving forward with GATK4. Do you want me to move this issue to GATK or do you already have an issue for this there?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288598906). The new qual score doesn't subset alleles at all because it doesn't need to. `AlleleSubsettingUt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:1661,Usability,Simpl,Simply,1661,"ithm:. For each sample, get its PLs, get the best genotype based on those. Then for the alleles included in that genotype increase their ""best allele score"" by the GQ of the genotype. Then we chose those alleles that have the highest scores. I guess often this is ok when we are dealing with many samples and the ""good"" alleles are present in the top genotypes with high confidence in a few samples and the ""bad"" alleles are not. However one can see how this criterion fails when either we are working with just a few samples (e.g. 1 sample in HC GVCF mode) or with low coverage data. . For example with a single sample only the alleles in the best genotype may have a score different than 0. All the rest have the same probability of been picked up if maxAltAlleles gives us room for more. Despite that the likelihoods of other genotypes may indicate which ones are a better choice amongst the ""loosers"" we throw that info away.; ### Proposed solution. Simply do a quick and dirty AF estimation and choose the alleles with the larger frequencies. This estimate should use all the genotype likelihoods rather than just the top genotype giving a nominal score for all the alleles that would allow us to sort them all and make a better and less arbitrary selection. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260486156). I believe this was done by @vruano and @SHuang-Broad already -- right guys? Can we close this? . ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260684553). My understanding of the current state is that there are several possible places with alt allele reduction in HC, in order:; 1. The fix I put in, to prevent the calculator from becoming too slow or blow up, so downstream steps won't even include these alleles in their likelihood calculations;; 2. The fix Valentine put in, which happens after the read likelihoods are calcula",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2958:6119,Usability,clear,clear,6119,"nstitute/gatk-protected/issues/950#issuecomment-288598906). The new qual score doesn't subset alleles at all because it doesn't need to. `AlleleSubsettingUtils` handles this upstream of the new qual. We're waiting on the HaplotypeCaller tie-out to eliminate the old qual from GATK 4, however. ---. @vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288599388). Ah, do I understand correctly that if the new qual checks out and the old one can be eliminated, this issue no longer applies?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288601472). Well, it's possible that people aren't satisfied with allele reduction in general (@vruano and @SHuang-Broad have thoughts on this, I believe), but it won't have anything to do with the `AFCalculator`. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288601998). @vdauwera And just to be clear, if I understood correctly everyone was satisfied in November with the new qual and we are just waiting for *HaplotypeCaller* to check out in GATK 4 before making any changes like pulling the plug on the old qual. The reason is that we're looking for GATK 4 results to match as exactly as possible and subbing in the new qual would complicate that. ---. @vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288604546). Ah yes my bad -- we are indeed just waiting on the tie outs to get rid of the old qual and make the new one default. I commented before the full context had resurfaced in my memory... Which means it's bedtime for my brain cells. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288606101). Seeing as I just pressed the ""close"" button by mistake my brain cells could benefit from the same.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958
https://github.com/broadinstitute/gatk/issues/2959:2959,Availability,ping,ping,2959,"joun is willing to help.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally painful to get; that dropped allele. I'd suggest making a unit test with some fake data.; You'll need two positions: one upstream with a deletion to generate the *; and one for the SNP. I think the dropped allele was a 1bp deletion at the; same position t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:303,Deployability,pipeline,pipeline,303,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/951). @lfrancioli commented on [Thu Oct 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489). ## Bug Report; ### Affected tool(s). HaplotypeCaller; ### Affected version(s). Cloud production pipeline; ### Description. This apparently happen when a longer allele was at the position but is no emitted. This issue can be e.g. found in the first VCF of gnomAD (gnomAD_20k.filtered.0.vcf.gz): . ```; 1 10153 . AC CC,*,GC 42563.06 PASS ; ```. As can be seen the last 'C' is superfluous.; #### Steps to reproduce. This was in a production VCF in the cloud ; #### Expected behavior. Minimal representation, e.g. ```; 1 10153 . A C,*,G 42563.06 PASS ; ```; #### Actual behavior. Extra 'C'. ---. @ldgauthier commented on [Thu Oct 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252063664). I think this happened when there was a AC ->C deletion originally called that got subset out because it didn't meet the QUAL threshold, but the trimming that was done on the final allele set didn't work right because of the *. ---. @vdauwera commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252204232). Do we have GVCF snippets to reproduce this? . ---. @ldgauthier commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496). I don't know what intermediates we save on the cloud but maybe @yfarjoun is willing to help. ---. @yfarjoun commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:1772,Deployability,pipeline,pipeline-help,1772,"``; #### Actual behavior. Extra 'C'. ---. @ldgauthier commented on [Thu Oct 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252063664). I think this happened when there was a AC ->C deletion originally called that got subset out because it didn't meet the QUAL threshold, but the trimming that was done on the final allele set didn't work right because of the *. ---. @vdauwera commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252204232). Do we have GVCF snippets to reproduce this? . ---. @ldgauthier commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496). I don't know what intermediates we save on the cloud but maybe @yfarjoun is willing to help. ---. @yfarjoun commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:2471,Deployability,pipeline,pipeline-help,2471," save on the cloud but maybe @yfarjoun is willing to help. ---. @yfarjoun commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:2625,Deployability,pipeline,pipeline,2625,"tps://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:2563,Integrability,protocol,protocol,2563,"tps://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:2982,Security,access,access,2982,"joun is willing to help.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally painful to get; that dropped allele. I'd suggest making a unit test with some fake data.; You'll need two positions: one upstream with a deletion to generate the *; and one for the SNP. I think the dropped allele was a 1bp deletion at the; same position t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:3345,Security,access,access,3345,"ps://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally painful to get; that dropped allele. I'd suggest making a unit test with some fake data.; You'll need two positions: one upstream with a deletion to generate the *; and one for the SNP. I think the dropped allele was a 1bp deletion at the; same position that generated the representation with the extra base at the; end. Give that one a really low quality in its gvcf so it gets dropped.; PLs don't really matter as long as they jive with the quals and aren't hom; ref. You can just grab numbers from any other valid vcf. I think you can do; it with three samples: one with the upstream deletion and *, one with the; AC SNP and one with the low quality deletion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:4669,Security,access,access,4669,"? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally painful to get; that dropped allele. I'd suggest making a unit test with some fake data.; You'll need two positions: one upstream with a deletion to generate the *; and one for the SNP. I think the dropped allele was a 1bp deletion at the; same position that generated the representation with the extra base at the; end. Give that one a really low quality in its gvcf so it gets dropped.; PLs don't really matter as long as they jive with the quals and aren't hom; ref. You can just grab numbers from any other valid vcf. I think you can do; it with three samples: one with the upstream deletion and *, one with the; AC SNP and one with the low quality deletion. Other combinations will; probably also produce the same bug. There may be an even simpler way to reproduce the bug without the low; quality deletion but I suspect this will work. On Jan 26, 2017 10:02 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. Oh, they gave me access to the files but I never took the next step of; figuring out which files are relevant. There are twenty thousand samples...; I'm not sure what is the best way to approach this. ; You are receiving this because you commented. Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/AGRhdKIgGAjH5_n3wlZ0E2A5xw1TeFg1ks5rWV5DgaJpZM4KQT_3>; .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:2721,Testability,mock,mock,2721," cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally pain",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:3783,Testability,test,test,3783," to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally painful to get; that dropped allele. I'd suggest making a unit test with some fake data.; You'll need two positions: one upstream with a deletion to generate the *; and one for the SNP. I think the dropped allele was a 1bp deletion at the; same position that generated the representation with the extra base at the; end. Give that one a really low quality in its gvcf so it gets dropped.; PLs don't really matter as long as they jive with the quals and aren't hom; ref. You can just grab numbers from any other valid vcf. I think you can do; it with three samples: one with the upstream deletion and *, one with the; AC SNP and one with the low quality deletion. Other combinations will; probably also produce the same bug. There may be an even simpler way to reproduce the bug without the low; quality deletion but I suspect this will work. On Jan 26, 2017 10:02 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. Oh, they gave me access to the files but I never took the next step of; figuring out which files are relevant. There are twenty th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2959:4465,Usability,simpl,simpler,4465,"? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally painful to get; that dropped allele. I'd suggest making a unit test with some fake data.; You'll need two positions: one upstream with a deletion to generate the *; and one for the SNP. I think the dropped allele was a 1bp deletion at the; same position that generated the representation with the extra base at the; end. Give that one a really low quality in its gvcf so it gets dropped.; PLs don't really matter as long as they jive with the quals and aren't hom; ref. You can just grab numbers from any other valid vcf. I think you can do; it with three samples: one with the upstream deletion and *, one with the; AC SNP and one with the low quality deletion. Other combinations will; probably also produce the same bug. There may be an even simpler way to reproduce the bug without the low; quality deletion but I suspect this will work. On Jan 26, 2017 10:02 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. Oh, they gave me access to the files but I never took the next step of; figuring out which files are relevant. There are twenty thousand samples...; I'm not sure what is the best way to approach this. ; You are receiving this because you commented. Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/AGRhdKIgGAjH5_n3wlZ0E2A5xw1TeFg1ks5rWV5DgaJpZM4KQT_3>; .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959
https://github.com/broadinstitute/gatk/issues/2960:3144,Deployability,release,release,3144,"/GATK/bundle/2.8/b37/human_g1k_v37.fasta -I /humgen/gsa-scr1/schandra/bgrenier_MixingAndMatchingGVCFAndBPRES/ind2.bam -L 9 -o Sheila.HaplotypeCallerGVCF.g.vcf -ERC GVCF`. GenotypeGVCFs:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /humgen/gsa-hpprojects/GATK/bundle/2.8/b37/human_g1k_v37.fasta -V Sheila.HaplotypeCallerGVCF.g.vcf -o Sheila.GenotypeGVCFsGVCF.vcf`. ---. @chandrans commented on [Sat Dec 03 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-264640941). User asked about this. Probably won't get to it anytime soon, but I told him/her I would check in. ---. @vdauwera commented on [Mon Dec 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-264964313). I responded that we're waiting on the tie-outs. . ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-265162286). Tie-outs?. I started on this, but I definitely won't finish it before the release. ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-265166347). Functional equivalence of GATK4 ports. That's my party line for now. Didn't realize you had actually started on this... but yeah, 3.7 is going out today. ---. @ronlevine commented on [Mon Jan 23 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-274513719). @davidbenjamin Any thoughts on this? Laura thought she implemented a fix but it have any effect. I am becoming familiar with the HC code but it's slow going. While stepping though the code, I noticed it correctly identifies the events for the deletion and SNP. ---. @davidbenjamin commented on [Mon Jan 23 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-274536654). @ronlevine I would put in a few breakpoints to see where the spanning deletion allele gets lost from the SNP site `VariantContext`. I just ran things wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2960
https://github.com/broadinstitute/gatk/issues/2960:1547,Testability,Test,Test,1547,"eQRankSum=0.359;ClippingRankSum=0.000;DP=36;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.90;MQRankSum=2.404;QD=11.11;ReadPosRankSum=0.041;SOR=0.914 GT:AD:DP:GQ:PL 0/1:21,12:33:99:404,0,760`. `9 418272 . T C 509.77 . AC=1;AF=0.500;AN=2;BaseQRankSum=-3.075;ClippingRankSum=0.000;DP=36;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.90;MQRankSum=-2.152;QD=14.56;ReadPosRankSum=-0.236;SOR=0.784 GT:AD:DP:GQ:PL 0/1:14,21:35:99:538,0,505`. The SNP/spanning deletion site is at position 418272. Notice HaplotypeCaller assigns a genotype of 0/1 (T/C). The genotype should really be 1/2 (C/*) with the \* allele as the other alternate allele. ; #### Steps to reproduce. Commands and files below. This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/gatk/discussion/comment/33849#Comment_33849) . ---. @chandrans commented on [Mon Oct 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-255851363). Test Files here:; `/humgen/gsa-scr1/schandra/bgrenier_MixingAndMatchingGVCFAndBPRES`. Command for HaplotypeCaller:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/b37/human_g1k_v37.fasta -I /humgen/gsa-scr1/schandra/bgrenier_MixingAndMatchingGVCFAndBPRES/ind2.bam -o Sheila.HaplotypeCaller.vcf -L 9 -bamout Sheila.HaplotypeCaller.bam`. HaplotypeCaller in GVCF mode:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/GATK/bundle/2.8/b37/human_g1k_v37.fasta -I /humgen/gsa-scr1/schandra/bgrenier_MixingAndMatchingGVCFAndBPRES/ind2.bam -L 9 -o Sheila.HaplotypeCallerGVCF.g.vcf -ERC GVCF`. GenotypeGVCFs:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /humgen/gsa-hpprojects/GATK/bundle/2.8/b37/human_g1k_v37.fasta -V Sheila.HaplotypeCallerGVCF.g.vcf -o Sheila.GenotypeGVCFsGVCF.vcf`. ---. @chandrans commented on [Sat Dec 03 2016](https://github",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2960
https://github.com/broadinstitute/gatk/issues/2960:5336,Testability,log,logic,5336,"idbenjamin That's what I have been doing. I could not find where the spanning deletion alleles was ever added. I agree with Laura's code that the spanning deletion alleles should be added to the events in `HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods`. ---. @ronlevine commented on [Tue Jan 24 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-274868889). @davidbenjamin It looks like the issue is with `ReferenceConfidenceModel.getOverlappingVariantContext(final GenomeLoc curPos, final Collection<VariantContext> maybeOverlapping)`, with the stack trace:; ```; ReferenceConfidenceModel.calculateRefConfidence; ReferenceConfidenceModel.getOverlappingVariantContext; HaplotypeCaller.map; ``` ; For `curPos=9:418272`, there are 2 variants, `9:418269-418273 TTTTG*,<NON_REF>,T` and `9:418272 T*,<NON_REF>,C`. This method returns the variant with the right-most start , so the variant with the deletion is ignored. This logic should be changed so that the variants that overlap `curPos` are merged and returned. A utility similar to `GATKVariantContextUtils.simpleMerge` might work. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-283367380). I thought the state I left that branch was that it would output a merged; representation of the two events starting at the deletion position- so the; reference is some string of bases, allele 1 is the deletion, and allele 2; matches the reference for the length of the deletion with the exception of; the SNP. (Allele 2 is not the minimal representation yet.) That's the first; step to get the genotype right. After that we need to break up the events,; clean up the representation, and assign the genotype from the combined; event to both of them. Hopefully that helps. (And hopefully I actually committed the version of; the branch that does what I said.). On Jan 24, 2017 12:06 PM, ""Ron Levine"" <notifications@github.com> wrote:. > @davidbenjami",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2960
https://github.com/broadinstitute/gatk/issues/2960:6975,Testability,log,logic,6975,"impleMerge` might work. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-283367380). I thought the state I left that branch was that it would output a merged; representation of the two events starting at the deletion position- so the; reference is some string of bases, allele 1 is the deletion, and allele 2; matches the reference for the length of the deletion with the exception of; the SNP. (Allele 2 is not the minimal representation yet.) That's the first; step to get the genotype right. After that we need to break up the events,; clean up the representation, and assign the genotype from the combined; event to both of them. Hopefully that helps. (And hopefully I actually committed the version of; the branch that does what I said.). On Jan 24, 2017 12:06 PM, ""Ron Levine"" <notifications@github.com> wrote:. > @davidbenjamin <https://github.com/davidbenjamin> It looks like the issue; > is with ReferenceConfidenceModel.getOverlappingVariantContext(final; > GenomeLoc curPos, final Collection<VariantContext> maybeOverlapping),; > with the stack trace:; >; > ReferenceConfidenceModel.calculateRefConfidence; > ReferenceConfidenceModel.getOverlappingVariantContext; > HaplotypeCaller.map; >; > For curPos=9:418272, there are 2 variants, 9:418269-418273; > TTTTG*,<NON_REF>,T and 9:418272 T*,<NON_REF>,C. This method returns the; > variant with the right-most start , so the variant with the deletion is; > ignored. This logic should be changed so that the variants that overlap; > curPos are merged and returned. A utility such as GATKVariantContextUtils.; > simpleMerge might work.; >; > ; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-274868889>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCAl7601U3RRbph6rOi0E7dqNawRks5rVi-fgaJpZM4KfOWm>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2960
https://github.com/broadinstitute/gatk/issues/2960:5474,Usability,simpl,simpleMerge,5474,"d. I agree with Laura's code that the spanning deletion alleles should be added to the events in `HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods`. ---. @ronlevine commented on [Tue Jan 24 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-274868889). @davidbenjamin It looks like the issue is with `ReferenceConfidenceModel.getOverlappingVariantContext(final GenomeLoc curPos, final Collection<VariantContext> maybeOverlapping)`, with the stack trace:; ```; ReferenceConfidenceModel.calculateRefConfidence; ReferenceConfidenceModel.getOverlappingVariantContext; HaplotypeCaller.map; ``` ; For `curPos=9:418272`, there are 2 variants, `9:418269-418273 TTTTG*,<NON_REF>,T` and `9:418272 T*,<NON_REF>,C`. This method returns the variant with the right-most start , so the variant with the deletion is ignored. This logic should be changed so that the variants that overlap `curPos` are merged and returned. A utility similar to `GATKVariantContextUtils.simpleMerge` might work. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-283367380). I thought the state I left that branch was that it would output a merged; representation of the two events starting at the deletion position- so the; reference is some string of bases, allele 1 is the deletion, and allele 2; matches the reference for the length of the deletion with the exception of; the SNP. (Allele 2 is not the minimal representation yet.) That's the first; step to get the genotype right. After that we need to break up the events,; clean up the representation, and assign the genotype from the combined; event to both of them. Hopefully that helps. (And hopefully I actually committed the version of; the branch that does what I said.). On Jan 24, 2017 12:06 PM, ""Ron Levine"" <notifications@github.com> wrote:. > @davidbenjamin <https://github.com/davidbenjamin> It looks like the issue; > is with ReferenceConfidenceModel.getOverlap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2960
https://github.com/broadinstitute/gatk/issues/2960:7114,Usability,simpl,simpleMerge,7114,"impleMerge` might work. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-283367380). I thought the state I left that branch was that it would output a merged; representation of the two events starting at the deletion position- so the; reference is some string of bases, allele 1 is the deletion, and allele 2; matches the reference for the length of the deletion with the exception of; the SNP. (Allele 2 is not the minimal representation yet.) That's the first; step to get the genotype right. After that we need to break up the events,; clean up the representation, and assign the genotype from the combined; event to both of them. Hopefully that helps. (And hopefully I actually committed the version of; the branch that does what I said.). On Jan 24, 2017 12:06 PM, ""Ron Levine"" <notifications@github.com> wrote:. > @davidbenjamin <https://github.com/davidbenjamin> It looks like the issue; > is with ReferenceConfidenceModel.getOverlappingVariantContext(final; > GenomeLoc curPos, final Collection<VariantContext> maybeOverlapping),; > with the stack trace:; >; > ReferenceConfidenceModel.calculateRefConfidence; > ReferenceConfidenceModel.getOverlappingVariantContext; > HaplotypeCaller.map; >; > For curPos=9:418272, there are 2 variants, 9:418269-418273; > TTTTG*,<NON_REF>,T and 9:418272 T*,<NON_REF>,C. This method returns the; > variant with the right-most start , so the variant with the deletion is; > ignored. This logic should be changed so that the variants that overlap; > curPos are merged and returned. A utility such as GATKVariantContextUtils.; > simpleMerge might work.; >; > ; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-274868889>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCAl7601U3RRbph6rOi0E7dqNawRks5rVi-fgaJpZM4KfOWm>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2960
https://github.com/broadinstitute/gatk/issues/2961:223,Security,validat,validations,223,"@davidbenjamin commented on [Mon Mar 27 2017](https://github.com/broadinstitute/gatk-protected/issues/958). We currently have an ICE exome normal-normal analysis set up i.e. you can `cd` into `/dsde/working/davidben/mutect/validations/normalNormal` and `/Users/home/davidben/cromwell/run_sge.sh normal_normal.wdl normal_normal.json` is all you need to get the analysis. Let's set this up for a few other replicate sets, which we can grab from the Palantir wiki.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2961
https://github.com/broadinstitute/gatk/issues/2962:718,Availability,fault,fault,718,"@takutosato commented on [Mon Mar 27 2017](https://github.com/broadinstitute/gatk-protected/issues/959). Mutect2.wdl is a monolithic beast that does way more than what it's designed to do i.e. run Mutect2. It's good style to split standalone tasks (Mutect2, Oncotator, OxoG filter) into modules. On the other hand, in the cloud it's cheaper to have all tasks in one wdl. We should find a good balance between these opposing forces. ---. @davidbenjamin commented on [Sun Apr 09 2017](https://github.com/broadinstitute/gatk-protected/issues/959#issuecomment-292795106). @takutosato I recently made a few modest (and orthogonal) steps to clean the wdl (PR #974). Let me know if the rebase gets messy, since it will be my fault for causing the mess.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2962
https://github.com/broadinstitute/gatk/issues/2963:251,Testability,test,tests,251,"@LeeTL1220 commented on [Wed Mar 29 2017](https://github.com/broadinstitute/gatk-protected/issues/961). ---. @LeeTL1220 commented on [Wed Mar 29 2017](https://github.com/broadinstitute/gatk-protected/issues/961#issuecomment-290092153). Currently, the tests do not use docker. The M2 WDL does use docker.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2963
https://github.com/broadinstitute/gatk/issues/2964:314,Energy Efficiency,reduce,reduce,314,"@samuelklee commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/975). Should be an equivalent of PadTargets for WGS that outputs a file specifying the bins. Alternatively, the WES coverage collection CLI should calculate padded targets on the fly. This will simplify the WDL and reduce the number of tasks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2964
https://github.com/broadinstitute/gatk/issues/2964:293,Usability,simpl,simplify,293,"@samuelklee commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/975). Should be an equivalent of PadTargets for WGS that outputs a file specifying the bins. Alternatively, the WES coverage collection CLI should calculate padded targets on the fly. This will simplify the WDL and reduce the number of tasks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2964
https://github.com/broadinstitute/gatk/issues/2965:228,Energy Efficiency,reduce,reduce,228,"@droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977). The docker tests take about 40 minutes in travis, while the next-slowest travis task takes about 30 minutes. We should try to reduce the runtime of the docker tests to <= 30 minutes. ---. @droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032382). For @LeeTL1220 . ---. @LeeTL1220 commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032644). I thought we were going to address this with a GATK base image... what is; target time? (Within reason). On Apr 5, 2017 20:08, ""droazen"" <notifications@github.com> wrote:. For @LeeTL1220 <https://github.com/LeeTL1220>. ; You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032382>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk_R_mkaEcEJlOt3lJntscqeum3-lks5rtC0SgaJpZM4M09tE>; . ---. @droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292033277). That is tracked at https://github.com/broadinstitute/gatk/issues/2457 and slated for beta (mid-May). This ticket in protected can be considered as blocked until https://github.com/broadinstitute/gatk/issues/2457 is done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2965
https://github.com/broadinstitute/gatk/issues/2965:113,Testability,test,tests,113,"@droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977). The docker tests take about 40 minutes in travis, while the next-slowest travis task takes about 30 minutes. We should try to reduce the runtime of the docker tests to <= 30 minutes. ---. @droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032382). For @LeeTL1220 . ---. @LeeTL1220 commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032644). I thought we were going to address this with a GATK base image... what is; target time? (Within reason). On Apr 5, 2017 20:08, ""droazen"" <notifications@github.com> wrote:. For @LeeTL1220 <https://github.com/LeeTL1220>. ; You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032382>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk_R_mkaEcEJlOt3lJntscqeum3-lks5rtC0SgaJpZM4M09tE>; . ---. @droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292033277). That is tracked at https://github.com/broadinstitute/gatk/issues/2457 and slated for beta (mid-May). This ticket in protected can be considered as blocked until https://github.com/broadinstitute/gatk/issues/2457 is done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2965
https://github.com/broadinstitute/gatk/issues/2965:261,Testability,test,tests,261,"@droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977). The docker tests take about 40 minutes in travis, while the next-slowest travis task takes about 30 minutes. We should try to reduce the runtime of the docker tests to <= 30 minutes. ---. @droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032382). For @LeeTL1220 . ---. @LeeTL1220 commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032644). I thought we were going to address this with a GATK base image... what is; target time? (Within reason). On Apr 5, 2017 20:08, ""droazen"" <notifications@github.com> wrote:. For @LeeTL1220 <https://github.com/LeeTL1220>. ; You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032382>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk_R_mkaEcEJlOt3lJntscqeum3-lks5rtC0SgaJpZM4M09tE>; . ---. @droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292033277). That is tracked at https://github.com/broadinstitute/gatk/issues/2457 and slated for beta (mid-May). This ticket in protected can be considered as blocked until https://github.com/broadinstitute/gatk/issues/2457 is done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2965
https://github.com/broadinstitute/gatk/issues/2967:139,Integrability,depend,dependency,139,"@cmnbroad commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982). By default, Gradle resolves version dependency conflicts amongst transitive dependencies by choosing the newest version of any particular module that has conflicts. GATK public overrides this and forces resolution to specific versions for some modules like htsjdk and protobuf, but gatk-protected doesn't. So gatk-protected can wind up using a different version of a dependent module than GATK uses. This is currently happening with protobuf, but it could happen with htsjdk or other modules. Short term, we probably want to use ""force"" resolution for htsjdk and protobuf in gatk-protected (ideally to force them to be resolved to the ones pulled in via the gatk dependency). Longer term, we should probably use a resolution policy of ""failOnVersionConflict"" in both gatk and protected, which will force us to manually resolve any conflicts as they arise, rather than depending on Gradle to do the resolution for us. ---. @droazen commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982#issuecomment-293032625). Let's treat this one as high priority -- we really need to fix this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2967
https://github.com/broadinstitute/gatk/issues/2967:179,Integrability,depend,dependencies,179,"@cmnbroad commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982). By default, Gradle resolves version dependency conflicts amongst transitive dependencies by choosing the newest version of any particular module that has conflicts. GATK public overrides this and forces resolution to specific versions for some modules like htsjdk and protobuf, but gatk-protected doesn't. So gatk-protected can wind up using a different version of a dependent module than GATK uses. This is currently happening with protobuf, but it could happen with htsjdk or other modules. Short term, we probably want to use ""force"" resolution for htsjdk and protobuf in gatk-protected (ideally to force them to be resolved to the ones pulled in via the gatk dependency). Longer term, we should probably use a resolution policy of ""failOnVersionConflict"" in both gatk and protected, which will force us to manually resolve any conflicts as they arise, rather than depending on Gradle to do the resolution for us. ---. @droazen commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982#issuecomment-293032625). Let's treat this one as high priority -- we really need to fix this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2967
https://github.com/broadinstitute/gatk/issues/2967:470,Integrability,depend,dependent,470,"@cmnbroad commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982). By default, Gradle resolves version dependency conflicts amongst transitive dependencies by choosing the newest version of any particular module that has conflicts. GATK public overrides this and forces resolution to specific versions for some modules like htsjdk and protobuf, but gatk-protected doesn't. So gatk-protected can wind up using a different version of a dependent module than GATK uses. This is currently happening with protobuf, but it could happen with htsjdk or other modules. Short term, we probably want to use ""force"" resolution for htsjdk and protobuf in gatk-protected (ideally to force them to be resolved to the ones pulled in via the gatk dependency). Longer term, we should probably use a resolution policy of ""failOnVersionConflict"" in both gatk and protected, which will force us to manually resolve any conflicts as they arise, rather than depending on Gradle to do the resolution for us. ---. @droazen commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982#issuecomment-293032625). Let's treat this one as high priority -- we really need to fix this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2967
https://github.com/broadinstitute/gatk/issues/2967:766,Integrability,depend,dependency,766,"@cmnbroad commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982). By default, Gradle resolves version dependency conflicts amongst transitive dependencies by choosing the newest version of any particular module that has conflicts. GATK public overrides this and forces resolution to specific versions for some modules like htsjdk and protobuf, but gatk-protected doesn't. So gatk-protected can wind up using a different version of a dependent module than GATK uses. This is currently happening with protobuf, but it could happen with htsjdk or other modules. Short term, we probably want to use ""force"" resolution for htsjdk and protobuf in gatk-protected (ideally to force them to be resolved to the ones pulled in via the gatk dependency). Longer term, we should probably use a resolution policy of ""failOnVersionConflict"" in both gatk and protected, which will force us to manually resolve any conflicts as they arise, rather than depending on Gradle to do the resolution for us. ---. @droazen commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982#issuecomment-293032625). Let's treat this one as high priority -- we really need to fix this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2967
https://github.com/broadinstitute/gatk/issues/2967:971,Integrability,depend,depending,971,"@cmnbroad commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982). By default, Gradle resolves version dependency conflicts amongst transitive dependencies by choosing the newest version of any particular module that has conflicts. GATK public overrides this and forces resolution to specific versions for some modules like htsjdk and protobuf, but gatk-protected doesn't. So gatk-protected can wind up using a different version of a dependent module than GATK uses. This is currently happening with protobuf, but it could happen with htsjdk or other modules. Short term, we probably want to use ""force"" resolution for htsjdk and protobuf in gatk-protected (ideally to force them to be resolved to the ones pulled in via the gatk dependency). Longer term, we should probably use a resolution policy of ""failOnVersionConflict"" in both gatk and protected, which will force us to manually resolve any conflicts as they arise, rather than depending on Gradle to do the resolution for us. ---. @droazen commented on [Mon Apr 10 2017](https://github.com/broadinstitute/gatk-protected/issues/982#issuecomment-293032625). Let's treat this one as high priority -- we really need to fix this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2967
https://github.com/broadinstitute/gatk/issues/2968:533,Integrability,Depend,Dependent,533,"@asmirnov239 commented on [Tue Apr 18 2017](https://github.com/broadinstitute/gatk-protected/issues/987). Replace the `AllelicCountCollector` with the implementation using LocusWalker. ---. @LeeTL1220 commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/987#issuecomment-295366237). Actually, I have to replace ``CollectAllelicCounts`` and ``AllelicCountCollector``. ---. @LeeTL1220 commented on [Fri May 12 2017](https://github.com/broadinstitute/gatk-protected/issues/987#issuecomment-301082304). Dependent on: . https://github.com/broadinstitute/gatk/pull/2661",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2968
https://github.com/broadinstitute/gatk/issues/2971:103,Availability,Robust,RobustBrentSolver,103,@mbabadi commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/990). `RobustBrentSolver` is a univariate solver developed as a part of GATK coverage model. It has a Brent solver at the core but tries to avoid spurious non-bracketing conditions by creating a collection of refined sub-brackets. The implementation needs to be made more flexible:; - Allow the user to specify how sub-brackets are generated. The default grid is a logarithmic grid concentrated about the leftmost endpoint followed by uniform refinement of each grid element.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2971
https://github.com/broadinstitute/gatk/issues/2971:368,Modifiability,flexible,flexible,368,@mbabadi commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/990). `RobustBrentSolver` is a univariate solver developed as a part of GATK coverage model. It has a Brent solver at the core but tries to avoid spurious non-bracketing conditions by creating a collection of refined sub-brackets. The implementation needs to be made more flexible:; - Allow the user to specify how sub-brackets are generated. The default grid is a logarithmic grid concentrated about the leftmost endpoint followed by uniform refinement of each grid element.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2971
https://github.com/broadinstitute/gatk/issues/2971:236,Safety,avoid,avoid,236,@mbabadi commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/990). `RobustBrentSolver` is a univariate solver developed as a part of GATK coverage model. It has a Brent solver at the core but tries to avoid spurious non-bracketing conditions by creating a collection of refined sub-brackets. The implementation needs to be made more flexible:; - Allow the user to specify how sub-brackets are generated. The default grid is a logarithmic grid concentrated about the leftmost endpoint followed by uniform refinement of each grid element.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2971
https://github.com/broadinstitute/gatk/issues/2971:461,Testability,log,logarithmic,461,@mbabadi commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/990). `RobustBrentSolver` is a univariate solver developed as a part of GATK coverage model. It has a Brent solver at the core but tries to avoid spurious non-bracketing conditions by creating a collection of refined sub-brackets. The implementation needs to be made more flexible:; - Allow the user to specify how sub-brackets are generated. The default grid is a logarithmic grid concentrated about the leftmost endpoint followed by uniform refinement of each grid element.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2971
https://github.com/broadinstitute/gatk/issues/2972:2021,Energy Efficiency,schedul,scheduler,2021,titute.hellbender.tools.exome.cnlohcaller.CNLOHCaller.calculateResponsibilities(CNLOHCaller.java:538); 	at org.broadinstitute.hellbender.tools.exome.cnlohcaller.CNLOHCaller.lambda$makeCalls$cbed43a$1(CNLOHCaller.java:307); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); 	at scala.collection.Iterator$class.foreach(Iterator.scala:727); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273); 	at scala.collection.AbstractIterator.to(Iterator.scala:1157); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2972
https://github.com/broadinstitute/gatk/issues/2972:2093,Energy Efficiency,schedul,scheduler,2093,titute.hellbender.tools.exome.cnlohcaller.CNLOHCaller.calculateResponsibilities(CNLOHCaller.java:538); 	at org.broadinstitute.hellbender.tools.exome.cnlohcaller.CNLOHCaller.lambda$makeCalls$cbed43a$1(CNLOHCaller.java:307); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); 	at scala.collection.Iterator$class.foreach(Iterator.scala:727); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273); 	at scala.collection.AbstractIterator.to(Iterator.scala:1157); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2972
https://github.com/broadinstitute/gatk/issues/2972:2217,Performance,concurren,concurrent,2217,titute.hellbender.tools.exome.cnlohcaller.CNLOHCaller.calculateResponsibilities(CNLOHCaller.java:538); 	at org.broadinstitute.hellbender.tools.exome.cnlohcaller.CNLOHCaller.lambda$makeCalls$cbed43a$1(CNLOHCaller.java:307); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); 	at scala.collection.Iterator$class.foreach(Iterator.scala:727); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273); 	at scala.collection.AbstractIterator.to(Iterator.scala:1157); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2972
https://github.com/broadinstitute/gatk/issues/2972:2302,Performance,concurren,concurrent,2302,titute.hellbender.tools.exome.cnlohcaller.CNLOHCaller.calculateResponsibilities(CNLOHCaller.java:538); 	at org.broadinstitute.hellbender.tools.exome.cnlohcaller.CNLOHCaller.lambda$makeCalls$cbed43a$1(CNLOHCaller.java:307); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); 	at scala.collection.Iterator$class.foreach(Iterator.scala:727); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273); 	at scala.collection.AbstractIterator.to(Iterator.scala:1157); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2972
https://github.com/broadinstitute/gatk/issues/2973:694,Availability,error,errors,694,"@davidbenjamin commented on [Thu Apr 20 2017](https://github.com/broadinstitute/gatk-protected/issues/994). The simplest idea is to take kmers (k = 5, 7, 10?) centered at variant positions and fit a distribution (beta distribution?) of artifact allele fractions for each kmer. . Back of the envelope: with k = 10 we have 4^10 ~ 1 million different kmers, so each kmer appears ~ 3000 times per genome or about 1 million times in our panel of normals. This is easily enough to fit the distribution of artifact fractions very precisely. In addition to beta distributions, we may wish to fit different distributions for artifact allele fractions, such as a mixture of no artifacts (other than base errors as expected from the base quals) and a beta.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973
https://github.com/broadinstitute/gatk/issues/2973:112,Usability,simpl,simplest,112,"@davidbenjamin commented on [Thu Apr 20 2017](https://github.com/broadinstitute/gatk-protected/issues/994). The simplest idea is to take kmers (k = 5, 7, 10?) centered at variant positions and fit a distribution (beta distribution?) of artifact allele fractions for each kmer. . Back of the envelope: with k = 10 we have 4^10 ~ 1 million different kmers, so each kmer appears ~ 3000 times per genome or about 1 million times in our panel of normals. This is easily enough to fit the distribution of artifact fractions very precisely. In addition to beta distributions, we may wish to fit different distributions for artifact allele fractions, such as a mixture of no artifacts (other than base errors as expected from the base quals) and a beta.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973
https://github.com/broadinstitute/gatk/issues/2974:266,Testability,test,testing,266,"@davidbenjamin commented on [Thu Apr 20 2017](https://github.com/broadinstitute/gatk-protected/issues/995). We have a Q script for in silico mixing, which we should port to wdl, and we have haploid exomes and genomes. We should use this to make great truth data for testing Mutect.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2974
https://github.com/broadinstitute/gatk/issues/2975:3156,Availability,down,down,3156,"y large numbers of false positives; > with bad mapping quality and very large normal artifact lods. The depth is; > often high due to mapping issues, which aggravates the problem. We should; > be able to modify our active region determination so that these bad sites; > don't trigger the assembly and likelihoods engines.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/997>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdKeWsrA1DojH_u7JMVCvec1o-zOtks5ryCXbgaJpZM4ND1FU>; > .; >. ---. @LeeTL1220 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296208775). We should leverage that list for CNV tools as well. We get a lot of false positive CNVs in centromeres (particularly chr9). ---. @davidbenjamin commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296262676). @ldgauthier Thank you! I have localized the regions, and that HaplotypeCaller interval list seems to exclude most or all of them (I can't say for sure because I have only localized down to about 100 kb). In our DREAM challenge wgs benchmarks we will lose about one in ten thousand true positives, which I can easily live with. This will save us a lot of time. ---. @davidbenjamin commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296267603). @samuelklee if you're not already aware of this wgs intervals whitelist. ---. @samuelklee commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296268137). Excellent, thanks. Looping in @asmirnov239, @mbabadi, and @achevali. ---. @samuelklee commented on [Wed May 03 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-299050342). Looping in @danielrosebrock and @dlivitz as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2975
https://github.com/broadinstitute/gatk/issues/2975:1443,Performance,optimiz,optimizing,1443,"f false positives with bad mapping quality and very large normal artifact lods. The depth is often high due to mapping issues, which aggravates the problem. We should be able to modify our active region determination so that these bad sites don't trigger the assembly and likelihoods engines. ---. @ldgauthier commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296196072). Have you localized the regions yet? Dave Shiga did some work for us on; slow regions that we causing problems for GenotypeGVCFs in production (not; quite apples to apples) and he found that the centromeres cause a lot of; problems and our MPG collaborators don't trust calls there anyway. In; production for HaplotypeCaller we use an interval list for genomes too; (/seq/references/Homo_sapiens_assembly19/v1/variant_calling/wgs_calling_regions.v1.interval_list); to avoid centromeres, telomeres, and gaps in the reference. That is to say, don't waste too much effort optimizing regions where we; won't trust any calls anyway. On Thu, Apr 20, 2017 at 11:43 PM, David Benjamin <notifications@github.com>; wrote:. > Mutect 2 spends a disproportionate amount of time in certain nasty; > regions. For example, on average a 500,000 bp chunk of DREAM challenge 3; > usually takes 30 seconds on a single core, but in some cases takes hours.; > This is very bad both for scattered jobs and total single-core run time.; >; > These bad regions are characterized by large numbers of false positives; > with bad mapping quality and very large normal artifact lods. The depth is; > often high due to mapping issues, which aggravates the problem. We should; > be able to modify our active region determination so that these bad sites; > don't trigger the assembly and likelihoods engines.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/997>, or mut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2975
https://github.com/broadinstitute/gatk/issues/2975:1342,Safety,avoid,avoid,1342,"bs and total single-core run time. These bad regions are characterized by large numbers of false positives with bad mapping quality and very large normal artifact lods. The depth is often high due to mapping issues, which aggravates the problem. We should be able to modify our active region determination so that these bad sites don't trigger the assembly and likelihoods engines. ---. @ldgauthier commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296196072). Have you localized the regions yet? Dave Shiga did some work for us on; slow regions that we causing problems for GenotypeGVCFs in production (not; quite apples to apples) and he found that the centromeres cause a lot of; problems and our MPG collaborators don't trust calls there anyway. In; production for HaplotypeCaller we use an interval list for genomes too; (/seq/references/Homo_sapiens_assembly19/v1/variant_calling/wgs_calling_regions.v1.interval_list); to avoid centromeres, telomeres, and gaps in the reference. That is to say, don't waste too much effort optimizing regions where we; won't trust any calls anyway. On Thu, Apr 20, 2017 at 11:43 PM, David Benjamin <notifications@github.com>; wrote:. > Mutect 2 spends a disproportionate amount of time in certain nasty; > regions. For example, on average a 500,000 bp chunk of DREAM challenge 3; > usually takes 30 seconds on a single core, but in some cases takes hours.; > This is very bad both for scattered jobs and total single-core run time.; >; > These bad regions are characterized by large numbers of false positives; > with bad mapping quality and very large normal artifact lods. The depth is; > often high due to mapping issues, which aggravates the problem. We should; > be able to modify our active region determination so that these bad sites; > don't trigger the assembly and likelihoods engines.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, v",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2975
https://github.com/broadinstitute/gatk/issues/2975:3206,Testability,benchmark,benchmarks,3206,"y large numbers of false positives; > with bad mapping quality and very large normal artifact lods. The depth is; > often high due to mapping issues, which aggravates the problem. We should; > be able to modify our active region determination so that these bad sites; > don't trigger the assembly and likelihoods engines.; >; > ; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/997>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdKeWsrA1DojH_u7JMVCvec1o-zOtks5ryCXbgaJpZM4ND1FU>; > .; >. ---. @LeeTL1220 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296208775). We should leverage that list for CNV tools as well. We get a lot of false positive CNVs in centromeres (particularly chr9). ---. @davidbenjamin commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296262676). @ldgauthier Thank you! I have localized the regions, and that HaplotypeCaller interval list seems to exclude most or all of them (I can't say for sure because I have only localized down to about 100 kb). In our DREAM challenge wgs benchmarks we will lose about one in ten thousand true positives, which I can easily live with. This will save us a lot of time. ---. @davidbenjamin commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296267603). @samuelklee if you're not already aware of this wgs intervals whitelist. ---. @samuelklee commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296268137). Excellent, thanks. Looping in @asmirnov239, @mbabadi, and @achevali. ---. @samuelklee commented on [Wed May 03 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-299050342). Looping in @danielrosebrock and @dlivitz as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2975
https://github.com/broadinstitute/gatk/issues/2976:500,Security,Hash,HashedListTargetCollection,500,"@asmirnov239 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/1000). Right now if a user disables MAPPED filter, which is a default filter for CalculateTargetCoverage tool, it will fail with the following uninformative exception (unless somehow all reads are mapped):; ```; java.lang.IllegalArgumentException: the input location cannot be null; 	at org.broadinstitute.hellbender.utils.Utils.nonNull(Utils.java:549); 	at org.broadinstitute.hellbender.tools.exome.HashedListTargetCollection.indexRange(HashedListTargetCollection.java:152); 	at org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage.apply(CalculateTargetCoverage.java:298); ```; We should guard against it and throw an exception before the traversal starts if MAPPED filter is disabled. ---. @asmirnov239 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/1000#issuecomment-296295294). Since there is no direct API call to access the list of resolved filters(after command line parsing) this bug fix will have to wait until [broadinstitute/barclay#38](https://github.com/broadinstitute/barclay/pull/38) is merged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2976
https://github.com/broadinstitute/gatk/issues/2976:538,Security,Hash,HashedListTargetCollection,538,"@asmirnov239 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/1000). Right now if a user disables MAPPED filter, which is a default filter for CalculateTargetCoverage tool, it will fail with the following uninformative exception (unless somehow all reads are mapped):; ```; java.lang.IllegalArgumentException: the input location cannot be null; 	at org.broadinstitute.hellbender.utils.Utils.nonNull(Utils.java:549); 	at org.broadinstitute.hellbender.tools.exome.HashedListTargetCollection.indexRange(HashedListTargetCollection.java:152); 	at org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage.apply(CalculateTargetCoverage.java:298); ```; We should guard against it and throw an exception before the traversal starts if MAPPED filter is disabled. ---. @asmirnov239 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/1000#issuecomment-296295294). Since there is no direct API call to access the list of resolved filters(after command line parsing) this bug fix will have to wait until [broadinstitute/barclay#38](https://github.com/broadinstitute/barclay/pull/38) is merged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2976
https://github.com/broadinstitute/gatk/issues/2976:972,Security,access,access,972,"@asmirnov239 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/1000). Right now if a user disables MAPPED filter, which is a default filter for CalculateTargetCoverage tool, it will fail with the following uninformative exception (unless somehow all reads are mapped):; ```; java.lang.IllegalArgumentException: the input location cannot be null; 	at org.broadinstitute.hellbender.utils.Utils.nonNull(Utils.java:549); 	at org.broadinstitute.hellbender.tools.exome.HashedListTargetCollection.indexRange(HashedListTargetCollection.java:152); 	at org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage.apply(CalculateTargetCoverage.java:298); ```; We should guard against it and throw an exception before the traversal starts if MAPPED filter is disabled. ---. @asmirnov239 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/1000#issuecomment-296295294). Since there is no direct API call to access the list of resolved filters(after command line parsing) this bug fix will have to wait until [broadinstitute/barclay#38](https://github.com/broadinstitute/barclay/pull/38) is merged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2976
https://github.com/broadinstitute/gatk/issues/2977:1453,Availability,down,down,1453,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2977:681,Deployability,pipeline,pipeline,681,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2977:1426,Deployability,pipeline,pipeline,1426,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2977:123,Modifiability,enhance,enhancement,123,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2977:799,Performance,perform,performing,799,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2977:1352,Performance,perform,performs,1352,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2977:1439,Safety,avoid,avoid,1439,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2977:733,Usability,simpl,simply,733,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2977:1333,Usability,simpl,simple,1333,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977
https://github.com/broadinstitute/gatk/issues/2979:225,Modifiability,refactor,refactor,225,@mbabadi commented on [Tue May 02 2017](https://github.com/broadinstitute/gatk-protected/issues/1021). - [ ] factor I/O methods out of `CoverageModelEMWorkspace` and to a new class; - [ ] shrink the exposed API; - [ ] rename/refactor `CopyRatioCallingMetadata` appropriately; - [ ] rename/move `MathObjectAsserts` to test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2979
https://github.com/broadinstitute/gatk/issues/2979:199,Security,expose,exposed,199,@mbabadi commented on [Tue May 02 2017](https://github.com/broadinstitute/gatk-protected/issues/1021). - [ ] factor I/O methods out of `CoverageModelEMWorkspace` and to a new class; - [ ] shrink the exposed API; - [ ] rename/refactor `CopyRatioCallingMetadata` appropriately; - [ ] rename/move `MathObjectAsserts` to test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2979
https://github.com/broadinstitute/gatk/issues/2979:317,Testability,test,test,317,@mbabadi commented on [Tue May 02 2017](https://github.com/broadinstitute/gatk-protected/issues/1021). - [ ] factor I/O methods out of `CoverageModelEMWorkspace` and to a new class; - [ ] shrink the exposed API; - [ ] rename/refactor `CopyRatioCallingMetadata` appropriately; - [ ] rename/move `MathObjectAsserts` to test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2979
https://github.com/broadinstitute/gatk/issues/2982:685,Usability,simpl,simple,685,"@davidbenjamin commented on [Tue May 09 2017](https://github.com/broadinstitute/gatk-protected/issues/1031). @fleharty has found a common artifact in his UMI work in which barcode swapping causes duplicate reads not to be correctly flagged as such, thereby overestimating the evidence in favor of a false positive variant. In his examples, several reads usually have the variant at *exactly* the same position. Less frequently but still often enough that we must deal with it, there will be two sets of artifacts and two distinct positions. Reasonable approaches to an annotation and corresponding filter include but are not limited to 1) a p-value for this degree of clustering, 2) a simple generative model, and 3) just counting the number of unique positions within the read.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2982
https://github.com/broadinstitute/gatk/issues/2984:2082,Availability,error,errors,2082,"y, which includes theano graph compilation. This includes 350 iterations of ADVI, but note that convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=le",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2984:2242,Availability,down,down,2242,"pping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droazen @lbergelson @LeeTL1220 @ldgauthier @yfarjoun This is just one example of how using recently developed M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2984:3655,Availability,down,downside,3655,"form(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droazen @lbergelson @LeeTL1220 @ldgauthier @yfarjoun This is just one example of how using recently developed ML frameworks could make our lives orders of magnitude easier. The sooner we can develop a strategy to leverage these in Java land, the better!. I'd expect that roughly the same amount of code would be needed to specify this model using Stan. Interfaces for Stan exist for many other languages, so it might be relatively easy to come up with some Java bindings. However, one downside is that Stan is not built on top of a computational graph (e.g., theano/tensorflow), so we don't get GPU/distributed computing for free. I don't think this is a deal breaker, but it's something we should consider. ---. @samuelklee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302429919). It should be said that I consider this a major blocker for the CNV team. I don't think it makes sense to rebuild the somatic pipeline to include the new coverage model until we move to this ADVI framework or something like it. I do think @mbabadi should continue adding features (such as common CNV calling) to his non-ADVI germline implementation, so that we can be in a position to start calling on gnomAD or other large cohorts, but that we should eventually move the germline tool over to this framework as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2984:2178,Deployability,configurat,configuration,2178,"t convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2984:4137,Deployability,pipeline,pipeline,4137,"form(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droazen @lbergelson @LeeTL1220 @ldgauthier @yfarjoun This is just one example of how using recently developed ML frameworks could make our lives orders of magnitude easier. The sooner we can develop a strategy to leverage these in Java land, the better!. I'd expect that roughly the same amount of code would be needed to specify this model using Stan. Interfaces for Stan exist for many other languages, so it might be relatively easy to come up with some Java bindings. However, one downside is that Stan is not built on top of a computational graph (e.g., theano/tensorflow), so we don't get GPU/distributed computing for free. I don't think this is a deal breaker, but it's something we should consider. ---. @samuelklee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302429919). It should be said that I consider this a major blocker for the CNV team. I don't think it makes sense to rebuild the somatic pipeline to include the new coverage model until we move to this ADVI framework or something like it. I do think @mbabadi should continue adding features (such as common CNV calling) to his non-ADVI germline implementation, so that we can be in a position to start calling on gnomAD or other large cohorts, but that we should eventually move the germline tool over to this framework as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2984:3523,Integrability,Interface,Interfaces,3523,"form(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droazen @lbergelson @LeeTL1220 @ldgauthier @yfarjoun This is just one example of how using recently developed ML frameworks could make our lives orders of magnitude easier. The sooner we can develop a strategy to leverage these in Java land, the better!. I'd expect that roughly the same amount of code would be needed to specify this model using Stan. Interfaces for Stan exist for many other languages, so it might be relatively easy to come up with some Java bindings. However, one downside is that Stan is not built on top of a computational graph (e.g., theano/tensorflow), so we don't get GPU/distributed computing for free. I don't think this is a deal breaker, but it's something we should consider. ---. @samuelklee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302429919). It should be said that I consider this a major blocker for the CNV team. I don't think it makes sense to rebuild the somatic pipeline to include the new coverage model until we move to this ADVI framework or something like it. I do think @mbabadi should continue adding features (such as common CNV calling) to his non-ADVI germline implementation, so that we can be in a position to start calling on gnomAD or other large cohorts, but that we should eventually move the germline tool over to this framework as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2984:2178,Modifiability,config,configuration,2178,"t convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2984:194,Performance,perform,performance,194,"@samuelklee commented on [Fri May 12 2017](https://github.com/broadinstitute/gatk-protected/issues/1038). - [x] PyMC3 (theano) implementation; - [x] PyStan implementation; - [x] investigate GPU performance; - [x] investigate batch/minibatch performance. See https://github.com/broadinstitute/dsde-methods-prototyping/tree/sl_advi for notebook on Bayesian GMM and instructions to set up the appropriate python environment with conda. ---. @samuelklee commented on [Sun May 14 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-301189833). Coverage model with ARD and full batch now in https://github.com/broadinstitute/dsde-methods-prototyping/blob/sl_advi/cnv-th/advi-prototypes/coverage.ipynb in the sl_advi branch. Toy number of targets (100) and samples (50) seems to work well and converge quickly with random initialization. ---. @samuelklee commented on [Sun May 14 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-301321417). T = 10^5 and N = 100 took 15 minutes running on one core of an i5-3570K with 32GB memory, which includes theano graph compilation. This includes 350 iterations of ADVI, but note that convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2984:241,Performance,perform,performance,241,"@samuelklee commented on [Fri May 12 2017](https://github.com/broadinstitute/gatk-protected/issues/1038). - [x] PyMC3 (theano) implementation; - [x] PyStan implementation; - [x] investigate GPU performance; - [x] investigate batch/minibatch performance. See https://github.com/broadinstitute/dsde-methods-prototyping/tree/sl_advi for notebook on Bayesian GMM and instructions to set up the appropriate python environment with conda. ---. @samuelklee commented on [Sun May 14 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-301189833). Coverage model with ARD and full batch now in https://github.com/broadinstitute/dsde-methods-prototyping/blob/sl_advi/cnv-th/advi-prototypes/coverage.ipynb in the sl_advi branch. Toy number of targets (100) and samples (50) seems to work well and converge quickly with random initialization. ---. @samuelklee commented on [Sun May 14 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-301321417). T = 10^5 and N = 100 took 15 minutes running on one core of an i5-3570K with 32GB memory, which includes theano graph compilation. This includes 350 iterations of ADVI, but note that convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984
https://github.com/broadinstitute/gatk/issues/2985:272,Performance,Perform,Performance,272,@samuelklee commented on [Fri May 12 2017](https://github.com/broadinstitute/gatk-protected/issues/1039). Hooking up a siHMM (or some variation) to the coverage model from https://github.com/broadinstitute/gatk-protected/issues/1038 will be our first cut at GATK CNV 2.0. Performance and convergence on WGS-size data should be a priority. The end goal will be a joint siHMM once we also have the 2.0 allele-fraction model prototyped.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2985
https://github.com/broadinstitute/gatk/issues/2987:174,Modifiability,variab,variable-ploidy,174,"@sooheelee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1045). For MuTect2 developers, a feature to keep in mind -- allowing moving-variable-ploidy calling on highly diverse pooled microbiome samples. ---; Hi, I am interested in knowing how to apply gatk tools to microbiome data. Specifically, I would like to override the assumption of ploidy in the HaplotypeCaller and making it flexible, in that one sample could have a unknown number of haplotypes at the same time, I know somatic mutation caller Mutect2 does not share the assumption but then it's designed to specifically deal with normal - tumor sample pair which is not really applicable in the microbiome studies. Thanks. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/9594/applying-gatk-to-microbiome-data/p1. ---. @davidbenjamin commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303273671). @sooheelee Tumor-only calling is fully supported in GATK 4 M2, but I will need to understand more about microbiome variant calling to know if M2 could be used. ---. @vdauwera commented on [Tue May 23 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303284834). We've had users trying to use MuTect this way for a long time; we just tell them it's unsupported. Actually putting effort into figuring this out and potentially supporting it would probably be a quarterly-goals level decision. Given everything on everyone's plate I wouldn't expect this to get on the QGs in a long while, unless there's a high-profile project that demands it. Don't get me wrong, I'd love to see this done, and I will bring it up, but realistically I'm not going to hold my breath... . ---. @davidbenjamin commented on [Tue May 23 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303413211). This could be the sort of thing like mitochondrial calli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2987
https://github.com/broadinstitute/gatk/issues/2987:424,Modifiability,flexible,flexible,424,"@sooheelee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1045). For MuTect2 developers, a feature to keep in mind -- allowing moving-variable-ploidy calling on highly diverse pooled microbiome samples. ---; Hi, I am interested in knowing how to apply gatk tools to microbiome data. Specifically, I would like to override the assumption of ploidy in the HaplotypeCaller and making it flexible, in that one sample could have a unknown number of haplotypes at the same time, I know somatic mutation caller Mutect2 does not share the assumption but then it's designed to specifically deal with normal - tumor sample pair which is not really applicable in the microbiome studies. Thanks. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/9594/applying-gatk-to-microbiome-data/p1. ---. @davidbenjamin commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303273671). @sooheelee Tumor-only calling is fully supported in GATK 4 M2, but I will need to understand more about microbiome variant calling to know if M2 could be used. ---. @vdauwera commented on [Tue May 23 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303284834). We've had users trying to use MuTect this way for a long time; we just tell them it's unsupported. Actually putting effort into figuring this out and potentially supporting it would probably be a quarterly-goals level decision. Given everything on everyone's plate I wouldn't expect this to get on the QGs in a long while, unless there's a high-profile project that demands it. Don't get me wrong, I'd love to see this done, and I will bring it up, but realistically I'm not going to hold my breath... . ---. @davidbenjamin commented on [Tue May 23 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303413211). This could be the sort of thing like mitochondrial calli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2987
https://github.com/broadinstitute/gatk/issues/2988:847,Availability,error,error,847,"@sooheelee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048). Hey @cmnbroad. Instead of slacking you this stacktrace, I thought I'd put it here. Let me know if you want the more detailed debug report, etc. ---; ```; WMCF9-CB5:hellbender shlee$ git branch; * master; shl_mark_documented_splitncigarreads; WMCF9-CB5:hellbender shlee$ git pull origin master; From github.com:broadinstitute/gatk; * branch master -> FETCH_HEAD; Already up-to-date.; WMCF9-CB5:hellbender shlee$ cd ../hellbender-protected/; WMCF9-CB5:hellbender-protected shlee$ ./gradlew -PgatkSourceDir=../hellbender gatkDoc; :compileJava UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :cleanGatkDoc; :gatkDoc; Note: The GATK source folder must contain the same version of GATK that is used to build gatk-protected; javadoc: error - In doclet class org.broadinstitute.hellbender.utils.help.GATKHelpDoclet, method start has thrown an exception java.lang.reflect.InvocationTargetException; java.lang.RuntimeException: Could not find the field corresponding to java.lang.Throwable.backtrace, presumably because the field is inaccessible; at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:604); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:591); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDocForCommandLineArgument(DefaultDocWorkUnitHandler.java:403); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processNamedArgument(DefaultDocWorkUnitHandler.java:357); at org.broadinstitute.barclay.help.DefaultD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988
https://github.com/broadinstitute/gatk/issues/2988:3787,Availability,error,error,3787,"ach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://gith",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988
https://github.com/broadinstitute/gatk/issues/2988:3811,Availability,FAILURE,FAILURE,3811,"g.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988
https://github.com/broadinstitute/gatk/issues/2988:4493,Integrability,depend,dependent,4493,"c.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302784016). Any word on this @cmnbroad? I'm running into pre-existing documentation whose formatting I need to check appears correctly. I think I will create a testing doc in gatk public to see how formatting appears. However, this has me going back and forth between two repos so adds to my mental overhead. ---. @cmnbroad commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-303094262). @sooheelee I have a fix, but reproducing this in a test is challenging. Will try to get something as soon as possible. ---. @sooheelee commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomme",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988
https://github.com/broadinstitute/gatk/issues/2988:4539,Performance,cache,cachemanager,4539,"c.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302784016). Any word on this @cmnbroad? I'm running into pre-existing documentation whose formatting I need to check appears correctly. I think I will create a testing doc in gatk public to see how formatting appears. However, this has me going back and forth between two repos so adds to my mental overhead. ---. @cmnbroad commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-303094262). @sooheelee I have a fix, but reproducing this in a test is challenging. Will try to get something as soon as possible. ---. @sooheelee commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomme",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988
https://github.com/broadinstitute/gatk/issues/2988:4198,Testability,log,log,4198,"mpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302784016). Any word on this @cmnbroad? I'm running into pre-existing documentation whose formatting I need to check appears correctly. I think I will create a testing doc in gatk public to see how formatting appears. However, this has me going back and forth between two repos so adds to my mental overhead. ---. @cmnbroad c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988
https://github.com/broadinstitute/gatk/issues/2988:5015,Testability,test,testing,5015,"What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302784016). Any word on this @cmnbroad? I'm running into pre-existing documentation whose formatting I need to check appears correctly. I think I will create a testing doc in gatk public to see how formatting appears. However, this has me going back and forth between two repos so adds to my mental overhead. ---. @cmnbroad commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-303094262). @sooheelee I have a fix, but reproducing this in a test is challenging. Will try to get something as soon as possible. ---. @sooheelee commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-303138110). Looking forward to it! . ---. @magicDGS commented on [Fri May 26 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-304247203). Because this repository is going to be merge into the ""public"" one at some point, should this issue be moved to Barclay @cmnbroad? Thanks in advance!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988
https://github.com/broadinstitute/gatk/issues/2988:5347,Testability,test,test,5347,"What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302784016). Any word on this @cmnbroad? I'm running into pre-existing documentation whose formatting I need to check appears correctly. I think I will create a testing doc in gatk public to see how formatting appears. However, this has me going back and forth between two repos so adds to my mental overhead. ---. @cmnbroad commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-303094262). @sooheelee I have a fix, but reproducing this in a test is challenging. Will try to get something as soon as possible. ---. @sooheelee commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-303138110). Looking forward to it! . ---. @magicDGS commented on [Fri May 26 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-304247203). Because this repository is going to be merge into the ""public"" one at some point, should this issue be moved to Barclay @cmnbroad? Thanks in advance!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988
https://github.com/broadinstitute/gatk/issues/2990:109,Deployability,upgrade,upgrade,109,"@LeeTL1220 commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1053). And upgrade ubuntu to 16.04. And leverage the ``broadinstitute/gatk:gatkbase-1.0`` image. Actually, this will handle the move to OpenJDK8 and ubuntu 16.04.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2990
https://github.com/broadinstitute/gatk/issues/2991:597,Availability,mask,masked,597,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1055). At the moment, we:; - Remove targets with possibly bad (NaN, infinity, negative) values; - Remove targets that have uniformly low coverage across all samples. Perhaps we should consider adding more filters:; - Remove targets with very high and very low GC content (can be done in the CalculateTargetCoverage step); - Remove targets with lots of repeats and anomalously low mappability (can be done in the CalculateTargetCoverage step); - In the learning mode, remove a target if _too many_ are masked across the samples (in that case, max likelihood parameter estimation is unreliable). This must be done after careful evaluations, i.e. only if certain features makes a target prone to bad calls no matter what.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2991
https://github.com/broadinstitute/gatk/issues/2991:548,Usability,learn,learning,548,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1055). At the moment, we:; - Remove targets with possibly bad (NaN, infinity, negative) values; - Remove targets that have uniformly low coverage across all samples. Perhaps we should consider adding more filters:; - Remove targets with very high and very low GC content (can be done in the CalculateTargetCoverage step); - Remove targets with lots of repeats and anomalously low mappability (can be done in the CalculateTargetCoverage step); - In the learning mode, remove a target if _too many_ are masked across the samples (in that case, max likelihood parameter estimation is unreliable). This must be done after careful evaluations, i.e. only if certain features makes a target prone to bad calls no matter what.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2991
https://github.com/broadinstitute/gatk/issues/2992:597,Availability,mask,masked,597,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1055). At the moment, we:; - Remove targets with possibly bad (NaN, infinity, negative) values; - Remove targets that have uniformly low coverage across all samples. Perhaps we should consider adding more filters:; - Remove targets with very high and very low GC content (can be done in the CalculateTargetCoverage step); - Remove targets with lots of repeats and anomalously low mappability (can be done in the CalculateTargetCoverage step); - In the learning mode, remove a target if _too many_ are masked across the samples (in that case, max likelihood parameter estimation is unreliable). This must be done after careful evaluations, i.e. only if certain features makes a target prone to bad calls no matter what.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2992
https://github.com/broadinstitute/gatk/issues/2992:548,Usability,learn,learning,548,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1055). At the moment, we:; - Remove targets with possibly bad (NaN, infinity, negative) values; - Remove targets that have uniformly low coverage across all samples. Perhaps we should consider adding more filters:; - Remove targets with very high and very low GC content (can be done in the CalculateTargetCoverage step); - Remove targets with lots of repeats and anomalously low mappability (can be done in the CalculateTargetCoverage step); - In the learning mode, remove a target if _too many_ are masked across the samples (in that case, max likelihood parameter estimation is unreliable). This must be done after careful evaluations, i.e. only if certain features makes a target prone to bad calls no matter what.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2992
https://github.com/broadinstitute/gatk/issues/2993:382,Usability,simpl,simplify,382,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1056). Here's a relevant conversation we had:. **David**: At some point we should consider ditching Viterbi altogether, since you can segment using the forward-backward result eg setting the call at each target to be the max posterior value. If it works equally well or better it would simplify stuff. **Mehrtash**: Sam and I had a lengthy discussion about this. The Viterbi result on one hand, and what you get from stacking MAP on each target one the other hand, can be (very) different. One can think of the former as the ground state of the entire system and the latter as the most favorable local state after tracing out the rest of the system. What we thought would be interesting to do is to:; (1) segment based on Viterbi,; (2) segment based on stacking local CR MAP, and; (3) generate a swarm of hidden state samples from the HMM, segment each sample, and create a 1D density plot for state transition ""hotspots"". The hope is that (1) gives us the best overall hidden chain (which may sacrifice local calls), (2) gives us target-resolved genotypes, and (3) gives us some insight about ""excitations"" about the Viterbi state, i.e. (3) is something between (1) and (2). **Samuel**: I think breakpoint probabilities based on sequences drawn from the joint posterior (3) would be most useful, but let's see how these look in real data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2993
https://github.com/broadinstitute/gatk/issues/2995:773,Availability,error,errors,773,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1058). - [ ] good choice of default parameters; - [ ] double-check assertion coverage in `CoverageModelArgumentCollection.validate()`; - [ ] if a model is provided, ARD and number of PCs must be overridden (currently, an exception is thrown if there is a discrepancy between model parameters and arguments). Relevant discussion:; **Mehrtash**: We may be able to get rid of a number of these parameters. Though, generally speaking, I'd rather expose more than less, with good default values and bold advanced disclaimers w/ proper documentation as you suggested. This is the case with sophisticated tools like HaplotypeCaller, StarAligner, etc. Soon enough, we will get strange errors from various users many of which can be resolved by changing a certain advanced parameter. Without exposing them, we will have to create patches for them and/or build custom jars.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2995
https://github.com/broadinstitute/gatk/issues/2995:917,Deployability,patch,patches,917,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1058). - [ ] good choice of default parameters; - [ ] double-check assertion coverage in `CoverageModelArgumentCollection.validate()`; - [ ] if a model is provided, ARD and number of PCs must be overridden (currently, an exception is thrown if there is a discrepancy between model parameters and arguments). Relevant discussion:; **Mehrtash**: We may be able to get rid of a number of these parameters. Though, generally speaking, I'd rather expose more than less, with good default values and bold advanced disclaimers w/ proper documentation as you suggested. This is the case with sophisticated tools like HaplotypeCaller, StarAligner, etc. Soon enough, we will get strange errors from various users many of which can be resolved by changing a certain advanced parameter. Without exposing them, we will have to create patches for them and/or build custom jars.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2995
https://github.com/broadinstitute/gatk/issues/2995:218,Security,validat,validate,218,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1058). - [ ] good choice of default parameters; - [ ] double-check assertion coverage in `CoverageModelArgumentCollection.validate()`; - [ ] if a model is provided, ARD and number of PCs must be overridden (currently, an exception is thrown if there is a discrepancy between model parameters and arguments). Relevant discussion:; **Mehrtash**: We may be able to get rid of a number of these parameters. Though, generally speaking, I'd rather expose more than less, with good default values and bold advanced disclaimers w/ proper documentation as you suggested. This is the case with sophisticated tools like HaplotypeCaller, StarAligner, etc. Soon enough, we will get strange errors from various users many of which can be resolved by changing a certain advanced parameter. Without exposing them, we will have to create patches for them and/or build custom jars.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2995
https://github.com/broadinstitute/gatk/issues/2995:538,Security,expose,expose,538,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1058). - [ ] good choice of default parameters; - [ ] double-check assertion coverage in `CoverageModelArgumentCollection.validate()`; - [ ] if a model is provided, ARD and number of PCs must be overridden (currently, an exception is thrown if there is a discrepancy between model parameters and arguments). Relevant discussion:; **Mehrtash**: We may be able to get rid of a number of these parameters. Though, generally speaking, I'd rather expose more than less, with good default values and bold advanced disclaimers w/ proper documentation as you suggested. This is the case with sophisticated tools like HaplotypeCaller, StarAligner, etc. Soon enough, we will get strange errors from various users many of which can be resolved by changing a certain advanced parameter. Without exposing them, we will have to create patches for them and/or build custom jars.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2995
https://github.com/broadinstitute/gatk/issues/2995:163,Testability,assert,assertion,163,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1058). - [ ] good choice of default parameters; - [ ] double-check assertion coverage in `CoverageModelArgumentCollection.validate()`; - [ ] if a model is provided, ARD and number of PCs must be overridden (currently, an exception is thrown if there is a discrepancy between model parameters and arguments). Relevant discussion:; **Mehrtash**: We may be able to get rid of a number of these parameters. Though, generally speaking, I'd rather expose more than less, with good default values and bold advanced disclaimers w/ proper documentation as you suggested. This is the case with sophisticated tools like HaplotypeCaller, StarAligner, etc. Soon enough, we will get strange errors from various users many of which can be resolved by changing a certain advanced parameter. Without exposing them, we will have to create patches for them and/or build custom jars.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2995
https://github.com/broadinstitute/gatk/issues/2996:243,Deployability,release,release,243,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:165,Energy Efficiency,reduce,reduce,165,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:840,Testability,Test,Tests,840,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:918,Testability,test,tests,918,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:928,Testability,test,test,928,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:1060,Testability,test,tests,1060,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:1070,Testability,test,test,1070,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:1204,Testability,test,tests,1204,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:1214,Testability,test,test,1214,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2996:114,Usability,learn,learn,114,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996
https://github.com/broadinstitute/gatk/issues/2998:163,Modifiability,polymorphi,polymorphic,163,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1062). - [ ] the ability to _override_ global transition priors in polymorphic regions, etc.; - [ ] the ability to ingest XHMM-like priors (i.e. parametrized models); - [ ] (bonus) different max copy number states for different genomic loci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2998
https://github.com/broadinstitute/gatk/issues/3000:480,Deployability,update,updated,480,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1063). Due to a bug in Nd4j library, INDArray.get() method does not work correctly for row or column vectors (that we work with when number of samples is 1). In these cases a call to `getNDArrayByIndices(array, indX, indY, int)` is made.; Note that this bug was fixed in later version of Nd4j (currently used version is 0.5.0), so the method can be removed when the dependency is updated",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3000
https://github.com/broadinstitute/gatk/issues/3000:466,Integrability,depend,dependency,466,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1063). Due to a bug in Nd4j library, INDArray.get() method does not work correctly for row or column vectors (that we work with when number of samples is 1). In these cases a call to `getNDArrayByIndices(array, indX, indY, int)` is made.; Note that this bug was fixed in later version of Nd4j (currently used version is 0.5.0), so the method can be removed when the dependency is updated",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3000
https://github.com/broadinstitute/gatk/issues/3001:538,Deployability,integrat,integrate,538,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1064). At the moment, the maximum copy number in gCNV is set by the dimension provided transition matrices. One needs to work with copy number states up to 10 ~ 20 in order to capture high copy number states seen in certain regions. It is desirable to limit the number of states, however, to designate a special state that represents _all copy number states at and above CN_max_. This can be achieved by adjusting the emission probability to integrate over all higher copy number states. There are other challenges too (e.g. calculating copy number posteriors from such states).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3001
https://github.com/broadinstitute/gatk/issues/3001:538,Integrability,integrat,integrate,538,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1064). At the moment, the maximum copy number in gCNV is set by the dimension provided transition matrices. One needs to work with copy number states up to 10 ~ 20 in order to capture high copy number states seen in certain regions. It is desirable to limit the number of states, however, to designate a special state that represents _all copy number states at and above CN_max_. This can be achieved by adjusting the emission probability to integrate over all higher copy number states. There are other challenges too (e.g. calculating copy number posteriors from such states).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3001
https://github.com/broadinstitute/gatk/issues/3002:535,Deployability,integrat,integration,535,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:535,Integrability,integrat,integration,535,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:1129,Modifiability,extend,extend,1129,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:409,Testability,test,test,409,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:419,Testability,test,test,419,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:547,Testability,test,test,547,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:731,Testability,test,tests,731,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:1140,Testability,test,test,1140,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:1407,Testability,test,test,1407,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3002:1522,Testability,test,test,1522,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002
https://github.com/broadinstitute/gatk/issues/3004:1280,Deployability,release,release,1280,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:499,Energy Efficiency,power,power,499,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:565,Energy Efficiency,adapt,adaptive,565,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:661,Energy Efficiency,adapt,adaptively,661,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:565,Modifiability,adapt,adaptive,565,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:661,Modifiability,adapt,adaptively,661,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:781,Performance,perform,perform,781,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:1249,Performance,perform,perform,1249,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:154,Usability,learn,learn,154,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:235,Usability,learn,learning,235,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3004:314,Usability,learn,learning,314,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004
https://github.com/broadinstitute/gatk/issues/3007:122,Performance,perform,perform,122,@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1071). It is desirable to perform an explicit bookkeeping of ICG function evaluation to ensure that gCNV is not performing unnecessary EM evaluation in each cycle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3007
https://github.com/broadinstitute/gatk/issues/3007:208,Performance,perform,performing,208,@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1071). It is desirable to perform an explicit bookkeeping of ICG function evaluation to ensure that gCNV is not performing unnecessary EM evaluation in each cycle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3007
https://github.com/broadinstitute/gatk/issues/3011:231,Usability,simpl,simply,231,@davidbenjamin commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1089). A large number of spurious active regions are due to non-reference bases with quals less than 10. The current approach of simply counting non-ref bases is too permissive.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3011
https://github.com/broadinstitute/gatk/issues/3013:2089,Availability,error,error,2089,".setPosition(SAMRecordToGATKReadAdapter.java:89); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHARDCLIP_BASES(ClippingOp.java:381); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:73); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:147); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:128); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:332); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:335); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:84); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:238); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:478); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$580(HaplotypeCallerSpark.java:203) ; ```. At first glance this looks like a problem with unmapped reads, but these are filtered out by the tool. So it's more likely to be in the clipping logic. It's hard to diagnose since it doesn't say which read caused it, and it's slow to reproduce as it is running on a large input. Any thoughts @lbergelson, @droazen?. ---. @lbergelson commented on [Sat May 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1091#issuecomment-304466112). @tomwhite Is it possible you could upload the bam file somewhere on google cloud along with the command line you used? It's not obvious to me where the error is being caused. It's painful to debug anything on a 160GB file, but I think we can probably do a binary search on the file and find the bad location pretty quickly. I.e. throw compute at the problem instead of human time...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3013
https://github.com/broadinstitute/gatk/issues/3013:1632,Testability,log,logic,1632,".setPosition(SAMRecordToGATKReadAdapter.java:89); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHARDCLIP_BASES(ClippingOp.java:381); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:73); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:147); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:128); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:332); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:335); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:84); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:238); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:478); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$580(HaplotypeCallerSpark.java:203) ; ```. At first glance this looks like a problem with unmapped reads, but these are filtered out by the tool. So it's more likely to be in the clipping logic. It's hard to diagnose since it doesn't say which read caused it, and it's slow to reproduce as it is running on a large input. Any thoughts @lbergelson, @droazen?. ---. @lbergelson commented on [Sat May 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1091#issuecomment-304466112). @tomwhite Is it possible you could upload the bam file somewhere on google cloud along with the command line you used? It's not obvious to me where the error is being caused. It's painful to debug anything on a 160GB file, but I think we can probably do a binary search on the file and find the bad location pretty quickly. I.e. throw compute at the problem instead of human time...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3013
https://github.com/broadinstitute/gatk/issues/3014:746,Availability,down,downstream,746,"@davidbenjamin commented on [Tue May 23 2017](https://github.com/broadinstitute/gatk-protected/issues/1094). In active region determination for Mutect, and I believe also HaplotypeCaller, we count soft clips as a potential sign of a variant. This is because the aligner might soft clip the last few bases of a read that follow a deletion rather than call the deletion. For example, if the reference and read are:. TTCCAGAGTGTGTCAC (reference); TTC____________GTCAC (read). the alignment might choose to soft clip the GTCAC rather than call a deletion on the CAGAGTGT. In somatic calling it is expensive to call too many active regions, so perhaps we should only count eg the soft-clipped bases GTCAC as evidence of variation if that kmer appears downstream in the reference. @fleharty is this understanding of soft-clips being possible deletions (but not insertions or SNVs) correct?. ---. @fleharty commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1094#issuecomment-303733706). @davidbenjamin . I certainly agree with you that soft-clips can be due to deletions. It's not at all clear to me that they wouldn't happen with an insertion. Consider:. ---ATGAACAGATATAACAGAT (reference); ---ATGAA(AGGTAA)CAGATATAACAGAT (read). I don't see why a soft clip might not show up on this read after ATGAA.; I'm not really sure I understand why some things are soft-clipped to be honest. I've seen plenty of things that were soft-clipped, but appear to match the reference perfectly (maybe I'm remembering this incorrectly). I suspect that soft-clips are hardly ever correctly associated with SNVs though. ---. @davidbenjamin commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1094#issuecomment-303771734). @fleharty Thanks for the input!. ---. @ldgauthier commented on [Thu May 25 2017](https://github.com/broadinstitute/gatk-protected/issues/1094#issuecomment-303996178). You'll also likely see a difference in behavior for exomes vs gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3014
https://github.com/broadinstitute/gatk/issues/3014:1118,Usability,clear,clear,1118,"on for Mutect, and I believe also HaplotypeCaller, we count soft clips as a potential sign of a variant. This is because the aligner might soft clip the last few bases of a read that follow a deletion rather than call the deletion. For example, if the reference and read are:. TTCCAGAGTGTGTCAC (reference); TTC____________GTCAC (read). the alignment might choose to soft clip the GTCAC rather than call a deletion on the CAGAGTGT. In somatic calling it is expensive to call too many active regions, so perhaps we should only count eg the soft-clipped bases GTCAC as evidence of variation if that kmer appears downstream in the reference. @fleharty is this understanding of soft-clips being possible deletions (but not insertions or SNVs) correct?. ---. @fleharty commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1094#issuecomment-303733706). @davidbenjamin . I certainly agree with you that soft-clips can be due to deletions. It's not at all clear to me that they wouldn't happen with an insertion. Consider:. ---ATGAACAGATATAACAGAT (reference); ---ATGAA(AGGTAA)CAGATATAACAGAT (read). I don't see why a soft clip might not show up on this read after ATGAA.; I'm not really sure I understand why some things are soft-clipped to be honest. I've seen plenty of things that were soft-clipped, but appear to match the reference perfectly (maybe I'm remembering this incorrectly). I suspect that soft-clips are hardly ever correctly associated with SNVs though. ---. @davidbenjamin commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1094#issuecomment-303771734). @fleharty Thanks for the input!. ---. @ldgauthier commented on [Thu May 25 2017](https://github.com/broadinstitute/gatk-protected/issues/1094#issuecomment-303996178). You'll also likely see a difference in behavior for exomes vs genomes; because exomes (still) use bwa-aln and genomes use bwa-mem. I don't; remember the details off the top of my head, but I looked into",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3014
https://github.com/broadinstitute/gatk/issues/3015:1191,Availability,robust,robustness,1191,"scovered by feeding the tool with coverage data on autosomes + X chromosome (no Y chromosome). Since the X chr in XX samples has 2x ploidy of X in XY samples, one expects the tool to be able to make the correct inference. However, the tool genotyped all samples as XX (see the attached figure -- left: autosome+X+Y, right:autosome+X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:1716,Availability,robust,robust,1716,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:1736,Deployability,Update,Update,1736,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:796,Performance,perform,performing,796,"@mbabadi commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1097). Devin McCabe discovered a _bug_ (read: bad model behavior) in TargetCoverageSexGenotyper. The bug was discovered by feeding the tool with coverage data on autosomes + X chromosome (no Y chromosome). Since the X chr in XX samples has 2x ploidy of X in XY samples, one expects the tool to be able to make the correct inference. However, the tool genotyped all samples as XX (see the attached figure -- left: autosome+X+Y, right:autosome+X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/2651",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:1601,Performance,perform,perform,1601,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:1526,Testability,log,log,1526,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:2432,Testability,test,test,2432,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:2497,Testability,test,tests,2497,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:1546,Usability,simpl,simply,1546,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3015:1788,Usability,simpl,simply,1788,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015
https://github.com/broadinstitute/gatk/issues/3016:145,Availability,error,error,145,"@davidbenjamin commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1098). The telltale sign of a substitution error occurring on a single strand of DNA is that supporting evidence is all on forward strand read 1 and reverse strand read 2, or vice versa. This lends itself to a graphical model, the hyperparameters of which can be learned from the data. Further down the road, we might use a neural network to learn the context-specific risk of such artifacts and attach it to the Bayesian model for forward/reverse and read 1/read 2. This would be our first experience with a deep generative model.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3016
https://github.com/broadinstitute/gatk/issues/3016:396,Availability,down,down,396,"@davidbenjamin commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1098). The telltale sign of a substitution error occurring on a single strand of DNA is that supporting evidence is all on forward strand read 1 and reverse strand read 2, or vice versa. This lends itself to a graphical model, the hyperparameters of which can be learned from the data. Further down the road, we might use a neural network to learn the context-specific risk of such artifacts and attach it to the Bayesian model for forward/reverse and read 1/read 2. This would be our first experience with a deep generative model.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3016
https://github.com/broadinstitute/gatk/issues/3016:471,Safety,risk,risk,471,"@davidbenjamin commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1098). The telltale sign of a substitution error occurring on a single strand of DNA is that supporting evidence is all on forward strand read 1 and reverse strand read 2, or vice versa. This lends itself to a graphical model, the hyperparameters of which can be learned from the data. Further down the road, we might use a neural network to learn the context-specific risk of such artifacts and attach it to the Bayesian model for forward/reverse and read 1/read 2. This would be our first experience with a deep generative model.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3016
https://github.com/broadinstitute/gatk/issues/3016:365,Usability,learn,learned,365,"@davidbenjamin commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1098). The telltale sign of a substitution error occurring on a single strand of DNA is that supporting evidence is all on forward strand read 1 and reverse strand read 2, or vice versa. This lends itself to a graphical model, the hyperparameters of which can be learned from the data. Further down the road, we might use a neural network to learn the context-specific risk of such artifacts and attach it to the Bayesian model for forward/reverse and read 1/read 2. This would be our first experience with a deep generative model.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3016
https://github.com/broadinstitute/gatk/issues/3016:444,Usability,learn,learn,444,"@davidbenjamin commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1098). The telltale sign of a substitution error occurring on a single strand of DNA is that supporting evidence is all on forward strand read 1 and reverse strand read 2, or vice versa. This lends itself to a graphical model, the hyperparameters of which can be learned from the data. Further down the road, we might use a neural network to learn the context-specific risk of such artifacts and attach it to the Bayesian model for forward/reverse and read 1/read 2. This would be our first experience with a deep generative model.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3016
https://github.com/broadinstitute/gatk/issues/3018:179,Availability,error,error,179,"@TianJin297 commented on [Fri May 26 2017](https://github.com/broadinstitute/gatk-protected/issues/1107). When I was running HaplotypeCallorSpark with one of my samples, I got an error as ""Duplicate key"". . The command I used is ""/gatk-protected HaplotypeCallerSpark -I XX_BQSRappliedspark.bam -O XX_525.gvcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence BP_RESOLUTION --TMP_DIR tmp"". And it runs on Amazon instance m4.2xlarge. 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1223,Availability,ERROR,ERROR,1223,"cted HaplotypeCallerSpark -I XX_BQSRappliedspark.bam -O XX_525.gvcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence BP_RESOLUTION --TMP_DIR tmp"". And it runs on Amazon instance m4.2xlarge. 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9194,Availability,ERROR,ERROR,9194,"nce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9444,Availability,down,down,9444,"rg.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9674,Availability,failure,failure,9674,"Context.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9732,Availability,failure,failure,9732,"n$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikeliho",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1564,Energy Efficiency,Reduce,ReduceOps,1564,"9 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1595,Energy Efficiency,Reduce,ReduceOps,1595,"eHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1874,Energy Efficiency,Reduce,ReduceOps,1874,"lculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1884,Energy Efficiency,Reduce,ReduceOp,1884,"lculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1912,Energy Efficiency,Reduce,ReduceOps,1912,"t called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipelin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:4874,Energy Efficiency,schedul,scheduler,4874," scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAnd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:4945,Energy Efficiency,schedul,scheduler,4945,"8); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$Redu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5626,Energy Efficiency,Reduce,ReduceOps,5626,"onfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5657,Energy Efficiency,Reduce,ReduceOps,5657,"nfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5936,Energy Efficiency,Reduce,ReduceOps,5936,"scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5946,Energy Efficiency,Reduce,ReduceOp,5946,"scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5974,Energy Efficiency,Reduce,ReduceOps,5974,"scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipelin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:8791,Energy Efficiency,schedul,scheduler,8791,"lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:8862,Energy Efficiency,schedul,scheduler,8862,"la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:10067,Energy Efficiency,Reduce,ReduceOps,10067,"ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:10098,Energy Efficiency,Reduce,ReduceOps,10098,"orker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:10377,Energy Efficiency,Reduce,ReduceOps,10377,"nally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:10387,Energy Efficiency,Reduce,ReduceOp,10387,"nally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:10415,Energy Efficiency,Reduce,ReduceOps,10415,"O HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipelin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13232,Energy Efficiency,schedul,scheduler,13232,lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13303,Energy Efficiency,schedul,scheduler,13303,la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13662,Energy Efficiency,schedul,scheduler,13662,2); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13702,Energy Efficiency,schedul,scheduler,13702,scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13800,Energy Efficiency,schedul,scheduler,13800,scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13897,Energy Efficiency,schedul,scheduler,13897,k.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14148,Energy Efficiency,schedul,scheduler,14148,kContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14228,Energy Efficiency,schedul,scheduler,14228,cheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14333,Energy Efficiency,schedul,scheduler,14333,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14481,Energy Efficiency,schedul,scheduler,14481,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14569,Energy Efficiency,schedul,scheduler,14569,617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14666,Energy Efficiency,schedul,scheduler,14666,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14761,Energy Efficiency,schedul,scheduler,14761,.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14924,Energy Efficiency,schedul,scheduler,14924,abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.writeVariants(HaplotypeCallerSpark.java:205); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:16958,Energy Efficiency,Reduce,ReduceOps,16958,.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:16989,Energy Efficiency,Reduce,ReduceOps,16989,park.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:17268,Energy Efficiency,Reduce,ReduceOps,17268,hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:17278,Energy Efficiency,Reduce,ReduceOp,17278,hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:17306,Energy Efficiency,Reduce,ReduceOps,17306,ndLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipelin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:20123,Energy Efficiency,schedul,scheduler,20123,ava:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:20194,Energy Efficiency,schedul,scheduler,20194,ava:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1810,Integrability,wrap,wrapAndCopyInto,1810,":45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:3042,Integrability,Wrap,WrappingSpliterator,3042,ne.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:3381,Integrability,Wrap,WrappingSpliterator,3381,irHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:3537,Integrability,Wrap,Wrappers,3537,irHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:3571,Integrability,Wrap,Wrappers,3571,ationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5872,Integrability,wrap,wrapAndCopyInto,5872,"ltTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:7104,Integrability,Wrap,WrappingSpliterator,7104,ne.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:7443,Integrability,Wrap,WrappingSpliterator,7443,irHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:7599,Integrability,Wrap,Wrappers,7599,irHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:7633,Integrability,Wrap,Wrappers,7633,ationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkCont,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:10313,Integrability,wrap,wrapAndCopyInto,10313,"age 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:11545,Integrability,Wrap,WrappingSpliterator,11545,ne.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:11884,Integrability,Wrap,WrappingSpliterator,11884,irHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:12040,Integrability,Wrap,Wrappers,12040,irHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:12074,Integrability,Wrap,Wrappers,12074,ationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkCont,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:17204,Integrability,wrap,wrapAndCopyInto,17204,Tool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:18436,Integrability,Wrap,WrappingSpliterator,18436,ne.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:18775,Integrability,Wrap,WrappingSpliterator,18775,irHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:18931,Integrability,Wrap,Wrappers,18931,irHMMLikelihoodCalculationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:18965,Integrability,Wrap,Wrappers,18965,ationEngi; ne.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:518); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$2(HaplotypeCallerSpark.java:192); at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkCont,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5067,Performance,concurren,concurrent,5067,"tor.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5151,Performance,concurren,concurrent,5151,"scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:8984,Performance,concurren,concurrent,8984,"$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9068,Performance,concurren,concurrent,9068,"nce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13425,Performance,concurren,concurrent,13425,$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Opti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13509,Performance,concurren,concurrent,13509,nce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:20316,Performance,concurren,concurrent,20316,ava:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:20400,Performance,concurren,concurrent,20400,ava:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9257,Safety,abort,aborting,9257,"terator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9653,Safety,abort,aborted,9653,"Context.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13832,Safety,abort,abortStage,13832,ollection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:13929,Safety,abort,abortStage,13929,lect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:14171,Safety,abort,abortStage,14171,ly(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1440,Security,Hash,HashMap,1440,".089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:1454,Security,Hash,HashMap,1454,"trandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5502,Security,Hash,HashMap,5502,"erator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:5516,Security,Hash,HashMap,5516,"a:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9943,Security,Hash,HashMap,9943,"un(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:9957,Security,Hash,HashMap,9957,".scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:16834,Security,Hash,HashMap,16834,java:205); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCallerSpark.java:115); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinsti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3018:16848,Security,Hash,HashMap,16848,at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCallerSpark.java:115); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018
https://github.com/broadinstitute/gatk/issues/3019:326,Availability,error,error,326,"@TianJin297 commented on [Fri May 26 2017](https://github.com/broadinstitute/gatk-protected/issues/1108). The command is gatk-protected HaplotypeCallerSpark -I XX_BQSRappliedspark.bam -O XX_spark.vcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence GVCF --TMP_DIR tmp. And it is run on an Amazon m4.2xlarge instance. The error messages are like below.; 04:39:06.415 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 05:09:00.269 ERROR Executor:91 - Exception in task 8.0 in stage 1.0 (TID 345); java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:500,Availability,ERROR,ERROR,500,"@TianJin297 commented on [Fri May 26 2017](https://github.com/broadinstitute/gatk-protected/issues/1108). The command is gatk-protected HaplotypeCallerSpark -I XX_BQSRappliedspark.bam -O XX_spark.vcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence GVCF --TMP_DIR tmp. And it is run on an Amazon m4.2xlarge instance. The error messages are like below.; 04:39:06.415 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 05:09:00.269 ERROR Executor:91 - Exception in task 8.0 in stage 1.0 (TID 345); java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:2142,Availability,ERROR,ERROR,2142,"tor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:2231,Availability,ERROR,ERROR,2231,"lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:2265,Availability,Error,Error,2265,"lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:4081,Availability,ERROR,ERROR,4081,nsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:4181,Availability,Error,Error,4181,cala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(Map,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:6135,Availability,down,down,6135,"a:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:6365,Availability,failure,failure,6365,"spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGSch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:6422,Availability,failure,failure,6422,"8); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGSchedul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:3678,Energy Efficiency,schedul,scheduler,3678,.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:3749,Energy Efficiency,schedul,scheduler,3749,ache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:5562,Energy Efficiency,schedul,scheduler,5562,".apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:5633,Energy Efficiency,schedul,scheduler,5633,"ache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7260,Energy Efficiency,schedul,scheduler,7260,"utes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7300,Energy Efficiency,schedul,scheduler,7300,"SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7398,Energy Efficiency,schedul,scheduler,7398,"times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7495,Energy Efficiency,schedul,scheduler,7495,exOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7746,Energy Efficiency,schedul,scheduler,7746,o.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7826,Energy Efficiency,schedul,scheduler,7826,yo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7931,Energy Efficiency,schedul,scheduler,7931,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:8079,Energy Efficiency,schedul,scheduler,8079,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:8167,Energy Efficiency,schedul,scheduler,8167,617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:8264,Energy Efficiency,schedul,scheduler,8264,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:8359,Energy Efficiency,schedul,scheduler,8359,.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:8522,Energy Efficiency,schedul,scheduler,8522,abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.writeVariants(HaplotypeCallerSpark.java:205); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:332,Integrability,message,messages,332,"@TianJin297 commented on [Fri May 26 2017](https://github.com/broadinstitute/gatk-protected/issues/1108). The command is gatk-protected HaplotypeCallerSpark -I XX_BQSRappliedspark.bam -O XX_spark.vcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence GVCF --TMP_DIR tmp. And it is run on an Amazon m4.2xlarge instance. The error messages are like below.; 04:39:06.415 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 05:09:00.269 ERROR Executor:91 - Exception in task 8.0 in stage 1.0 (TID 345); java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:1107,Performance,concurren,concurrent,1107,"mmand is gatk-protected HaplotypeCallerSpark -I XX_BQSRappliedspark.bam -O XX_spark.vcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence GVCF --TMP_DIR tmp. And it is run on an Amazon m4.2xlarge instance. The error messages are like below.; 04:39:06.415 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 05:09:00.269 ERROR Executor:91 - Exception in task 8.0 in stage 1.0 (TID 345); java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:1191,Performance,concurren,concurrent,1191,"vcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence GVCF --TMP_DIR tmp. And it is run on an Amazon m4.2xlarge instance. The error messages are like below.; 04:39:06.415 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 05:09:00.269 ERROR Executor:91 - Exception in task 8.0 in stage 1.0 (TID 345); java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:1932,Performance,concurren,concurrent,1932,"pache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:2016,Performance,concurren,concurrent,2016,"t org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:3871,Performance,concurren,concurrent,3871,che.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:3955,Performance,concurren,concurrent,3955,rk.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:5755,Performance,concurren,concurrent,5755,"che.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:5839,Performance,concurren,concurrent,5839,"rk.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7023,Performance,concurren,concurrent,7023,"ker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Opti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7107,Performance,concurren,concurrent,7107,"CallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:10823,Performance,concurren,concurrent,10823,DDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.writeVariants(HaplotypeCallerSpark.java:205); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCallerSpark.java:115); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:10907,Performance,concurren,concurrent,10907,DDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.writeVariants(HaplotypeCallerSpark.java:205); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCallerSpark.java:115); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:2204,Safety,abort,aborting,2204,"adPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:6344,Safety,abort,aborted,6344,"spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGSch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7430,Safety,abort,abortStage,7430,"lure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7527,Safety,abort,abortStage,7527,: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:7769,Safety,abort,abortStage,7769,.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:673,Usability,clear,clear,673,"@TianJin297 commented on [Fri May 26 2017](https://github.com/broadinstitute/gatk-protected/issues/1108). The command is gatk-protected HaplotypeCallerSpark -I XX_BQSRappliedspark.bam -O XX_spark.vcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence GVCF --TMP_DIR tmp. And it is run on an Amazon m4.2xlarge instance. The error messages are like below.; 04:39:06.415 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 05:09:00.269 ERROR Executor:91 - Exception in task 8.0 in stage 1.0 (TID 345); java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:1498,Usability,clear,clear,1498,"tor:91 - Exception in task 8.0 in stage 1.0 (TID 345); java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:6589,Usability,clear,clear,6589,"scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.Res",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3019:10389,Usability,clear,clear,10389,DDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.writeVariants(HaplotypeCallerSpark.java:205); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCallerSpark.java:115); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019
https://github.com/broadinstitute/gatk/issues/3020:192,Usability,simpl,simple,192,"@davidbenjamin commented on [Sat May 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1110). The traditional approach in Mutect and HaplotypeCaller is to filter reads based on simple properties, such as low mapping quality, lack of mate, etc. This still leaves a lot of fairly bad reads. These probably don't affect calls because they don't support any allele well according to `PairHMM`. Still, they probably waste a lot of time by generating unnecessary paths in the assembly graph. Some possible filters include an excessive number of different CIGAR elements, and too many low-quality bases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3020
https://github.com/broadinstitute/gatk/issues/3022:611,Security,validat,validations,611,"@davidbenjamin commented on [Sat May 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1112). In `SomaticGenotypingEngine::callMutations` and `HaplotypeCallerGenotypingEngine::assignGenotypeLikelihoods` there is a line of code after the call is made but before the variant is annotated:; ```java; ReadLikelihoods annotationLikelihoods = prepareReadAlleleLikelihoodsForAnnotation(likelihoods...); ```; Is this really necessary? It seems quite defensible to annotate using the same likelihoods from which the variant call is derived. As far as Mutect is concerned, the standard will be whether our validations are better or at least no worse without it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3022
https://github.com/broadinstitute/gatk/issues/3023:281,Performance,perform,performance,281,"@davidbenjamin commented on [Sun May 28 2017](https://github.com/broadinstitute/gatk-protected/issues/1113). This sets the minimum base quality to consider a base in the assembly graph. By default it's 10, but in practice anything under 20 is usually junk. We could probably get a performance boost just by running Mutect with a higher value than the default.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3023
https://github.com/broadinstitute/gatk/issues/3024:231,Availability,error,error,231,"@davidbenjamin commented on [Sun May 28 2017](https://github.com/broadinstitute/gatk-protected/issues/1114). HaplotypeCaller and Mutect by default assemble reads with kmer sizes of 10 and 25. 10 seems extremely small given the low error rates of Illumina sequencing. It's worth investigating how the Mutect validations are affected by increasing these values. ---. @ldgauthier commented on [Tue May 30 2017](https://github.com/broadinstitute/gatk-protected/issues/1114#issuecomment-305032024). Investigate away, but keep in mind bigger kmers introduce more ""dangling tails"", which may end up dropping evidence at the ends of reads. If you end up diving into the assembly graphs, I'm happy to consult. It's a deep, dark rabbit hole, but I've been there before and I know the way. ;)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3024
https://github.com/broadinstitute/gatk/issues/3024:307,Security,validat,validations,307,"@davidbenjamin commented on [Sun May 28 2017](https://github.com/broadinstitute/gatk-protected/issues/1114). HaplotypeCaller and Mutect by default assemble reads with kmer sizes of 10 and 25. 10 seems extremely small given the low error rates of Illumina sequencing. It's worth investigating how the Mutect validations are affected by increasing these values. ---. @ldgauthier commented on [Tue May 30 2017](https://github.com/broadinstitute/gatk-protected/issues/1114#issuecomment-305032024). Investigate away, but keep in mind bigger kmers introduce more ""dangling tails"", which may end up dropping evidence at the ends of reads. If you end up diving into the assembly graphs, I'm happy to consult. It's a deep, dark rabbit hole, but I've been there before and I know the way. ;)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3024
https://github.com/broadinstitute/gatk/pull/3027:139,Performance,Perform,PerformAlleleFractionSegmentation,139,parate PRs. ### Here are the twelve tools with the BETA tag:; ```; CountFalsePositives; HaplotypeCaller; Concordance; GetPileupSummaries; PerformAlleleFractionSegmentation; PerformCopyRatioSegmentation; PerformJointSegmentation; TargetCoverageSexGenotyper; GermlineCNVCaller; CreateAllelicPanelOfNormals; HaplotypeCallerSpark; ConvertACNVResults; ```. These include true BETA tools as well as experimental tools. The tag show up as:. ![screenshot 2017-06-05 17 58 09](https://cloud.githubusercontent.com/assets/11543866/26805139/a346eff8-4a18-11e7-873b-4964dbf00aaa.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3027
https://github.com/broadinstitute/gatk/pull/3027:174,Performance,Perform,PerformCopyRatioSegmentation,174,parate PRs. ### Here are the twelve tools with the BETA tag:; ```; CountFalsePositives; HaplotypeCaller; Concordance; GetPileupSummaries; PerformAlleleFractionSegmentation; PerformCopyRatioSegmentation; PerformJointSegmentation; TargetCoverageSexGenotyper; GermlineCNVCaller; CreateAllelicPanelOfNormals; HaplotypeCallerSpark; ConvertACNVResults; ```. These include true BETA tools as well as experimental tools. The tag show up as:. ![screenshot 2017-06-05 17 58 09](https://cloud.githubusercontent.com/assets/11543866/26805139/a346eff8-4a18-11e7-873b-4964dbf00aaa.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3027
https://github.com/broadinstitute/gatk/pull/3027:204,Performance,Perform,PerformJointSegmentation,204,parate PRs. ### Here are the twelve tools with the BETA tag:; ```; CountFalsePositives; HaplotypeCaller; Concordance; GetPileupSummaries; PerformAlleleFractionSegmentation; PerformCopyRatioSegmentation; PerformJointSegmentation; TargetCoverageSexGenotyper; GermlineCNVCaller; CreateAllelicPanelOfNormals; HaplotypeCallerSpark; ConvertACNVResults; ```. These include true BETA tools as well as experimental tools. The tag show up as:. ![screenshot 2017-06-05 17 58 09](https://cloud.githubusercontent.com/assets/11543866/26805139/a346eff8-4a18-11e7-873b-4964dbf00aaa.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3027
https://github.com/broadinstitute/gatk/issues/3029:130,Deployability,integrat,integration,130,"This applies to projects that import the GATK jar as part of the build process, but are not part of the GATK itself. All unit and integration tests are (by default) broken, since the BaseTest class requires the mini fasta, even when it should not be required. This causes breakage, since a project built on the GATK should not be expected to have that file at the exact correct place in the filesystem. The tests do not even start.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3029
https://github.com/broadinstitute/gatk/issues/3029:130,Integrability,integrat,integration,130,"This applies to projects that import the GATK jar as part of the build process, but are not part of the GATK itself. All unit and integration tests are (by default) broken, since the BaseTest class requires the mini fasta, even when it should not be required. This causes breakage, since a project built on the GATK should not be expected to have that file at the exact correct place in the filesystem. The tests do not even start.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3029
https://github.com/broadinstitute/gatk/issues/3029:142,Testability,test,tests,142,"This applies to projects that import the GATK jar as part of the build process, but are not part of the GATK itself. All unit and integration tests are (by default) broken, since the BaseTest class requires the mini fasta, even when it should not be required. This causes breakage, since a project built on the GATK should not be expected to have that file at the exact correct place in the filesystem. The tests do not even start.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3029
https://github.com/broadinstitute/gatk/issues/3029:407,Testability,test,tests,407,"This applies to projects that import the GATK jar as part of the build process, but are not part of the GATK itself. All unit and integration tests are (by default) broken, since the BaseTest class requires the mini fasta, even when it should not be required. This causes breakage, since a project built on the GATK should not be expected to have that file at the exact correct place in the filesystem. The tests do not even start.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3029
https://github.com/broadinstitute/gatk/issues/3030:837,Availability,error,errors,837,"Currently, FilterByOrientation requires replacing a required input's contents with a sed command: . ```; sed -r ""s/picard\.analysis\.artifacts\.SequencingArtifactMetrics\\\$PreAdapterDetailMetrics/org\.broadinstitute\.; hellbender\.tools\.picard\.analysis\.artifacts\.SequencingArtifactMetrics\$PreAdapterDetailMetrics/g"" \; ""metrics.pre_adapter_detail_metrics"" > ""gatk.pre_adapter_detail_metrics""; ```. The required input is the detailed pre-adapter summary metrics from Picard's CollectSequencingArtifactMetrics. The sed command replaces; ```; ## METRICS CLASS	picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```; with ; ```; ## METRICS CLASS	org.broadinstitute.hellbender.tools.picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```. If this class is not replaced, then the tool errors with: ; ```; htsjdk.samtools.SAMException: Could not locate class with name gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadCla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:2436,Availability,error,errors,2436,"seArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:2464,Availability,error,error,2464,"seArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:3981,Availability,error,error,3981,"ncher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```. It does not matter if I produce the pre-adapter metrics with the latest Picard jar v2.9.2. I get the same error. . I'm using a M2 callset from GATK3. Even so, I don't think I should get the above error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:4071,Availability,error,error,4071,"ncher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```. It does not matter if I produce the pre-adapter metrics with the latest Picard jar v2.9.2. I get the same error. . I'm using a M2 callset from GATK3. Even so, I don't think I should get the above error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:443,Energy Efficiency,adapt,adapter,443,"Currently, FilterByOrientation requires replacing a required input's contents with a sed command: . ```; sed -r ""s/picard\.analysis\.artifacts\.SequencingArtifactMetrics\\\$PreAdapterDetailMetrics/org\.broadinstitute\.; hellbender\.tools\.picard\.analysis\.artifacts\.SequencingArtifactMetrics\$PreAdapterDetailMetrics/g"" \; ""metrics.pre_adapter_detail_metrics"" > ""gatk.pre_adapter_detail_metrics""; ```. The required input is the detailed pre-adapter summary metrics from Picard's CollectSequencingArtifactMetrics. The sed command replaces; ```; ## METRICS CLASS	picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```; with ; ```; ## METRICS CLASS	org.broadinstitute.hellbender.tools.picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```. If this class is not replaced, then the tool errors with: ; ```; htsjdk.samtools.SAMException: Could not locate class with name gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadCla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:3915,Energy Efficiency,adapt,adapter,3915,"ncher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```. It does not matter if I produce the pre-adapter metrics with the latest Picard jar v2.9.2. I get the same error. . I'm using a M2 callset from GATK3. Even so, I don't think I should get the above error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:443,Integrability,adapter,adapter,443,"Currently, FilterByOrientation requires replacing a required input's contents with a sed command: . ```; sed -r ""s/picard\.analysis\.artifacts\.SequencingArtifactMetrics\\\$PreAdapterDetailMetrics/org\.broadinstitute\.; hellbender\.tools\.picard\.analysis\.artifacts\.SequencingArtifactMetrics\$PreAdapterDetailMetrics/g"" \; ""metrics.pre_adapter_detail_metrics"" > ""gatk.pre_adapter_detail_metrics""; ```. The required input is the detailed pre-adapter summary metrics from Picard's CollectSequencingArtifactMetrics. The sed command replaces; ```; ## METRICS CLASS	picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```; with ; ```; ## METRICS CLASS	org.broadinstitute.hellbender.tools.picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```. If this class is not replaced, then the tool errors with: ; ```; htsjdk.samtools.SAMException: Could not locate class with name gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadCla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:3915,Integrability,adapter,adapter,3915,"ncher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```. It does not matter if I produce the pre-adapter metrics with the latest Picard jar v2.9.2. I get the same error. . I'm using a M2 callset from GATK3. Even so, I don't think I should get the above error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:443,Modifiability,adapt,adapter,443,"Currently, FilterByOrientation requires replacing a required input's contents with a sed command: . ```; sed -r ""s/picard\.analysis\.artifacts\.SequencingArtifactMetrics\\\$PreAdapterDetailMetrics/org\.broadinstitute\.; hellbender\.tools\.picard\.analysis\.artifacts\.SequencingArtifactMetrics\$PreAdapterDetailMetrics/g"" \; ""metrics.pre_adapter_detail_metrics"" > ""gatk.pre_adapter_detail_metrics""; ```. The required input is the detailed pre-adapter summary metrics from Picard's CollectSequencingArtifactMetrics. The sed command replaces; ```; ## METRICS CLASS	picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```; with ; ```; ## METRICS CLASS	org.broadinstitute.hellbender.tools.picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```. If this class is not replaced, then the tool errors with: ; ```; htsjdk.samtools.SAMException: Could not locate class with name gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadCla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:3915,Modifiability,adapt,adapter,3915,"ncher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```. It does not matter if I produce the pre-adapter metrics with the latest Picard jar v2.9.2. I get the same error. . I'm using a M2 callset from GATK3. Even so, I don't think I should get the above error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:1994,Performance,load,loadClass,1994,".samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:2064,Performance,load,loadClass,2064,"oadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:2120,Performance,load,loadClass,2120,"s.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/issues/3030:2283,Performance,load,loadClass,2283,"r.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030
https://github.com/broadinstitute/gatk/pull/3031:72,Availability,error,errors,72,"Please see <https://github.com/broadinstitute/gatk/issues/3030> for the errors I encountered when running this tool. I have not been able to run the tool successfully and so ask what are we missing in the documentation that will help users get this tool running. Otherwise, I recommend labeling this tool experimental as well as in BETA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3031
https://github.com/broadinstitute/gatk/pull/3032:40,Usability,clear,clear,40,"### Question for @davidbenjamin ; - Not clear if SplitIntervals actually divides by the count of intervals or by genomic territory covered. Specifically, what is the difference between --subdivision_mode INTERVAL_SUBDIVISION and BALANCING_WITHOUT_INTERVAL_SUBDIVISION? The argument description is unclear.; - What is the point of having the default scatter count 1? For future reference, can we change this default.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3032
https://github.com/broadinstitute/gatk/pull/3034:124,Performance,cache,caches,124,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034
https://github.com/broadinstitute/gatk/pull/3034:143,Performance,cache,cache,143,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034
https://github.com/broadinstitute/gatk/pull/3034:607,Performance,Cache,CacheNode,607,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034
https://github.com/broadinstitute/gatk/pull/3034:30,Security,access,access,30,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034
https://github.com/broadinstitute/gatk/pull/3034:450,Testability,assert,asserts,450,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034
https://github.com/broadinstitute/gatk/pull/3034:478,Testability,test,tests,478,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034
https://github.com/broadinstitute/gatk/pull/3034:521,Testability,test,tests,521,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034
https://github.com/broadinstitute/gatk/pull/3034:564,Testability,test,tests,564,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034
https://github.com/broadinstitute/gatk/pull/3035:124,Performance,cache,caches,124,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035
https://github.com/broadinstitute/gatk/pull/3035:143,Performance,cache,cache,143,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035
https://github.com/broadinstitute/gatk/pull/3035:30,Security,access,access,30,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035
https://github.com/broadinstitute/gatk/pull/3035:450,Testability,assert,asserts,450,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035
https://github.com/broadinstitute/gatk/pull/3035:478,Testability,test,tests,478,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035
https://github.com/broadinstitute/gatk/pull/3035:521,Testability,test,tests,521,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035
https://github.com/broadinstitute/gatk/pull/3035:564,Testability,test,tests,564,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035
https://github.com/broadinstitute/gatk/pull/3036:778,Performance,perform,performs,778,"Migrated from https://github.com/broadinstitute/gatk-protected/pull/962. Changes by @davidbenjamin; -changes to docs; -got rid of weights and concentration; -got rid of constant states to simplify before CRP pre-training; -smarter transition matrix; -switched to binomial AF likelihoods for segmentation; -got rid of attempt big change in memory length; -fixed outlier likelihood; Changes by @samuelklee; -ACNV with joint segmentation; -tweaked convergence criteria and removed extraneous MCMC fit; -sorted acc in AF segmentation; -NaN fixes in binomial likelihood; -fixed some tests and added EXPERIMENTAL tags; -disabled JointAFCRSegmenterUnitTest. This introduces a new command line (AllelicCNVHMM---@sooheelee, this command line is experimental and should not be used) that performs joint segmentation and then fits model parameters using MCMC. It performs relatively well on some samples (and was used to generate results for the AACR poster), but others result in oversegmentation and convergence issues. It's possible that this could be due to the naive copy-ratio model used. @davidbenjamin may want to do some additional tweaking, but I think we will also explore other iHMM variants concurrently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036
https://github.com/broadinstitute/gatk/pull/3036:852,Performance,perform,performs,852,"Migrated from https://github.com/broadinstitute/gatk-protected/pull/962. Changes by @davidbenjamin; -changes to docs; -got rid of weights and concentration; -got rid of constant states to simplify before CRP pre-training; -smarter transition matrix; -switched to binomial AF likelihoods for segmentation; -got rid of attempt big change in memory length; -fixed outlier likelihood; Changes by @samuelklee; -ACNV with joint segmentation; -tweaked convergence criteria and removed extraneous MCMC fit; -sorted acc in AF segmentation; -NaN fixes in binomial likelihood; -fixed some tests and added EXPERIMENTAL tags; -disabled JointAFCRSegmenterUnitTest. This introduces a new command line (AllelicCNVHMM---@sooheelee, this command line is experimental and should not be used) that performs joint segmentation and then fits model parameters using MCMC. It performs relatively well on some samples (and was used to generate results for the AACR poster), but others result in oversegmentation and convergence issues. It's possible that this could be due to the naive copy-ratio model used. @davidbenjamin may want to do some additional tweaking, but I think we will also explore other iHMM variants concurrently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036
https://github.com/broadinstitute/gatk/pull/3036:1193,Performance,concurren,concurrently,1193,"Migrated from https://github.com/broadinstitute/gatk-protected/pull/962. Changes by @davidbenjamin; -changes to docs; -got rid of weights and concentration; -got rid of constant states to simplify before CRP pre-training; -smarter transition matrix; -switched to binomial AF likelihoods for segmentation; -got rid of attempt big change in memory length; -fixed outlier likelihood; Changes by @samuelklee; -ACNV with joint segmentation; -tweaked convergence criteria and removed extraneous MCMC fit; -sorted acc in AF segmentation; -NaN fixes in binomial likelihood; -fixed some tests and added EXPERIMENTAL tags; -disabled JointAFCRSegmenterUnitTest. This introduces a new command line (AllelicCNVHMM---@sooheelee, this command line is experimental and should not be used) that performs joint segmentation and then fits model parameters using MCMC. It performs relatively well on some samples (and was used to generate results for the AACR poster), but others result in oversegmentation and convergence issues. It's possible that this could be due to the naive copy-ratio model used. @davidbenjamin may want to do some additional tweaking, but I think we will also explore other iHMM variants concurrently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036
https://github.com/broadinstitute/gatk/pull/3036:578,Testability,test,tests,578,"Migrated from https://github.com/broadinstitute/gatk-protected/pull/962. Changes by @davidbenjamin; -changes to docs; -got rid of weights and concentration; -got rid of constant states to simplify before CRP pre-training; -smarter transition matrix; -switched to binomial AF likelihoods for segmentation; -got rid of attempt big change in memory length; -fixed outlier likelihood; Changes by @samuelklee; -ACNV with joint segmentation; -tweaked convergence criteria and removed extraneous MCMC fit; -sorted acc in AF segmentation; -NaN fixes in binomial likelihood; -fixed some tests and added EXPERIMENTAL tags; -disabled JointAFCRSegmenterUnitTest. This introduces a new command line (AllelicCNVHMM---@sooheelee, this command line is experimental and should not be used) that performs joint segmentation and then fits model parameters using MCMC. It performs relatively well on some samples (and was used to generate results for the AACR poster), but others result in oversegmentation and convergence issues. It's possible that this could be due to the naive copy-ratio model used. @davidbenjamin may want to do some additional tweaking, but I think we will also explore other iHMM variants concurrently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036
https://github.com/broadinstitute/gatk/pull/3036:188,Usability,simpl,simplify,188,"Migrated from https://github.com/broadinstitute/gatk-protected/pull/962. Changes by @davidbenjamin; -changes to docs; -got rid of weights and concentration; -got rid of constant states to simplify before CRP pre-training; -smarter transition matrix; -switched to binomial AF likelihoods for segmentation; -got rid of attempt big change in memory length; -fixed outlier likelihood; Changes by @samuelklee; -ACNV with joint segmentation; -tweaked convergence criteria and removed extraneous MCMC fit; -sorted acc in AF segmentation; -NaN fixes in binomial likelihood; -fixed some tests and added EXPERIMENTAL tags; -disabled JointAFCRSegmenterUnitTest. This introduces a new command line (AllelicCNVHMM---@sooheelee, this command line is experimental and should not be used) that performs joint segmentation and then fits model parameters using MCMC. It performs relatively well on some samples (and was used to generate results for the AACR poster), but others result in oversegmentation and convergence issues. It's possible that this could be due to the naive copy-ratio model used. @davidbenjamin may want to do some additional tweaking, but I think we will also explore other iHMM variants concurrently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036
https://github.com/broadinstitute/gatk/issues/3038:267,Availability,error,errors,267,"We've discovered a confusing git-lfs bug that effects people who have a git fork of gatk that started before the gatk/gatk-protected merge. . The symptom is that if you try to update your fork from the gatk master branch, and then push to your master, you'll see git errors like this:. ```; $ git push; open/Users/louisb/tmp/gatk-1/src/test/resources/large-protected/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx: no such file or directory; ```. This will prevent you from updating your master. The solution is to checkout the commit `32a2b39` which will manually trigger a git-lfs download of 1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx. Then checkout master again and push. @magicDGS This probably affects you. . We're going to open a git-lfs issue since it seems to be a pretty serious git-lfs bug, I'll update this ticket with more information when we have it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3038
https://github.com/broadinstitute/gatk/issues/3038:592,Availability,down,download,592,"We've discovered a confusing git-lfs bug that effects people who have a git fork of gatk that started before the gatk/gatk-protected merge. . The symptom is that if you try to update your fork from the gatk master branch, and then push to your master, you'll see git errors like this:. ```; $ git push; open/Users/louisb/tmp/gatk-1/src/test/resources/large-protected/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx: no such file or directory; ```. This will prevent you from updating your master. The solution is to checkout the commit `32a2b39` which will manually trigger a git-lfs download of 1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx. Then checkout master again and push. @magicDGS This probably affects you. . We're going to open a git-lfs issue since it seems to be a pretty serious git-lfs bug, I'll update this ticket with more information when we have it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3038
https://github.com/broadinstitute/gatk/issues/3038:176,Deployability,update,update,176,"We've discovered a confusing git-lfs bug that effects people who have a git fork of gatk that started before the gatk/gatk-protected merge. . The symptom is that if you try to update your fork from the gatk master branch, and then push to your master, you'll see git errors like this:. ```; $ git push; open/Users/louisb/tmp/gatk-1/src/test/resources/large-protected/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx: no such file or directory; ```. This will prevent you from updating your master. The solution is to checkout the commit `32a2b39` which will manually trigger a git-lfs download of 1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx. Then checkout master again and push. @magicDGS This probably affects you. . We're going to open a git-lfs issue since it seems to be a pretty serious git-lfs bug, I'll update this ticket with more information when we have it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3038
https://github.com/broadinstitute/gatk/issues/3038:829,Deployability,update,update,829,"We've discovered a confusing git-lfs bug that effects people who have a git fork of gatk that started before the gatk/gatk-protected merge. . The symptom is that if you try to update your fork from the gatk master branch, and then push to your master, you'll see git errors like this:. ```; $ git push; open/Users/louisb/tmp/gatk-1/src/test/resources/large-protected/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx: no such file or directory; ```. This will prevent you from updating your master. The solution is to checkout the commit `32a2b39` which will manually trigger a git-lfs download of 1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx. Then checkout master again and push. @magicDGS This probably affects you. . We're going to open a git-lfs issue since it seems to be a pretty serious git-lfs bug, I'll update this ticket with more information when we have it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3038
https://github.com/broadinstitute/gatk/issues/3038:336,Testability,test,test,336,"We've discovered a confusing git-lfs bug that effects people who have a git fork of gatk that started before the gatk/gatk-protected merge. . The symptom is that if you try to update your fork from the gatk master branch, and then push to your master, you'll see git errors like this:. ```; $ git push; open/Users/louisb/tmp/gatk-1/src/test/resources/large-protected/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx: no such file or directory; ```. This will prevent you from updating your master. The solution is to checkout the commit `32a2b39` which will manually trigger a git-lfs download of 1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx. Then checkout master again and push. @magicDGS This probably affects you. . We're going to open a git-lfs issue since it seems to be a pretty serious git-lfs bug, I'll update this ticket with more information when we have it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3038
https://github.com/broadinstitute/gatk/issues/3039:10,Deployability,update,update,10,"We should update all command lines in the docs to use `gatk-launch`. Running with `java -jar` and bypassing `gatk-launch` causes several important system properties to not get set, including htsjdk compression level.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3039
https://github.com/broadinstitute/gatk/pull/3043:203,Deployability,install,install,203,this should save a non-trivial amount of time in the docker builds; updating from 1.0 -> 1.1 which includes the necessary RScripts; moving scripts/install_R_packages.R -> scripts/docker/gatkbase/. don't install Rscripts during docker builds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3043
https://github.com/broadinstitute/gatk/pull/3044:72,Integrability,interface,interfaces,72,Along with minor changes needed to track the moved classes; and changed interfaces. This is related to issue #2822,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3044
https://github.com/broadinstitute/gatk/pull/3045:46,Deployability,update,updated,46,Added some websites with tool information and updated the command,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3045
https://github.com/broadinstitute/gatk/issues/3047:30,Integrability,depend,dependencies,30,We have a number of R package dependencies and it's not clear if we actually need all of them or if some of them are there for historical reasons. We should review them and identify which we actually need.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3047
https://github.com/broadinstitute/gatk/issues/3047:56,Usability,clear,clear,56,We have a number of R package dependencies and it's not clear if we actually need all of them or if some of them are there for historical reasons. We should review them and identify which we actually need.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3047
https://github.com/broadinstitute/gatk/issues/3050:347,Availability,down,downloading,347,"I met this exception when I was trying to run spark commands on a standalone Spark server. I searched for possible causes, and I found two results. One is the spark lacks some jar files to handle the file system. The other says the version of spark on the server is not the same as the one codes are compiled with. ; So for the first one, I tried downloading jars from Zookeeper, Hive and Hbase, and implemented them as said in ""https://stackoverflow.com/questions/34901331/spark-hbase-error-java-lang-illegalstateexception-unread-block-data"", but it doesn't really change anything. ; And for the other one, I tried spark-2.0.0-hadoop-2.6, spark-2.0.0-hadoop-2.7, spark-2.1.1-hadoop-2.7 and spark-1.6.1-hadoop-2.6. But none of them changed the error message. **So I want to ask what version of Spark should I use actually?**. And I will put the error message here:; Using GATK jar /curr/tianj/gatk/build/libs/gatk-spark.jar; Running:; ```; spark-submit --master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:486,Availability,error,error-java-lang-illegalstateexception-unread-block-data,486,"I met this exception when I was trying to run spark commands on a standalone Spark server. I searched for possible causes, and I found two results. One is the spark lacks some jar files to handle the file system. The other says the version of spark on the server is not the same as the one codes are compiled with. ; So for the first one, I tried downloading jars from Zookeeper, Hive and Hbase, and implemented them as said in ""https://stackoverflow.com/questions/34901331/spark-hbase-error-java-lang-illegalstateexception-unread-block-data"", but it doesn't really change anything. ; And for the other one, I tried spark-2.0.0-hadoop-2.6, spark-2.0.0-hadoop-2.7, spark-2.1.1-hadoop-2.7 and spark-1.6.1-hadoop-2.6. But none of them changed the error message. **So I want to ask what version of Spark should I use actually?**. And I will put the error message here:; Using GATK jar /curr/tianj/gatk/build/libs/gatk-spark.jar; Running:; ```; spark-submit --master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:744,Availability,error,error,744,"I met this exception when I was trying to run spark commands on a standalone Spark server. I searched for possible causes, and I found two results. One is the spark lacks some jar files to handle the file system. The other says the version of spark on the server is not the same as the one codes are compiled with. ; So for the first one, I tried downloading jars from Zookeeper, Hive and Hbase, and implemented them as said in ""https://stackoverflow.com/questions/34901331/spark-hbase-error-java-lang-illegalstateexception-unread-block-data"", but it doesn't really change anything. ; And for the other one, I tried spark-2.0.0-hadoop-2.6, spark-2.0.0-hadoop-2.7, spark-2.1.1-hadoop-2.7 and spark-1.6.1-hadoop-2.6. But none of them changed the error message. **So I want to ask what version of Spark should I use actually?**. And I will put the error message here:; Using GATK jar /curr/tianj/gatk/build/libs/gatk-spark.jar; Running:; ```; spark-submit --master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:845,Availability,error,error,845,"I met this exception when I was trying to run spark commands on a standalone Spark server. I searched for possible causes, and I found two results. One is the spark lacks some jar files to handle the file system. The other says the version of spark on the server is not the same as the one codes are compiled with. ; So for the first one, I tried downloading jars from Zookeeper, Hive and Hbase, and implemented them as said in ""https://stackoverflow.com/questions/34901331/spark-hbase-error-java-lang-illegalstateexception-unread-block-data"", but it doesn't really change anything. ; And for the other one, I tried spark-2.0.0-hadoop-2.6, spark-2.0.0-hadoop-2.7, spark-2.1.1-hadoop-2.7 and spark-1.6.1-hadoop-2.6. But none of them changed the error message. **So I want to ask what version of Spark should I use actually?**. And I will put the error message here:; Using GATK jar /curr/tianj/gatk/build/libs/gatk-spark.jar; Running:; ```; spark-submit --master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:3737,Availability,ERROR,ERROR,3737,"ut false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.total",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:3855,Availability,ERROR,ERROR,3855,"alse --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:3920,Availability,ERROR,ERROR,3920," 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4000,Availability,ERROR,ERROR,4000,"n Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4121,Availability,ERROR,ERROR,4121," - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4532,Availability,down,down,4532,"r; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.seri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4791,Availability,failure,failure,4791,"ct is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4849,Availability,failure,failure,4849,"e.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:9835,Deployability,deploy,deploy,9835,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:9872,Deployability,deploy,deploy,9872,Tool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:9944,Deployability,deploy,deploy,9944,arkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstanc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:10020,Deployability,deploy,deploy,10020,nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:10091,Deployability,deploy,deploy,10091,am.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:10160,Deployability,deploy,deploy,10160,gram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.conc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6023,Energy Efficiency,schedul,scheduler,6023,BlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6063,Energy Efficiency,schedul,scheduler,6063,ctInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6161,Energy Efficiency,schedul,scheduler,6161,tream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6258,Energy Efficiency,schedul,scheduler,6258,(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6509,Energy Efficiency,schedul,scheduler,6509, org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6589,Energy Efficiency,schedul,scheduler,6589,.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6694,Energy Efficiency,schedul,scheduler,6694,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6842,Energy Efficiency,schedul,scheduler,6842,va:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6930,Energy Efficiency,schedul,scheduler,6930,615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:7027,Energy Efficiency,schedul,scheduler,7027,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:7122,Energy Efficiency,schedul,scheduler,7122,.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:748); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:7285,Energy Efficiency,schedul,scheduler,7285,abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:748); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:747); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:750,Integrability,message,message,750,"I met this exception when I was trying to run spark commands on a standalone Spark server. I searched for possible causes, and I found two results. One is the spark lacks some jar files to handle the file system. The other says the version of spark on the server is not the same as the one codes are compiled with. ; So for the first one, I tried downloading jars from Zookeeper, Hive and Hbase, and implemented them as said in ""https://stackoverflow.com/questions/34901331/spark-hbase-error-java-lang-illegalstateexception-unread-block-data"", but it doesn't really change anything. ; And for the other one, I tried spark-2.0.0-hadoop-2.6, spark-2.0.0-hadoop-2.7, spark-2.1.1-hadoop-2.7 and spark-1.6.1-hadoop-2.6. But none of them changed the error message. **So I want to ask what version of Spark should I use actually?**. And I will put the error message here:; Using GATK jar /curr/tianj/gatk/build/libs/gatk-spark.jar; Running:; ```; spark-submit --master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:851,Integrability,message,message,851,"I met this exception when I was trying to run spark commands on a standalone Spark server. I searched for possible causes, and I found two results. One is the spark lacks some jar files to handle the file system. The other says the version of spark on the server is not the same as the one codes are compiled with. ; So for the first one, I tried downloading jars from Zookeeper, Hive and Hbase, and implemented them as said in ""https://stackoverflow.com/questions/34901331/spark-hbase-error-java-lang-illegalstateexception-unread-block-data"", but it doesn't really change anything. ; And for the other one, I tried spark-2.0.0-hadoop-2.6, spark-2.0.0-hadoop-2.7, spark-2.1.1-hadoop-2.7 and spark-1.6.1-hadoop-2.6. But none of them changed the error message. **So I want to ask what version of Spark should I use actually?**. And I will put the error message here:; Using GATK jar /curr/tianj/gatk/build/libs/gatk-spark.jar; Running:; ```; spark-submit --master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:3838,Modifiability,variab,variable,3838,"lse --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:1955,Performance,Load,Loading,1955,"--master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 7, 2017 12:48:13 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark --output xx_markduplicatespark.bam --METRICS_FILE xx.m --input /curr/tianj/data/sortedbam/xx_sort.bam --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx --TMP_DIR tmp --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --READ_NAME_REGEX [a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:3903,Performance,load,loaded,3903," 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4045,Performance,load,loaded,4045,"er VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:5786,Performance,concurren,concurrent,5786,"failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Opti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:5870,Performance,concurren,concurrent,5870," stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:11076,Performance,concurren,concurrent,11076,Program.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:11160,Performance,concurren,concurrent,11160,Program.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4770,Safety,abort,aborted,4770,"ct is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6193,Safety,abort,abortStage,6193,s(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6290,Safety,abort,abortStage,6290,a:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:6532,Safety,abort,abortStage,6532,avaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4300,Testability,log,logger,4300,"e; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3050:4420,Testability,log,logging,4420,"ark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050
https://github.com/broadinstitute/gatk/issues/3055:14025,Deployability,a/b,a/broadinstitute,14025,r the second pass.; 44	GetPileupSummaries	beta; helper tool for CalculateContamination	6/5/2017	https://github.com/broadinstitute/gatk-protected/blob/2bf35790393332da5414b42ec6dca813fcc63202/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/GetPileupSummaries.java	scripts/mutect2_wdl/mutect2.wdl	https://github.com/broadinstitute/gatk/pull/3006	yes	; 33	AnnotateVcfWithBamDepth	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithBamDepth.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 34	AnnotateVcfWithExpectedAlleleFraction	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithExpectedAlleleFraction.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 37	CalculateMixingFractions	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/CalculateMixingFractions.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 47	RemoveNearbyIndels	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/RemoveNearbyIndels.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes		; ```. This is also at <https://docs.google.com/a/broadinstitute.org/spreadsheets/d/15xviLwYUjU82MtYwxxPINiJAkovmaHpRGhqkghiEATQ/edit?usp=sharing>.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:2030,Performance,Perform,PerformSegmentation,2030, 7	CombineReadCounts		19-May	https://github.com/broadinstitute/gatk-protected/blob/3e6142ad4eb23d4d9227fafd8e52b498263b4369/src/main/java/org/broadinstitute/hellbender/tools/exome/CombineReadCounts.java	scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1072	yes	; 13	CreatePanelOfNormals		19-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/CreatePanelOfNormals.java	scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1073	yes	; 20	NormalizeSomaticReadCounts		19-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/NormalizeSomaticReadCounts.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1074	yes	; 25	PerformSegmentation		19-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/PerformSegmentation.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1075	yes	; 27	PlotSegmentedCopyRatio		21-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/plotting/PlotSegmentedCopyRatio.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1077	yes	; 5	CallSegments		21-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CallSegments.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:2209,Performance,Perform,PerformSegmentation,2209,c/main/java/org/broadinstitute/hellbender/tools/exome/CombineReadCounts.java	scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1072	yes	; 13	CreatePanelOfNormals		19-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/CreatePanelOfNormals.java	scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1073	yes	; 20	NormalizeSomaticReadCounts		19-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/NormalizeSomaticReadCounts.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1074	yes	; 25	PerformSegmentation		19-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/PerformSegmentation.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1075	yes	; 27	PlotSegmentedCopyRatio		21-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/plotting/PlotSegmentedCopyRatio.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1077	yes	; 5	CallSegments		21-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CallSegments.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1078	yes	; 2	AnnotateTargets		21-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:4647,Performance,Perform,PerformAlleleFractionSegmentation,4647,"titute/gatk-protected/pull/1080	yes	; 9	ConvertBedToTargetFile		done	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/convertbed/ConvertBedToTargetFile.java	no but mentioned in scripts/cnv_wdl/somatic/README.md	https://github.com/broadinstitute/gatk-protected/pull/1082	yes	; 21	PadTargets		done	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/PadTargets.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1093	yes	; 11	CorrectGCBias		5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/gcbias/CorrectGCBias.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1118	yes	; 22	PerformAlleleFractionSegmentation	yes	5/30/2017, 6/3/2017	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformAlleleFractionSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1120	yes, but	https://github.com/broadinstitute/gatk/pull/2811; 23	PerformCopyRatioSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformCopyRatioSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1121	yes	no example command; 24	PerformJointSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformJointSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1122	yes	; 26	P",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:4869,Performance,Perform,PerformAlleleFractionSegmentation,4869,"d9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/convertbed/ConvertBedToTargetFile.java	no but mentioned in scripts/cnv_wdl/somatic/README.md	https://github.com/broadinstitute/gatk-protected/pull/1082	yes	; 21	PadTargets		done	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/PadTargets.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1093	yes	; 11	CorrectGCBias		5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/gcbias/CorrectGCBias.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1118	yes	; 22	PerformAlleleFractionSegmentation	yes	5/30/2017, 6/3/2017	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformAlleleFractionSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1120	yes, but	https://github.com/broadinstitute/gatk/pull/2811; 23	PerformCopyRatioSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformCopyRatioSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1121	yes	no example command; 24	PerformJointSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformJointSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1122	yes	; 26	PlotACNVResults		5/30	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:5032,Performance,Perform,PerformCopyRatioSegmentation,5032,"adTargets		done	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/PadTargets.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1093	yes	; 11	CorrectGCBias		5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/gcbias/CorrectGCBias.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1118	yes	; 22	PerformAlleleFractionSegmentation	yes	5/30/2017, 6/3/2017	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformAlleleFractionSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1120	yes, but	https://github.com/broadinstitute/gatk/pull/2811; 23	PerformCopyRatioSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformCopyRatioSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1121	yes	no example command; 24	PerformJointSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformJointSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1122	yes	; 26	PlotACNVResults		5/30	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/plotting/PlotACNVResults.java	scripts/cnv_wdl/somatic/cnv_somatic_allele_fraction_pair_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1123	yes	needs review in 2nd round; 18	GetBayesianHetCove",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:5234,Performance,Perform,PerformCopyRatioSegmentation,5234,"nstitute/hellbender/tools/exome/PadTargets.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1093	yes	; 11	CorrectGCBias		5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/gcbias/CorrectGCBias.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1118	yes	; 22	PerformAlleleFractionSegmentation	yes	5/30/2017, 6/3/2017	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformAlleleFractionSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1120	yes, but	https://github.com/broadinstitute/gatk/pull/2811; 23	PerformCopyRatioSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformCopyRatioSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1121	yes	no example command; 24	PerformJointSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformJointSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1122	yes	; 26	PlotACNVResults		5/30	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/plotting/PlotACNVResults.java	scripts/cnv_wdl/somatic/cnv_somatic_allele_fraction_pair_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1123	yes	needs review in 2nd round; 18	GetBayesianHetCoverage		5/30	https://github.com/broadinstitute/gatk-protected/blob/91336c9aefb077d1dc7daf7aaae3a8dc3e007ffe/src/main/java/org/broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:5357,Performance,Perform,PerformJointSegmentation,5357,"ias		5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/gcbias/CorrectGCBias.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1118	yes	; 22	PerformAlleleFractionSegmentation	yes	5/30/2017, 6/3/2017	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformAlleleFractionSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1120	yes, but	https://github.com/broadinstitute/gatk/pull/2811; 23	PerformCopyRatioSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformCopyRatioSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1121	yes	no example command; 24	PerformJointSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformJointSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1122	yes	; 26	PlotACNVResults		5/30	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/plotting/PlotACNVResults.java	scripts/cnv_wdl/somatic/cnv_somatic_allele_fraction_pair_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1123	yes	needs review in 2nd round; 18	GetBayesianHetCoverage		5/30	https://github.com/broadinstitute/gatk-protected/blob/91336c9aefb077d1dc7daf7aaae3a8dc3e007ffe/src/main/java/org/broadinstitute/hellbender/tools/exome/GetBayesianHetCoverage.java	scripts/cnv_wdl/somatic/cnv_somatic_allele_fraction_pair_workflow.wdl	https://github.com/broadinstitute/gatk-protect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:5555,Performance,Perform,PerformJointSegmentation,5555,"r/tools/exome/gcbias/CorrectGCBias.java	scripts/cnv_wdl/somatic/cnv_somatic_tasks.wdl	https://github.com/broadinstitute/gatk-protected/pull/1118	yes	; 22	PerformAlleleFractionSegmentation	yes	5/30/2017, 6/3/2017	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformAlleleFractionSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1120	yes, but	https://github.com/broadinstitute/gatk/pull/2811; 23	PerformCopyRatioSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformCopyRatioSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1121	yes	no example command; 24	PerformJointSegmentation	yes	5/30	https://github.com/broadinstitute/gatk-protected/blob/087505afd217d32589cda152ac9ee4e7a7061572/src/main/java/org/broadinstitute/hellbender/tools/exome/segmentation/PerformJointSegmentation.java	no	https://github.com/broadinstitute/gatk-protected/pull/1122	yes	; 26	PlotACNVResults		5/30	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/plotting/PlotACNVResults.java	scripts/cnv_wdl/somatic/cnv_somatic_allele_fraction_pair_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1123	yes	needs review in 2nd round; 18	GetBayesianHetCoverage		5/30	https://github.com/broadinstitute/gatk-protected/blob/91336c9aefb077d1dc7daf7aaae3a8dc3e007ffe/src/main/java/org/broadinstitute/hellbender/tools/exome/GetBayesianHetCoverage.java	scripts/cnv_wdl/somatic/cnv_somatic_allele_fraction_pair_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1125	yes	https://github.com/broadinstitute/gatk/pull/2812; 19	GetHetCoverage		5/30	https://github.com/broadinstitute/gatk-protected/blob/91",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:9206,Safety,Detect,DetectCoverageDropout,9206,alls.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 10	ConvertGSVariantsToSegments	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/eval/ConvertGSVariantsToSegments.java	no	https://github.com/broadinstitute/gatk-protected/pull/1117	yes	needs review in 2nd round; 30	XHMMSegmentCaller	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/3e6142ad4eb23d4d9227fafd8e52b498263b4369/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentCaller.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 31	XHMMSegmentGenotyper	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentGenotyper.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 15	DetectCoverageDropout	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/DetectCoverageDropout.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		1; 14	DecomposeSingularValues	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/DecomposeSingularValues.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		; 3	CalculatePulldownPhasePosteriors	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CalculatePulldownPhasePosteriors.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130	; 46	Mutect2		6/4/2017	https://github.com/broadinstitute/gatk-protected/blob/91336c9aefb077d1dc7daf7aaae3a8dc3e007,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:9392,Safety,detect,detectcoveragedropout,9392,broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/eval/ConvertGSVariantsToSegments.java	no	https://github.com/broadinstitute/gatk-protected/pull/1117	yes	needs review in 2nd round; 30	XHMMSegmentCaller	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/3e6142ad4eb23d4d9227fafd8e52b498263b4369/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentCaller.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 31	XHMMSegmentGenotyper	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentGenotyper.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 15	DetectCoverageDropout	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/DetectCoverageDropout.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		1; 14	DecomposeSingularValues	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/DecomposeSingularValues.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		; 3	CalculatePulldownPhasePosteriors	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CalculatePulldownPhasePosteriors.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130	; 46	Mutect2		6/4/2017	https://github.com/broadinstitute/gatk-protected/blob/91336c9aefb077d1dc7daf7aaae3a8dc3e007ffe/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java	scripts/mutect2_wdl/mutect2_multi_sample.wdl and more	https,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:9414,Safety,Detect,DetectCoverageDropout,9414,broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/eval/ConvertGSVariantsToSegments.java	no	https://github.com/broadinstitute/gatk-protected/pull/1117	yes	needs review in 2nd round; 30	XHMMSegmentCaller	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/3e6142ad4eb23d4d9227fafd8e52b498263b4369/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentCaller.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 31	XHMMSegmentGenotyper	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentGenotyper.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 15	DetectCoverageDropout	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/DetectCoverageDropout.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		1; 14	DecomposeSingularValues	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/DecomposeSingularValues.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		; 3	CalculatePulldownPhasePosteriors	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CalculatePulldownPhasePosteriors.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130	; 46	Mutect2		6/4/2017	https://github.com/broadinstitute/gatk-protected/blob/91336c9aefb077d1dc7daf7aaae3a8dc3e007ffe/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java	scripts/mutect2_wdl/mutect2_multi_sample.wdl and more	https,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:12697,Security,validat,validation,12697,litIntervals		6/5/2017	https://github.com/broadinstitute/gatk-protected/blob/ec40da398e4185fa8fb0c62453304e8315f8f4e1/src/main/java/org/broadinstitute/hellbender/tools/walkers/SplitIntervals.java	scripts/mutect2_wdl/mutect2.wdl	https://github.com/broadinstitute/gatk/pull/3032	yes	Default value: INTERVAL_SUBDIVISION. warn users to be careful when dividing lengthy genomic intervals. Perhaps it would be wise to specify the workflow in which this tool would be used. Something for the second pass.; 44	GetPileupSummaries	beta; helper tool for CalculateContamination	6/5/2017	https://github.com/broadinstitute/gatk-protected/blob/2bf35790393332da5414b42ec6dca813fcc63202/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/GetPileupSummaries.java	scripts/mutect2_wdl/mutect2.wdl	https://github.com/broadinstitute/gatk/pull/3006	yes	; 33	AnnotateVcfWithBamDepth	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithBamDepth.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 34	AnnotateVcfWithExpectedAlleleFraction	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithExpectedAlleleFraction.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 37	CalculateMixingFractions	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/CalculateMixingFractions.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 47	RemoveNearbyIndels	i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:13076,Security,validat,validation,13076,rhaps it would be wise to specify the workflow in which this tool would be used. Something for the second pass.; 44	GetPileupSummaries	beta; helper tool for CalculateContamination	6/5/2017	https://github.com/broadinstitute/gatk-protected/blob/2bf35790393332da5414b42ec6dca813fcc63202/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/GetPileupSummaries.java	scripts/mutect2_wdl/mutect2.wdl	https://github.com/broadinstitute/gatk/pull/3006	yes	; 33	AnnotateVcfWithBamDepth	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithBamDepth.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 34	AnnotateVcfWithExpectedAlleleFraction	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithExpectedAlleleFraction.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 37	CalculateMixingFractions	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/CalculateMixingFractions.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 47	RemoveNearbyIndels	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/RemoveNearbyIndels.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes		; ```. This is also at <https://docs.google.com/a/broad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:13456,Security,validat,validation,13456,r the second pass.; 44	GetPileupSummaries	beta; helper tool for CalculateContamination	6/5/2017	https://github.com/broadinstitute/gatk-protected/blob/2bf35790393332da5414b42ec6dca813fcc63202/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/GetPileupSummaries.java	scripts/mutect2_wdl/mutect2.wdl	https://github.com/broadinstitute/gatk/pull/3006	yes	; 33	AnnotateVcfWithBamDepth	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithBamDepth.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 34	AnnotateVcfWithExpectedAlleleFraction	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithExpectedAlleleFraction.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 37	CalculateMixingFractions	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/CalculateMixingFractions.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 47	RemoveNearbyIndels	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/RemoveNearbyIndels.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes		; ```. This is also at <https://docs.google.com/a/broadinstitute.org/spreadsheets/d/15xviLwYUjU82MtYwxxPINiJAkovmaHpRGhqkghiEATQ/edit?usp=sharing>.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/issues/3055:13817,Security,validat,validation,13817,r the second pass.; 44	GetPileupSummaries	beta; helper tool for CalculateContamination	6/5/2017	https://github.com/broadinstitute/gatk-protected/blob/2bf35790393332da5414b42ec6dca813fcc63202/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/GetPileupSummaries.java	scripts/mutect2_wdl/mutect2.wdl	https://github.com/broadinstitute/gatk/pull/3006	yes	; 33	AnnotateVcfWithBamDepth	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithBamDepth.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 34	AnnotateVcfWithExpectedAlleleFraction	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithExpectedAlleleFraction.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 37	CalculateMixingFractions	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/CalculateMixingFractions.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 47	RemoveNearbyIndels	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/RemoveNearbyIndels.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes		; ```. This is also at <https://docs.google.com/a/broadinstitute.org/spreadsheets/d/15xviLwYUjU82MtYwxxPINiJAkovmaHpRGhqkghiEATQ/edit?usp=sharing>.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055
https://github.com/broadinstitute/gatk/pull/3057:96,Availability,Error,Error,96,Not sure if there is a better way to do this. UserException seems wrong since its displays USER Error.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3057
https://github.com/broadinstitute/gatk/pull/3058:217,Security,validat,validateSequenceDictionaries,217,Now can specify a master sequence dictionary that preempts all other; dictionaries that are found (in GATKTool.getBestAvailableSequenceDictionary). Added in associated validity checks with new dictionary in; GATKTool.validateSequenceDictionaries. Fixes #2410,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3058
https://github.com/broadinstitute/gatk/issues/3059:85,Deployability,update,update,85,Currently we have a useless assembleDist task and separate zipBundle task. We should update assembleDist so that it does the right thing.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3059
https://github.com/broadinstitute/gatk/issues/3061:1353,Availability,error,error,1353,"For discussion with @davidbenjamin. Conversation will benefit Mutect2 workflow documentation. The Mutect2 WDL pipeline at <https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl> uses the following:. 1. SplitIntervals in default mode (clarified in #3032 that default INTERVAL_SUBDIVISION mode can cut into an interval in the intervals list); ```; java -jar $GATK_JAR SplitIntervals -R ${ref_fasta} ${""-L "" + intervals} -scatter ${scatter_count} -O interval-files; cp interval-files/*.intervals .; ```; 2. Scatter Mutect2 over files of intervals; ```; scatter (subintervals in SplitIntervals.interval_files ) {; call M2 {; ```; 3. MergeVCFs to collate the resulting VCF callsets; ```; java -Xmx2g -jar $GATK_JAR MergeVcfs -I ${sep=' -I ' input_vcfs} -O ${output_vcf_name}.vcf; ```. Assuming Mutect2 handles active regions in the same manner as HaplotypeCaller, which will expand/pad active regions, then a consequence of the current workflow is potential _duplicate calls_ (that can also differ slightly from each other e.g. due to rounding) for the same genomic locus that result from expansion of the active region into the edges of the intervals being split. MergeVCFs as well as GatherVCFs, allows duplicate calls in the final VCF without checks. GatherVCFs does not allow for out-of-genomic-order-inputs and will give an error. However, I'm noticing something interesting (see below). I would recommend creating intervals without splitting, i.e. with the BALANCING_WITHOUT_INTERVAL_SUBDIVISION option and setting the default of the tool to such to ward against accidental misuse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061
https://github.com/broadinstitute/gatk/issues/3061:110,Deployability,pipeline,pipeline,110,"For discussion with @davidbenjamin. Conversation will benefit Mutect2 workflow documentation. The Mutect2 WDL pipeline at <https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl> uses the following:. 1. SplitIntervals in default mode (clarified in #3032 that default INTERVAL_SUBDIVISION mode can cut into an interval in the intervals list); ```; java -jar $GATK_JAR SplitIntervals -R ${ref_fasta} ${""-L "" + intervals} -scatter ${scatter_count} -O interval-files; cp interval-files/*.intervals .; ```; 2. Scatter Mutect2 over files of intervals; ```; scatter (subintervals in SplitIntervals.interval_files ) {; call M2 {; ```; 3. MergeVCFs to collate the resulting VCF callsets; ```; java -Xmx2g -jar $GATK_JAR MergeVcfs -I ${sep=' -I ' input_vcfs} -O ${output_vcf_name}.vcf; ```. Assuming Mutect2 handles active regions in the same manner as HaplotypeCaller, which will expand/pad active regions, then a consequence of the current workflow is potential _duplicate calls_ (that can also differ slightly from each other e.g. due to rounding) for the same genomic locus that result from expansion of the active region into the edges of the intervals being split. MergeVCFs as well as GatherVCFs, allows duplicate calls in the final VCF without checks. GatherVCFs does not allow for out-of-genomic-order-inputs and will give an error. However, I'm noticing something interesting (see below). I would recommend creating intervals without splitting, i.e. with the BALANCING_WITHOUT_INTERVAL_SUBDIVISION option and setting the default of the tool to such to ward against accidental misuse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061
https://github.com/broadinstitute/gatk/pull/3063:11,Availability,error,errorOnOutOfDateIndex,11,"Adding the errorOnOutOfDateIndex option, which will cause a; UserException to be thrown when an index file is opened that is out of; date with respect to its data file. This option defaults to false to; preserve baseline behavior. Split tests apart and added a method to change the last modified time of a file. Fixes #1683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3063
https://github.com/broadinstitute/gatk/pull/3063:237,Testability,test,tests,237,"Adding the errorOnOutOfDateIndex option, which will cause a; UserException to be thrown when an index file is opened that is out of; date with respect to its data file. This option defaults to false to; preserve baseline behavior. Split tests apart and added a method to change the last modified time of a file. Fixes #1683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3063
https://github.com/broadinstitute/gatk/pull/3064:81,Deployability,integrat,integration,81,Placed a call to forceJVMLocaleToUSEnglish in Main.runCommandLineProgram so that integration tests can take advantage of this. Fixes #2557,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3064
https://github.com/broadinstitute/gatk/pull/3064:81,Integrability,integrat,integration,81,Placed a call to forceJVMLocaleToUSEnglish in Main.runCommandLineProgram so that integration tests can take advantage of this. Fixes #2557,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3064
https://github.com/broadinstitute/gatk/pull/3064:93,Testability,test,tests,93,Placed a call to forceJVMLocaleToUSEnglish in Main.runCommandLineProgram so that integration tests can take advantage of this. Fixes #2557,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3064
https://github.com/broadinstitute/gatk/pull/3065:454,Testability,assert,assertOutBamContainsInBamProgramRecords,454,"Implements https://github.com/broadinstitute/gatk/issues/2787.; **Summary of Changes**; * HaplotypeBAMDestination.java - Added a statement to put the the input `@PG` header lines in the output BAM.; * ArtificialReadUtils.java - Added a methods to create a SAM header with Program Records. Fixed a bug in `createArtificialSamHeaderWithReadGroup` where the Read Group was using `PG` instead of the `RG` tag.; * SamAssertionUtils.java - Created a utility, `assertOutBamContainsInBamProgramRecords`, that checks that output BAM file header contains the input BAM file header Program Records.; * SamAssertionUtilsUnitTest.java - Unit tests `assertOutBamContainsInBamProgramRecords`.; * HaplotypeCallerIntegrationTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output BAM.; * HaplotypeBAMWriterUnitTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output SAM and BAM.; * testSAM.sam and testBAM.sam - Modified to reflect forwarded PG tags in HaplotypeBAMWriterUnitTest.; * ArtificialReadUtilsUnitTest.java and ReadPileupUnitTest.java - change null to the RG value due to using the correct tag in `ArtificialReadUtils.createArtificialSamHeaderWithReadGroup`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3065
https://github.com/broadinstitute/gatk/pull/3065:629,Testability,test,tests,629,"Implements https://github.com/broadinstitute/gatk/issues/2787.; **Summary of Changes**; * HaplotypeBAMDestination.java - Added a statement to put the the input `@PG` header lines in the output BAM.; * ArtificialReadUtils.java - Added a methods to create a SAM header with Program Records. Fixed a bug in `createArtificialSamHeaderWithReadGroup` where the Read Group was using `PG` instead of the `RG` tag.; * SamAssertionUtils.java - Created a utility, `assertOutBamContainsInBamProgramRecords`, that checks that output BAM file header contains the input BAM file header Program Records.; * SamAssertionUtilsUnitTest.java - Unit tests `assertOutBamContainsInBamProgramRecords`.; * HaplotypeCallerIntegrationTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output BAM.; * HaplotypeBAMWriterUnitTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output SAM and BAM.; * testSAM.sam and testBAM.sam - Modified to reflect forwarded PG tags in HaplotypeBAMWriterUnitTest.; * ArtificialReadUtilsUnitTest.java and ReadPileupUnitTest.java - change null to the RG value due to using the correct tag in `ArtificialReadUtils.createArtificialSamHeaderWithReadGroup`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3065
https://github.com/broadinstitute/gatk/pull/3065:636,Testability,assert,assertOutBamContainsInBamProgramRecords,636,"Implements https://github.com/broadinstitute/gatk/issues/2787.; **Summary of Changes**; * HaplotypeBAMDestination.java - Added a statement to put the the input `@PG` header lines in the output BAM.; * ArtificialReadUtils.java - Added a methods to create a SAM header with Program Records. Fixed a bug in `createArtificialSamHeaderWithReadGroup` where the Read Group was using `PG` instead of the `RG` tag.; * SamAssertionUtils.java - Created a utility, `assertOutBamContainsInBamProgramRecords`, that checks that output BAM file header contains the input BAM file header Program Records.; * SamAssertionUtilsUnitTest.java - Unit tests `assertOutBamContainsInBamProgramRecords`.; * HaplotypeCallerIntegrationTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output BAM.; * HaplotypeBAMWriterUnitTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output SAM and BAM.; * testSAM.sam and testBAM.sam - Modified to reflect forwarded PG tags in HaplotypeBAMWriterUnitTest.; * ArtificialReadUtilsUnitTest.java and ReadPileupUnitTest.java - change null to the RG value due to using the correct tag in `ArtificialReadUtils.createArtificialSamHeaderWithReadGroup`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3065
https://github.com/broadinstitute/gatk/pull/3065:719,Testability,Test,Tested,719,"Implements https://github.com/broadinstitute/gatk/issues/2787.; **Summary of Changes**; * HaplotypeBAMDestination.java - Added a statement to put the the input `@PG` header lines in the output BAM.; * ArtificialReadUtils.java - Added a methods to create a SAM header with Program Records. Fixed a bug in `createArtificialSamHeaderWithReadGroup` where the Read Group was using `PG` instead of the `RG` tag.; * SamAssertionUtils.java - Created a utility, `assertOutBamContainsInBamProgramRecords`, that checks that output BAM file header contains the input BAM file header Program Records.; * SamAssertionUtilsUnitTest.java - Unit tests `assertOutBamContainsInBamProgramRecords`.; * HaplotypeCallerIntegrationTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output BAM.; * HaplotypeBAMWriterUnitTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output SAM and BAM.; * testSAM.sam and testBAM.sam - Modified to reflect forwarded PG tags in HaplotypeBAMWriterUnitTest.; * ArtificialReadUtilsUnitTest.java and ReadPileupUnitTest.java - change null to the RG value due to using the correct tag in `ArtificialReadUtils.createArtificialSamHeaderWithReadGroup`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3065
https://github.com/broadinstitute/gatk/pull/3065:835,Testability,Test,Tested,835,"Implements https://github.com/broadinstitute/gatk/issues/2787.; **Summary of Changes**; * HaplotypeBAMDestination.java - Added a statement to put the the input `@PG` header lines in the output BAM.; * ArtificialReadUtils.java - Added a methods to create a SAM header with Program Records. Fixed a bug in `createArtificialSamHeaderWithReadGroup` where the Read Group was using `PG` instead of the `RG` tag.; * SamAssertionUtils.java - Created a utility, `assertOutBamContainsInBamProgramRecords`, that checks that output BAM file header contains the input BAM file header Program Records.; * SamAssertionUtilsUnitTest.java - Unit tests `assertOutBamContainsInBamProgramRecords`.; * HaplotypeCallerIntegrationTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output BAM.; * HaplotypeBAMWriterUnitTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output SAM and BAM.; * testSAM.sam and testBAM.sam - Modified to reflect forwarded PG tags in HaplotypeBAMWriterUnitTest.; * ArtificialReadUtilsUnitTest.java and ReadPileupUnitTest.java - change null to the RG value due to using the correct tag in `ArtificialReadUtils.createArtificialSamHeaderWithReadGroup`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3065
https://github.com/broadinstitute/gatk/pull/3065:925,Testability,test,testSAM,925,"Implements https://github.com/broadinstitute/gatk/issues/2787.; **Summary of Changes**; * HaplotypeBAMDestination.java - Added a statement to put the the input `@PG` header lines in the output BAM.; * ArtificialReadUtils.java - Added a methods to create a SAM header with Program Records. Fixed a bug in `createArtificialSamHeaderWithReadGroup` where the Read Group was using `PG` instead of the `RG` tag.; * SamAssertionUtils.java - Created a utility, `assertOutBamContainsInBamProgramRecords`, that checks that output BAM file header contains the input BAM file header Program Records.; * SamAssertionUtilsUnitTest.java - Unit tests `assertOutBamContainsInBamProgramRecords`.; * HaplotypeCallerIntegrationTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output BAM.; * HaplotypeBAMWriterUnitTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output SAM and BAM.; * testSAM.sam and testBAM.sam - Modified to reflect forwarded PG tags in HaplotypeBAMWriterUnitTest.; * ArtificialReadUtilsUnitTest.java and ReadPileupUnitTest.java - change null to the RG value due to using the correct tag in `ArtificialReadUtils.createArtificialSamHeaderWithReadGroup`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3065
https://github.com/broadinstitute/gatk/pull/3065:941,Testability,test,testBAM,941,"Implements https://github.com/broadinstitute/gatk/issues/2787.; **Summary of Changes**; * HaplotypeBAMDestination.java - Added a statement to put the the input `@PG` header lines in the output BAM.; * ArtificialReadUtils.java - Added a methods to create a SAM header with Program Records. Fixed a bug in `createArtificialSamHeaderWithReadGroup` where the Read Group was using `PG` instead of the `RG` tag.; * SamAssertionUtils.java - Created a utility, `assertOutBamContainsInBamProgramRecords`, that checks that output BAM file header contains the input BAM file header Program Records.; * SamAssertionUtilsUnitTest.java - Unit tests `assertOutBamContainsInBamProgramRecords`.; * HaplotypeCallerIntegrationTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output BAM.; * HaplotypeBAMWriterUnitTest.java - Tested the input BAM Program Record (PG) tags are forwarded to the output SAM and BAM.; * testSAM.sam and testBAM.sam - Modified to reflect forwarded PG tags in HaplotypeBAMWriterUnitTest.; * ArtificialReadUtilsUnitTest.java and ReadPileupUnitTest.java - change null to the RG value due to using the correct tag in `ArtificialReadUtils.createArtificialSamHeaderWithReadGroup`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3065
https://github.com/broadinstitute/gatk/issues/3066:68,Availability,error,error,68,"I just try to run PrintReadsSpark on cloudera cluster and meet this error. Command:; ```; $ ./gatk-launch PrintReadsSpark -I NA12878.chr17_69k_70k.dictFix.bam -O /user/yaron/output.bam -- --sparkRunner SPARK --sparkMaster yarn --num-executors 5 --executor-cores 2 --executor-memory 1g; ```; Results:; ```; Using GATK jar /home/yaron/gatk/build/libs/gatk-spark.jar; Running:; spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --num-executors 5 --executor-cores 2 --executor-memory 1g /home/yaron/gatk/build/libs/gatk-spark.jar PrintReadsSpark -I NA12878.chr17_69k_70k.dictFix.bam -O /user/yaron/output.bam --sparkMaster yarn; 09:14:13.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yaron/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 8, 2017 9:14:13 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /user/yaron/output.bam --input NA12878.chr17_69k_70k.dictFix.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:2846,Availability,ERROR,ERROR,2846,"isableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN N",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:2964,Availability,ERROR,ERROR,2964,"UIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3029,Availability,ERROR,ERROR,3029,"later false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3109,Availability,ERROR,ERROR,3109,"7 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3230,Availability,ERROR,ERROR,3230,":13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3291,Availability,ERROR,ERROR,3291,"aults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; *********************************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3409,Availability,ERROR,ERROR,3409,"9:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3474,Availability,ERROR,ERROR,3474,"s.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3554,Availability,ERROR,ERROR,3554,"intReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. *****************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3675,Availability,ERROR,ERROR,3675,"nflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:4082,Availability,down,down,4082,"object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:4341,Availability,ERROR,ERROR,4341,"rg.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.Comm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:1568,Deployability,pipeline,pipelines,1568,"ark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --num-executors 5 --executor-cores 2 --executor-memory 1g /home/yaron/gatk/build/libs/gatk-spark.jar PrintReadsSpark -I NA12878.chr17_69k_70k.dictFix.bam -O /user/yaron/output.bam --sparkMaster yarn; 09:14:13.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yaron/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 8, 2017 9:14:13 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /user/yaron/output.bam --input NA12878.chr17_69k_70k.dictFix.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:4167,Deployability,pipeline,pipelines,4167,"che.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:4956,Deployability,pipeline,pipelines,4956,"N See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:6049,Deployability,deploy,deploy,6049,"ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:6086,Deployability,deploy,deploy,6086,"Tool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls outp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:6158,Deployability,deploy,deploy,6158,"arkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls output.bam.parts; Found 3 items; -rw-r--r-- 3 yaron yaron 0 2017-06-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:6234,Deployability,deploy,deploy,6234,"nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls output.bam.parts; Found 3 items; -rw-r--r-- 3 yaron yaron 0 2017-06-08 09:14 output.bam.parts/_SUCCESS; -rw-r--r-- 3 yaron yaron 62019 2017-06-0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:6305,Deployability,deploy,deploy,6305,"am.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls output.bam.parts; Found 3 items; -rw-r--r-- 3 yaron yaron 0 2017-06-08 09:14 output.bam.parts/_SUCCESS; -rw-r--r-- 3 yaron yaron 62019 2017-06-08 09:14 output.bam.parts/part-r-00000.bam; -rw-r--r-- 3 yaron yaron 16 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:6374,Deployability,deploy,deploy,6374,"gram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls output.bam.parts; Found 3 items; -rw-r--r-- 3 yaron yaron 0 2017-06-08 09:14 output.bam.parts/_SUCCESS; -rw-r--r-- 3 yaron yaron 62019 2017-06-08 09:14 output.bam.parts/part-r-00000.bam; -rw-r--r-- 3 yaron yaron 16 2017-06-08 09:14 output.bam.parts/part-r-00000.bam.splitting-bai; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:2947,Modifiability,variab,variable,2947,"lse --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system proper",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3392,Modifiability,variab,variable,3392,"_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:1364,Performance,Load,Loading,1364,"r; Running:; spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --num-executors 5 --executor-cores 2 --executor-memory 1g /home/yaron/gatk/build/libs/gatk-spark.jar PrintReadsSpark -I NA12878.chr17_69k_70k.dictFix.bam -O /user/yaron/output.bam --sparkMaster yarn; 09:14:13.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yaron/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 8, 2017 9:14:13 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /user/yaron/output.bam --input NA12878.chr17_69k_70k.dictFix.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3012,Performance,load,loaded,3012,"later false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3154,Performance,load,loaded,3154,"d64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3457,Performance,load,loaded,3457,"s.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3599,Performance,load,loaded,3599,"e; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3854,Testability,log,logger,3854,"engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3066:3974,Testability,log,logging,3974,"ROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066
https://github.com/broadinstitute/gatk/issues/3068:135,Availability,down,downloads,135,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3068:308,Deployability,integrat,integration,308,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3068:364,Deployability,integrat,integration,364,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3068:308,Integrability,integrat,integration,308,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3068:364,Integrability,integrat,integration,364,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3068:529,Modifiability,variab,variable,529,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3068:585,Performance,perform,perform,585,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3068:320,Testability,test,test,320,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3068:376,Testability,test,test,376,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068
https://github.com/broadinstitute/gatk/issues/3069:64,Testability,log,log,64,"The pair hmm is emitting this warning which doesn't respect the log settings. ; It would be nice if it respected our logging settings. . Maybe we should add a ""setLogLevel"" hook to GatkNativeBindings?. I'm not sure why this is being triggered on travis, since I would have thought travis supports OpenMP, so either travis isn't testing what we think it is, or the warning is being emitted for a slightly different reason than it claims.; `[WARNING] Ignoring request for 4 threads; not using OpenMP implementation`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3069
https://github.com/broadinstitute/gatk/issues/3069:117,Testability,log,logging,117,"The pair hmm is emitting this warning which doesn't respect the log settings. ; It would be nice if it respected our logging settings. . Maybe we should add a ""setLogLevel"" hook to GatkNativeBindings?. I'm not sure why this is being triggered on travis, since I would have thought travis supports OpenMP, so either travis isn't testing what we think it is, or the warning is being emitted for a slightly different reason than it claims.; `[WARNING] Ignoring request for 4 threads; not using OpenMP implementation`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3069
https://github.com/broadinstitute/gatk/issues/3069:328,Testability,test,testing,328,"The pair hmm is emitting this warning which doesn't respect the log settings. ; It would be nice if it respected our logging settings. . Maybe we should add a ""setLogLevel"" hook to GatkNativeBindings?. I'm not sure why this is being triggered on travis, since I would have thought travis supports OpenMP, so either travis isn't testing what we think it is, or the warning is being emitted for a slightly different reason than it claims.; `[WARNING] Ignoring request for 4 threads; not using OpenMP implementation`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3069
https://github.com/broadinstitute/gatk/pull/3070:296,Availability,error,errors,296,"This was a very useful debug tool when working on issue #2685. It sends many parallel reads for a long time. This makes sure that the combination of the cloud provider's throttling and our own retry parameters allows us to eventually read everything to completion and not fail with disconnection errors. This test is disabled by default, because it takes too long to be run every time. But if there's a doubt about retries we can dust it off and run it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3070
https://github.com/broadinstitute/gatk/pull/3070:309,Testability,test,test,309,"This was a very useful debug tool when working on issue #2685. It sends many parallel reads for a long time. This makes sure that the combination of the cloud provider's throttling and our own retry parameters allows us to eventually read everything to completion and not fail with disconnection errors. This test is disabled by default, because it takes too long to be run every time. But if there's a doubt about retries we can dust it off and run it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3070
https://github.com/broadinstitute/gatk/issues/3073:755,Testability,test,testing,755,"gatk-launch crashes with ; ```; Traceback (most recent call last):; File ""/Users/louisb/Workspace/gatk//gatk-launch"", line 469, in <module>; main(sys.argv[1:]); File ""/Users/louisb/Workspace/gatk//gatk-launch"", line 161, in main; runGATK(sparkRunner, sparkSubmitCommand, dryRun, gatkArgs, sparkArgs, javaOptions); File ""/Users/louisb/Workspace/gatk//gatk-launch"", line 345, in runGATK; dataprocargs = convertSparkSubmitToDataprocArgs(sparkConfArgs, sparkArgs); File ""/Users/louisb/Workspace/gatk//gatk-launch"", line 439, in convertSparkSubmitToDataprocArgs; property = args[i]; NameError: global name 'args' is not defined; ``` ; When trying to run on spark. We introduced this bug in the last commit to gatk launch. I'm not sure how it passed our manual testing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3073
https://github.com/broadinstitute/gatk/pull/3075:0,Deployability,update,update,0,update the artifactory url to point to the new artifactory; update the travis build with another environment variable called UPLOAD which determines if that build should upload a snapshot or not; fixes #3068,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3075
https://github.com/broadinstitute/gatk/pull/3075:60,Deployability,update,update,60,update the artifactory url to point to the new artifactory; update the travis build with another environment variable called UPLOAD which determines if that build should upload a snapshot or not; fixes #3068,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3075
https://github.com/broadinstitute/gatk/pull/3075:109,Modifiability,variab,variable,109,update the artifactory url to point to the new artifactory; update the travis build with another environment variable called UPLOAD which determines if that build should upload a snapshot or not; fixes #3068,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3075
https://github.com/broadinstitute/gatk/pull/3076:138,Deployability,pipeline,pipeline,138,"rather than the default boot HDD.; Thanks to @mwalker174 for discovering this. In terms of runtime, this didn't change runtime for the SV pipeline much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3076
https://github.com/broadinstitute/gatk/issues/3077:113,Availability,failure,failures,113,"Our jenkins nightly tests are failing, but they're reporting success. This shouldn't be happening. . Some of the failures are due to #3067, but the spark failures look like something else is causing them. Notice the very short runtimes because nothing is actually happening.; <img width=""958"" alt=""screen shot 2017-06-09 at 2 04 31 pm"" src=""https://user-images.githubusercontent.com/4700332/26988271-a61ab3fc-4d1c-11e7-9110-9941888b66ce.png"">. This ticket is to fix the fact that the tests report success even when they fail, not to fix the tests themselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3077
https://github.com/broadinstitute/gatk/issues/3077:154,Availability,failure,failures,154,"Our jenkins nightly tests are failing, but they're reporting success. This shouldn't be happening. . Some of the failures are due to #3067, but the spark failures look like something else is causing them. Notice the very short runtimes because nothing is actually happening.; <img width=""958"" alt=""screen shot 2017-06-09 at 2 04 31 pm"" src=""https://user-images.githubusercontent.com/4700332/26988271-a61ab3fc-4d1c-11e7-9110-9941888b66ce.png"">. This ticket is to fix the fact that the tests report success even when they fail, not to fix the tests themselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3077
https://github.com/broadinstitute/gatk/issues/3077:20,Testability,test,tests,20,"Our jenkins nightly tests are failing, but they're reporting success. This shouldn't be happening. . Some of the failures are due to #3067, but the spark failures look like something else is causing them. Notice the very short runtimes because nothing is actually happening.; <img width=""958"" alt=""screen shot 2017-06-09 at 2 04 31 pm"" src=""https://user-images.githubusercontent.com/4700332/26988271-a61ab3fc-4d1c-11e7-9110-9941888b66ce.png"">. This ticket is to fix the fact that the tests report success even when they fail, not to fix the tests themselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3077
https://github.com/broadinstitute/gatk/issues/3077:484,Testability,test,tests,484,"Our jenkins nightly tests are failing, but they're reporting success. This shouldn't be happening. . Some of the failures are due to #3067, but the spark failures look like something else is causing them. Notice the very short runtimes because nothing is actually happening.; <img width=""958"" alt=""screen shot 2017-06-09 at 2 04 31 pm"" src=""https://user-images.githubusercontent.com/4700332/26988271-a61ab3fc-4d1c-11e7-9110-9941888b66ce.png"">. This ticket is to fix the fact that the tests report success even when they fail, not to fix the tests themselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3077
https://github.com/broadinstitute/gatk/issues/3077:541,Testability,test,tests,541,"Our jenkins nightly tests are failing, but they're reporting success. This shouldn't be happening. . Some of the failures are due to #3067, but the spark failures look like something else is causing them. Notice the very short runtimes because nothing is actually happening.; <img width=""958"" alt=""screen shot 2017-06-09 at 2 04 31 pm"" src=""https://user-images.githubusercontent.com/4700332/26988271-a61ab3fc-4d1c-11e7-9110-9941888b66ce.png"">. This ticket is to fix the fact that the tests report success even when they fail, not to fix the tests themselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3077
https://github.com/broadinstitute/gatk/issues/3078:31,Modifiability,config,config,31,"Needs to:. -support overriding config settings via a simple mechanism (like providing an override config file); -use a simple, easy-to-edit file format like Java Properties (name = value); -be widely used in the Java community & well-maintained.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078
https://github.com/broadinstitute/gatk/issues/3078:98,Modifiability,config,config,98,"Needs to:. -support overriding config settings via a simple mechanism (like providing an override config file); -use a simple, easy-to-edit file format like Java Properties (name = value); -be widely used in the Java community & well-maintained.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078
https://github.com/broadinstitute/gatk/issues/3078:53,Usability,simpl,simple,53,"Needs to:. -support overriding config settings via a simple mechanism (like providing an override config file); -use a simple, easy-to-edit file format like Java Properties (name = value); -be widely used in the Java community & well-maintained.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078
https://github.com/broadinstitute/gatk/issues/3078:119,Usability,simpl,simple,119,"Needs to:. -support overriding config settings via a simple mechanism (like providing an override config file); -use a simple, easy-to-edit file format like Java Properties (name = value); -be widely used in the Java community & well-maintained.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078
https://github.com/broadinstitute/gatk/issues/3079:147,Availability,down,downstream,147,"Once we choose the library to use for GATK configuration, let's have a design meeting to make sure we come up with something that works for Spark, downstream projects, our users, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079
https://github.com/broadinstitute/gatk/issues/3079:43,Deployability,configurat,configuration,43,"Once we choose the library to use for GATK configuration, let's have a design meeting to make sure we come up with something that works for Spark, downstream projects, our users, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079
https://github.com/broadinstitute/gatk/issues/3079:43,Modifiability,config,configuration,43,"Once we choose the library to use for GATK configuration, let's have a design meeting to make sure we come up with something that works for Spark, downstream projects, our users, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079
https://github.com/broadinstitute/gatk/issues/3084:89,Availability,avail,available,89,"Yes, what are the plans? I too would like to know. ---; What are the plans for the tools available in Picard 2.9.2 or GATK3.7 that are not in GATK4 alpha? Is the plan eventually to port everything to GATK4? Or are some being permanently sent out to pasture?. I have specifically noticed as missing:; - CollectVariantCallingMetrics; - SetNmMdAndUqTags; - the -gvcf option for ValidateVariants. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/9736/gatk4-status-of-some-picard-gatk3-7-tools-missing-from-alpha/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3084
https://github.com/broadinstitute/gatk/issues/3084:375,Security,Validat,ValidateVariants,375,"Yes, what are the plans? I too would like to know. ---; What are the plans for the tools available in Picard 2.9.2 or GATK3.7 that are not in GATK4 alpha? Is the plan eventually to port everything to GATK4? Or are some being permanently sent out to pasture?. I have specifically noticed as missing:; - CollectVariantCallingMetrics; - SetNmMdAndUqTags; - the -gvcf option for ValidateVariants. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/9736/gatk4-status-of-some-picard-gatk3-7-tools-missing-from-alpha/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3084
https://github.com/broadinstitute/gatk/issues/3085:20,Usability,guid,guides,20,"Using this: https://guides.github.com/activities/citable-code/; To track GATK in scientific publications as we do not currently publish our work in scientific journals otherwise. Being able to track our use will come in handy some day, I promise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3085
https://github.com/broadinstitute/gatk/issues/3086:126,Safety,predict,predict,126,"The current Mutect panel of normals has been quite effective, but we can probably improve upon it by *learning* features that predict artifacts rather than *memorizing* problematic sites. As of June, 2017 we imagine an end goal of a deep learning model that predicts a fraction of artifact reads from reference context, annotations, and perhaps additional information such as chromatin states. This prediction could then be compared with the actual allele fraction to determine whether a true variant or artifact is a better explanation. One virtue of a regressor versus a classifier is that it doesn't require labeled data, which is hard to come by in the somatic context. It also gets around the risk of a classifier simply learning to throw out all low allele fractions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3086
https://github.com/broadinstitute/gatk/issues/3086:258,Safety,predict,predicts,258,"The current Mutect panel of normals has been quite effective, but we can probably improve upon it by *learning* features that predict artifacts rather than *memorizing* problematic sites. As of June, 2017 we imagine an end goal of a deep learning model that predicts a fraction of artifact reads from reference context, annotations, and perhaps additional information such as chromatin states. This prediction could then be compared with the actual allele fraction to determine whether a true variant or artifact is a better explanation. One virtue of a regressor versus a classifier is that it doesn't require labeled data, which is hard to come by in the somatic context. It also gets around the risk of a classifier simply learning to throw out all low allele fractions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3086
https://github.com/broadinstitute/gatk/issues/3086:399,Safety,predict,prediction,399,"The current Mutect panel of normals has been quite effective, but we can probably improve upon it by *learning* features that predict artifacts rather than *memorizing* problematic sites. As of June, 2017 we imagine an end goal of a deep learning model that predicts a fraction of artifact reads from reference context, annotations, and perhaps additional information such as chromatin states. This prediction could then be compared with the actual allele fraction to determine whether a true variant or artifact is a better explanation. One virtue of a regressor versus a classifier is that it doesn't require labeled data, which is hard to come by in the somatic context. It also gets around the risk of a classifier simply learning to throw out all low allele fractions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3086
https://github.com/broadinstitute/gatk/issues/3086:698,Safety,risk,risk,698,"The current Mutect panel of normals has been quite effective, but we can probably improve upon it by *learning* features that predict artifacts rather than *memorizing* problematic sites. As of June, 2017 we imagine an end goal of a deep learning model that predicts a fraction of artifact reads from reference context, annotations, and perhaps additional information such as chromatin states. This prediction could then be compared with the actual allele fraction to determine whether a true variant or artifact is a better explanation. One virtue of a regressor versus a classifier is that it doesn't require labeled data, which is hard to come by in the somatic context. It also gets around the risk of a classifier simply learning to throw out all low allele fractions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3086
https://github.com/broadinstitute/gatk/issues/3086:102,Usability,learn,learning,102,"The current Mutect panel of normals has been quite effective, but we can probably improve upon it by *learning* features that predict artifacts rather than *memorizing* problematic sites. As of June, 2017 we imagine an end goal of a deep learning model that predicts a fraction of artifact reads from reference context, annotations, and perhaps additional information such as chromatin states. This prediction could then be compared with the actual allele fraction to determine whether a true variant or artifact is a better explanation. One virtue of a regressor versus a classifier is that it doesn't require labeled data, which is hard to come by in the somatic context. It also gets around the risk of a classifier simply learning to throw out all low allele fractions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3086
https://github.com/broadinstitute/gatk/issues/3086:238,Usability,learn,learning,238,"The current Mutect panel of normals has been quite effective, but we can probably improve upon it by *learning* features that predict artifacts rather than *memorizing* problematic sites. As of June, 2017 we imagine an end goal of a deep learning model that predicts a fraction of artifact reads from reference context, annotations, and perhaps additional information such as chromatin states. This prediction could then be compared with the actual allele fraction to determine whether a true variant or artifact is a better explanation. One virtue of a regressor versus a classifier is that it doesn't require labeled data, which is hard to come by in the somatic context. It also gets around the risk of a classifier simply learning to throw out all low allele fractions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3086
https://github.com/broadinstitute/gatk/issues/3086:719,Usability,simpl,simply,719,"The current Mutect panel of normals has been quite effective, but we can probably improve upon it by *learning* features that predict artifacts rather than *memorizing* problematic sites. As of June, 2017 we imagine an end goal of a deep learning model that predicts a fraction of artifact reads from reference context, annotations, and perhaps additional information such as chromatin states. This prediction could then be compared with the actual allele fraction to determine whether a true variant or artifact is a better explanation. One virtue of a regressor versus a classifier is that it doesn't require labeled data, which is hard to come by in the somatic context. It also gets around the risk of a classifier simply learning to throw out all low allele fractions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3086
https://github.com/broadinstitute/gatk/issues/3086:726,Usability,learn,learning,726,"The current Mutect panel of normals has been quite effective, but we can probably improve upon it by *learning* features that predict artifacts rather than *memorizing* problematic sites. As of June, 2017 we imagine an end goal of a deep learning model that predicts a fraction of artifact reads from reference context, annotations, and perhaps additional information such as chromatin states. This prediction could then be compared with the actual allele fraction to determine whether a true variant or artifact is a better explanation. One virtue of a regressor versus a classifier is that it doesn't require labeled data, which is hard to come by in the somatic context. It also gets around the risk of a classifier simply learning to throw out all low allele fractions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3086
https://github.com/broadinstitute/gatk/issues/3089:178,Availability,error,errors,178,"We need a plan for this. It would be nice to use the SV team's in-memory BWA-mem binding, but it's not clear that tweaking settings would be enough to capture possible alignment errors in a BWA-aligned bam file. CGA has used Novo-align and BLAT in the past. We'll want to talk with Chris, Heng, and Julian about this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3089
https://github.com/broadinstitute/gatk/issues/3089:103,Usability,clear,clear,103,"We need a plan for this. It would be nice to use the SV team's in-memory BWA-mem binding, but it's not clear that tweaking settings would be enough to capture possible alignment errors in a BWA-aligned bam file. CGA has used Novo-align and BLAT in the past. We'll want to talk with Chris, Heng, and Julian about this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3089
https://github.com/broadinstitute/gatk/issues/3091:269,Safety,predict,predicting,269,"At deep learning club, @lh3 suggested a kmer-based approach as the non-deep baseline for the new Mutect PoN. We like this idea and are adopting it. The basic idea is that some regression or binning model of kmers will do what a convolutional network might later do for predicting whether a site is prone to artifacts. The goal here is to get intuition as to how much information is contained in the local sequence context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3091
https://github.com/broadinstitute/gatk/issues/3091:8,Usability,learn,learning,8,"At deep learning club, @lh3 suggested a kmer-based approach as the non-deep baseline for the new Mutect PoN. We like this idea and are adopting it. The basic idea is that some regression or binning model of kmers will do what a convolutional network might later do for predicting whether a site is prone to artifacts. The goal here is to get intuition as to how much information is contained in the local sequence context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3091
https://github.com/broadinstitute/gatk/issues/3091:342,Usability,intuit,intuition,342,"At deep learning club, @lh3 suggested a kmer-based approach as the non-deep baseline for the new Mutect PoN. We like this idea and are adopting it. The basic idea is that some regression or binning model of kmers will do what a convolutional network might later do for predicting whether a site is prone to artifacts. The goal here is to get intuition as to how much information is contained in the local sequence context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3091
https://github.com/broadinstitute/gatk/issues/3092:159,Availability,error,error,159,"This should emit a big file with lines consisting of: a kmer, and the count in the pileup of each of the following to occur at the *center* of the kmer:. * no error; * A substitution; * C substitution; * G substitution; * T substitution; * *beginning* of deletion; * *beginning* of insertion. We might later get fancier and distinguish between different lengths of indels.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3092
https://github.com/broadinstitute/gatk/issues/3094:13,Usability,learn,learn,13,Take what we learn in #2973 and apply it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3094
https://github.com/broadinstitute/gatk/issues/3095:303,Deployability,pipeline,pipelines,303,"`SortReadFileSpark` gives a log message like this:; ```; 17:31:00.062 INFO SortReadFileSpark - Using %s reducers2744; ```; It's trivial, but I think the [logger](https://github.com/broadinstitute/gatk/blob/c18e7800ed85c55f81387cf02fdcbf6cb3aaaf5e/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/SortReadFileSpark.java#L41) needs a small fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3095
https://github.com/broadinstitute/gatk/issues/3095:32,Integrability,message,message,32,"`SortReadFileSpark` gives a log message like this:; ```; 17:31:00.062 INFO SortReadFileSpark - Using %s reducers2744; ```; It's trivial, but I think the [logger](https://github.com/broadinstitute/gatk/blob/c18e7800ed85c55f81387cf02fdcbf6cb3aaaf5e/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/SortReadFileSpark.java#L41) needs a small fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3095
https://github.com/broadinstitute/gatk/issues/3095:28,Testability,log,log,28,"`SortReadFileSpark` gives a log message like this:; ```; 17:31:00.062 INFO SortReadFileSpark - Using %s reducers2744; ```; It's trivial, but I think the [logger](https://github.com/broadinstitute/gatk/blob/c18e7800ed85c55f81387cf02fdcbf6cb3aaaf5e/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/SortReadFileSpark.java#L41) needs a small fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3095
https://github.com/broadinstitute/gatk/issues/3095:154,Testability,log,logger,154,"`SortReadFileSpark` gives a log message like this:; ```; 17:31:00.062 INFO SortReadFileSpark - Using %s reducers2744; ```; It's trivial, but I think the [logger](https://github.com/broadinstitute/gatk/blob/c18e7800ed85c55f81387cf02fdcbf6cb3aaaf5e/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/SortReadFileSpark.java#L41) needs a small fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3095
https://github.com/broadinstitute/gatk/issues/3098:915,Availability,error,error,915,"Hey @mbabadi, I've answered this user question but I think my answer needs your review. Also, DR mentioned perhaps this option could be set automatically for users? Thanks. ---; Thanks @shlee for the update,; I used the latest jar from the gatk4 repo as recommended. And managed to derive the read count input file and sex genotype table. I just wanted to confirm whether Nd4j also needed to be installed if not using Spark. #script run; ./gatk-launch GermlineCNVCaller --contigAnnotationsTable ../gatk4_Hellbender/grch37_contig_annotations.tsv --copyNumberTransitionPriorTable ../gatk4_Hellbender/grch37_germline_CN_priors.tsv --jobType LEARN_AND_CALL --outputPath ./TS1 --input ../gatk4_Hellbender/target_cov.tsv --targets ../gatk4_Hellbender/targets.txt --disableSpark true --sexGenotypeTable ../gatk4_Hellbender/TS1_genotype --rddCheckpointing false --biasCovariateSolverType LOCAL. #I am getting the following error which seems to be linked with Nd4j:. Using GATK jar ~/localwork/playground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098
https://github.com/broadinstitute/gatk/issues/3098:2420,Availability,down,down,2420,"yground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896 INFO GermlineCNVCaller - Parsing the germline contig ploidy annotation table...; 16:55:24.906 INFO ContigGermlinePloidyAnnotationTableReader - Ploidy tags: SEX_XX, SEX_XY; 16:55:25.056 INFO GermlineCNVCaller - Parsing the copy number transition prior table and initializing the caches...; 16:55:28.634 INFO GermlineCNVCaller - Initializing the EM algorithm workspace...; 16:55:32.861 INFO GermlineCNVCaller - Shutting down engine; [June 12, 2017 4:55:32 PM ACST] org.broadinstitute.hellbender.tools.coveragemodel.germline.GermlineCNVCaller done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=1364721664; org.broadinstitute.hellbender.exceptions.GATKException: Nd4j data type must be set to double for coverage modeller routines to function properly. This can be done by setting JVM system property ""dtype"" to ""double"". Can not continue. Thanks. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/39376#Comment_39376",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098
https://github.com/broadinstitute/gatk/issues/3098:200,Deployability,update,update,200,"Hey @mbabadi, I've answered this user question but I think my answer needs your review. Also, DR mentioned perhaps this option could be set automatically for users? Thanks. ---; Thanks @shlee for the update,; I used the latest jar from the gatk4 repo as recommended. And managed to derive the read count input file and sex genotype table. I just wanted to confirm whether Nd4j also needed to be installed if not using Spark. #script run; ./gatk-launch GermlineCNVCaller --contigAnnotationsTable ../gatk4_Hellbender/grch37_contig_annotations.tsv --copyNumberTransitionPriorTable ../gatk4_Hellbender/grch37_germline_CN_priors.tsv --jobType LEARN_AND_CALL --outputPath ./TS1 --input ../gatk4_Hellbender/target_cov.tsv --targets ../gatk4_Hellbender/targets.txt --disableSpark true --sexGenotypeTable ../gatk4_Hellbender/TS1_genotype --rddCheckpointing false --biasCovariateSolverType LOCAL. #I am getting the following error which seems to be linked with Nd4j:. Using GATK jar ~/localwork/playground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098
https://github.com/broadinstitute/gatk/issues/3098:395,Deployability,install,installed,395,"Hey @mbabadi, I've answered this user question but I think my answer needs your review. Also, DR mentioned perhaps this option could be set automatically for users? Thanks. ---; Thanks @shlee for the update,; I used the latest jar from the gatk4 repo as recommended. And managed to derive the read count input file and sex genotype table. I just wanted to confirm whether Nd4j also needed to be installed if not using Spark. #script run; ./gatk-launch GermlineCNVCaller --contigAnnotationsTable ../gatk4_Hellbender/grch37_contig_annotations.tsv --copyNumberTransitionPriorTable ../gatk4_Hellbender/grch37_germline_CN_priors.tsv --jobType LEARN_AND_CALL --outputPath ./TS1 --input ../gatk4_Hellbender/target_cov.tsv --targets ../gatk4_Hellbender/targets.txt --disableSpark true --sexGenotypeTable ../gatk4_Hellbender/TS1_genotype --rddCheckpointing false --biasCovariateSolverType LOCAL. #I am getting the following error which seems to be linked with Nd4j:. Using GATK jar ~/localwork/playground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098
https://github.com/broadinstitute/gatk/issues/3098:2726,Integrability,rout,routines,2726,"yground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896 INFO GermlineCNVCaller - Parsing the germline contig ploidy annotation table...; 16:55:24.906 INFO ContigGermlinePloidyAnnotationTableReader - Ploidy tags: SEX_XX, SEX_XY; 16:55:25.056 INFO GermlineCNVCaller - Parsing the copy number transition prior table and initializing the caches...; 16:55:28.634 INFO GermlineCNVCaller - Initializing the EM algorithm workspace...; 16:55:32.861 INFO GermlineCNVCaller - Shutting down engine; [June 12, 2017 4:55:32 PM ACST] org.broadinstitute.hellbender.tools.coveragemodel.germline.GermlineCNVCaller done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=1364721664; org.broadinstitute.hellbender.exceptions.GATKException: Nd4j data type must be set to double for coverage modeller routines to function properly. This can be done by setting JVM system property ""dtype"" to ""double"". Can not continue. Thanks. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/39376#Comment_39376",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098
https://github.com/broadinstitute/gatk/issues/3098:2280,Performance,cache,caches,2280,"yground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896 INFO GermlineCNVCaller - Parsing the germline contig ploidy annotation table...; 16:55:24.906 INFO ContigGermlinePloidyAnnotationTableReader - Ploidy tags: SEX_XX, SEX_XY; 16:55:25.056 INFO GermlineCNVCaller - Parsing the copy number transition prior table and initializing the caches...; 16:55:28.634 INFO GermlineCNVCaller - Initializing the EM algorithm workspace...; 16:55:32.861 INFO GermlineCNVCaller - Shutting down engine; [June 12, 2017 4:55:32 PM ACST] org.broadinstitute.hellbender.tools.coveragemodel.germline.GermlineCNVCaller done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=1364721664; org.broadinstitute.hellbender.exceptions.GATKException: Nd4j data type must be set to double for coverage modeller routines to function properly. This can be done by setting JVM system property ""dtype"" to ""double"". Can not continue. Thanks. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/39376#Comment_39376",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098
https://github.com/broadinstitute/gatk/pull/3101:90,Performance,cache,cache,90,cleaning up some old lines that are no longer necessary; adding additional directories to cache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3101
https://github.com/broadinstitute/gatk/issues/3102:67,Integrability,depend,depends,67,"Hi, . I am trying to run GATK with against a Java NIO library that depends on `com.google.guave:22:0`.; In the GATK `build.gradle` there is a `force 'com.google.guava:guava:18.0'` at line 71. This version is 3 years old. Can you consider updating it to a newer version? 22.0 is the latest. Thanks,; David",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102
https://github.com/broadinstitute/gatk/issues/3104:72,Deployability,pipeline,pipeline,72,"After discussion in #3084, I offer myself to port the indel realignment pipeline. After exploring the GATK3 implementation, I will split the port in the following independent tasks:. - [ ] Port `RealignerTargetCreator` (require test data after run with GATK3); - [ ] Port `ConstrainedMateFixingManager`; - [ ] Port `NWaySAMFileWriter` (requires some change in the engine to get the ID for the inputs). The previous port will be integrated in the `IndelRealigner` tool implementation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104
https://github.com/broadinstitute/gatk/issues/3104:428,Deployability,integrat,integrated,428,"After discussion in #3084, I offer myself to port the indel realignment pipeline. After exploring the GATK3 implementation, I will split the port in the following independent tasks:. - [ ] Port `RealignerTargetCreator` (require test data after run with GATK3); - [ ] Port `ConstrainedMateFixingManager`; - [ ] Port `NWaySAMFileWriter` (requires some change in the engine to get the ID for the inputs). The previous port will be integrated in the `IndelRealigner` tool implementation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104
https://github.com/broadinstitute/gatk/issues/3104:428,Integrability,integrat,integrated,428,"After discussion in #3084, I offer myself to port the indel realignment pipeline. After exploring the GATK3 implementation, I will split the port in the following independent tasks:. - [ ] Port `RealignerTargetCreator` (require test data after run with GATK3); - [ ] Port `ConstrainedMateFixingManager`; - [ ] Port `NWaySAMFileWriter` (requires some change in the engine to get the ID for the inputs). The previous port will be integrated in the `IndelRealigner` tool implementation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104
https://github.com/broadinstitute/gatk/issues/3104:228,Testability,test,test,228,"After discussion in #3084, I offer myself to port the indel realignment pipeline. After exploring the GATK3 implementation, I will split the port in the following independent tasks:. - [ ] Port `RealignerTargetCreator` (require test data after run with GATK3); - [ ] Port `ConstrainedMateFixingManager`; - [ ] Port `NWaySAMFileWriter` (requires some change in the engine to get the ID for the inputs). The previous port will be integrated in the `IndelRealigner` tool implementation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104
https://github.com/broadinstitute/gatk/pull/3106:44,Deployability,pipeline,pipeline,44,Here are the changes needed to get the full pipeline running on WGS BAMs. This contains commits from other PRs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3106
https://github.com/broadinstitute/gatk/pull/3108:0,Usability,Simpl,Simple,0,Simple fixes to capitalize VCF and changed command to reflect ./gatk-launch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3108
https://github.com/broadinstitute/gatk/pull/3110:319,Availability,failure,failures,319,"Empirical testing has shown that this tool performs best at scale with cloud buffering; disabled. With cloud buffering on and thousands of concurrent GenomicsDBImport tasks,; we do too many simultaneous GCS accesses (since the prefetcher spawns a new thread for each; reader upon a query) and start seeing intermittent failures, even with aggressive retries.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3110
https://github.com/broadinstitute/gatk/pull/3110:43,Performance,perform,performs,43,"Empirical testing has shown that this tool performs best at scale with cloud buffering; disabled. With cloud buffering on and thousands of concurrent GenomicsDBImport tasks,; we do too many simultaneous GCS accesses (since the prefetcher spawns a new thread for each; reader upon a query) and start seeing intermittent failures, even with aggressive retries.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3110
https://github.com/broadinstitute/gatk/pull/3110:139,Performance,concurren,concurrent,139,"Empirical testing has shown that this tool performs best at scale with cloud buffering; disabled. With cloud buffering on and thousands of concurrent GenomicsDBImport tasks,; we do too many simultaneous GCS accesses (since the prefetcher spawns a new thread for each; reader upon a query) and start seeing intermittent failures, even with aggressive retries.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3110
https://github.com/broadinstitute/gatk/pull/3110:207,Security,access,accesses,207,"Empirical testing has shown that this tool performs best at scale with cloud buffering; disabled. With cloud buffering on and thousands of concurrent GenomicsDBImport tasks,; we do too many simultaneous GCS accesses (since the prefetcher spawns a new thread for each; reader upon a query) and start seeing intermittent failures, even with aggressive retries.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3110
https://github.com/broadinstitute/gatk/pull/3110:10,Testability,test,testing,10,"Empirical testing has shown that this tool performs best at scale with cloud buffering; disabled. With cloud buffering on and thousands of concurrent GenomicsDBImport tasks,; we do too many simultaneous GCS accesses (since the prefetcher spawns a new thread for each; reader upon a query) and start seeing intermittent failures, even with aggressive retries.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3110
https://github.com/broadinstitute/gatk/pull/3113:44,Deployability,pipeline,pipeline,44,"Implements a Bwa Spark tool for the PathSeq pipeline. . Tool input:; 1) BAM of paired reads; 2) BAM of unpaired reads; 3) Bwa index image file. Output:; 1) BAM of paired alignments; 2) BAM of unpaired alignments. Notes:; - The tool does not generate secondary/supplementary alignments. ; - Alternate alignments are written to the SA tag. ; - Only the sequences for which there was at least 1 alignment are written to the BAM headers (instead of writing all sequences in the reference, which is 10,000's for the pathogen database and substantially increases the run time of the subsequent scoring tool).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3113
https://github.com/broadinstitute/gatk/pull/3114:53,Deployability,pipeline,pipeline,53,"Adds pathogen abundance scoring tool for the PathSeq pipeline. . Tool input:; 1) BAM of paired reads; 2) BAM of unpaired reads; 3) Taxonomy database file generated from PathSeqBuildReferenceTaxonomy. Tool output:; 1) Tab-delimited table of taxa scores; 2) Combined BAM with tags indicating which taxa each read was assigned to (optional). In Spark, each read pair / single read is paired with a list of taxa to which it aligned (with sufficient coverage/identity). These lists are collected to the driver and transformed into a map from taxonomic ID to scores (abundance score, number of reads, number of unambiguous reads). Details of the scoring can be found in the tool header comments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3114
https://github.com/broadinstitute/gatk/pull/3115:470,Availability,mask,mask,470,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:657,Availability,mask,masked,657,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:1187,Availability,mask,mask,1187,"ry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:1259,Availability,mask,masking,1259,"**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:5,Deployability,update,updated,5,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:800,Integrability,depend,depending,800,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:903,Integrability,wrap,wrappers,903,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:706,Modifiability,variab,variable,706,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:983,Modifiability,inherit,inherit,983," (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:2175,Modifiability,variab,variable,2175,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:1098,Performance,load,loading,1098," (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:1647,Performance,perform,performed,1647,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:383,Security,hash,hash,383,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:2256,Security,hash,hash,2256,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:2327,Security,hash,hash,2327,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:2363,Security,hash,hashes,2363,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:2425,Security,hash,hash,2425,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3115:2562,Security,hash,hashes,2562,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115
https://github.com/broadinstitute/gatk/pull/3116:5,Modifiability,variab,variable,5,(The variable was renamed to GATK_STACKTRACE_ON_USER_EXCEPTION). There are no mentions left of STACK_TRACE_ON_USEREXCEPTION. Thanks to David who reported this bug on gatk-dev-public.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3116
https://github.com/broadinstitute/gatk/issues/3117:148,Availability,failure,failure,148,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:702,Availability,FAILURE,FAILURE,702,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:737,Availability,failure,failures,737,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:70,Testability,test,test,70,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:143,Testability,test,test,143,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:185,Testability,test,test,185,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:254,Testability,test,testBlockGather,254,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:308,Testability,test,test,308,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:404,Testability,Assert,AssertionError,404,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:491,Testability,test,test,491,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:520,Testability,assert,assertEqualVariants,520,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:640,Testability,test,testBlockGather,640,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:714,Testability,test,tests,714,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:874,Testability,test,test,874,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:960,Testability,test,testBlockGather,960,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:1257,Testability,Assert,Assert,1257,"rationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GATK is able to read many fewer records from the final output file than it does at compression level 5. As mentioned above, at compression levels 1 or 2 it's able to read all the records correctly. If you manually decompress the output file it a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:1264,Testability,assert,assertEquals,1264,"/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GATK is able to read many fewer records from the final output file than it does at compression level 5. As mentioned above, at compression levels 1 or 2 it's able to read all the records correctly. If you manually decompress the output file it appears to have all the correct",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:1367,Testability,test,testBlockGather,1367,".7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GATK is able to read many fewer records from the final output file than it does at compression level 5. As mentioned above, at compression levels 1 or 2 it's able to read all the records correctly. If you manually decompress the output file it appears to have all the correct records in all cases, so it's something wrong with the compression itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:1516,Testability,test,test,1516,".7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GATK is able to read many fewer records from the final output file than it does at compression level 5. As mentioned above, at compression levels 1 or 2 it's able to read all the records correctly. If you manually decompress the output file it appears to have all the correct records in all cases, so it's something wrong with the compression itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:1626,Testability,test,test,1626,".7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GATK is able to read many fewer records from the final output file than it does at compression level 5. As mentioned above, at compression levels 1 or 2 it's able to read all the records correctly. If you manually decompress the output file it appears to have all the correct records in all cases, so it's something wrong with the compression itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:1670,Testability,test,test,1670,".7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GATK is able to read many fewer records from the final output file than it does at compression level 5. As mentioned above, at compression levels 1 or 2 it's able to read all the records correctly. If you manually decompress the output file it appears to have all the correct records in all cases, so it's something wrong with the compression itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:1819,Testability,test,test,1819,".7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GATK is able to read many fewer records from the final output file than it does at compression level 5. As mentioned above, at compression levels 1 or 2 it's able to read all the records correctly. If you manually decompress the output file it appears to have all the correct records in all cases, so it's something wrong with the compression itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3117:1903,Testability,test,test,1903,".7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GATK is able to read many fewer records from the final output file than it does at compression level 5. As mentioned above, at compression levels 1 or 2 it's able to read all the records correctly. If you manually decompress the output file it appears to have all the correct records in all cases, so it's something wrong with the compression itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117
https://github.com/broadinstitute/gatk/issues/3118:104,Deployability,release,release,104,This will provide a temporary fix for https://github.com/broadinstitute/gatk/issues/2793 so that we can release beta,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3118
https://github.com/broadinstitute/gatk/issues/3119:396,Deployability,pipeline,pipeline,396,"Somehow Kris managed to generate a VCF with an index that doesn't have a properly sorted sequence dictionary: gs://broad-dsde-methods/kcibul/bug_for_louis I think it was with GATK4 SelectVariants (with a version prior to the commandline being put in the header), but I'm not 100% sure. Generating the index on the fly with GATK3 works fine. I'm not sure if the original tabix index from the GotC pipeline is okay: gs://broad-jg-dev-storage/temp/09C99383.91c5a812-70c5-4526-a3a2-3e99b9cf08fb.g.vcf.gz.tbi. I found this because GATK3 complained about the contig order.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3119
https://github.com/broadinstitute/gatk/issues/3120:449,Modifiability,inherit,inherit,449,"Currently we set NIO retry settings on a per-Path basis in `BucketUtils.getPathOnGcs()`. This is a bit brittle, since there are places in other libraries like htsjdk that create their own `Path` objects on-the-fly, and these new Path objects don't respect our retry settings (see https://github.com/broadinstitute/gatk/issues/2749). Ideally we'd be able to set these retry settings globally on startup, and all `Path` objects created anywhere would inherit these settings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3120
https://github.com/broadinstitute/gatk/pull/3122:28,Availability,fault,faulty,28,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122
https://github.com/broadinstitute/gatk/pull/3122:198,Availability,fault,faulty,198,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122
https://github.com/broadinstitute/gatk/pull/3122:306,Availability,error,error,306,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122
https://github.com/broadinstitute/gatk/pull/3122:288,Deployability,patch,patch,288,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122
https://github.com/broadinstitute/gatk/pull/3122:35,Energy Efficiency,adapt,adapter,35,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122
https://github.com/broadinstitute/gatk/pull/3122:35,Integrability,adapter,adapter,35,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122
https://github.com/broadinstitute/gatk/pull/3122:35,Modifiability,adapt,adapter,35,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122
https://github.com/broadinstitute/gatk/pull/3124:28,Availability,error,errorProbability,28,"I also removed the obsolete errorProbability variable line of code in the SomaticGenotypingEngine.java and noted this argument is deprecated in the M2ArgumentCollection. Somewhat relatedly, see request in #3123.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3124
https://github.com/broadinstitute/gatk/pull/3124:45,Modifiability,variab,variable,45,"I also removed the obsolete errorProbability variable line of code in the SomaticGenotypingEngine.java and noted this argument is deprecated in the M2ArgumentCollection. Somewhat relatedly, see request in #3123.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3124
https://github.com/broadinstitute/gatk/issues/3125:121,Integrability,depend,dependency,121,"As a stopgap solution to allow `gs://` access on Spark with the local runner, let's add the `gcs-connector` as a project dependency, and craft a test case the runs a simple Spark tool like `PrintReadsSpark` using the local runner with GCS inputs and outputs. I've already started this in the branch https://github.com/broadinstitute/gatk/compare/dr_fix_gcs_spark_writing, but it's not working yet since the gcs-connector requires some extra authentication-related setup.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3125
https://github.com/broadinstitute/gatk/issues/3125:39,Security,access,access,39,"As a stopgap solution to allow `gs://` access on Spark with the local runner, let's add the `gcs-connector` as a project dependency, and craft a test case the runs a simple Spark tool like `PrintReadsSpark` using the local runner with GCS inputs and outputs. I've already started this in the branch https://github.com/broadinstitute/gatk/compare/dr_fix_gcs_spark_writing, but it's not working yet since the gcs-connector requires some extra authentication-related setup.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3125
https://github.com/broadinstitute/gatk/issues/3125:441,Security,authenticat,authentication-related,441,"As a stopgap solution to allow `gs://` access on Spark with the local runner, let's add the `gcs-connector` as a project dependency, and craft a test case the runs a simple Spark tool like `PrintReadsSpark` using the local runner with GCS inputs and outputs. I've already started this in the branch https://github.com/broadinstitute/gatk/compare/dr_fix_gcs_spark_writing, but it's not working yet since the gcs-connector requires some extra authentication-related setup.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3125
https://github.com/broadinstitute/gatk/issues/3125:145,Testability,test,test,145,"As a stopgap solution to allow `gs://` access on Spark with the local runner, let's add the `gcs-connector` as a project dependency, and craft a test case the runs a simple Spark tool like `PrintReadsSpark` using the local runner with GCS inputs and outputs. I've already started this in the branch https://github.com/broadinstitute/gatk/compare/dr_fix_gcs_spark_writing, but it's not working yet since the gcs-connector requires some extra authentication-related setup.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3125
https://github.com/broadinstitute/gatk/issues/3125:166,Usability,simpl,simple,166,"As a stopgap solution to allow `gs://` access on Spark with the local runner, let's add the `gcs-connector` as a project dependency, and craft a test case the runs a simple Spark tool like `PrintReadsSpark` using the local runner with GCS inputs and outputs. I've already started this in the branch https://github.com/broadinstitute/gatk/compare/dr_fix_gcs_spark_writing, but it's not working yet since the gcs-connector requires some extra authentication-related setup.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3125
https://github.com/broadinstitute/gatk/issues/3126:44,Deployability,configurat,configuration,44,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/issues/3126:167,Deployability,configurat,configuration,167,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/issues/3126:798,Integrability,inject,inject,798,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/issues/3126:44,Modifiability,config,configuration,44,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/issues/3126:167,Modifiability,config,configuration,167,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/issues/3126:375,Modifiability,config,config,375,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/issues/3126:798,Security,inject,inject,798,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/issues/3126:15,Testability,mock,mock,15,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/issues/3126:134,Testability,mock,mock,134,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126
https://github.com/broadinstitute/gatk/pull/3127:18,Deployability,release,released,18,from snapshots to released versions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3127
https://github.com/broadinstitute/gatk/pull/3140:1190,Availability,avail,available,1190,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:1479,Availability,down,down,1479,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:1455,Deployability,integrat,integration,1455,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:1455,Integrability,integrat,integration,1455,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:23,Testability,test,tests,23,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:63,Testability,test,tests,63,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:190,Testability,test,test,190,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:307,Testability,test,test,307,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:572,Testability,test,tests,572,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:732,Testability,test,tests,732,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:1089,Testability,test,tests,1089,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:1467,Testability,test,tests,1467,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/pull/3140:1488,Testability,test,test,1488,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140
https://github.com/broadinstitute/gatk/issues/3141:1206,Deployability,pipeline,pipeline,1206,"We found that many GATK4 commands accept an option to let them output ""sharded"" files. But we didn't find how those commands accept ""sharded"" data that generated from the last step. For example,. ```; gatk ReadsPipelineSpark \; -I hdfs://ip-/user/tianj/${sample}_sort.bam \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -O hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; --knownSites hdfs://ip-/genome/ref/Mills_and_1000G_gold_standard.indels.b37.vcf \; --shardedOutput true \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip- \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp"". HaplotypeCallerSpark \; -I hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; -O hdfs://ip-/user/tianj/$sample.vcf \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip-172-31-2-45:7077 \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp""; ```; This pipeline makes ${sample}_ReadsPipelineSpark.bam a directory and it doesn't work. I didn't find any option to specify that the input file is ""sharded"" or not.**How should we use the ""shardedoutput"" option?**. And also, we noticed that for SV calling, there is a whole pipeline command, but for SNP&Indels calling, we only found partial pipeline such as ReadPipelineSpark. **Is there a whole SNP&Indels calling pipeline script we can use?** Though it still seems to cost some unnecessary time keeps writing and reading files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3141
https://github.com/broadinstitute/gatk/issues/3141:1473,Deployability,pipeline,pipeline,1473,"We found that many GATK4 commands accept an option to let them output ""sharded"" files. But we didn't find how those commands accept ""sharded"" data that generated from the last step. For example,. ```; gatk ReadsPipelineSpark \; -I hdfs://ip-/user/tianj/${sample}_sort.bam \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -O hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; --knownSites hdfs://ip-/genome/ref/Mills_and_1000G_gold_standard.indels.b37.vcf \; --shardedOutput true \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip- \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp"". HaplotypeCallerSpark \; -I hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; -O hdfs://ip-/user/tianj/$sample.vcf \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip-172-31-2-45:7077 \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp""; ```; This pipeline makes ${sample}_ReadsPipelineSpark.bam a directory and it doesn't work. I didn't find any option to specify that the input file is ""sharded"" or not.**How should we use the ""shardedoutput"" option?**. And also, we noticed that for SV calling, there is a whole pipeline command, but for SNP&Indels calling, we only found partial pipeline such as ReadPipelineSpark. **Is there a whole SNP&Indels calling pipeline script we can use?** Though it still seems to cost some unnecessary time keeps writing and reading files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3141
https://github.com/broadinstitute/gatk/issues/3141:1541,Deployability,pipeline,pipeline,1541,"We found that many GATK4 commands accept an option to let them output ""sharded"" files. But we didn't find how those commands accept ""sharded"" data that generated from the last step. For example,. ```; gatk ReadsPipelineSpark \; -I hdfs://ip-/user/tianj/${sample}_sort.bam \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -O hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; --knownSites hdfs://ip-/genome/ref/Mills_and_1000G_gold_standard.indels.b37.vcf \; --shardedOutput true \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip- \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp"". HaplotypeCallerSpark \; -I hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; -O hdfs://ip-/user/tianj/$sample.vcf \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip-172-31-2-45:7077 \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp""; ```; This pipeline makes ${sample}_ReadsPipelineSpark.bam a directory and it doesn't work. I didn't find any option to specify that the input file is ""sharded"" or not.**How should we use the ""shardedoutput"" option?**. And also, we noticed that for SV calling, there is a whole pipeline command, but for SNP&Indels calling, we only found partial pipeline such as ReadPipelineSpark. **Is there a whole SNP&Indels calling pipeline script we can use?** Though it still seems to cost some unnecessary time keeps writing and reading files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3141
https://github.com/broadinstitute/gatk/issues/3141:1615,Deployability,pipeline,pipeline,1615,"We found that many GATK4 commands accept an option to let them output ""sharded"" files. But we didn't find how those commands accept ""sharded"" data that generated from the last step. For example,. ```; gatk ReadsPipelineSpark \; -I hdfs://ip-/user/tianj/${sample}_sort.bam \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -O hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; --knownSites hdfs://ip-/genome/ref/Mills_and_1000G_gold_standard.indels.b37.vcf \; --shardedOutput true \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip- \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp"". HaplotypeCallerSpark \; -I hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; -O hdfs://ip-/user/tianj/$sample.vcf \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip-172-31-2-45:7077 \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp""; ```; This pipeline makes ${sample}_ReadsPipelineSpark.bam a directory and it doesn't work. I didn't find any option to specify that the input file is ""sharded"" or not.**How should we use the ""shardedoutput"" option?**. And also, we noticed that for SV calling, there is a whole pipeline command, but for SNP&Indels calling, we only found partial pipeline such as ReadPipelineSpark. **Is there a whole SNP&Indels calling pipeline script we can use?** Though it still seems to cost some unnecessary time keeps writing and reading files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3141
https://github.com/broadinstitute/gatk/issues/3142:25,Integrability,message,message,25,"In the command line help message, it says . ```--input,-I:String BAM/SAM/CRAM file containing reads This argument must be specified at least once.```. However, if we actually give multiple input files, it says. ```org.broadinstitute.hellbender.exceptions.UserException: Sorry, we only support a single reads input for spark tools for now.```. On the other hand, if we specify the input parameter as the folder containing all partial bam files, it actually works. Could you explain how this feature works now?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3142
https://github.com/broadinstitute/gatk/issues/3150:1020,Energy Efficiency,reduce,reduce,1020,"Pair-HMM costs O(read length x haplotype length) because these are the dimensions of the matrix of pairwise alignments that it fills. A common use of Pair-HMM is inter-species sequence comparison, in which the sequences in question may be very different. In this case, one needs to account for many potential indels, that is, horizontal and vertical moves in the alignment matrix. However, the realignment step in HaplotypeCaller and Mutect does not seek to align two distantly-related sequences. Rather, we use Pair-HMM to find evidence of variant alleles. When a read aligns poorly to a variant haplotype, we usually don't care exactly *how* poor the alignment is. Either way, it can't convince us to make a call. We therefore might not need a fully general alignment of reads to haplotypes. Evaluating a diagonal of band of width n on either side corresponds to considering only alignments in which the read gets no more than n insertions ahead or n deletions behind the haplotype. Since these indels would seriously reduce the alignment likelihood, this effectively means that we would replace very small likelihoods with zero. Note that we would not want to do this with the reference haplotype because we make calls based on the relative likelihood of ref versus alt. This would require determining the alignment start of the read versus the haplotype, but that should be easily doable in linear time, which, by the way, is how band diagonal Pair-HMM would scale. Perhaps this would let us get away with bigger haplotypes. This seems like the sort of thing Valentin may have thought about already. I'll need to vet this idea with him.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3150
https://github.com/broadinstitute/gatk/issues/3154:140,Availability,down,downloading,140,"This **CRAM file** and **its reference** are identical to those I have in the cloud because I uploaded these exact files to the cloud after downloading them from the 1000 Genomes Project FTP site. Back then, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3154:447,Availability,error,error,447,"This **CRAM file** and **its reference** are identical to those I have in the cloud because I uploaded these exact files to the cloud after downloading them from the 1000 Genomes Project FTP site. Back then, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3154:879,Availability,error,error,879,"This **CRAM file** and **its reference** are identical to those I have in the cloud because I uploaded these exact files to the cloud after downloading them from the 1000 Genomes Project FTP site. Back then, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3154:1037,Availability,ERROR,ERROR,1037,"ecause I uploaded these exact files to the cloud after downloading them from the 1000 Genomes Project FTP site. Back then, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3154:1203,Availability,down,down,1203,"n, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3154:3183,Availability,error,error,3183,"getCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:779); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); at org.broadinstitute.hellbender.Main.main(Main.java:221); ```; This error occurs fairly soon after launching the job, after the progress meter shows the tool iterating over chromosome 1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3154:3252,Energy Efficiency,meter,meter,3252,"getCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:779); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); at org.broadinstitute.hellbender.Main.main(Main.java:221); ```; This error occurs fairly soon after launching the job, after the progress meter shows the tool iterating over chromosome 1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3154:2137,Integrability,wrap,wrapAndCopyInto,2137,"47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:779); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); at org.broadinstitute.hellbender.Main.ma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3154:1717,Testability,Assert,AssertingIterator,1717,"hlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:779); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154
https://github.com/broadinstitute/gatk/issues/3156:339,Deployability,pipeline,pipeline,339,"As requested in the forum by SL. ----; @esalinas The segment means in the .seg file should be non log_2 (this is the output of the GATK CNV tool, as indicated in the javadoc), but the tangent normalized coverages in the tn.tsv should be log_2. @shlee Perhaps it's worth making this more explicit in the javadoc. Future versions of the CNV pipeline will only deal with raw (integer) read-count coverage and only output non log_2 copy-ratio estimates, which will hopefully prevent such sources of confusion. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/39744#Comment_39744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3156
https://github.com/broadinstitute/gatk/pull/3158:165,Availability,down,downloading,165,* Add a table of contents.; * Update out-of-date information.; * Merge in information from the old gatk-protected README; * Add section on git-lfs; * Add section on downloading GATK4; * Add section on documentation generation; * Add section on zenhub; * Remove no-longer-needed protected-root directory. Resolves #2775; Resolves #2978; Resolves #2487; Resolves #2461,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158
https://github.com/broadinstitute/gatk/pull/3158:30,Deployability,Update,Update,30,* Add a table of contents.; * Update out-of-date information.; * Merge in information from the old gatk-protected README; * Add section on git-lfs; * Add section on downloading GATK4; * Add section on documentation generation; * Add section on zenhub; * Remove no-longer-needed protected-root directory. Resolves #2775; Resolves #2978; Resolves #2487; Resolves #2461,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158
https://github.com/broadinstitute/gatk/pull/3159:187,Deployability,update,updated,187,For the tests to work we must define:; HELLBENDER_TEST_PROJECT : Google Project to use; HELLBENDER_JSON_SERVICE_ACCOUNT_KEY : path to a JSON file with service; account credentials. (I've updated the README accordingly). (We've used both before so they should already be configured). This fixes issue #3125,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159
https://github.com/broadinstitute/gatk/pull/3159:270,Modifiability,config,configured,270,For the tests to work we must define:; HELLBENDER_TEST_PROJECT : Google Project to use; HELLBENDER_JSON_SERVICE_ACCOUNT_KEY : path to a JSON file with service; account credentials. (I've updated the README accordingly). (We've used both before so they should already be configured). This fixes issue #3125,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159
https://github.com/broadinstitute/gatk/pull/3159:8,Testability,test,tests,8,For the tests to work we must define:; HELLBENDER_TEST_PROJECT : Google Project to use; HELLBENDER_JSON_SERVICE_ACCOUNT_KEY : path to a JSON file with service; account credentials. (I've updated the README accordingly). (We've used both before so they should already be configured). This fixes issue #3125,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159
https://github.com/broadinstitute/gatk/issues/3160:140,Security,HASH,HASH,140,"Rightnow doing the manual docker build locally would fail. One needs to copy the line in `.travis.yml`. ```; sudo bash build_docker.sh -e ${HASH} -s -u -d $PWD/temp_staging/;; sudo docker run -v $(pwd)/src/test/resources:/testdata --rm -e ""TEST_VERBOSITY=minimal"" -e ""TEST_TYPE=${TEST_TYPE}"" -t broadinstitute/gatk:${HASH} bash /root/run_unit_tests.sh;; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3160
https://github.com/broadinstitute/gatk/issues/3160:317,Security,HASH,HASH,317,"Rightnow doing the manual docker build locally would fail. One needs to copy the line in `.travis.yml`. ```; sudo bash build_docker.sh -e ${HASH} -s -u -d $PWD/temp_staging/;; sudo docker run -v $(pwd)/src/test/resources:/testdata --rm -e ""TEST_VERBOSITY=minimal"" -e ""TEST_TYPE=${TEST_TYPE}"" -t broadinstitute/gatk:${HASH} bash /root/run_unit_tests.sh;; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3160
https://github.com/broadinstitute/gatk/issues/3160:206,Testability,test,test,206,"Rightnow doing the manual docker build locally would fail. One needs to copy the line in `.travis.yml`. ```; sudo bash build_docker.sh -e ${HASH} -s -u -d $PWD/temp_staging/;; sudo docker run -v $(pwd)/src/test/resources:/testdata --rm -e ""TEST_VERBOSITY=minimal"" -e ""TEST_TYPE=${TEST_TYPE}"" -t broadinstitute/gatk:${HASH} bash /root/run_unit_tests.sh;; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3160
https://github.com/broadinstitute/gatk/issues/3160:222,Testability,test,testdata,222,"Rightnow doing the manual docker build locally would fail. One needs to copy the line in `.travis.yml`. ```; sudo bash build_docker.sh -e ${HASH} -s -u -d $PWD/temp_staging/;; sudo docker run -v $(pwd)/src/test/resources:/testdata --rm -e ""TEST_VERBOSITY=minimal"" -e ""TEST_TYPE=${TEST_TYPE}"" -t broadinstitute/gatk:${HASH} bash /root/run_unit_tests.sh;; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3160
https://github.com/broadinstitute/gatk/issues/3161:240,Availability,avail,available,240,"Port https://github.com/broadgsa/gatk-protected/pull/24; ```; Without this patch the stream is only closed (thus, flushed) when the; object is garbage collected. This is problematic when subsequent jobs; proceed and expect the output to be available, for example; AnalyzeCovariates. We see failures in approximately 50% of runs due to; this issue and they are confirmed as fixed when applying the patch (on; a busy machine using NFS storage).; ```. The tool `BaseRecalibratorSparkSharded` is affected. The fix will be in `BaseRecalibratorEngineSparkWrapper.saveTextualReport`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3161
https://github.com/broadinstitute/gatk/issues/3161:290,Availability,failure,failures,290,"Port https://github.com/broadgsa/gatk-protected/pull/24; ```; Without this patch the stream is only closed (thus, flushed) when the; object is garbage collected. This is problematic when subsequent jobs; proceed and expect the output to be available, for example; AnalyzeCovariates. We see failures in approximately 50% of runs due to; this issue and they are confirmed as fixed when applying the patch (on; a busy machine using NFS storage).; ```. The tool `BaseRecalibratorSparkSharded` is affected. The fix will be in `BaseRecalibratorEngineSparkWrapper.saveTextualReport`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3161
https://github.com/broadinstitute/gatk/issues/3161:75,Deployability,patch,patch,75,"Port https://github.com/broadgsa/gatk-protected/pull/24; ```; Without this patch the stream is only closed (thus, flushed) when the; object is garbage collected. This is problematic when subsequent jobs; proceed and expect the output to be available, for example; AnalyzeCovariates. We see failures in approximately 50% of runs due to; this issue and they are confirmed as fixed when applying the patch (on; a busy machine using NFS storage).; ```. The tool `BaseRecalibratorSparkSharded` is affected. The fix will be in `BaseRecalibratorEngineSparkWrapper.saveTextualReport`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3161
https://github.com/broadinstitute/gatk/issues/3161:397,Deployability,patch,patch,397,"Port https://github.com/broadgsa/gatk-protected/pull/24; ```; Without this patch the stream is only closed (thus, flushed) when the; object is garbage collected. This is problematic when subsequent jobs; proceed and expect the output to be available, for example; AnalyzeCovariates. We see failures in approximately 50% of runs due to; this issue and they are confirmed as fixed when applying the patch (on; a busy machine using NFS storage).; ```. The tool `BaseRecalibratorSparkSharded` is affected. The fix will be in `BaseRecalibratorEngineSparkWrapper.saveTextualReport`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3161
https://github.com/broadinstitute/gatk/issues/3163:108,Availability,error,error,108,"For discussion with @LeeTL1220 @samuelklee @mbabadi @vruano. ---; ### CreatePanelOfNormals gives unexpected error; Running CNV's CreatePanelOfNormals on forty 1000 Genomes Project WES samples using conservatively derived and padded target intervals (189,493 targets). Tool gives the following error. ```; Using GATK jar /usr/shlee/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-package-4.alpha.2-1134-ga9d9d91-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -Xmx16g -jar /usr/shlee/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-package-4.alpha.2-1134-ga9d9d91-SNAPSHOT-local.jar CreatePanelOfNormals -I shlee-dev/CNV/1kgmix_gccorrect_pon/CreateCNVPon/95bf88a5-c5aa-45eb-9178-efc7d5c75946/call-CombineReadCounts/String_combinedcoverage.tsv -O String.pon --disableSpark; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [June 23, 2017 6:54:00 PM UTC] CreatePanelOfNormals --input shlee-dev/CNV/1kgmix_gccorrect_pon/CreateCNVPon/95bf88a5-c5aa-45eb-9178-efc7d5c75946/call-CombineReadCounts/String_combinedcoverage.tsv --output String.pon --disableSpark true --minimumTargetFactorPercentileThreshold 25.0 --maximumColumnZerosPercentage 2.0 --maximumTargetZerosPercentage 5.0 --extremeColumnMedianCountPercentileThreshold 2.5 --truncatePercentileThreshold 0.1 --numberOfEigensamples auto --noQC false --dryRun false --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false; [June 23, 2017 6:54:00 PM UTC] Executing as root@b4f42b5ba157 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_91-8u91-b14-1~bpo8+1-b14; Version: 4.alpha.2-1134-ga9d9d91-SNAPSHOT; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://l",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163
https://github.com/broadinstitute/gatk/issues/3163:293,Availability,error,error,293,"For discussion with @LeeTL1220 @samuelklee @mbabadi @vruano. ---; ### CreatePanelOfNormals gives unexpected error; Running CNV's CreatePanelOfNormals on forty 1000 Genomes Project WES samples using conservatively derived and padded target intervals (189,493 targets). Tool gives the following error. ```; Using GATK jar /usr/shlee/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-package-4.alpha.2-1134-ga9d9d91-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -Xmx16g -jar /usr/shlee/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-package-4.alpha.2-1134-ga9d9d91-SNAPSHOT-local.jar CreatePanelOfNormals -I shlee-dev/CNV/1kgmix_gccorrect_pon/CreateCNVPon/95bf88a5-c5aa-45eb-9178-efc7d5c75946/call-CombineReadCounts/String_combinedcoverage.tsv -O String.pon --disableSpark; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [June 23, 2017 6:54:00 PM UTC] CreatePanelOfNormals --input shlee-dev/CNV/1kgmix_gccorrect_pon/CreateCNVPon/95bf88a5-c5aa-45eb-9178-efc7d5c75946/call-CombineReadCounts/String_combinedcoverage.tsv --output String.pon --disableSpark true --minimumTargetFactorPercentileThreshold 25.0 --maximumColumnZerosPercentage 2.0 --maximumTargetZerosPercentage 5.0 --extremeColumnMedianCountPercentileThreshold 2.5 --truncatePercentileThreshold 0.1 --numberOfEigensamples auto --noQC false --dryRun false --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false; [June 23, 2017 6:54:00 PM UTC] Executing as root@b4f42b5ba157 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_91-8u91-b14-1~bpo8+1-b14; Version: 4.alpha.2-1134-ga9d9d91-SNAPSHOT; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://l",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163
https://github.com/broadinstitute/gatk/issues/3163:2306,Availability,ERROR,ERROR,2306,"tPercentileThreshold 2.5 --truncatePercentileThreshold 0.1 --numberOfEigensamples auto --noQC false --dryRun false --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false; [June 23, 2017 6:54:00 PM UTC] Executing as root@b4f42b5ba157 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_91-8u91-b14-1~bpo8+1-b14; Version: 4.alpha.2-1134-ga9d9d91-SNAPSHOT; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; [June 23, 2017 6:54:09 PM UTC] org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormals done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=1400373248; ***********************************************************************. A USER ERROR has occurred: Bad input: The number of zeros per count column is too large resulting in all count columns to be dropped. ***********************************************************************; Use -DSTACK_TRACE_ON_USEREXCEPTION to print the stack trace.; ```. I am running the tool with parameters that should be the standard, i.e. with QC, unlike the settings in our repo's WDL scripts ([WDL](https://github.com/broadinstitute/gatk/blob/502fd4119ebde964d24d39aafd1b7346ac5d84d5/scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl#L137), [JSON](https://github.com/broadinstitute/gatk/blob/56e6baa79b4e56ebee5fb8d2b2288373a4269fa8/scripts/cnv_cromwell_tests/somatic/cnv_somatic_panel_wes_workflow.json#L9)). The command I use is:; ```; 	command {; 	/usr/shlee/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; 		--javaOptions ""-Xmx16g"" \; 		CreatePanelOfNormals \; 	-I ${combined_coverage} \; 	-O ${basename}.pon ${additional_options} \; 	--disableSpark; 	}; ```. Where there are no additional options. ---; The coverage counts were processed with",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163
https://github.com/broadinstitute/gatk/issues/3163:11684,Modifiability,variab,variables,11684,"ional coverages, e.g. a value of 0.499 rounds to zero. The QC step does not transform data per se but does then throw out data points (targets, samples) from consideration and inclusion in the PoN. VR's debugging traces this QC step to HDF5PCACoveragePoNCreationUtils.java, line 212. It appears the current workaround in the official WDL scripts towards preventing data from being thrown out is to disable the QC step altogether, with the `--noQC true` option. However, this appears to me a hack that does not allow us to create a more refined PoN that the QC is meant to enable. **Can someone explain the impact of skipping QC?**. @mbabadi has said for me to go ahead and fix this line of code by changing the `roundToInteger` to true. I am placing this issue here for further discussion and to separate the discussion from the PR. ---; ### Information is lacking in our repo regards to certain settings; Our understanding of the CNV workflow was to use proportional coverage (PCOV) for CalculateTargetCoverage, whether for the PoN or samples. However, @LeeTL1220, there isn't any example INPUTS JSON or discussion of settings for such for the somatic WDL variables pertaining to `gatk/scripts/cnv_wdl/cnv_common_tasks.wdl`'s CalculateTargetCoverage task, where `transform` is the variable that defines whether counts ought to be raw or proportional. . If I search the repo for the WDL variable ""transform PCOV json"", then I get no hits. However, if I search the repo for ""transform RAW json"", then I get germline calling workflows that show '""transform = ""RAW""'. For example, '""transform = ""RAW""' is in `scripts/cnv_wdl/germline/cnv_germline_single_sample_calling_workflow.wdl` and `scripts/cnv_wdl/germline/cnv_germline_panel_creation_workflow.wdl`. Please correct me if I am wrong but It seems to me that this setting shouldn't be different between the binned targets of the germline workflow and a somatic WES workflow. What is the reason we use proportional (PCOV) counts instead of RAW counts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163
https://github.com/broadinstitute/gatk/issues/3163:11809,Modifiability,variab,variable,11809,"ional coverages, e.g. a value of 0.499 rounds to zero. The QC step does not transform data per se but does then throw out data points (targets, samples) from consideration and inclusion in the PoN. VR's debugging traces this QC step to HDF5PCACoveragePoNCreationUtils.java, line 212. It appears the current workaround in the official WDL scripts towards preventing data from being thrown out is to disable the QC step altogether, with the `--noQC true` option. However, this appears to me a hack that does not allow us to create a more refined PoN that the QC is meant to enable. **Can someone explain the impact of skipping QC?**. @mbabadi has said for me to go ahead and fix this line of code by changing the `roundToInteger` to true. I am placing this issue here for further discussion and to separate the discussion from the PR. ---; ### Information is lacking in our repo regards to certain settings; Our understanding of the CNV workflow was to use proportional coverage (PCOV) for CalculateTargetCoverage, whether for the PoN or samples. However, @LeeTL1220, there isn't any example INPUTS JSON or discussion of settings for such for the somatic WDL variables pertaining to `gatk/scripts/cnv_wdl/cnv_common_tasks.wdl`'s CalculateTargetCoverage task, where `transform` is the variable that defines whether counts ought to be raw or proportional. . If I search the repo for the WDL variable ""transform PCOV json"", then I get no hits. However, if I search the repo for ""transform RAW json"", then I get germline calling workflows that show '""transform = ""RAW""'. For example, '""transform = ""RAW""' is in `scripts/cnv_wdl/germline/cnv_germline_single_sample_calling_workflow.wdl` and `scripts/cnv_wdl/germline/cnv_germline_panel_creation_workflow.wdl`. Please correct me if I am wrong but It seems to me that this setting shouldn't be different between the binned targets of the germline workflow and a somatic WES workflow. What is the reason we use proportional (PCOV) counts instead of RAW counts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163
https://github.com/broadinstitute/gatk/issues/3163:11914,Modifiability,variab,variable,11914,"ional coverages, e.g. a value of 0.499 rounds to zero. The QC step does not transform data per se but does then throw out data points (targets, samples) from consideration and inclusion in the PoN. VR's debugging traces this QC step to HDF5PCACoveragePoNCreationUtils.java, line 212. It appears the current workaround in the official WDL scripts towards preventing data from being thrown out is to disable the QC step altogether, with the `--noQC true` option. However, this appears to me a hack that does not allow us to create a more refined PoN that the QC is meant to enable. **Can someone explain the impact of skipping QC?**. @mbabadi has said for me to go ahead and fix this line of code by changing the `roundToInteger` to true. I am placing this issue here for further discussion and to separate the discussion from the PR. ---; ### Information is lacking in our repo regards to certain settings; Our understanding of the CNV workflow was to use proportional coverage (PCOV) for CalculateTargetCoverage, whether for the PoN or samples. However, @LeeTL1220, there isn't any example INPUTS JSON or discussion of settings for such for the somatic WDL variables pertaining to `gatk/scripts/cnv_wdl/cnv_common_tasks.wdl`'s CalculateTargetCoverage task, where `transform` is the variable that defines whether counts ought to be raw or proportional. . If I search the repo for the WDL variable ""transform PCOV json"", then I get no hits. However, if I search the repo for ""transform RAW json"", then I get germline calling workflows that show '""transform = ""RAW""'. For example, '""transform = ""RAW""' is in `scripts/cnv_wdl/germline/cnv_germline_single_sample_calling_workflow.wdl` and `scripts/cnv_wdl/germline/cnv_germline_panel_creation_workflow.wdl`. Please correct me if I am wrong but It seems to me that this setting shouldn't be different between the binned targets of the germline workflow and a somatic WES workflow. What is the reason we use proportional (PCOV) counts instead of RAW counts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163
https://github.com/broadinstitute/gatk/issues/3163:1874,Testability,log,logger,1874,"call-CombineReadCounts/String_combinedcoverage.tsv -O String.pon --disableSpark; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [June 23, 2017 6:54:00 PM UTC] CreatePanelOfNormals --input shlee-dev/CNV/1kgmix_gccorrect_pon/CreateCNVPon/95bf88a5-c5aa-45eb-9178-efc7d5c75946/call-CombineReadCounts/String_combinedcoverage.tsv --output String.pon --disableSpark true --minimumTargetFactorPercentileThreshold 25.0 --maximumColumnZerosPercentage 2.0 --maximumTargetZerosPercentage 5.0 --extremeColumnMedianCountPercentileThreshold 2.5 --truncatePercentileThreshold 0.1 --numberOfEigensamples auto --noQC false --dryRun false --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false; [June 23, 2017 6:54:00 PM UTC] Executing as root@b4f42b5ba157 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_91-8u91-b14-1~bpo8+1-b14; Version: 4.alpha.2-1134-ga9d9d91-SNAPSHOT; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; [June 23, 2017 6:54:09 PM UTC] org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormals done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=1400373248; ***********************************************************************. A USER ERROR has occurred: Bad input: The number of zeros per count column is too large resulting in all count columns to be dropped. ***********************************************************************; Use -DSTACK_TRACE_ON_USEREXCEPTION to print the stack trace.; ```. I am running the tool with parameters that should be the standard, i.e. with QC, unlike the settings in our repo's WDL scripts ([WDL](https://github.com/broadinstitute/gatk/blob/502fd4119ebde964d24d39aafd1b7346ac5d84d5/scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163
https://github.com/broadinstitute/gatk/issues/3163:2000,Testability,log,logging,2000,"e 23, 2017 6:54:00 PM UTC] CreatePanelOfNormals --input shlee-dev/CNV/1kgmix_gccorrect_pon/CreateCNVPon/95bf88a5-c5aa-45eb-9178-efc7d5c75946/call-CombineReadCounts/String_combinedcoverage.tsv --output String.pon --disableSpark true --minimumTargetFactorPercentileThreshold 25.0 --maximumColumnZerosPercentage 2.0 --maximumTargetZerosPercentage 5.0 --extremeColumnMedianCountPercentileThreshold 2.5 --truncatePercentileThreshold 0.1 --numberOfEigensamples auto --noQC false --dryRun false --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false; [June 23, 2017 6:54:00 PM UTC] Executing as root@b4f42b5ba157 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_91-8u91-b14-1~bpo8+1-b14; Version: 4.alpha.2-1134-ga9d9d91-SNAPSHOT; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; [June 23, 2017 6:54:09 PM UTC] org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormals done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=1400373248; ***********************************************************************. A USER ERROR has occurred: Bad input: The number of zeros per count column is too large resulting in all count columns to be dropped. ***********************************************************************; Use -DSTACK_TRACE_ON_USEREXCEPTION to print the stack trace.; ```. I am running the tool with parameters that should be the standard, i.e. with QC, unlike the settings in our repo's WDL scripts ([WDL](https://github.com/broadinstitute/gatk/blob/502fd4119ebde964d24d39aafd1b7346ac5d84d5/scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl#L137), [JSON](https://github.com/broadinstitute/gatk/blob/56e6baa79b4e56ebee5fb8d2b2288373a4269fa8/scripts/cnv_cromwell_tests/somatic/cnv_somatic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163
https://github.com/broadinstitute/gatk/pull/3164:0,Energy Efficiency,Green,Green,0,"Green light from devs to change this. However, perhaps more discussion is warranted. Please see #3163 for details leading to this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3164
https://github.com/broadinstitute/gatk/pull/3165:8,Deployability,update,updates,8,"This PR updates the Freemarker templates so that the resulting pages will work with the current state of the website code. Most of the changes have to do with functionality I put in to enable hosting multiple doc versions and easy switching between them via a dropdown menu. . I had already done some retrofitting on older tooldocs so the versioned tool docs go back to 3.5, and we can add beta versions of 4 without changing the ""latest supported version"". . The only remaining problem is that I couldn't figure out how to output php instead of html. To test the web integration, I just renamed all *.html to *.php with `for f in *.html; do mv -- ""$f"" ""${f%.html}.php""; done` but that doesn't take care of internal links, which are of course broken as a result. @cmnbroad please let me know if I missed something obvious on this front ^^. . That being said this PR is fully functional as far as I'm concerned.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3165
https://github.com/broadinstitute/gatk/pull/3165:568,Deployability,integrat,integration,568,"This PR updates the Freemarker templates so that the resulting pages will work with the current state of the website code. Most of the changes have to do with functionality I put in to enable hosting multiple doc versions and easy switching between them via a dropdown menu. . I had already done some retrofitting on older tooldocs so the versioned tool docs go back to 3.5, and we can add beta versions of 4 without changing the ""latest supported version"". . The only remaining problem is that I couldn't figure out how to output php instead of html. To test the web integration, I just renamed all *.html to *.php with `for f in *.html; do mv -- ""$f"" ""${f%.html}.php""; done` but that doesn't take care of internal links, which are of course broken as a result. @cmnbroad please let me know if I missed something obvious on this front ^^. . That being said this PR is fully functional as far as I'm concerned.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3165
https://github.com/broadinstitute/gatk/pull/3165:568,Integrability,integrat,integration,568,"This PR updates the Freemarker templates so that the resulting pages will work with the current state of the website code. Most of the changes have to do with functionality I put in to enable hosting multiple doc versions and easy switching between them via a dropdown menu. . I had already done some retrofitting on older tooldocs so the versioned tool docs go back to 3.5, and we can add beta versions of 4 without changing the ""latest supported version"". . The only remaining problem is that I couldn't figure out how to output php instead of html. To test the web integration, I just renamed all *.html to *.php with `for f in *.html; do mv -- ""$f"" ""${f%.html}.php""; done` but that doesn't take care of internal links, which are of course broken as a result. @cmnbroad please let me know if I missed something obvious on this front ^^. . That being said this PR is fully functional as far as I'm concerned.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3165
https://github.com/broadinstitute/gatk/pull/3165:555,Testability,test,test,555,"This PR updates the Freemarker templates so that the resulting pages will work with the current state of the website code. Most of the changes have to do with functionality I put in to enable hosting multiple doc versions and easy switching between them via a dropdown menu. . I had already done some retrofitting on older tooldocs so the versioned tool docs go back to 3.5, and we can add beta versions of 4 without changing the ""latest supported version"". . The only remaining problem is that I couldn't figure out how to output php instead of html. To test the web integration, I just renamed all *.html to *.php with `for f in *.html; do mv -- ""$f"" ""${f%.html}.php""; done` but that doesn't take care of internal links, which are of course broken as a result. @cmnbroad please let me know if I missed something obvious on this front ^^. . That being said this PR is fully functional as far as I'm concerned.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3165
https://github.com/broadinstitute/gatk/pull/3169:0,Integrability,Depend,Depends,0,Depends on https://github.com/broadinstitute/gatk/pull/3165.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3169
https://github.com/broadinstitute/gatk/pull/3171:64,Security,validat,validation,64,@takutosato These are the changes I made for the most recent GP validation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3171
https://github.com/broadinstitute/gatk/pull/3172:57,Testability,test,tests,57,@LeeTL1220 Can I assign the review to you once automatic tests are back?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3172
https://github.com/broadinstitute/gatk/issues/3173:51,Availability,error,errors,51,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173
https://github.com/broadinstitute/gatk/issues/3173:83,Availability,error,errors,83,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173
https://github.com/broadinstitute/gatk/issues/3173:163,Availability,error,errors,163,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173
https://github.com/broadinstitute/gatk/issues/3173:275,Availability,error,error,275,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173
https://github.com/broadinstitute/gatk/issues/3173:316,Modifiability,extend,extend,316,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173
https://github.com/broadinstitute/gatk/issues/3173:478,Modifiability,extend,extended,478,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173
https://github.com/broadinstitute/gatk/pull/3176:58,Testability,Test,TestResources,58,* Remove repo-specific files from `BaseTest` in favor of `TestResources` constants; * Remove `GenomeLocParser`initialization in `BaseTest` in favor of on-demand initialization in `TestResources` and getters for related objects. Closes #3029 ; Closes #2125,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3176
https://github.com/broadinstitute/gatk/pull/3176:180,Testability,Test,TestResources,180,* Remove repo-specific files from `BaseTest` in favor of `TestResources` constants; * Remove `GenomeLocParser`initialization in `BaseTest` in favor of on-demand initialization in `TestResources` and getters for related objects. Closes #3029 ; Closes #2125,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3176
https://github.com/broadinstitute/gatk/pull/3177:113,Integrability,message,messages,113,"es #1572. This commit also addresses #3069, by virtue of building against GKL 0.5.3, which pushes INFO and WARN messages to the Java logger.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3177
https://github.com/broadinstitute/gatk/pull/3177:134,Testability,log,logger,134,"es #1572. This commit also addresses #3069, by virtue of building against GKL 0.5.3, which pushes INFO and WARN messages to the Java logger.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3177
https://github.com/broadinstitute/gatk/pull/3179:127,Security,authenticat,authenticated,127,"This should resolve our git-lfs quota issues, since the quotas; for unauthenticated requests are much stingier than those for; authenticated requests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3179
https://github.com/broadinstitute/gatk/issues/3180:440,Availability,avail,availability,440,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:597,Availability,avail,available,597,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:369,Deployability,integrat,integrate,369,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:481,Deployability,integrat,integrate,481,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:112,Energy Efficiency,POWER,POWER,112,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:226,Energy Efficiency,POWER,POWER,226,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:369,Integrability,integrat,integrate,369,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:481,Integrability,integrat,integrate,481,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:199,Performance,perform,performance,199,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:193,Safety,avoid,avoid,193,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3180:611,Testability,test,test,611,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180
https://github.com/broadinstitute/gatk/issues/3181:80,Deployability,integrat,integration,80,"Regression test for #3163. A unit test was added in #3164, but we should add an integration test as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181
https://github.com/broadinstitute/gatk/issues/3181:80,Integrability,integrat,integration,80,"Regression test for #3163. A unit test was added in #3164, but we should add an integration test as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181
https://github.com/broadinstitute/gatk/issues/3181:11,Testability,test,test,11,"Regression test for #3163. A unit test was added in #3164, but we should add an integration test as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181
https://github.com/broadinstitute/gatk/issues/3181:34,Testability,test,test,34,"Regression test for #3163. A unit test was added in #3164, but we should add an integration test as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181
https://github.com/broadinstitute/gatk/issues/3181:92,Testability,test,test,92,"Regression test for #3163. A unit test was added in #3164, but we should add an integration test as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181
https://github.com/broadinstitute/gatk/pull/3183:312,Availability,mask,masks,312,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:341,Deployability,integrat,integration,341,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:393,Deployability,integrat,integration,393,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:176,Integrability,message,messages,176,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:341,Integrability,integrat,integration,341,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:393,Integrability,integrat,integration,393,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:172,Testability,log,log,172,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:353,Testability,test,tests,353,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:405,Testability,test,test,405,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/pull/3183:457,Testability,test,test,457,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183
https://github.com/broadinstitute/gatk/issues/3184:234,Energy Efficiency,adapt,adaptor,234,"@sooheelee has some serious concerns about `ReadClipper.hardClipAdaptorSequence()`, which is called in `Mutect2` and `HaplotypeCaller` via `AssemblyBasedCallerUtils.finalizeRegion()`. She thinks that the method being used to find the adaptor boundary for clipping purposes is completely bogus!. This is some pretty old code that was also in the GATK3 versions of these tools, so if it's crazy, then we can at least take ""comfort"" in the fact that it's been like this for a very long time...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184
https://github.com/broadinstitute/gatk/issues/3184:234,Modifiability,adapt,adaptor,234,"@sooheelee has some serious concerns about `ReadClipper.hardClipAdaptorSequence()`, which is called in `Mutect2` and `HaplotypeCaller` via `AssemblyBasedCallerUtils.finalizeRegion()`. She thinks that the method being used to find the adaptor boundary for clipping purposes is completely bogus!. This is some pretty old code that was also in the GATK3 versions of these tools, so if it's crazy, then we can at least take ""comfort"" in the fact that it's been like this for a very long time...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184
https://github.com/broadinstitute/gatk/issues/3186:4156,Availability,error,error,4156,"pPartitions$1$$anonfun$apply$23.apply(RDD.scala:796); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; And I notice `gatk/src/main/java/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.java` doesn't exist, and there is no class file `gatk/build/classes/main/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.class` either. Is that causing this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186
https://github.com/broadinstitute/gatk/issues/3186:3424,Energy Efficiency,schedul,scheduler,3424,"pPartitions$1$$anonfun$apply$23.apply(RDD.scala:796); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; And I notice `gatk/src/main/java/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.java` doesn't exist, and there is no class file `gatk/build/classes/main/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.class` either. Is that causing this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186
https://github.com/broadinstitute/gatk/issues/3186:3503,Energy Efficiency,schedul,scheduler,3503,"pPartitions$1$$anonfun$apply$23.apply(RDD.scala:796); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; And I notice `gatk/src/main/java/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.java` doesn't exist, and there is no class file `gatk/build/classes/main/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.class` either. Is that causing this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186
https://github.com/broadinstitute/gatk/issues/3186:3582,Energy Efficiency,schedul,scheduler,3582,"pPartitions$1$$anonfun$apply$23.apply(RDD.scala:796); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; And I notice `gatk/src/main/java/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.java` doesn't exist, and there is no class file `gatk/build/classes/main/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.class` either. Is that causing this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186
https://github.com/broadinstitute/gatk/issues/3186:3704,Performance,concurren,concurrent,3704,"pPartitions$1$$anonfun$apply$23.apply(RDD.scala:796); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; And I notice `gatk/src/main/java/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.java` doesn't exist, and there is no class file `gatk/build/classes/main/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.class` either. Is that causing this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186
https://github.com/broadinstitute/gatk/issues/3186:3788,Performance,concurren,concurrent,3788,"pPartitions$1$$anonfun$apply$23.apply(RDD.scala:796); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; And I notice `gatk/src/main/java/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.java` doesn't exist, and there is no class file `gatk/build/classes/main/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.class` either. Is that causing this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186
https://github.com/broadinstitute/gatk/issues/3191:143,Availability,down,downloaded,143,"Currently the script fails when configured to run tests, since it doesn't have access to the large files in the docker image. These need to be downloaded and mounted for the tests to pass, as we do in the docker tests on travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191
https://github.com/broadinstitute/gatk/issues/3191:32,Modifiability,config,configured,32,"Currently the script fails when configured to run tests, since it doesn't have access to the large files in the docker image. These need to be downloaded and mounted for the tests to pass, as we do in the docker tests on travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191
https://github.com/broadinstitute/gatk/issues/3191:79,Security,access,access,79,"Currently the script fails when configured to run tests, since it doesn't have access to the large files in the docker image. These need to be downloaded and mounted for the tests to pass, as we do in the docker tests on travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191
https://github.com/broadinstitute/gatk/issues/3191:50,Testability,test,tests,50,"Currently the script fails when configured to run tests, since it doesn't have access to the large files in the docker image. These need to be downloaded and mounted for the tests to pass, as we do in the docker tests on travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191
https://github.com/broadinstitute/gatk/issues/3191:174,Testability,test,tests,174,"Currently the script fails when configured to run tests, since it doesn't have access to the large files in the docker image. These need to be downloaded and mounted for the tests to pass, as we do in the docker tests on travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191
https://github.com/broadinstitute/gatk/issues/3191:212,Testability,test,tests,212,"Currently the script fails when configured to run tests, since it doesn't have access to the large files in the docker image. These need to be downloaded and mounted for the tests to pass, as we do in the docker tests on travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191
https://github.com/broadinstitute/gatk/issues/3193:61,Energy Efficiency,consumption,consumption,61,For use in genotyping and interpretation of events for human consumption.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3193
https://github.com/broadinstitute/gatk/pull/3198:28,Security,expose,exposed,28,Added experimental tool and exposed some of the AllelicCNV file extension constants. I am adding the tools as requested. Minor additional changes.; @gtiao and @sooheelee . Closes #3196,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3198
https://github.com/broadinstitute/gatk/issues/3199:45,Usability,user-friendly,user-friendly,45,"Readme packaged with the jars should be more user-friendly, omit dev instructions such as how to compile (since the package by definition does not include source code) and start right away with quickstart instructions:. - how to run ; - where to find docs; - where to get help. Could be largely lifted from http://gatkforums.broadinstitute.org/gatk/discussion/9881/howto-get-started-with-gatk4-beta",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3199
https://github.com/broadinstitute/gatk/pull/3203:183,Testability,test,test,183,"Making the CollectAllelicCounts tool a LocusWalker. . Some notes:. - had to create indices for bam files.; - had to convert a sam file to a bam file so that I could index it.; - Made test bam files have reads that pass wellformed read filter.; - deleted a test that would no longer throw an exception, since the reads all passed wellformed read filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3203
https://github.com/broadinstitute/gatk/pull/3203:256,Testability,test,test,256,"Making the CollectAllelicCounts tool a LocusWalker. . Some notes:. - had to create indices for bam files.; - had to convert a sam file to a bam file so that I could index it.; - Made test bam files have reads that pass wellformed read filter.; - deleted a test that would no longer throw an exception, since the reads all passed wellformed read filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3203
https://github.com/broadinstitute/gatk/pull/3204:27,Testability,test,test,27,"@LeeTL1220 For all PathSeq test BAMs, all reads have read groups with an SM tag. This will make them pass the WellFormedReadFilter. . Also the BAMs are now properly sorted and made a small fix so the flags are set properly for filter tool output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3204
https://github.com/broadinstitute/gatk/issues/3205:8,Usability,simpl,simple,8,"Write a simple annotator that annotates a vcf with a GC context in the reference near a variant as an INFO field. Ideally it would take a k-mer size as an argument, but I'm not sure if VariantAnnotator supports that.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3205
https://github.com/broadinstitute/gatk/pull/3206:190,Security,Validat,ValidateSameFile,190,"@LeeTL1220 Fixed PathSeq test BAMs so that all reads have read groups with an SM tag. This will make them pass the WellFormedReadFilter. To be thorough, I made sure they also check out with ValidateSameFile. The BAMs are now properly sorted, and also I made a small fix so that unmapped mate flags are set properly in the filter tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3206
https://github.com/broadinstitute/gatk/pull/3206:25,Testability,test,test,25,"@LeeTL1220 Fixed PathSeq test BAMs so that all reads have read groups with an SM tag. This will make them pass the WellFormedReadFilter. To be thorough, I made sure they also check out with ValidateSameFile. The BAMs are now properly sorted, and also I made a small fix so that unmapped mate flags are set properly in the filter tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3206
https://github.com/broadinstitute/gatk/issues/3208:175,Availability,error,errors,175,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:304,Availability,error,error,304,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:526,Availability,error,error,526,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:643,Availability,error,error,643,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:665,Availability,error,error,665,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:855,Availability,error,error,855,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:1081,Availability,error,error,1081,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:1259,Availability,error,error,1259,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:1701,Availability,avail,available,1701,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:318,Energy Efficiency,allocate,allocate,318,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:869,Energy Efficiency,allocate,allocate,869,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:27,Testability,test,testBlockGather,27,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:637,Testability,log,log,637,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:703,Testability,test,test,703,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:1192,Testability,log,log,1192,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:1432,Testability,test,test,1432,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:1468,Testability,Test,Test,1468,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:1656,Testability,test,test,1656,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3208:1772,Testability,test,test,1772,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208
https://github.com/broadinstitute/gatk/issues/3209:51,Availability,failure,failures,51,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:195,Availability,error,error,195,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:261,Availability,error,error,261,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:649,Availability,error,error,649,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:875,Availability,error,error,875,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:1011,Availability,error,error,1011," in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:1449,Availability,error,error,1449,"0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we were more careful about checking return values from system calls. Eg., in the function below from the BWA bindings we could check the return values of the `mmap()` and `calloc()` calls, and die with an appropriate error message if they fail:. ```; bwaidx_t* jnibwa_openIndex( int fd ) {; struct stat statBuf;; if ( fstat(fd, &statBuf) == -1 ) return 0;; uint8_t* mem = mmap(0, statBuf.st_size, PROT_READ, MAP_SHARED, fd, 0);; close(fd);; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:1975,Availability,fault,faults,1975,"ufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we were more careful about checking return values from system calls. Eg., in the function below from the BWA bindings we could check the return values of the `mmap()` and `calloc()` calls, and die with an appropriate error message if they fail:. ```; bwaidx_t* jnibwa_openIndex( int fd ) {; struct stat statBuf;; if ( fstat(fd, &statBuf) == -1 ) return 0;; uint8_t* mem = mmap(0, statBuf.st_size, PROT_READ, MAP_SHARED, fd, 0);; close(fd);; bwaidx_t* pIdx = calloc(1, sizeof(bwaidx_t));; bwa_mem2idx(statBuf.st_size, mem, pIdx);; pIdx->is_shm = 1;; mem_fmt_fnc = &fmt_BAMish;; bwa_verbose = 0;; return pIdx;; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:2082,Availability,failure,failures,2082,"ufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we were more careful about checking return values from system calls. Eg., in the function below from the BWA bindings we could check the return values of the `mmap()` and `calloc()` calls, and die with an appropriate error message if they fail:. ```; bwaidx_t* jnibwa_openIndex( int fd ) {; struct stat statBuf;; if ( fstat(fd, &statBuf) == -1 ) return 0;; uint8_t* mem = mmap(0, statBuf.st_size, PROT_READ, MAP_SHARED, fd, 0);; close(fd);; bwaidx_t* pIdx = calloc(1, sizeof(bwaidx_t));; bwa_mem2idx(statBuf.st_size, mem, pIdx);; pIdx->is_shm = 1;; mem_fmt_fnc = &fmt_BAMish;; bwa_verbose = 0;; return pIdx;; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:2311,Availability,error,error,2311,"ufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we were more careful about checking return values from system calls. Eg., in the function below from the BWA bindings we could check the return values of the `mmap()` and `calloc()` calls, and die with an appropriate error message if they fail:. ```; bwaidx_t* jnibwa_openIndex( int fd ) {; struct stat statBuf;; if ( fstat(fd, &statBuf) == -1 ) return 0;; uint8_t* mem = mmap(0, statBuf.st_size, PROT_READ, MAP_SHARED, fd, 0);; close(fd);; bwaidx_t* pIdx = calloc(1, sizeof(bwaidx_t));; bwa_mem2idx(statBuf.st_size, mem, pIdx);; pIdx->is_shm = 1;; mem_fmt_fnc = &fmt_BAMish;; bwa_verbose = 0;; return pIdx;; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:663,Energy Efficiency,allocate,allocate,663,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:2317,Integrability,message,message,2317,"ufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we were more careful about checking return values from system calls. Eg., in the function below from the BWA bindings we could check the return values of the `mmap()` and `calloc()` calls, and die with an appropriate error message if they fail:. ```; bwaidx_t* jnibwa_openIndex( int fd ) {; struct stat statBuf;; if ( fstat(fd, &statBuf) == -1 ) return 0;; uint8_t* mem = mmap(0, statBuf.st_size, PROT_READ, MAP_SHARED, fd, 0);; close(fd);; bwaidx_t* pIdx = calloc(1, sizeof(bwaidx_t));; bwa_mem2idx(statBuf.st_size, mem, pIdx);; pIdx->is_shm = 1;; mem_fmt_fnc = &fmt_BAMish;; bwa_verbose = 0;; return pIdx;; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:1026,Safety,detect,detected,1026," in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:1464,Safety,detect,detected,1464,"0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we were more careful about checking return values from system calls. Eg., in the function below from the BWA bindings we could check the return values of the `mmap()` and `calloc()` calls, and die with an appropriate error message if they fail:. ```; bwaidx_t* jnibwa_openIndex( int fd ) {; struct stat statBuf;; if ( fstat(fd, &statBuf) == -1 ) return 0;; uint8_t* mem = mmap(0, statBuf.st_size, PROT_READ, MAP_SHARED, fd, 0);; close(fd);; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:394,Testability,test,test,394,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:430,Testability,Test,Test,430,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:497,Testability,test,test,497,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:986,Testability,log,log,986,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3209:139,Usability,simpl,simple,139,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209
https://github.com/broadinstitute/gatk/issues/3210:1334,Availability,ping,ping,1334,"In a joint calling run with 11,000 samples, and broken up into over 10,000 scatters, a single one failed with a NPE. I was able to get around it for now by just ignoring that scatter for the output, but that's really not an ideal thing to do for joint calling (and we cannot do that for the CCDG callset). I can't give you the inputs because it was running on so many samples (and via GenomicsDB), but hopefully the stacktrace will help here:. java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.calculateLikelihoodSums(AlleleSubsettingUtils.java:234); at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.calculateMostLikelyAlleles(AlleleSubsettingUtils.java:199); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:241); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:205); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:276); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:234); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:213). I'm not sure who now owns this code, so will ping @davidbenjamin, @ldgauthier, @droazen.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3210
https://github.com/broadinstitute/gatk/issues/3211:7,Deployability,update,update,7,"Please update the GitHub description to use https://www.broadinstitute.org/gatk/ which saves one redirect, and is more secure with rogue DNS servers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3211
https://github.com/broadinstitute/gatk/issues/3211:119,Security,secur,secure,119,"Please update the GitHub description to use https://www.broadinstitute.org/gatk/ which saves one redirect, and is more secure with rogue DNS servers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3211
https://github.com/broadinstitute/gatk/pull/3213:425,Testability,log,logs,425,"Currently, if `ReadsDataSource` comes from several files, the `getHeader()` method is always returning a coordinate sorted header. Thus, it is difficult to assume that the input is pre-sorted or not for other sorting orders. This PR implements a method to obtain the consensus between the files (ignoring the unknown sort order), and if there is no consensus falls back in the previous behavior (coordinate). In addition, it logs a warning with the current assumptions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3213
https://github.com/broadinstitute/gatk/issues/3216:180,Performance,perform,performs,180,Currently it seems like the pull request build on travis is running the wrong commit's tests in docker. The docker build script takes a commit hash as part of it's inputs and then performs a checkout of that when building the docker. This is failing for the PR builds because the .travis.yml is currently getting the hash from calling rev-parse on the current branch which gives the commit number of master. It should be using $TRAVIS_COMMIT as the hash.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3216
https://github.com/broadinstitute/gatk/issues/3216:143,Security,hash,hash,143,Currently it seems like the pull request build on travis is running the wrong commit's tests in docker. The docker build script takes a commit hash as part of it's inputs and then performs a checkout of that when building the docker. This is failing for the PR builds because the .travis.yml is currently getting the hash from calling rev-parse on the current branch which gives the commit number of master. It should be using $TRAVIS_COMMIT as the hash.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3216
https://github.com/broadinstitute/gatk/issues/3216:317,Security,hash,hash,317,Currently it seems like the pull request build on travis is running the wrong commit's tests in docker. The docker build script takes a commit hash as part of it's inputs and then performs a checkout of that when building the docker. This is failing for the PR builds because the .travis.yml is currently getting the hash from calling rev-parse on the current branch which gives the commit number of master. It should be using $TRAVIS_COMMIT as the hash.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3216
https://github.com/broadinstitute/gatk/issues/3216:449,Security,hash,hash,449,Currently it seems like the pull request build on travis is running the wrong commit's tests in docker. The docker build script takes a commit hash as part of it's inputs and then performs a checkout of that when building the docker. This is failing for the PR builds because the .travis.yml is currently getting the hash from calling rev-parse on the current branch which gives the commit number of master. It should be using $TRAVIS_COMMIT as the hash.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3216
https://github.com/broadinstitute/gatk/issues/3216:87,Testability,test,tests,87,Currently it seems like the pull request build on travis is running the wrong commit's tests in docker. The docker build script takes a commit hash as part of it's inputs and then performs a checkout of that when building the docker. This is failing for the PR builds because the .travis.yml is currently getting the hash from calling rev-parse on the current branch which gives the commit number of master. It should be using $TRAVIS_COMMIT as the hash.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3216
https://github.com/broadinstitute/gatk/pull/3217:465,Modifiability,variab,variable,465,"Previously we had an issue where our travis builds would use the wrong; commit for the docker builds in the travis pull-request builds but not for the push; builds. This caused the tests from master to run and usually pass. However,; since we are mounting the test data from the correct commit into the; docker, this would result in confusing mismatches where old tests would; try to run on new test data. Fixing the problem by using the $TRAVIS_COMMIT environment variable; instead of the TRAVIS_BRANCH. fixes #3216",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3217
https://github.com/broadinstitute/gatk/pull/3217:181,Testability,test,tests,181,"Previously we had an issue where our travis builds would use the wrong; commit for the docker builds in the travis pull-request builds but not for the push; builds. This caused the tests from master to run and usually pass. However,; since we are mounting the test data from the correct commit into the; docker, this would result in confusing mismatches where old tests would; try to run on new test data. Fixing the problem by using the $TRAVIS_COMMIT environment variable; instead of the TRAVIS_BRANCH. fixes #3216",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3217
https://github.com/broadinstitute/gatk/pull/3217:260,Testability,test,test,260,"Previously we had an issue where our travis builds would use the wrong; commit for the docker builds in the travis pull-request builds but not for the push; builds. This caused the tests from master to run and usually pass. However,; since we are mounting the test data from the correct commit into the; docker, this would result in confusing mismatches where old tests would; try to run on new test data. Fixing the problem by using the $TRAVIS_COMMIT environment variable; instead of the TRAVIS_BRANCH. fixes #3216",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3217
https://github.com/broadinstitute/gatk/pull/3217:364,Testability,test,tests,364,"Previously we had an issue where our travis builds would use the wrong; commit for the docker builds in the travis pull-request builds but not for the push; builds. This caused the tests from master to run and usually pass. However,; since we are mounting the test data from the correct commit into the; docker, this would result in confusing mismatches where old tests would; try to run on new test data. Fixing the problem by using the $TRAVIS_COMMIT environment variable; instead of the TRAVIS_BRANCH. fixes #3216",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3217
https://github.com/broadinstitute/gatk/pull/3217:395,Testability,test,test,395,"Previously we had an issue where our travis builds would use the wrong; commit for the docker builds in the travis pull-request builds but not for the push; builds. This caused the tests from master to run and usually pass. However,; since we are mounting the test data from the correct commit into the; docker, this would result in confusing mismatches where old tests would; try to run on new test data. Fixing the problem by using the $TRAVIS_COMMIT environment variable; instead of the TRAVIS_BRANCH. fixes #3216",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3217
https://github.com/broadinstitute/gatk/issues/3218:256,Availability,avail,available,256,"If one of the block compressed VCFs in the list is empty (i.e. it does have proper header lines but there are no variant records, which is perfectly valid) then the tool fails with an IllegalStateException:. java.lang.IllegalStateException: Could not read available bytes from BlockCompressedInputStream.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:697); at org.broadinstitute.hellbender.tools.GatherVcfs.gatherWithBlockCopying(GatherVcfs.java:354)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3218
https://github.com/broadinstitute/gatk/issues/3218:351,Security,validat,validate,351,"If one of the block compressed VCFs in the list is empty (i.e. it does have proper header lines but there are no variant records, which is perfectly valid) then the tool fails with an IllegalStateException:. java.lang.IllegalStateException: Could not read available bytes from BlockCompressedInputStream.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:697); at org.broadinstitute.hellbender.tools.GatherVcfs.gatherWithBlockCopying(GatherVcfs.java:354)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3218
https://github.com/broadinstitute/gatk/issues/3221:260,Availability,down,downgrading,260,"Right now the alignments are filtered in various places in SV discovery stage. ; Having a single logic unit for doing this makes; * debugging and modular development, ; * complex SV discovery/interpretation, and ; * future improvements (e.g. not filtering but downgrading certain alignments and use an optimization approach). easier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3221
https://github.com/broadinstitute/gatk/issues/3221:302,Performance,optimiz,optimization,302,"Right now the alignments are filtered in various places in SV discovery stage. ; Having a single logic unit for doing this makes; * debugging and modular development, ; * complex SV discovery/interpretation, and ; * future improvements (e.g. not filtering but downgrading certain alignments and use an optimization approach). easier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3221
https://github.com/broadinstitute/gatk/issues/3221:97,Testability,log,logic,97,"Right now the alignments are filtered in various places in SV discovery stage. ; Having a single logic unit for doing this makes; * debugging and modular development, ; * complex SV discovery/interpretation, and ; * future improvements (e.g. not filtering but downgrading certain alignments and use an optimization approach). easier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3221
https://github.com/broadinstitute/gatk/pull/3223:134,Security,hash,hashCode,134,"For `byte[]` attributes, the `GATKSAMRecordToGATKReadAdapter.getAttributesAsString()` uses the default `Object.toString()` (className@hashCode). This is something unexpected for my point of view, because the following code may fail:. ```java; public void testGATKReadGetAttributeAsString(final GATKRead read) {; read.setAttribute(""BC"", new byte[]{'A', 'C', 'T', 'G'});; // this will fail with the current implementation; Assert.assertEquals(read.getAttributesAsString(""BC"").getBytes(Charset.forName(""UTF-8"")),; read.getAttributeAsByteArray(""BC""));; }; ```. This PR fixes the issue by identifying `byte[]` attributes and converting them to Strings by using the default GATKRead charset (UTF-8).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3223
https://github.com/broadinstitute/gatk/pull/3223:255,Testability,test,testGATKReadGetAttributeAsString,255,"For `byte[]` attributes, the `GATKSAMRecordToGATKReadAdapter.getAttributesAsString()` uses the default `Object.toString()` (className@hashCode). This is something unexpected for my point of view, because the following code may fail:. ```java; public void testGATKReadGetAttributeAsString(final GATKRead read) {; read.setAttribute(""BC"", new byte[]{'A', 'C', 'T', 'G'});; // this will fail with the current implementation; Assert.assertEquals(read.getAttributesAsString(""BC"").getBytes(Charset.forName(""UTF-8"")),; read.getAttributeAsByteArray(""BC""));; }; ```. This PR fixes the issue by identifying `byte[]` attributes and converting them to Strings by using the default GATKRead charset (UTF-8).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3223
https://github.com/broadinstitute/gatk/pull/3223:421,Testability,Assert,Assert,421,"For `byte[]` attributes, the `GATKSAMRecordToGATKReadAdapter.getAttributesAsString()` uses the default `Object.toString()` (className@hashCode). This is something unexpected for my point of view, because the following code may fail:. ```java; public void testGATKReadGetAttributeAsString(final GATKRead read) {; read.setAttribute(""BC"", new byte[]{'A', 'C', 'T', 'G'});; // this will fail with the current implementation; Assert.assertEquals(read.getAttributesAsString(""BC"").getBytes(Charset.forName(""UTF-8"")),; read.getAttributeAsByteArray(""BC""));; }; ```. This PR fixes the issue by identifying `byte[]` attributes and converting them to Strings by using the default GATKRead charset (UTF-8).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3223
https://github.com/broadinstitute/gatk/pull/3223:428,Testability,assert,assertEquals,428,"For `byte[]` attributes, the `GATKSAMRecordToGATKReadAdapter.getAttributesAsString()` uses the default `Object.toString()` (className@hashCode). This is something unexpected for my point of view, because the following code may fail:. ```java; public void testGATKReadGetAttributeAsString(final GATKRead read) {; read.setAttribute(""BC"", new byte[]{'A', 'C', 'T', 'G'});; // this will fail with the current implementation; Assert.assertEquals(read.getAttributesAsString(""BC"").getBytes(Charset.forName(""UTF-8"")),; read.getAttributeAsByteArray(""BC""));; }; ```. This PR fixes the issue by identifying `byte[]` attributes and converting them to Strings by using the default GATKRead charset (UTF-8).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3223
https://github.com/broadinstitute/gatk/issues/3224:15,Deployability,pipeline,pipeline,15,The current SV pipeline is producing invalid VCF files that are missing a header line for these attributes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3224
https://github.com/broadinstitute/gatk/pull/3226:69,Deployability,install,installation,69,Travis-CI now has git-lfs baked into the images so we can delete our installation script and just use their instance. ; deleted download_lfs_files.sh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3226
https://github.com/broadinstitute/gatk/pull/3229:90,Availability,down,downstream,90,"fixes a flaw where i was looking at the 1st read, and the event could've been upstream or downstream. now i always choose the upstream read so that the event will be downstream.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3229
https://github.com/broadinstitute/gatk/pull/3229:166,Availability,down,downstream,166,"fixes a flaw where i was looking at the 1st read, and the event could've been upstream or downstream. now i always choose the upstream read so that the event will be downstream.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3229
https://github.com/broadinstitute/gatk/issues/3231:147,Testability,log,logging,147,I'm using:. ```; gatk-launch CreatePanelOfNormals \; -I combinedcoverage_C.tsv \; -O ponC.pon \; ```; With or without `--disableSpark`. The stdout logging is super verbose when using Spark. Changing with `--verbosity` does not allow for a summary level stdout that I can get with `--disableSpark`. . @samuelklee notes that many of the Spark tools are very verbose. Is there a reason why we need the Spark mode to be so verbose @droazen @lbergelson?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3231
https://github.com/broadinstitute/gatk/issues/3234:11,Availability,down,downstream,11,The hstjdk downstream tests are failing and have been since we merged the repos. It looks like the failure are due to missing R dependencies on the worker nodes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3234
https://github.com/broadinstitute/gatk/issues/3234:99,Availability,failure,failure,99,The hstjdk downstream tests are failing and have been since we merged the repos. It looks like the failure are due to missing R dependencies on the worker nodes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3234
https://github.com/broadinstitute/gatk/issues/3234:128,Integrability,depend,dependencies,128,The hstjdk downstream tests are failing and have been since we merged the repos. It looks like the failure are due to missing R dependencies on the worker nodes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3234
https://github.com/broadinstitute/gatk/issues/3234:22,Testability,test,tests,22,The hstjdk downstream tests are failing and have been since we merged the repos. It looks like the failure are due to missing R dependencies on the worker nodes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3234
https://github.com/broadinstitute/gatk/issues/3235:11,Availability,down,downstream,11,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235
https://github.com/broadinstitute/gatk/issues/3235:162,Deployability,install,installing,162,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235
https://github.com/broadinstitute/gatk/issues/3235:246,Deployability,integrat,integration,246,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235
https://github.com/broadinstitute/gatk/issues/3235:267,Energy Efficiency,reduce,reduce,267,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235
https://github.com/broadinstitute/gatk/issues/3235:246,Integrability,integrat,integration,246,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235
https://github.com/broadinstitute/gatk/issues/3235:116,Modifiability,refactor,refactored,116,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235
https://github.com/broadinstitute/gatk/issues/3235:22,Testability,test,tests,22,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235
https://github.com/broadinstitute/gatk/issues/3235:258,Testability,test,tests,258,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235
https://github.com/broadinstitute/gatk/issues/3236:136,Availability,avail,available,136,We think we're hitting disk space caps on travis in some builds. Travis support suggested we move to their minimal image which has more available space. We should investigate if this is possible without major pain. Particularly it may have problems with git-lfs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3236
https://github.com/broadinstitute/gatk/pull/3238:74,Availability,down,downsample,74,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238
https://github.com/broadinstitute/gatk/pull/3238:158,Availability,error,errors,158,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238
https://github.com/broadinstitute/gatk/pull/3238:347,Availability,down,downsampling,347,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238
https://github.com/broadinstitute/gatk/pull/3238:371,Availability,down,downsample,371,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238
https://github.com/broadinstitute/gatk/pull/3238:521,Availability,down,downsampling,521,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238
https://github.com/broadinstitute/gatk/pull/3238:830,Availability,down,downsampling,830,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238
https://github.com/broadinstitute/gatk/pull/3238:795,Testability,test,tests,795,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238
https://github.com/broadinstitute/gatk/pull/3238:909,Testability,test,tests,909,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238
https://github.com/broadinstitute/gatk/issues/3239:222,Availability,down,downsampled,222,Implement a convolutional network for the reference context merged with a dense network for annotations. The training data for the classifier consists of low-AF false positives from a normal and true hets with alt alleles downsampled to look like somatic variants. (This is in the db_m2_pon_mode branch).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3239
https://github.com/broadinstitute/gatk/pull/3243:62,Security,validat,validations,62,This simplifies the code and didn't affect specificity in our validations. @takutosato can you review this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3243
https://github.com/broadinstitute/gatk/pull/3243:5,Usability,simpl,simplifies,5,This simplifies the code and didn't affect specificity in our validations. @takutosato can you review this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3243
https://github.com/broadinstitute/gatk/issues/3246:559,Modifiability,refactor,refactored,559,"The use of Targets to refer to genomic intervals is unnecessary and confusing. It obfuscates the fact that most of the tools and code can be applied to not only counts from WES targets, but also counts from WES baits, WGS bins, etc. Requiring that Targets be named also adds unnecessary storage and memory burden. We should just use SimpleIntervals everywhere. We should also get rid of the target file format. In terms of external visibility, we can just rename tools and edit javadoc. Internally, there will be many classes that need to be both renamed and refactored. I instead suggest that we rebuild new versions of the classes and tools as necessary in the tools/copynumber package. - [ ] Rename tools: AnnotateTargets -> AnnotateIntervals, TargetCoverageSexGenotyper -> ReadCountSexGenotyper. ; - [ ] Deprecate tools: CalculateTargetCoverage, ConvertBedToTargetFile, and PadTargets will be replaced by @asmirnov239's new CollectReadCounts tool and on-the-fly padding specified by --interval_padding parameters.; - [ ] Deprecate target file format and change all other affected file formats.; - [ ] Refactor/rename/rebuild classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3246
https://github.com/broadinstitute/gatk/issues/3246:1105,Modifiability,Refactor,Refactor,1105,"The use of Targets to refer to genomic intervals is unnecessary and confusing. It obfuscates the fact that most of the tools and code can be applied to not only counts from WES targets, but also counts from WES baits, WGS bins, etc. Requiring that Targets be named also adds unnecessary storage and memory burden. We should just use SimpleIntervals everywhere. We should also get rid of the target file format. In terms of external visibility, we can just rename tools and edit javadoc. Internally, there will be many classes that need to be both renamed and refactored. I instead suggest that we rebuild new versions of the classes and tools as necessary in the tools/copynumber package. - [ ] Rename tools: AnnotateTargets -> AnnotateIntervals, TargetCoverageSexGenotyper -> ReadCountSexGenotyper. ; - [ ] Deprecate tools: CalculateTargetCoverage, ConvertBedToTargetFile, and PadTargets will be replaced by @asmirnov239's new CollectReadCounts tool and on-the-fly padding specified by --interval_padding parameters.; - [ ] Deprecate target file format and change all other affected file formats.; - [ ] Refactor/rename/rebuild classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3246
https://github.com/broadinstitute/gatk/issues/3246:333,Usability,Simpl,SimpleIntervals,333,"The use of Targets to refer to genomic intervals is unnecessary and confusing. It obfuscates the fact that most of the tools and code can be applied to not only counts from WES targets, but also counts from WES baits, WGS bins, etc. Requiring that Targets be named also adds unnecessary storage and memory burden. We should just use SimpleIntervals everywhere. We should also get rid of the target file format. In terms of external visibility, we can just rename tools and edit javadoc. Internally, there will be many classes that need to be both renamed and refactored. I instead suggest that we rebuild new versions of the classes and tools as necessary in the tools/copynumber package. - [ ] Rename tools: AnnotateTargets -> AnnotateIntervals, TargetCoverageSexGenotyper -> ReadCountSexGenotyper. ; - [ ] Deprecate tools: CalculateTargetCoverage, ConvertBedToTargetFile, and PadTargets will be replaced by @asmirnov239's new CollectReadCounts tool and on-the-fly padding specified by --interval_padding parameters.; - [ ] Deprecate target file format and change all other affected file formats.; - [ ] Refactor/rename/rebuild classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3246
https://github.com/broadinstitute/gatk/issues/3247:50,Availability,echo,echoed,50,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/issues/3247:65,Deployability,integrat,integration,65,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/issues/3247:629,Deployability,integrat,integration,629,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/issues/3247:65,Integrability,integrat,integration,65,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/issues/3247:629,Integrability,integrat,integration,629,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/issues/3247:77,Testability,test,test,77,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/issues/3247:160,Testability,test,test,160,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/issues/3247:377,Testability,test,test,377,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/issues/3247:641,Testability,test,tests,641,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247
https://github.com/broadinstitute/gatk/pull/3250:36,Testability,test,testing,36,Default is previous behavior. Added testing. Note that this change may need to be made in picard as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3250
https://github.com/broadinstitute/gatk/issues/3251:88,Security,expose,expose,88,"As per the discussion in https://github.com/broadinstitute/gatk/issues/3246, we need to expose the `IntervalArgumentCollection.IntervalMergingRule` setting as a command-line argument in `IntervalArgumentCollection`. This was exposed in GATK3 as `--interval_merging`/`-im` (see GATK3's `IntervalArgumentCollection`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3251
https://github.com/broadinstitute/gatk/issues/3251:225,Security,expose,exposed,225,"As per the discussion in https://github.com/broadinstitute/gatk/issues/3246, we need to expose the `IntervalArgumentCollection.IntervalMergingRule` setting as a command-line argument in `IntervalArgumentCollection`. This was exposed in GATK3 as `--interval_merging`/`-im` (see GATK3's `IntervalArgumentCollection`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3251
https://github.com/broadinstitute/gatk/pull/3252:160,Availability,error,errorIfMissingData,160,"Related to https://github.com/broadgsa/gatk/pull/22.; Port of https://github.com/broadinstitute/gsa-unstable/pull/1606. Also replaced --allowMissingData with --errorIfMissingData since we'd rather not fail by default for missing data.; The root cause of the problem is due the implementation of `Genotype.hasAnyAttribute`. It always returns true for FT, even if it's missing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3252
https://github.com/broadinstitute/gatk/issues/3253:290,Availability,failure,failures,290,"`CloudStorageReadChannel.create()` appears to do a GCS access outside of the retry mechanism in `CloudStorageReadChannel.read()`. It calls the constructor, which calls `CloudStorageReadChannel.fetchSize()`, which does a `gcsStorage.get(file)` followed by a `getSize()`. . We are seeing 503 failures specifically from the GCS access in `CloudStorageReadChannel.fetchSize()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:202); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:234); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:78); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:68); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:304); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:265); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:101); at htsjdk.tribble.readers.Ta",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253
https://github.com/broadinstitute/gatk/issues/3253:2188,Availability,down,down,2188,s specifically from the GCS access in `CloudStorageReadChannel.fetchSize()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:202); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:234); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:78); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:68); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:304); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:265); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:101); at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:270). [2:58] ; From stdout:. [2:58] ; 15:55:58.059 INFO GenomicsDBImport - Done importing batch 19/444; 15:56:21.780 INFO GenomicsDBImport - Shutting down engine; code: 503; message: 503 Service Unavailable; reason: null; location: null; retryable: false; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253
https://github.com/broadinstitute/gatk/issues/3253:2212,Integrability,message,message,2212,s specifically from the GCS access in `CloudStorageReadChannel.fetchSize()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:202); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:234); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:78); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:68); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:304); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:265); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:101); at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:270). [2:58] ; From stdout:. [2:58] ; 15:55:58.059 INFO GenomicsDBImport - Done importing batch 19/444; 15:56:21.780 INFO GenomicsDBImport - Shutting down engine; code: 503; message: 503 Service Unavailable; reason: null; location: null; retryable: false; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253
https://github.com/broadinstitute/gatk/issues/3253:55,Security,access,access,55,"`CloudStorageReadChannel.create()` appears to do a GCS access outside of the retry mechanism in `CloudStorageReadChannel.read()`. It calls the constructor, which calls `CloudStorageReadChannel.fetchSize()`, which does a `gcsStorage.get(file)` followed by a `getSize()`. . We are seeing 503 failures specifically from the GCS access in `CloudStorageReadChannel.fetchSize()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:202); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:234); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:78); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:68); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:304); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:265); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:101); at htsjdk.tribble.readers.Ta",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253
https://github.com/broadinstitute/gatk/issues/3253:325,Security,access,access,325,"`CloudStorageReadChannel.create()` appears to do a GCS access outside of the retry mechanism in `CloudStorageReadChannel.read()`. It calls the constructor, which calls `CloudStorageReadChannel.fetchSize()`, which does a `gcsStorage.get(file)` followed by a `getSize()`. . We are seeing 503 failures specifically from the GCS access in `CloudStorageReadChannel.fetchSize()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:202); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:234); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:78); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:68); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:304); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:265); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:101); at htsjdk.tribble.readers.Ta",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253
https://github.com/broadinstitute/gatk/pull/3254:16,Testability,test,tests,16,As well as some tests to the test suite.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3254
https://github.com/broadinstitute/gatk/pull/3254:29,Testability,test,test,29,As well as some tests to the test suite.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3254
https://github.com/broadinstitute/gatk/pull/3255:128,Availability,down,downsamples,128,"New functionality to limit the amount of memory needed to read in all the data, intended for use with large WGS callsets. (VQSR downsamples training data if there are more than 2.5M variants anyway.). Also contains port of broadinstitute/gsa-unstable#1608 and broadinstitute/gsa-unstable#1575. Addresses #3230",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3255
https://github.com/broadinstitute/gatk/pull/3257:207,Integrability,depend,depend,207,"Now the assemblies file will be sorted by coordinates by default.... one can indicate the sorting order using an additional option (-sort coordinate|queryname|...). The assemblies sam/bam output format will depend on the file name extension: .bam -> binary BAM, .sam -> text SAM and anything else will result in an exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3257
https://github.com/broadinstitute/gatk/pull/3262:380,Deployability,configurat,configuration,380,"This PR fixes two problems I noticed relative to how we are treating cross-contig evidence that comes from the alt contigs:. - The cross-contig exclusion rule in `ReadClassifier` was incorrect. Due to the structure of the `if`-`else if` logic in `checkDiscordantPair()`, `SameStrandPair` or `OutiesPair` evidence was still being created for cross-contig pairs based on the strand configuration of the two reads (even though they were aligned to different contigs).; - The `runWholePipeline` script we've been using was not actually setting the cross-contig kill list parameter, meaning no cross-contig evidence was being filtered out. The change makes the number of intervals for CMHMIX WGS1 drop from 31962 to 27645.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3262
https://github.com/broadinstitute/gatk/pull/3262:380,Modifiability,config,configuration,380,"This PR fixes two problems I noticed relative to how we are treating cross-contig evidence that comes from the alt contigs:. - The cross-contig exclusion rule in `ReadClassifier` was incorrect. Due to the structure of the `if`-`else if` logic in `checkDiscordantPair()`, `SameStrandPair` or `OutiesPair` evidence was still being created for cross-contig pairs based on the strand configuration of the two reads (even though they were aligned to different contigs).; - The `runWholePipeline` script we've been using was not actually setting the cross-contig kill list parameter, meaning no cross-contig evidence was being filtered out. The change makes the number of intervals for CMHMIX WGS1 drop from 31962 to 27645.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3262
https://github.com/broadinstitute/gatk/pull/3262:237,Testability,log,logic,237,"This PR fixes two problems I noticed relative to how we are treating cross-contig evidence that comes from the alt contigs:. - The cross-contig exclusion rule in `ReadClassifier` was incorrect. Due to the structure of the `if`-`else if` logic in `checkDiscordantPair()`, `SameStrandPair` or `OutiesPair` evidence was still being created for cross-contig pairs based on the strand configuration of the two reads (even though they were aligned to different contigs).; - The `runWholePipeline` script we've been using was not actually setting the cross-contig kill list parameter, meaning no cross-contig evidence was being filtered out. The change makes the number of intervals for CMHMIX WGS1 drop from 31962 to 27645.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3262
https://github.com/broadinstitute/gatk/pull/3264:93,Usability,simpl,simpler,93,"@takutosato @LeeTL1220 I set out in all good faith to port deTiN, but I believe this will be simpler, faster, and more accurate, while taking much less of our time. The proposal is so simple that it won't take long to build it and see if I'm right.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3264
https://github.com/broadinstitute/gatk/pull/3264:184,Usability,simpl,simple,184,"@takutosato @LeeTL1220 I set out in all good faith to port deTiN, but I believe this will be simpler, faster, and more accurate, while taking much less of our time. The proposal is so simple that it won't take long to build it and see if I'm right.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3264
https://github.com/broadinstitute/gatk/issues/3265:12,Availability,ERROR,ERROR,12,"```; A USER ERROR has occurred: Bad input: BAM header sample names [TCGA-D7-6520-01A-11D-1800-08, TCGA-D7-6520-10A-01D-1800-08]does not contain given tumor sample name C440.TCGA-D7-6520-10A-01D-1800-08.5; ```; Possible solutions:; - (hacky) A fix needs to be put into the WDL that calls samtools to alter the provided sample name:; ```; if [[ ""_${normal_bam}"" == *.bam ]]; then; samtools view -H ${normal_bam} | sed -n ""/SM:/{s/.*SM:\\(\\)/\\1/; s/\\t.*//p ;q};"" > normal_name.txt; normal_command_line=""-I ${normal_bam} -normal `cat normal_name.txt`""; fi; ....snip....; samtools view -H ${tumor_bam} | sed -n ""/SM:/{s/.*SM:\\(\\)/\\1/; s/\\t.*//p ;q};"" > tumor_name.txt; java -Xmx4g -jar $GATK_JAR Mutect2 \; -R ${ref_fasta} \; -I ${tumor_bam} \; -tumor `cat tumor_name.txt` \; $normal_command_line \; ${""--dbsnp "" + dbsnp} \; ${""--cosmic "" + cosmic} \; ${""--normal_panel "" + pon} \; ${""-L "" + intervals} \; -O ""${output_vcf_name}.vcf""; ```; Pro: Already tested.; Con: Requires samtools.; Con: Annoying for users not running the WDL.; Con: Easy to forget/bad logistically. - Use labeled parameters if supported in GATK. @davidbenjamin @vdauwera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3265
https://github.com/broadinstitute/gatk/issues/3265:955,Testability,test,tested,955,"```; A USER ERROR has occurred: Bad input: BAM header sample names [TCGA-D7-6520-01A-11D-1800-08, TCGA-D7-6520-10A-01D-1800-08]does not contain given tumor sample name C440.TCGA-D7-6520-10A-01D-1800-08.5; ```; Possible solutions:; - (hacky) A fix needs to be put into the WDL that calls samtools to alter the provided sample name:; ```; if [[ ""_${normal_bam}"" == *.bam ]]; then; samtools view -H ${normal_bam} | sed -n ""/SM:/{s/.*SM:\\(\\)/\\1/; s/\\t.*//p ;q};"" > normal_name.txt; normal_command_line=""-I ${normal_bam} -normal `cat normal_name.txt`""; fi; ....snip....; samtools view -H ${tumor_bam} | sed -n ""/SM:/{s/.*SM:\\(\\)/\\1/; s/\\t.*//p ;q};"" > tumor_name.txt; java -Xmx4g -jar $GATK_JAR Mutect2 \; -R ${ref_fasta} \; -I ${tumor_bam} \; -tumor `cat tumor_name.txt` \; $normal_command_line \; ${""--dbsnp "" + dbsnp} \; ${""--cosmic "" + cosmic} \; ${""--normal_panel "" + pon} \; ${""-L "" + intervals} \; -O ""${output_vcf_name}.vcf""; ```; Pro: Already tested.; Con: Requires samtools.; Con: Annoying for users not running the WDL.; Con: Easy to forget/bad logistically. - Use labeled parameters if supported in GATK. @davidbenjamin @vdauwera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3265
https://github.com/broadinstitute/gatk/issues/3265:1059,Testability,log,logistically,1059,"```; A USER ERROR has occurred: Bad input: BAM header sample names [TCGA-D7-6520-01A-11D-1800-08, TCGA-D7-6520-10A-01D-1800-08]does not contain given tumor sample name C440.TCGA-D7-6520-10A-01D-1800-08.5; ```; Possible solutions:; - (hacky) A fix needs to be put into the WDL that calls samtools to alter the provided sample name:; ```; if [[ ""_${normal_bam}"" == *.bam ]]; then; samtools view -H ${normal_bam} | sed -n ""/SM:/{s/.*SM:\\(\\)/\\1/; s/\\t.*//p ;q};"" > normal_name.txt; normal_command_line=""-I ${normal_bam} -normal `cat normal_name.txt`""; fi; ....snip....; samtools view -H ${tumor_bam} | sed -n ""/SM:/{s/.*SM:\\(\\)/\\1/; s/\\t.*//p ;q};"" > tumor_name.txt; java -Xmx4g -jar $GATK_JAR Mutect2 \; -R ${ref_fasta} \; -I ${tumor_bam} \; -tumor `cat tumor_name.txt` \; $normal_command_line \; ${""--dbsnp "" + dbsnp} \; ${""--cosmic "" + cosmic} \; ${""--normal_panel "" + pon} \; ${""-L "" + intervals} \; -O ""${output_vcf_name}.vcf""; ```; Pro: Already tested.; Con: Requires samtools.; Con: Annoying for users not running the WDL.; Con: Easy to forget/bad logistically. - Use labeled parameters if supported in GATK. @davidbenjamin @vdauwera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3265
https://github.com/broadinstitute/gatk/issues/3267:289,Availability,down,downstream,289,"Hi,. I have the allele counts from ASEReadCounter and I would like to do some statistical test on the output. You suggest MAMBA in your manual but in their manual they say the input file should have the required field: `EXON_INFO - variant annotation label`. . Can you recommend any other downstream tools to analyze ASEReadCounter output?. Thanks,; Komal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3267
https://github.com/broadinstitute/gatk/issues/3267:90,Testability,test,test,90,"Hi,. I have the allele counts from ASEReadCounter and I would like to do some statistical test on the output. You suggest MAMBA in your manual but in their manual they say the input file should have the required field: `EXON_INFO - variant annotation label`. . Can you recommend any other downstream tools to analyze ASEReadCounter output?. Thanks,; Komal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3267
https://github.com/broadinstitute/gatk/pull/3268:33,Usability,clear,clear,33,lines 125-127 fix a bug: need to clear out 1st half window when we switch contigs; changes on lines 62-67 are cosmetic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3268
https://github.com/broadinstitute/gatk/pull/3271:189,Availability,down,down,189,"Adds PathSeqPipelineSpark master tool, which required some minor changes:; -Refactored PSScoreUtils class to a PSScorer, which now includes the main Score tool code; -Moved code for paring down the pathogen header into a new function removeUnmappedHeaderSequences(). Spark-related optimizations:; -Removed cache() calls when possible, and replaced with persist(), spilling to disk with serialization, if necessary; -Removed try-with-resources in Filter and Bwa tools, which seemed to be causing the BWA/kmer references to be unloaded prematurely. Other changes:; -Changed ambiguous base filter from using a fraction of bases to number of bases; -Added function for closing all kmer filter instances; -Optimized PSBwaAligner SA tag construction; -Renamed repartitionPairedReads() to repartitionReadsByName(); -Resolved some conflicting tool argument names; -Created PathSeq tool program group; -Filled out tool summary strings",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3271
https://github.com/broadinstitute/gatk/pull/3271:76,Modifiability,Refactor,Refactored,76,"Adds PathSeqPipelineSpark master tool, which required some minor changes:; -Refactored PSScoreUtils class to a PSScorer, which now includes the main Score tool code; -Moved code for paring down the pathogen header into a new function removeUnmappedHeaderSequences(). Spark-related optimizations:; -Removed cache() calls when possible, and replaced with persist(), spilling to disk with serialization, if necessary; -Removed try-with-resources in Filter and Bwa tools, which seemed to be causing the BWA/kmer references to be unloaded prematurely. Other changes:; -Changed ambiguous base filter from using a fraction of bases to number of bases; -Added function for closing all kmer filter instances; -Optimized PSBwaAligner SA tag construction; -Renamed repartitionPairedReads() to repartitionReadsByName(); -Resolved some conflicting tool argument names; -Created PathSeq tool program group; -Filled out tool summary strings",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3271
https://github.com/broadinstitute/gatk/pull/3271:281,Performance,optimiz,optimizations,281,"Adds PathSeqPipelineSpark master tool, which required some minor changes:; -Refactored PSScoreUtils class to a PSScorer, which now includes the main Score tool code; -Moved code for paring down the pathogen header into a new function removeUnmappedHeaderSequences(). Spark-related optimizations:; -Removed cache() calls when possible, and replaced with persist(), spilling to disk with serialization, if necessary; -Removed try-with-resources in Filter and Bwa tools, which seemed to be causing the BWA/kmer references to be unloaded prematurely. Other changes:; -Changed ambiguous base filter from using a fraction of bases to number of bases; -Added function for closing all kmer filter instances; -Optimized PSBwaAligner SA tag construction; -Renamed repartitionPairedReads() to repartitionReadsByName(); -Resolved some conflicting tool argument names; -Created PathSeq tool program group; -Filled out tool summary strings",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3271
https://github.com/broadinstitute/gatk/pull/3271:306,Performance,cache,cache,306,"Adds PathSeqPipelineSpark master tool, which required some minor changes:; -Refactored PSScoreUtils class to a PSScorer, which now includes the main Score tool code; -Moved code for paring down the pathogen header into a new function removeUnmappedHeaderSequences(). Spark-related optimizations:; -Removed cache() calls when possible, and replaced with persist(), spilling to disk with serialization, if necessary; -Removed try-with-resources in Filter and Bwa tools, which seemed to be causing the BWA/kmer references to be unloaded prematurely. Other changes:; -Changed ambiguous base filter from using a fraction of bases to number of bases; -Added function for closing all kmer filter instances; -Optimized PSBwaAligner SA tag construction; -Renamed repartitionPairedReads() to repartitionReadsByName(); -Resolved some conflicting tool argument names; -Created PathSeq tool program group; -Filled out tool summary strings",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3271
https://github.com/broadinstitute/gatk/pull/3271:701,Performance,Optimiz,Optimized,701,"Adds PathSeqPipelineSpark master tool, which required some minor changes:; -Refactored PSScoreUtils class to a PSScorer, which now includes the main Score tool code; -Moved code for paring down the pathogen header into a new function removeUnmappedHeaderSequences(). Spark-related optimizations:; -Removed cache() calls when possible, and replaced with persist(), spilling to disk with serialization, if necessary; -Removed try-with-resources in Filter and Bwa tools, which seemed to be causing the BWA/kmer references to be unloaded prematurely. Other changes:; -Changed ambiguous base filter from using a fraction of bases to number of bases; -Added function for closing all kmer filter instances; -Optimized PSBwaAligner SA tag construction; -Renamed repartitionPairedReads() to repartitionReadsByName(); -Resolved some conflicting tool argument names; -Created PathSeq tool program group; -Filled out tool summary strings",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3271
https://github.com/broadinstitute/gatk/issues/3274:82,Deployability,release,release,82,This epic will track tickets related to work in the annotation engine for the 4.0 release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3274
https://github.com/broadinstitute/gatk/issues/3275:139,Availability,toler,tolerance,139,"We need a framework for easily comparing annotation values in test suite outputs (actual vs. expected), with a per-annotation configurable tolerance before failure. Exact-match comparisons are too brittle for our needs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3275
https://github.com/broadinstitute/gatk/issues/3275:156,Availability,failure,failure,156,"We need a framework for easily comparing annotation values in test suite outputs (actual vs. expected), with a per-annotation configurable tolerance before failure. Exact-match comparisons are too brittle for our needs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3275
https://github.com/broadinstitute/gatk/issues/3275:126,Modifiability,config,configurable,126,"We need a framework for easily comparing annotation values in test suite outputs (actual vs. expected), with a per-annotation configurable tolerance before failure. Exact-match comparisons are too brittle for our needs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3275
https://github.com/broadinstitute/gatk/issues/3275:62,Testability,test,test,62,"We need a framework for easily comparing annotation values in test suite outputs (actual vs. expected), with a per-annotation configurable tolerance before failure. Exact-match comparisons are too brittle for our needs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3275
https://github.com/broadinstitute/gatk/issues/3282:93,Safety,Predict,Predictor,93,Can look at what other similar tools have done:. * SnpEff ; * Annovar; * VEP (Variant Effect Predictor). SnpEff in particular already has a scheme for annotating the VCF INFO field with info from all transcripts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3282
https://github.com/broadinstitute/gatk/issues/3283:217,Usability,simpl,simple,217,"Once we have support for the necessary formats (https://github.com/broadinstitute/gatk/issues/3277, https://github.com/broadinstitute/gatk/issues/3278, https://github.com/broadinstitute/gatk/issues/3279), implement a simple, non-Spark prototype functional annotator just to check that everything is working. Ie., implement something approximating the ""main loop"" of the Oncotator tool:. ```; for each mutation; 	for each datasource compatible with your reference; 		annotate; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3283
https://github.com/broadinstitute/gatk/issues/3285:164,Performance,scalab,scalable,164,"After we have the non-Spark prototype in https://github.com/broadinstitute/gatk/issues/3283, write a Spark version that partitions the input data in a sensible and scalable way (this ticket itself might need to spawn many sub-tickets).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3285
https://github.com/broadinstitute/gatk/issues/3286:305,Deployability,integrat,integration,305,Create a plugin for IGV that allows it to display the reads as haplotype caller has assembled them. This would be extremely useful for analysts who are doing manual review and would mostly obviate the need for bamOut. Jim Robinson seems interested in this so we may be able to get help from IGV to do the integration https://github.com/igvteam/igv/issues/428.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286
https://github.com/broadinstitute/gatk/issues/3286:305,Integrability,integrat,integration,305,Create a plugin for IGV that allows it to display the reads as haplotype caller has assembled them. This would be extremely useful for analysts who are doing manual review and would mostly obviate the need for bamOut. Jim Robinson seems interested in this so we may be able to get help from IGV to do the integration https://github.com/igvteam/igv/issues/428.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286
https://github.com/broadinstitute/gatk/issues/3286:9,Modifiability,plugin,plugin,9,Create a plugin for IGV that allows it to display the reads as haplotype caller has assembled them. This would be extremely useful for analysts who are doing manual review and would mostly obviate the need for bamOut. Jim Robinson seems interested in this so we may be able to get help from IGV to do the integration https://github.com/igvteam/igv/issues/428.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286
https://github.com/broadinstitute/gatk/pull/3290:0,Safety,Avoid,Avoid,0,Avoid spark crash by switching from preview to latest supported Dataproc image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3290
https://github.com/broadinstitute/gatk/issues/3291:178,Availability,error,error,178,"Out of 11 runs on exactly the same data, FilterByOrientationBias fails 6 times and succeeds 5 times. Assigning @LeeTL1220, given prior interaction with user. - User reports this error in: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40412#Comment_40412; - My recapitulation is in: https://github.com/broadinstitute/dsde-docs/issues/2294. Data is at `/humgen/gsa-scr1/pub/incoming/byoo_FilterByOrientationBias.zip`. Command is:; ```; gatk-launch FilterByOrientationBias \; -A 'G/T' -A 'C/T' \; -V test2.vcf \; -P test2.pre_adapter_detail_metrics \; --output ob_filtered2.vcf; ```. Error message changes between:; ```; java.lang.IllegalStateException: Allele in genotype C* not in the variant context [G*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3291:597,Availability,Error,Error,597,"Out of 11 runs on exactly the same data, FilterByOrientationBias fails 6 times and succeeds 5 times. Assigning @LeeTL1220, given prior interaction with user. - User reports this error in: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40412#Comment_40412; - My recapitulation is in: https://github.com/broadinstitute/dsde-docs/issues/2294. Data is at `/humgen/gsa-scr1/pub/incoming/byoo_FilterByOrientationBias.zip`. Command is:; ```; gatk-launch FilterByOrientationBias \; -A 'G/T' -A 'C/T' \; -V test2.vcf \; -P test2.pre_adapter_detail_metrics \; --output ob_filtered2.vcf; ```. Error message changes between:; ```; java.lang.IllegalStateException: Allele in genotype C* not in the variant context [G*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3291:3587,Deployability,release,release,3587,"gram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```; and; ```; java.lang.IllegalStateException: Allele in genotype G* not in the variant context [C*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:211); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:840); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); ```. I tested using `gatk-package-4.beta.1-local.jar` and pre-release alpha version `gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-package-4.alpha.2-1134-ga9d9d91-SNAPSHOT-local.jar`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3291:603,Integrability,message,message,603,"Out of 11 runs on exactly the same data, FilterByOrientationBias fails 6 times and succeeds 5 times. Assigning @LeeTL1220, given prior interaction with user. - User reports this error in: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40412#Comment_40412; - My recapitulation is in: https://github.com/broadinstitute/dsde-docs/issues/2294. Data is at `/humgen/gsa-scr1/pub/incoming/byoo_FilterByOrientationBias.zip`. Command is:; ```; gatk-launch FilterByOrientationBias \; -A 'G/T' -A 'C/T' \; -V test2.vcf \; -P test2.pre_adapter_detail_metrics \; --output ob_filtered2.vcf; ```. Error message changes between:; ```; java.lang.IllegalStateException: Allele in genotype C* not in the variant context [G*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3291:774,Security,validat,validateGenotypes,774,"Out of 11 runs on exactly the same data, FilterByOrientationBias fails 6 times and succeeds 5 times. Assigning @LeeTL1220, given prior interaction with user. - User reports this error in: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40412#Comment_40412; - My recapitulation is in: https://github.com/broadinstitute/dsde-docs/issues/2294. Data is at `/humgen/gsa-scr1/pub/incoming/byoo_FilterByOrientationBias.zip`. Command is:; ```; gatk-launch FilterByOrientationBias \; -A 'G/T' -A 'C/T' \; -V test2.vcf \; -P test2.pre_adapter_detail_metrics \; --output ob_filtered2.vcf; ```. Error message changes between:; ```; java.lang.IllegalStateException: Allele in genotype C* not in the variant context [G*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3291:868,Security,validat,validate,868,"Out of 11 runs on exactly the same data, FilterByOrientationBias fails 6 times and succeeds 5 times. Assigning @LeeTL1220, given prior interaction with user. - User reports this error in: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40412#Comment_40412; - My recapitulation is in: https://github.com/broadinstitute/dsde-docs/issues/2294. Data is at `/humgen/gsa-scr1/pub/incoming/byoo_FilterByOrientationBias.zip`. Command is:; ```; gatk-launch FilterByOrientationBias \; -A 'G/T' -A 'C/T' \; -V test2.vcf \; -P test2.pre_adapter_detail_metrics \; --output ob_filtered2.vcf; ```. Error message changes between:; ```; java.lang.IllegalStateException: Allele in genotype C* not in the variant context [G*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3291:2227,Security,validat,validateGenotypes,2227,"ilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```; and; ```; java.lang.IllegalStateException: Allele in genotype G* not in the variant context [C*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:211); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:840); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3291:2321,Security,validat,validate,2321,"erer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```; and; ```; java.lang.IllegalStateException: Allele in genotype G* not in the variant context [C*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:211); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:840); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3291:3532,Testability,test,tested,3532,"gram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```; and; ```; java.lang.IllegalStateException: Allele in genotype G* not in the variant context [C*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:211); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:840); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); ```. I tested using `gatk-package-4.beta.1-local.jar` and pre-release alpha version `gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-package-4.alpha.2-1134-ga9d9d91-SNAPSHOT-local.jar`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291
https://github.com/broadinstitute/gatk/issues/3292:374,Modifiability,plugin,plugin,374,"Currently tools using the annotation engine are manually defining arguments to control which annotations should be enabled/disabled. See `HaplotypeCallerArgumentCollection` `annotationsToUse`/`annotationGroupsToUse` for an example. We should bundle these into a reusable argument collection, like we did for read filters, as part of the task of making annotations a barclay plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3292
https://github.com/broadinstitute/gatk/issues/3293:23,Energy Efficiency,reduce,reduce,23,"**After** we've ported reduce support for allele-specific annotations in https://github.com/broadinstitute/gatk/issues/1893 (and not as we're porting!), we should refactor the relevant interfaces to clean them up a bit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3293
https://github.com/broadinstitute/gatk/issues/3293:185,Integrability,interface,interfaces,185,"**After** we've ported reduce support for allele-specific annotations in https://github.com/broadinstitute/gatk/issues/1893 (and not as we're porting!), we should refactor the relevant interfaces to clean them up a bit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3293
https://github.com/broadinstitute/gatk/issues/3293:163,Modifiability,refactor,refactor,163,"**After** we've ported reduce support for allele-specific annotations in https://github.com/broadinstitute/gatk/issues/1893 (and not as we're porting!), we should refactor the relevant interfaces to clean them up a bit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3293
https://github.com/broadinstitute/gatk/issues/3294:102,Deployability,integrat,integration,102,"Once we do https://github.com/broadinstitute/gatk/issues/2817, we can disable the non-docker unit and integration tests in travis, saving a huge amount of time and resources. (We should keep the instance of the tests that run on the Oracle JDK, however).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3294
https://github.com/broadinstitute/gatk/issues/3294:102,Integrability,integrat,integration,102,"Once we do https://github.com/broadinstitute/gatk/issues/2817, we can disable the non-docker unit and integration tests in travis, saving a huge amount of time and resources. (We should keep the instance of the tests that run on the Oracle JDK, however).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3294
https://github.com/broadinstitute/gatk/issues/3294:114,Testability,test,tests,114,"Once we do https://github.com/broadinstitute/gatk/issues/2817, we can disable the non-docker unit and integration tests in travis, saving a huge amount of time and resources. (We should keep the instance of the tests that run on the Oracle JDK, however).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3294
https://github.com/broadinstitute/gatk/issues/3294:211,Testability,test,tests,211,"Once we do https://github.com/broadinstitute/gatk/issues/2817, we can disable the non-docker unit and integration tests in travis, saving a huge amount of time and resources. (We should keep the instance of the tests that run on the Oracle JDK, however).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3294
https://github.com/broadinstitute/gatk/pull/3295:287,Availability,error,errors,287,"This moves us to a snapshot of google-cloud-java based off of a branch in my fork here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. This patch wraps many more operations within retries, and in our tests resolves the intermittent 503/SSL errors completely when running at scale. This PR also migrates us from setting retry settings per-Path to setting it globally, using a new API from that google-cloud-java branch. This fixes an issue where the number of reopens was getting set to 0 deep in the google-cloud-java library. Resolves #2749; Resolves #2685; Resolves #3118; Resolves #3120; Resolves #3253",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295
https://github.com/broadinstitute/gatk/pull/3295:187,Deployability,patch,patch,187,"This moves us to a snapshot of google-cloud-java based off of a branch in my fork here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. This patch wraps many more operations within retries, and in our tests resolves the intermittent 503/SSL errors completely when running at scale. This PR also migrates us from setting retry settings per-Path to setting it globally, using a new API from that google-cloud-java branch. This fixes an issue where the number of reopens was getting set to 0 deep in the google-cloud-java library. Resolves #2749; Resolves #2685; Resolves #3118; Resolves #3120; Resolves #3253",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295
https://github.com/broadinstitute/gatk/pull/3295:193,Integrability,wrap,wraps,193,"This moves us to a snapshot of google-cloud-java based off of a branch in my fork here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. This patch wraps many more operations within retries, and in our tests resolves the intermittent 503/SSL errors completely when running at scale. This PR also migrates us from setting retry settings per-Path to setting it globally, using a new API from that google-cloud-java branch. This fixes an issue where the number of reopens was getting set to 0 deep in the google-cloud-java library. Resolves #2749; Resolves #2685; Resolves #3118; Resolves #3120; Resolves #3253",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295
https://github.com/broadinstitute/gatk/pull/3295:247,Testability,test,tests,247,"This moves us to a snapshot of google-cloud-java based off of a branch in my fork here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. This patch wraps many more operations within retries, and in our tests resolves the intermittent 503/SSL errors completely when running at scale. This PR also migrates us from setting retry settings per-Path to setting it globally, using a new API from that google-cloud-java branch. This fixes an issue where the number of reopens was getting set to 0 deep in the google-cloud-java library. Resolves #2749; Resolves #2685; Resolves #3118; Resolves #3120; Resolves #3253",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295
https://github.com/broadinstitute/gatk/issues/3296:1161,Availability,error,errors,1161,"Using VCFTools to validate:; ```; vcf-validator SAMPLE7T-vs-SAMPLE7N-filtered.vcf; ```. I get a massive amount of messages:; ```; .....snip.......; column SAMPLE7N at 19:49136721 .. Could not validate the float [NaN],FORMAT tag [MPOS] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MBQ] expected different number of values (expected 1, found 2); .....snip.......; column SAMPLE7T at 19:45901415 .. FORMAT tag [MBQ] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MPOS] expected different number of values (expected 1, found 2); .....snip.......; ```. Sure enough, the header does not match the values for those fields (in the header number=""A""), so the validation errors are correct. Not sure what is the deal with FOXOG, but that may not be a big deal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3296
https://github.com/broadinstitute/gatk/issues/3296:114,Integrability,message,messages,114,"Using VCFTools to validate:; ```; vcf-validator SAMPLE7T-vs-SAMPLE7N-filtered.vcf; ```. I get a massive amount of messages:; ```; .....snip.......; column SAMPLE7N at 19:49136721 .. Could not validate the float [NaN],FORMAT tag [MPOS] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MBQ] expected different number of values (expected 1, found 2); .....snip.......; column SAMPLE7T at 19:45901415 .. FORMAT tag [MBQ] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MPOS] expected different number of values (expected 1, found 2); .....snip.......; ```. Sure enough, the header does not match the values for those fields (in the header number=""A""), so the validation errors are correct. Not sure what is the deal with FOXOG, but that may not be a big deal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3296
https://github.com/broadinstitute/gatk/issues/3296:18,Security,validat,validate,18,"Using VCFTools to validate:; ```; vcf-validator SAMPLE7T-vs-SAMPLE7N-filtered.vcf; ```. I get a massive amount of messages:; ```; .....snip.......; column SAMPLE7N at 19:49136721 .. Could not validate the float [NaN],FORMAT tag [MPOS] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MBQ] expected different number of values (expected 1, found 2); .....snip.......; column SAMPLE7T at 19:45901415 .. FORMAT tag [MBQ] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MPOS] expected different number of values (expected 1, found 2); .....snip.......; ```. Sure enough, the header does not match the values for those fields (in the header number=""A""), so the validation errors are correct. Not sure what is the deal with FOXOG, but that may not be a big deal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3296
https://github.com/broadinstitute/gatk/issues/3296:38,Security,validat,validator,38,"Using VCFTools to validate:; ```; vcf-validator SAMPLE7T-vs-SAMPLE7N-filtered.vcf; ```. I get a massive amount of messages:; ```; .....snip.......; column SAMPLE7N at 19:49136721 .. Could not validate the float [NaN],FORMAT tag [MPOS] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MBQ] expected different number of values (expected 1, found 2); .....snip.......; column SAMPLE7T at 19:45901415 .. FORMAT tag [MBQ] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MPOS] expected different number of values (expected 1, found 2); .....snip.......; ```. Sure enough, the header does not match the values for those fields (in the header number=""A""), so the validation errors are correct. Not sure what is the deal with FOXOG, but that may not be a big deal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3296
https://github.com/broadinstitute/gatk/issues/3296:192,Security,validat,validate,192,"Using VCFTools to validate:; ```; vcf-validator SAMPLE7T-vs-SAMPLE7N-filtered.vcf; ```. I get a massive amount of messages:; ```; .....snip.......; column SAMPLE7N at 19:49136721 .. Could not validate the float [NaN],FORMAT tag [MPOS] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MBQ] expected different number of values (expected 1, found 2); .....snip.......; column SAMPLE7T at 19:45901415 .. FORMAT tag [MBQ] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MPOS] expected different number of values (expected 1, found 2); .....snip.......; ```. Sure enough, the header does not match the values for those fields (in the header number=""A""), so the validation errors are correct. Not sure what is the deal with FOXOG, but that may not be a big deal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3296
https://github.com/broadinstitute/gatk/issues/3296:1150,Security,validat,validation,1150,"Using VCFTools to validate:; ```; vcf-validator SAMPLE7T-vs-SAMPLE7N-filtered.vcf; ```. I get a massive amount of messages:; ```; .....snip.......; column SAMPLE7N at 19:49136721 .. Could not validate the float [NaN],FORMAT tag [MPOS] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MBQ] expected different number of values (expected 1, found 2); .....snip.......; column SAMPLE7T at 19:45901415 .. FORMAT tag [MBQ] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MPOS] expected different number of values (expected 1, found 2); .....snip.......; ```. Sure enough, the header does not match the values for those fields (in the header number=""A""), so the validation errors are correct. Not sure what is the deal with FOXOG, but that may not be a big deal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3296
https://github.com/broadinstitute/gatk/issues/3297:69,Availability,error,error,69,"User has reported default settings in other countries cause tools to error because of the switch between commas and periods. The tools themselves generate the data with the commas that then error downstream tools. User reports a workaround solution of adding `-Duser.language=en -Duser.country=US` to commands but this is cumbersome. . Should our tools should be internationally compatible?. ---; Hi @shlee, @slee,. yes it is working when I replace the commas with points, but since I am using the wdl-pipeline which was provided by slee on gitHub, this is not a sufficient workaround. Due to my expectation that the problem (commas instead of points) arise from my language settings I try different thing to change my actual language on the system. To make the long story short the solution was to start CalculateTargetCoverage with the flags (I added them in the cnv_somatic_tasks.wdl):; ```; java -Duser.language=en -Duser.country=US -Xmx${default=4 mem}g -jar ${gatk_jar} CalculateTargetCoverage; ```; And now the cnv_somatic_panel_workflow.wdl runs smoothly to the end, with points instead of commas :). An other solution would be to running the pipeline in a DockerContainer wich have an english java-version installed, like the one which is provide by the gatk-team. I hope this helps other user too which are working with non-englisch java versions. Greetings EADG. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40481#Comment_40481",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3297
https://github.com/broadinstitute/gatk/issues/3297:190,Availability,error,error,190,"User has reported default settings in other countries cause tools to error because of the switch between commas and periods. The tools themselves generate the data with the commas that then error downstream tools. User reports a workaround solution of adding `-Duser.language=en -Duser.country=US` to commands but this is cumbersome. . Should our tools should be internationally compatible?. ---; Hi @shlee, @slee,. yes it is working when I replace the commas with points, but since I am using the wdl-pipeline which was provided by slee on gitHub, this is not a sufficient workaround. Due to my expectation that the problem (commas instead of points) arise from my language settings I try different thing to change my actual language on the system. To make the long story short the solution was to start CalculateTargetCoverage with the flags (I added them in the cnv_somatic_tasks.wdl):; ```; java -Duser.language=en -Duser.country=US -Xmx${default=4 mem}g -jar ${gatk_jar} CalculateTargetCoverage; ```; And now the cnv_somatic_panel_workflow.wdl runs smoothly to the end, with points instead of commas :). An other solution would be to running the pipeline in a DockerContainer wich have an english java-version installed, like the one which is provide by the gatk-team. I hope this helps other user too which are working with non-englisch java versions. Greetings EADG. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40481#Comment_40481",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3297
https://github.com/broadinstitute/gatk/issues/3297:196,Availability,down,downstream,196,"User has reported default settings in other countries cause tools to error because of the switch between commas and periods. The tools themselves generate the data with the commas that then error downstream tools. User reports a workaround solution of adding `-Duser.language=en -Duser.country=US` to commands but this is cumbersome. . Should our tools should be internationally compatible?. ---; Hi @shlee, @slee,. yes it is working when I replace the commas with points, but since I am using the wdl-pipeline which was provided by slee on gitHub, this is not a sufficient workaround. Due to my expectation that the problem (commas instead of points) arise from my language settings I try different thing to change my actual language on the system. To make the long story short the solution was to start CalculateTargetCoverage with the flags (I added them in the cnv_somatic_tasks.wdl):; ```; java -Duser.language=en -Duser.country=US -Xmx${default=4 mem}g -jar ${gatk_jar} CalculateTargetCoverage; ```; And now the cnv_somatic_panel_workflow.wdl runs smoothly to the end, with points instead of commas :). An other solution would be to running the pipeline in a DockerContainer wich have an english java-version installed, like the one which is provide by the gatk-team. I hope this helps other user too which are working with non-englisch java versions. Greetings EADG. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40481#Comment_40481",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3297
https://github.com/broadinstitute/gatk/issues/3297:502,Deployability,pipeline,pipeline,502,"User has reported default settings in other countries cause tools to error because of the switch between commas and periods. The tools themselves generate the data with the commas that then error downstream tools. User reports a workaround solution of adding `-Duser.language=en -Duser.country=US` to commands but this is cumbersome. . Should our tools should be internationally compatible?. ---; Hi @shlee, @slee,. yes it is working when I replace the commas with points, but since I am using the wdl-pipeline which was provided by slee on gitHub, this is not a sufficient workaround. Due to my expectation that the problem (commas instead of points) arise from my language settings I try different thing to change my actual language on the system. To make the long story short the solution was to start CalculateTargetCoverage with the flags (I added them in the cnv_somatic_tasks.wdl):; ```; java -Duser.language=en -Duser.country=US -Xmx${default=4 mem}g -jar ${gatk_jar} CalculateTargetCoverage; ```; And now the cnv_somatic_panel_workflow.wdl runs smoothly to the end, with points instead of commas :). An other solution would be to running the pipeline in a DockerContainer wich have an english java-version installed, like the one which is provide by the gatk-team. I hope this helps other user too which are working with non-englisch java versions. Greetings EADG. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40481#Comment_40481",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3297
https://github.com/broadinstitute/gatk/issues/3297:1151,Deployability,pipeline,pipeline,1151,"User has reported default settings in other countries cause tools to error because of the switch between commas and periods. The tools themselves generate the data with the commas that then error downstream tools. User reports a workaround solution of adding `-Duser.language=en -Duser.country=US` to commands but this is cumbersome. . Should our tools should be internationally compatible?. ---; Hi @shlee, @slee,. yes it is working when I replace the commas with points, but since I am using the wdl-pipeline which was provided by slee on gitHub, this is not a sufficient workaround. Due to my expectation that the problem (commas instead of points) arise from my language settings I try different thing to change my actual language on the system. To make the long story short the solution was to start CalculateTargetCoverage with the flags (I added them in the cnv_somatic_tasks.wdl):; ```; java -Duser.language=en -Duser.country=US -Xmx${default=4 mem}g -jar ${gatk_jar} CalculateTargetCoverage; ```; And now the cnv_somatic_panel_workflow.wdl runs smoothly to the end, with points instead of commas :). An other solution would be to running the pipeline in a DockerContainer wich have an english java-version installed, like the one which is provide by the gatk-team. I hope this helps other user too which are working with non-englisch java versions. Greetings EADG. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40481#Comment_40481",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3297
https://github.com/broadinstitute/gatk/issues/3297:1215,Deployability,install,installed,1215,"User has reported default settings in other countries cause tools to error because of the switch between commas and periods. The tools themselves generate the data with the commas that then error downstream tools. User reports a workaround solution of adding `-Duser.language=en -Duser.country=US` to commands but this is cumbersome. . Should our tools should be internationally compatible?. ---; Hi @shlee, @slee,. yes it is working when I replace the commas with points, but since I am using the wdl-pipeline which was provided by slee on gitHub, this is not a sufficient workaround. Due to my expectation that the problem (commas instead of points) arise from my language settings I try different thing to change my actual language on the system. To make the long story short the solution was to start CalculateTargetCoverage with the flags (I added them in the cnv_somatic_tasks.wdl):; ```; java -Duser.language=en -Duser.country=US -Xmx${default=4 mem}g -jar ${gatk_jar} CalculateTargetCoverage; ```; And now the cnv_somatic_panel_workflow.wdl runs smoothly to the end, with points instead of commas :). An other solution would be to running the pipeline in a DockerContainer wich have an english java-version installed, like the one which is provide by the gatk-team. I hope this helps other user too which are working with non-englisch java versions. Greetings EADG. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40481#Comment_40481",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3297
https://github.com/broadinstitute/gatk/issues/3301:812,Availability,error,error,812,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301
https://github.com/broadinstitute/gatk/issues/3301:466,Deployability,update,updated,466,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301
https://github.com/broadinstitute/gatk/issues/3301:271,Integrability,depend,dependencies,271,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301
https://github.com/broadinstitute/gatk/issues/3301:731,Modifiability,sandbox,sandbox,731,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301
https://github.com/broadinstitute/gatk/issues/3301:731,Testability,sandbox,sandbox,731,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301
https://github.com/broadinstitute/gatk/issues/3301:780,Testability,LOG,LOG,780,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301
https://github.com/broadinstitute/gatk/issues/3303:59,Deployability,release,released,59,"I tried to process data with BQSRPipelineSpark( the latest released gatk4 beta version), but the job always failed at middle of processing.; I use ERR000589, the bam file size is 1.3G.; knownSites uses dbsnp_138.hg19.vcf, and the data size is 10G.; reference is ucsc.hg19.2bit, data size is 0.8G .; it was running on spark2.0, and there are 4 worker in total. Each node has 16 physical cores and 64G data memory.; Below is my command.; ./gatk-launch BQSRPipelineSpark -I hdfs:///user/xxx/ERR000589.bwa.mark.bam -O hdfs:///user/xxx/ERR000589.bwa.marked.bqsr.bam -R hdfs:///user/liucheng/refs/ucsc.hg19.2bit --knownSites hdfs:///user/liucheng/dbsnp/dbsnp_138.hg19.vcf -- --sparkRunner SPARK --sparkMaster spark://cu11:7077 --total-executor-cores 48 --executor-cores 6 --executor-memory 25G --driver-memory 30G. The log is attached as follow:; [July 19, 2017 2:39:55 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BQSRPipelineSpark done. Elapsed time: 3.24 minutes.; Runtime.totalMemory()=23515365376; com.esotericsoftware.kryo.KryoException: java.lang.NegativeArraySizeException; Serialization trace:; vs (org.broadinstitute.hellbender.utils.collections.IntervalsSkipListOneContig); intervals (org.broadinstitute.hellbender.utils.collections.IntervalsSkipList); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:109); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:914,Deployability,pipeline,pipelines,914,"I tried to process data with BQSRPipelineSpark( the latest released gatk4 beta version), but the job always failed at middle of processing.; I use ERR000589, the bam file size is 1.3G.; knownSites uses dbsnp_138.hg19.vcf, and the data size is 10G.; reference is ucsc.hg19.2bit, data size is 0.8G .; it was running on spark2.0, and there are 4 worker in total. Each node has 16 physical cores and 64G data memory.; Below is my command.; ./gatk-launch BQSRPipelineSpark -I hdfs:///user/xxx/ERR000589.bwa.mark.bam -O hdfs:///user/xxx/ERR000589.bwa.marked.bqsr.bam -R hdfs:///user/liucheng/refs/ucsc.hg19.2bit --knownSites hdfs:///user/liucheng/dbsnp/dbsnp_138.hg19.vcf -- --sparkRunner SPARK --sparkMaster spark://cu11:7077 --total-executor-cores 48 --executor-cores 6 --executor-memory 25G --driver-memory 30G. The log is attached as follow:; [July 19, 2017 2:39:55 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BQSRPipelineSpark done. Elapsed time: 3.24 minutes.; Runtime.totalMemory()=23515365376; com.esotericsoftware.kryo.KryoException: java.lang.NegativeArraySizeException; Serialization trace:; vs (org.broadinstitute.hellbender.utils.collections.IntervalsSkipListOneContig); intervals (org.broadinstitute.hellbender.utils.collections.IntervalsSkipList); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:109); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:3276,Deployability,pipeline,pipelines,3276,orrentBroadcast.scala:268); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303); 	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:269); 	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:126); 	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88); 	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); 	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56); 	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1411); 	at org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:650); 	at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.java:27); 	at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:68); 	at org.broadinstitute.hellbender.tools.spark.pipelines.BQSRPipelineSpark.runTool(BQSRPipelineSpark.java:108); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delegati,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:4387,Deployability,deploy,deploy,4387,TKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esoteri,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:4424,Deployability,deploy,deploy,4424,3); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:4497,Deployability,deploy,deploy,4497,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.uti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:4574,Deployability,deploy,deploy,4574,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.MapReferenceResolver.addWrittenObject(MapReferenceResolver.java:41); 	at co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:4646,Deployability,deploy,deploy,4646, 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.MapReferenceResolver.addWrittenObject(MapReferenceResolver.java:41); 	at com.esotericsoftware.kryo.Kryo.writeReferenceOrNull(Kryo.java:658); 	at co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:4716,Deployability,deploy,deploy,4716,MainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.MapReferenceResolver.addWrittenObject(MapReferenceResolver.java:41); 	at com.esotericsoftware.kryo.Kryo.writeReferenceOrNull(Kryo.java:658); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:623); 	at c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/issues/3303:813,Testability,log,log,813,"I tried to process data with BQSRPipelineSpark( the latest released gatk4 beta version), but the job always failed at middle of processing.; I use ERR000589, the bam file size is 1.3G.; knownSites uses dbsnp_138.hg19.vcf, and the data size is 10G.; reference is ucsc.hg19.2bit, data size is 0.8G .; it was running on spark2.0, and there are 4 worker in total. Each node has 16 physical cores and 64G data memory.; Below is my command.; ./gatk-launch BQSRPipelineSpark -I hdfs:///user/xxx/ERR000589.bwa.mark.bam -O hdfs:///user/xxx/ERR000589.bwa.marked.bqsr.bam -R hdfs:///user/liucheng/refs/ucsc.hg19.2bit --knownSites hdfs:///user/liucheng/dbsnp/dbsnp_138.hg19.vcf -- --sparkRunner SPARK --sparkMaster spark://cu11:7077 --total-executor-cores 48 --executor-cores 6 --executor-memory 25G --driver-memory 30G. The log is attached as follow:; [July 19, 2017 2:39:55 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BQSRPipelineSpark done. Elapsed time: 3.24 minutes.; Runtime.totalMemory()=23515365376; com.esotericsoftware.kryo.KryoException: java.lang.NegativeArraySizeException; Serialization trace:; vs (org.broadinstitute.hellbender.utils.collections.IntervalsSkipListOneContig); intervals (org.broadinstitute.hellbender.utils.collections.IntervalsSkipList); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:109); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303
https://github.com/broadinstitute/gatk/pull/3304:242,Energy Efficiency,reduce,reduced,242,"@takutosato @LeeTL1220 As mentioned, this change scraps all the p values and replaces it with a simple and cheap probabilistic model. All our validations either improve or stay the same and speed is much better. * Spurious active regions are reduced by almost 50%.; * DREAM 4 goes from 40 hours total CPU time to 20 hours. All DREAM genomes now take less than a day.; * Hapmap sensitivity is the same.; * DREAM sensitivities for SNVs and indels all go up a bit.; * Upon manual review we no longer make any obviously bad inactive calls, except for very long deletions, which remain an issue. @takutosato This is a higher priority review than either of the documentation PRs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3304
https://github.com/broadinstitute/gatk/pull/3304:142,Security,validat,validations,142,"@takutosato @LeeTL1220 As mentioned, this change scraps all the p values and replaces it with a simple and cheap probabilistic model. All our validations either improve or stay the same and speed is much better. * Spurious active regions are reduced by almost 50%.; * DREAM 4 goes from 40 hours total CPU time to 20 hours. All DREAM genomes now take less than a day.; * Hapmap sensitivity is the same.; * DREAM sensitivities for SNVs and indels all go up a bit.; * Upon manual review we no longer make any obviously bad inactive calls, except for very long deletions, which remain an issue. @takutosato This is a higher priority review than either of the documentation PRs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3304
https://github.com/broadinstitute/gatk/pull/3304:96,Usability,simpl,simple,96,"@takutosato @LeeTL1220 As mentioned, this change scraps all the p values and replaces it with a simple and cheap probabilistic model. All our validations either improve or stay the same and speed is much better. * Spurious active regions are reduced by almost 50%.; * DREAM 4 goes from 40 hours total CPU time to 20 hours. All DREAM genomes now take less than a day.; * Hapmap sensitivity is the same.; * DREAM sensitivities for SNVs and indels all go up a bit.; * Upon manual review we no longer make any obviously bad inactive calls, except for very long deletions, which remain an issue. @takutosato This is a higher priority review than either of the documentation PRs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3304
https://github.com/broadinstitute/gatk/pull/3305:67,Availability,error,error,67,Closes #3291 . - Detects the issue earlier and fails with a better error message that is local (in code) to the problem.; - Removes the spew of warnings that are not useful except in debug.; - Fixes an issue where filter values were being replaced instead of appended.; - Implemented a wrapper to always guarantee sorting never collides.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305
https://github.com/broadinstitute/gatk/pull/3305:73,Integrability,message,message,73,Closes #3291 . - Detects the issue earlier and fails with a better error message that is local (in code) to the problem.; - Removes the spew of warnings that are not useful except in debug.; - Fixes an issue where filter values were being replaced instead of appended.; - Implemented a wrapper to always guarantee sorting never collides.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305
https://github.com/broadinstitute/gatk/pull/3305:286,Integrability,wrap,wrapper,286,Closes #3291 . - Detects the issue earlier and fails with a better error message that is local (in code) to the problem.; - Removes the spew of warnings that are not useful except in debug.; - Fixes an issue where filter values were being replaced instead of appended.; - Implemented a wrapper to always guarantee sorting never collides.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305
https://github.com/broadinstitute/gatk/pull/3305:17,Safety,Detect,Detects,17,Closes #3291 . - Detects the issue earlier and fails with a better error message that is local (in code) to the problem.; - Removes the spew of warnings that are not useful except in debug.; - Fixes an issue where filter values were being replaced instead of appended.; - Implemented a wrapper to always guarantee sorting never collides.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305
https://github.com/broadinstitute/gatk/issues/3306:297,Availability,ERROR,ERROR,297,"This issue is for tracking https://github.com/samtools/samtools/issues/704. The samtools-generated .idx and .dict entries for a reference sequence that contains a ""."" have different lengths (the .dict appears to not count the "".""), and the MD5s are different. This causes GATK to throw:. > A USER ERROR has occurred: Couldn't read file /Users/cnorman/projects/htsjdk/src/test/resources/htsjdk/samtools/cram/amb.fa. Error was: Index length does not match dictionary length for contig: iupac with exception: Index length does not match dictionary length for contig: iupac. and causes failures when reading a CRAM file using that reference. It works fine if the .dict is regenerated using GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3306
https://github.com/broadinstitute/gatk/issues/3306:415,Availability,Error,Error,415,"This issue is for tracking https://github.com/samtools/samtools/issues/704. The samtools-generated .idx and .dict entries for a reference sequence that contains a ""."" have different lengths (the .dict appears to not count the "".""), and the MD5s are different. This causes GATK to throw:. > A USER ERROR has occurred: Couldn't read file /Users/cnorman/projects/htsjdk/src/test/resources/htsjdk/samtools/cram/amb.fa. Error was: Index length does not match dictionary length for contig: iupac with exception: Index length does not match dictionary length for contig: iupac. and causes failures when reading a CRAM file using that reference. It works fine if the .dict is regenerated using GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3306
https://github.com/broadinstitute/gatk/issues/3306:582,Availability,failure,failures,582,"This issue is for tracking https://github.com/samtools/samtools/issues/704. The samtools-generated .idx and .dict entries for a reference sequence that contains a ""."" have different lengths (the .dict appears to not count the "".""), and the MD5s are different. This causes GATK to throw:. > A USER ERROR has occurred: Couldn't read file /Users/cnorman/projects/htsjdk/src/test/resources/htsjdk/samtools/cram/amb.fa. Error was: Index length does not match dictionary length for contig: iupac with exception: Index length does not match dictionary length for contig: iupac. and causes failures when reading a CRAM file using that reference. It works fine if the .dict is regenerated using GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3306
https://github.com/broadinstitute/gatk/issues/3306:371,Testability,test,test,371,"This issue is for tracking https://github.com/samtools/samtools/issues/704. The samtools-generated .idx and .dict entries for a reference sequence that contains a ""."" have different lengths (the .dict appears to not count the "".""), and the MD5s are different. This causes GATK to throw:. > A USER ERROR has occurred: Couldn't read file /Users/cnorman/projects/htsjdk/src/test/resources/htsjdk/samtools/cram/amb.fa. Error was: Index length does not match dictionary length for contig: iupac with exception: Index length does not match dictionary length for contig: iupac. and causes failures when reading a CRAM file using that reference. It works fine if the .dict is regenerated using GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3306
https://github.com/broadinstitute/gatk/pull/3307:8,Availability,error,error,8,"When an error occurs, we currently print this:. > Use -DGATK_STACKTRACE_ON_USER_EXCEPTIONto print the stack trace. In addition to missing a space, this doesn't work with gatk-launch. Replace this with:. > Use the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3307
https://github.com/broadinstitute/gatk/issues/3312:749,Availability,down,downstream,749,"Currently, we recommend the use of gatk-launch, but we're inconsistent about how we display the app name in output, including in command lines embedded in output files. We alternatively use 'gatk', gatk-launch', or '':. > Using GATK wrapper script /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk; > Running:; > /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk CalculateTargetCoverage -I ...; > [July 19, 2017 3:31:55 PM EDT] CalculateTargetCoverage --input ... Note there is no app name listed in the last line above, due to [this](https://github.com/broadinstitute/gatk/blob/33d316f0e8e35572bb60c83a144297c8557bb37d/src/main/java/org/broadinstitute/hellbender/Main.java#L103). This also affects embedded command line output. Also, downstream projects need some way to customize this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3312
https://github.com/broadinstitute/gatk/issues/3312:283,Deployability,install,install,283,"Currently, we recommend the use of gatk-launch, but we're inconsistent about how we display the app name in output, including in command lines embedded in output files. We alternatively use 'gatk', gatk-launch', or '':. > Using GATK wrapper script /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk; > Running:; > /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk CalculateTargetCoverage -I ...; > [July 19, 2017 3:31:55 PM EDT] CalculateTargetCoverage --input ... Note there is no app name listed in the last line above, due to [this](https://github.com/broadinstitute/gatk/blob/33d316f0e8e35572bb60c83a144297c8557bb37d/src/main/java/org/broadinstitute/hellbender/Main.java#L103). This also affects embedded command line output. Also, downstream projects need some way to customize this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3312
https://github.com/broadinstitute/gatk/issues/3312:355,Deployability,install,install,355,"Currently, we recommend the use of gatk-launch, but we're inconsistent about how we display the app name in output, including in command lines embedded in output files. We alternatively use 'gatk', gatk-launch', or '':. > Using GATK wrapper script /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk; > Running:; > /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk CalculateTargetCoverage -I ...; > [July 19, 2017 3:31:55 PM EDT] CalculateTargetCoverage --input ... Note there is no app name listed in the last line above, due to [this](https://github.com/broadinstitute/gatk/blob/33d316f0e8e35572bb60c83a144297c8557bb37d/src/main/java/org/broadinstitute/hellbender/Main.java#L103). This also affects embedded command line output. Also, downstream projects need some way to customize this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3312
https://github.com/broadinstitute/gatk/issues/3312:233,Integrability,wrap,wrapper,233,"Currently, we recommend the use of gatk-launch, but we're inconsistent about how we display the app name in output, including in command lines embedded in output files. We alternatively use 'gatk', gatk-launch', or '':. > Using GATK wrapper script /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk; > Running:; > /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk CalculateTargetCoverage -I ...; > [July 19, 2017 3:31:55 PM EDT] CalculateTargetCoverage --input ... Note there is no app name listed in the last line above, due to [this](https://github.com/broadinstitute/gatk/blob/33d316f0e8e35572bb60c83a144297c8557bb37d/src/main/java/org/broadinstitute/hellbender/Main.java#L103). This also affects embedded command line output. Also, downstream projects need some way to customize this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3312
https://github.com/broadinstitute/gatk/issues/3315:150,Availability,error,errors,150,"This constant controls both the maximum number of retries and the maximum number of reopens the GCS NIO library will perform in the face of transient errors. It's currently hardcoded, but should be exposed as an engine argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3315
https://github.com/broadinstitute/gatk/issues/3315:117,Performance,perform,perform,117,"This constant controls both the maximum number of retries and the maximum number of reopens the GCS NIO library will perform in the face of transient errors. It's currently hardcoded, but should be exposed as an engine argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3315
https://github.com/broadinstitute/gatk/issues/3315:198,Security,expose,exposed,198,"This constant controls both the maximum number of retries and the maximum number of reopens the GCS NIO library will perform in the face of transient errors. It's currently hardcoded, but should be exposed as an engine argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3315
https://github.com/broadinstitute/gatk/issues/3316:20,Availability,error,error,20,"I got the following error and log file from BaseRecalibrator: . ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHMI_CHMI3_WGS2.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr11:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.WXYB31; [July 20, 2017 2:18:26 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHMI_CHMI3_WGS2.recal_data.csv --intervals chr11:1+ --input gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --reference /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --misma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:3939,Availability,avail,available,3939,"root@ab23aa1f82e3 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.1; [July 20, 2017 2:22:06 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 3.65 minutes.; Runtime.totalMemory()=4188012544; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:537); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:519); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:455); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:445); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:326); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:366); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:209); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:7353,Deployability,pipeline,pipeline,7353,Iterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); ```. I ran the same command again from my computer (not in the cloud) still using the NIO paths and it ran successfully. I've also seen it run successfully when running the same pipeline in the cloud. The only thing I think I've changed is the disk size I'm asking for. I'm in the process of validating the input bam right now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:6127,Integrability,wrap,wrapAndCopyInto,6127,ools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:5352,Performance,load,loadNextRecord,5352,mtools.BAMRecordCodec.decode(BAMRecordCodec.java:209); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:7467,Security,validat,validating,7467,Iterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); ```. I ran the same command again from my computer (not in the cloud) still using the NIO paths and it ran successfully. I've also seen it run successfully when running the same pipeline in the cloud. The only thing I think I've changed is the disk size I'm asking for. I'm in the process of validating the input bam right now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:30,Testability,log,log,30,"I got the following error and log file from BaseRecalibrator: . ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHMI_CHMI3_WGS2.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr11:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.WXYB31; [July 20, 2017 2:18:26 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHMI_CHMI3_WGS2.recal_data.csv --intervals chr11:1+ --input gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --reference /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --misma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:469,Testability,log,log,469,"I got the following error and log file from BaseRecalibrator: . ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHMI_CHMI3_WGS2.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr11:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.WXYB31; [July 20, 2017 2:18:26 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHMI_CHMI3_WGS2.recal_data.csv --intervals chr11:1+ --input gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --reference /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --misma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:5158,Testability,Assert,AssertingIterator,5158,util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:366); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:209); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3316:5232,Testability,Assert,AssertingIterator,5232,ls.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:366); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:209); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316
https://github.com/broadinstitute/gatk/issues/3317:22,Deployability,integrat,integration,22,"We use Gauss-Legendre integration in the strand bias model. The number of subdivisions increases with the read count and for very deep coverage this can cause a stack overflow because, unfortunately, Apache Commons has a very questionable recursive implementation. The short-term fix is to cap the number of subdivisions. The long-term fix is to write some sort of simple adaptive 1D and 2D quadrature method. This ticket is for the short-term fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317
https://github.com/broadinstitute/gatk/issues/3317:372,Energy Efficiency,adapt,adaptive,372,"We use Gauss-Legendre integration in the strand bias model. The number of subdivisions increases with the read count and for very deep coverage this can cause a stack overflow because, unfortunately, Apache Commons has a very questionable recursive implementation. The short-term fix is to cap the number of subdivisions. The long-term fix is to write some sort of simple adaptive 1D and 2D quadrature method. This ticket is for the short-term fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317
https://github.com/broadinstitute/gatk/issues/3317:22,Integrability,integrat,integration,22,"We use Gauss-Legendre integration in the strand bias model. The number of subdivisions increases with the read count and for very deep coverage this can cause a stack overflow because, unfortunately, Apache Commons has a very questionable recursive implementation. The short-term fix is to cap the number of subdivisions. The long-term fix is to write some sort of simple adaptive 1D and 2D quadrature method. This ticket is for the short-term fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317
https://github.com/broadinstitute/gatk/issues/3317:372,Modifiability,adapt,adaptive,372,"We use Gauss-Legendre integration in the strand bias model. The number of subdivisions increases with the read count and for very deep coverage this can cause a stack overflow because, unfortunately, Apache Commons has a very questionable recursive implementation. The short-term fix is to cap the number of subdivisions. The long-term fix is to write some sort of simple adaptive 1D and 2D quadrature method. This ticket is for the short-term fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317
https://github.com/broadinstitute/gatk/issues/3317:365,Usability,simpl,simple,365,"We use Gauss-Legendre integration in the strand bias model. The number of subdivisions increases with the read count and for very deep coverage this can cause a stack overflow because, unfortunately, Apache Commons has a very questionable recursive implementation. The short-term fix is to cap the number of subdivisions. The long-term fix is to write some sort of simple adaptive 1D and 2D quadrature method. This ticket is for the short-term fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317
https://github.com/broadinstitute/gatk/issues/3322:151,Energy Efficiency,consumption,consumption,151,The alignment we currently get from naively calling into `bwa mem` produces significantly overlapping alignments both on their reference span and read consumption. Hopefully realignment (with `SWPairwiseAlignment`?) could make this easier. Deciding which part of reference and (long) read to align against might be tricky.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3322
https://github.com/broadinstitute/gatk/issues/3324:1395,Availability,error,error,1395,"I have to deal with this component recently and I found the design rather awkward.... In general between GATK and htsjdk we don't seem to have a proper support for managing and querying Supplementary alignment information from read alignment records:. 1. Querying: implemented in htsjdk consists in forging artificial SAMRecords that contain only the alignment info in the SA tag element... It seems to me that it makes more sense to create class to hold this information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder already has defined a private inner class with that in mind ""SARead"" so why not flesh it out and make it public. 2. Writing: currently SATagBuilder gets attached to a read, parsing its current SA attribute content into SARead instances. It provides the possibility adding additional SAM record one by one or clearing the list. ... then it actually updates the SA attribute on the original read when a method (setTag) is explicitly called.; I don't see the need to attach the SATag Builder to a read... it could perfectly be free standing; the same builder could be re-apply to several reads for that matter and I don't see any gain in hiding the read SA tag setting process,... even if typically this builder output would go to the ""SA"" tag, perhaps at some point we would like to also write SA coordinate list somewhere else, some other tag name or perhaps an error message... why impose this single purpose limitation?; I suggest to drop the notion of a builder for a more general custom ReadAlignmentInfo (or whatever name) list. Such list could be making reference to a dictionary to validate its elements, prevent duplicates, keep the primary SA in the first position... etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324
https://github.com/broadinstitute/gatk/issues/3324:882,Deployability,update,updates,882,"I have to deal with this component recently and I found the design rather awkward.... In general between GATK and htsjdk we don't seem to have a proper support for managing and querying Supplementary alignment information from read alignment records:. 1. Querying: implemented in htsjdk consists in forging artificial SAMRecords that contain only the alignment info in the SA tag element... It seems to me that it makes more sense to create class to hold this information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder already has defined a private inner class with that in mind ""SARead"" so why not flesh it out and make it public. 2. Writing: currently SATagBuilder gets attached to a read, parsing its current SA attribute content into SARead instances. It provides the possibility adding additional SAM record one by one or clearing the list. ... then it actually updates the SA attribute on the original read when a method (setTag) is explicitly called.; I don't see the need to attach the SATag Builder to a read... it could perfectly be free standing; the same builder could be re-apply to several reads for that matter and I don't see any gain in hiding the read SA tag setting process,... even if typically this builder output would go to the ""SA"" tag, perhaps at some point we would like to also write SA coordinate list somewhere else, some other tag name or perhaps an error message... why impose this single purpose limitation?; I suggest to drop the notion of a builder for a more general custom ReadAlignmentInfo (or whatever name) list. Such list could be making reference to a dictionary to validate its elements, prevent duplicates, keep the primary SA in the first position... etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324
https://github.com/broadinstitute/gatk/issues/3324:1401,Integrability,message,message,1401,"I have to deal with this component recently and I found the design rather awkward.... In general between GATK and htsjdk we don't seem to have a proper support for managing and querying Supplementary alignment information from read alignment records:. 1. Querying: implemented in htsjdk consists in forging artificial SAMRecords that contain only the alignment info in the SA tag element... It seems to me that it makes more sense to create class to hold this information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder already has defined a private inner class with that in mind ""SARead"" so why not flesh it out and make it public. 2. Writing: currently SATagBuilder gets attached to a read, parsing its current SA attribute content into SARead instances. It provides the possibility adding additional SAM record one by one or clearing the list. ... then it actually updates the SA attribute on the original read when a method (setTag) is explicitly called.; I don't see the need to attach the SATag Builder to a read... it could perfectly be free standing; the same builder could be re-apply to several reads for that matter and I don't see any gain in hiding the read SA tag setting process,... even if typically this builder output would go to the ""SA"" tag, perhaps at some point we would like to also write SA coordinate list somewhere else, some other tag name or perhaps an error message... why impose this single purpose limitation?; I suggest to drop the notion of a builder for a more general custom ReadAlignmentInfo (or whatever name) list. Such list could be making reference to a dictionary to validate its elements, prevent duplicates, keep the primary SA in the first position... etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324
https://github.com/broadinstitute/gatk/issues/3324:1622,Security,validat,validate,1622,"I have to deal with this component recently and I found the design rather awkward.... In general between GATK and htsjdk we don't seem to have a proper support for managing and querying Supplementary alignment information from read alignment records:. 1. Querying: implemented in htsjdk consists in forging artificial SAMRecords that contain only the alignment info in the SA tag element... It seems to me that it makes more sense to create class to hold this information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder already has defined a private inner class with that in mind ""SARead"" so why not flesh it out and make it public. 2. Writing: currently SATagBuilder gets attached to a read, parsing its current SA attribute content into SARead instances. It provides the possibility adding additional SAM record one by one or clearing the list. ... then it actually updates the SA attribute on the original read when a method (setTag) is explicitly called.; I don't see the need to attach the SATag Builder to a read... it could perfectly be free standing; the same builder could be re-apply to several reads for that matter and I don't see any gain in hiding the read SA tag setting process,... even if typically this builder output would go to the ""SA"" tag, perhaps at some point we would like to also write SA coordinate list somewhere else, some other tag name or perhaps an error message... why impose this single purpose limitation?; I suggest to drop the notion of a builder for a more general custom ReadAlignmentInfo (or whatever name) list. Such list could be making reference to a dictionary to validate its elements, prevent duplicates, keep the primary SA in the first position... etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324
https://github.com/broadinstitute/gatk/issues/3324:842,Usability,clear,clearing,842,"I have to deal with this component recently and I found the design rather awkward.... In general between GATK and htsjdk we don't seem to have a proper support for managing and querying Supplementary alignment information from read alignment records:. 1. Querying: implemented in htsjdk consists in forging artificial SAMRecords that contain only the alignment info in the SA tag element... It seems to me that it makes more sense to create class to hold this information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder already has defined a private inner class with that in mind ""SARead"" so why not flesh it out and make it public. 2. Writing: currently SATagBuilder gets attached to a read, parsing its current SA attribute content into SARead instances. It provides the possibility adding additional SAM record one by one or clearing the list. ... then it actually updates the SA attribute on the original read when a method (setTag) is explicitly called.; I don't see the need to attach the SATag Builder to a read... it could perfectly be free standing; the same builder could be re-apply to several reads for that matter and I don't see any gain in hiding the read SA tag setting process,... even if typically this builder output would go to the ""SA"" tag, perhaps at some point we would like to also write SA coordinate list somewhere else, some other tag name or perhaps an error message... why impose this single purpose limitation?; I suggest to drop the notion of a builder for a more general custom ReadAlignmentInfo (or whatever name) list. Such list could be making reference to a dictionary to validate its elements, prevent duplicates, keep the primary SA in the first position... etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324
https://github.com/broadinstitute/gatk/issues/3325:276,Deployability,release,releases,276,"Some are camel case already, some are python-style underscored arguments, and none are in the new standard format of #2596. Note that this requires carefully changing our wdls!!! @LeeTL1220 and @vdauwera this is easy enough to do, but any considerations on timing relative to releases or other logistical thoughts?. I notice that this inconsistency is shared with HaplotypeCaller. . .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3325
https://github.com/broadinstitute/gatk/issues/3325:294,Testability,log,logistical,294,"Some are camel case already, some are python-style underscored arguments, and none are in the new standard format of #2596. Note that this requires carefully changing our wdls!!! @LeeTL1220 and @vdauwera this is easy enough to do, but any considerations on timing relative to releases or other logistical thoughts?. I notice that this inconsistency is shared with HaplotypeCaller. . .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3325
https://github.com/broadinstitute/gatk/pull/3326:34,Testability,test,tests,34,@LeeTL1220 can you review this if tests pass?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3326
https://github.com/broadinstitute/gatk/pull/3327:46,Performance,load,load,46,This adds the option to use multithreading to load multiple headers at the same time when initializing FeatureReaders. . Empirically this speeds up large runs of GenomicsDBImport over GCS by 20-30%.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3327
https://github.com/broadinstitute/gatk/pull/3330:99,Deployability,integrat,integrated,99,"Updating the AUTHORS file to include authors who contributed to gatk-protected who's work has been integrated into GATK by the merger. I need to find out the preferred emails for the newly listed authors. Anders Peterson; Ayman Abdel Ghany <aymana.ghany@devfactory.com>; Kenji Kaneda ; Nils Homer. @apete @AymanDF @kkaneda @nh13 Would you like to be included here and if so, what email address would you like listed? Have I spelled your name correctly?. Resolves #3048",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3330
https://github.com/broadinstitute/gatk/pull/3330:99,Integrability,integrat,integrated,99,"Updating the AUTHORS file to include authors who contributed to gatk-protected who's work has been integrated into GATK by the merger. I need to find out the preferred emails for the newly listed authors. Anders Peterson; Ayman Abdel Ghany <aymana.ghany@devfactory.com>; Kenji Kaneda ; Nils Homer. @apete @AymanDF @kkaneda @nh13 Would you like to be included here and if so, what email address would you like listed? Have I spelled your name correctly?. Resolves #3048",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3330
https://github.com/broadinstitute/gatk/pull/3335:278,Availability,error,error,278,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335
https://github.com/broadinstitute/gatk/pull/3335:448,Deployability,integrat,integration,448,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335
https://github.com/broadinstitute/gatk/pull/3335:448,Integrability,integrat,integration,448,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335
https://github.com/broadinstitute/gatk/pull/3335:175,Safety,avoid,avoid,175,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335
https://github.com/broadinstitute/gatk/pull/3335:213,Testability,test,tested,213,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335
https://github.com/broadinstitute/gatk/pull/3335:108,Usability,simpl,simple,108,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335
https://github.com/broadinstitute/gatk/issues/3341:986,Availability,toler,tolerate,986,"ts index) for each sample ; ##; ## Description of inputs :; ##; ## ** Runtime ** (requires Docker; to use on-premises without Docker, change gatk4_jar from String to File); ## gatk4_jar: path to the java jar file containing GATK 4 (beta.2 or later) in the specified docker; ## picard_jar: path to retrieve a Picard jar (will be replaced by a docker image in a future version); ## m2_docker, oncotator_docker: docker images to use for GATK4 Mutect2 and for Oncotator; ## preemptible_attempts: how many preemptions to tolerate before switching to a non-preemptible machine (on Google); ##; ## ** Workflow options **; ## intervals: genomic intervals (will be used for scatter); ## scatter_count: number of parallel jobs to generate when scattering over intervals; ## artifact_modes: filtering options; ## m2_extra_args, m2_extra_filtering_args: additional arguments for Mutect2 calling and filtering (optional); ## is_run_orientation_bias_filter: if true, run the orientation bias filter post-processing step; ## is_run_oncotator: if true, annotate the M2 VCFs using oncotator (to produce a TCGA MAF); ##; ## ** Primary inputs **; ## ref_fasta, ref_fasta_index, ref_dict: reference genome, index, and dictionary; ## tumor_bam, tumor_bam_index, and tumor_sample_name: BAM, index and sample name for the tumor sample (sample name used for output naming); ## normal_bam, normal_bam_index, and normal_sample_name: BAM, index and sample name for the normal sample (optional if running tumor-only); ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_index: optional panel of normals in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_index: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_index: VCF of common variants with allele frequencies fo calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## onco_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341
https://github.com/broadinstitute/gatk/issues/3341:2253,Availability,down,downloads,2253," if true, annotate the M2 VCFs using oncotator (to produce a TCGA MAF); ##; ## ** Primary inputs **; ## ref_fasta, ref_fasta_index, ref_dict: reference genome, index, and dictionary; ## tumor_bam, tumor_bam_index, and tumor_sample_name: BAM, index and sample name for the tumor sample (sample name used for output naming); ## normal_bam, normal_bam_index, and normal_sample_name: BAM, index and sample name for the normal sample (optional if running tumor-only); ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_index: optional panel of normals in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_index: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_index: VCF of common variants with allele frequencies fo calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## onco_ds_tar_gz, default_config_file: Oncotator datasources and config file; ## sequencing_center, sequence_source: metadata for Oncotator; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested.; ##; ## Cromwell version support ; ## - Successfully tested on v27; ##; ## LICENSING : ; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in ; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may ; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker ; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information ; ## pertaining to the included programs. workflow Mutect2 {; # Runtime; String gatk4_jar; File picard_jar; String m2_docker; String oncotator_docker; Int preemptible_attempts; # Workflow options; Int scatter_count; File? in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341
https://github.com/broadinstitute/gatk/issues/3341:2853,Deployability,release,released,2853,"bam_index, and normal_sample_name: BAM, index and sample name for the normal sample (optional if running tumor-only); ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_index: optional panel of normals in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_index: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_index: VCF of common variants with allele frequencies fo calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## onco_ds_tar_gz, default_config_file: Oncotator datasources and config file; ## sequencing_center, sequence_source: metadata for Oncotator; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested.; ##; ## Cromwell version support ; ## - Successfully tested on v27; ##; ## LICENSING : ; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in ; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may ; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker ; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information ; ## pertaining to the included programs. workflow Mutect2 {; # Runtime; String gatk4_jar; File picard_jar; String m2_docker; String oncotator_docker; Int preemptible_attempts; # Workflow options; Int scatter_count; File? intervals ; Array[String] artifact_modes; String? m2_extra_args; String? m2_extra_filtering_args; Boolean is_run_orientation_bias_filter; Boolean is_run_oncotator; # Primary inputs ; File ref_fasta; File ref_fasta_index; File ref_dict; File tumor_bam; File tumor_bam_index; String tumor_sample_name; File? normal_bam; File? normal_bam_index; String",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341
https://github.com/broadinstitute/gatk/issues/3341:2528,Modifiability,config,config,2528," if true, annotate the M2 VCFs using oncotator (to produce a TCGA MAF); ##; ## ** Primary inputs **; ## ref_fasta, ref_fasta_index, ref_dict: reference genome, index, and dictionary; ## tumor_bam, tumor_bam_index, and tumor_sample_name: BAM, index and sample name for the tumor sample (sample name used for output naming); ## normal_bam, normal_bam_index, and normal_sample_name: BAM, index and sample name for the normal sample (optional if running tumor-only); ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_index: optional panel of normals in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_index: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_index: VCF of common variants with allele frequencies fo calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## onco_ds_tar_gz, default_config_file: Oncotator datasources and config file; ## sequencing_center, sequence_source: metadata for Oncotator; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested.; ##; ## Cromwell version support ; ## - Successfully tested on v27; ##; ## LICENSING : ; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in ; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may ; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker ; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information ; ## pertaining to the included programs. workflow Mutect2 {; # Runtime; String gatk4_jar; File picard_jar; String m2_docker; String oncotator_docker; Int preemptible_attempts; # Workflow options; Int scatter_count; File? in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341
https://github.com/broadinstitute/gatk/issues/3341:325,Performance,perform,performs,325,"comment block in the FC WDL is a good description, though not exactly the same as the GATK M2 WDL.; Also, the input parameters are re-ordered in a way that makes sense.; ```; ## Copyright Broad Institute, 2017; ## ; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample, ; ## and performs additional filtering and functional annotation tasks. ; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample ; ##; ## Description of inputs :; ##; ## ** Runtime ** (requires Docker; to use on-premises without Docker, change gatk4_jar from String to File); ## gatk4_jar: path to the java jar file containing GATK 4 (beta.2 or later) in the specified docker; ## picard_jar: path to retrieve a Picard jar (will be replaced by a docker image in a future version); ## m2_docker, oncotator_docker: docker images to use for GATK4 Mutect2 and for Oncotator; ## preemptible_attempts: how many preemptions to tolerate before switching to a non-preemptible machine (on Google); ##; ## ** Workflow options **; ## intervals: genomic intervals (will be used for scatter); ## scatter_count: number of parallel jobs to generate when scattering over intervals; ## artifact_modes: filtering options; ## m2_extra_args, m2_extra_filtering_args: additional arguments for Mutect2 calling and filtering (optional); ## is_run_orientation_bias_filter: if true, run the orientation bias filter post-processing step; ## is_run_oncotator: if true, annotate the M2 VCFs using oncotator (to produce a TCGA MAF); ##; ## ** Primary inputs **; ## ref_fasta, ref_fasta_index, ref_dict: reference genome, index, and dictionary; ## tumor_bam, tumor_bam_index, and tumor_sample_name: BAM, index and sample name for the tumor sample (sample name used for output naming); ## normal_bam, normal_bam_index, and normal_sample_name: BAM, index and sample name for the normal sample (optional if running tumor-only); ##; ## ** Primary resources ** (optional ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341
https://github.com/broadinstitute/gatk/issues/3341:3101,Security,authoriz,authorized,3101,"ble technical artifacts (false positves); ## gnomad, gnomad_index: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_index: VCF of common variants with allele frequencies fo calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## onco_ds_tar_gz, default_config_file: Oncotator datasources and config file; ## sequencing_center, sequence_source: metadata for Oncotator; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested.; ##; ## Cromwell version support ; ## - Successfully tested on v27; ##; ## LICENSING : ; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in ; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may ; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker ; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information ; ## pertaining to the included programs. workflow Mutect2 {; # Runtime; String gatk4_jar; File picard_jar; String m2_docker; String oncotator_docker; Int preemptible_attempts; # Workflow options; Int scatter_count; File? intervals ; Array[String] artifact_modes; String? m2_extra_args; String? m2_extra_filtering_args; Boolean is_run_orientation_bias_filter; Boolean is_run_oncotator; # Primary inputs ; File ref_fasta; File ref_fasta_index; File ref_dict; File tumor_bam; File tumor_bam_index; String tumor_sample_name; File? normal_bam; File? normal_bam_index; String? normal_sample_name; # Primary resources; File? pon; File? pon_index; File? gnomad; File? gnomad_index; File? variants_for_contamination; File? variants_for_contamination_index; # Secondary resources / inputs; File? onco_ds_tar_gz; File? default_config_file; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341
https://github.com/broadinstitute/gatk/issues/3341:2799,Testability,test,tested,2799,"bam_index, and normal_sample_name: BAM, index and sample name for the normal sample (optional if running tumor-only); ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_index: optional panel of normals in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_index: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_index: VCF of common variants with allele frequencies fo calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## onco_ds_tar_gz, default_config_file: Oncotator datasources and config file; ## sequencing_center, sequence_source: metadata for Oncotator; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested.; ##; ## Cromwell version support ; ## - Successfully tested on v27; ##; ## LICENSING : ; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in ; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may ; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker ; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information ; ## pertaining to the included programs. workflow Mutect2 {; # Runtime; String gatk4_jar; File picard_jar; String m2_docker; String oncotator_docker; Int preemptible_attempts; # Workflow options; Int scatter_count; File? intervals ; Array[String] artifact_modes; String? m2_extra_args; String? m2_extra_filtering_args; Boolean is_run_orientation_bias_filter; Boolean is_run_oncotator; # Primary inputs ; File ref_fasta; File ref_fasta_index; File ref_dict; File tumor_bam; File tumor_bam_index; String tumor_sample_name; File? normal_bam; File? normal_bam_index; String",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341
https://github.com/broadinstitute/gatk/issues/3344:543,Deployability,update,updates,543,"In this case, the FDR threshold is not honored. The explanation of this is complex, but essentially has to do with the Benjamini-Hochberg procedure not playing well with suppression factor when extended to more than one artifact mode. The definition of ``FilterByOrientationBias`` will have to be changes from ""guaranteeing less than 1% FDR over all mutations"" to ""guaranteeing less than 1% FDR in each specified artifact mode"". This could make the filter more aggressive, so we may have to adjust the FDR threshold. - [x] code fix; - [x] doc updates",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3344
https://github.com/broadinstitute/gatk/issues/3344:194,Modifiability,extend,extended,194,"In this case, the FDR threshold is not honored. The explanation of this is complex, but essentially has to do with the Benjamini-Hochberg procedure not playing well with suppression factor when extended to more than one artifact mode. The definition of ``FilterByOrientationBias`` will have to be changes from ""guaranteeing less than 1% FDR over all mutations"" to ""guaranteeing less than 1% FDR in each specified artifact mode"". This could make the filter more aggressive, so we may have to adjust the FDR threshold. - [x] code fix; - [x] doc updates",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3344
https://github.com/broadinstitute/gatk/issues/3348:23,Deployability,update,updated,23,"Currently it's getting updated during an active build, so we see things like ""coverage 10%"", then ""coverage 50%"", then ""coverage 80%"", etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3348
https://github.com/broadinstitute/gatk/pull/3350:56,Testability,test,test,56,Fixed issue were the travis script would fail to upload test reports because gsutils was not being found.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3350
https://github.com/broadinstitute/gatk/pull/3353:87,Deployability,integrat,integration,87,Also reduced the overall number of tests to execute by eliminating non-docker unit and integration tests from the travis.yml. I would suggest checking that the docker tests in this travis execution actually uploaded to where it says they do on gcloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3353
https://github.com/broadinstitute/gatk/pull/3353:5,Energy Efficiency,reduce,reduced,5,Also reduced the overall number of tests to execute by eliminating non-docker unit and integration tests from the travis.yml. I would suggest checking that the docker tests in this travis execution actually uploaded to where it says they do on gcloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3353
https://github.com/broadinstitute/gatk/pull/3353:87,Integrability,integrat,integration,87,Also reduced the overall number of tests to execute by eliminating non-docker unit and integration tests from the travis.yml. I would suggest checking that the docker tests in this travis execution actually uploaded to where it says they do on gcloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3353
https://github.com/broadinstitute/gatk/pull/3353:35,Testability,test,tests,35,Also reduced the overall number of tests to execute by eliminating non-docker unit and integration tests from the travis.yml. I would suggest checking that the docker tests in this travis execution actually uploaded to where it says they do on gcloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3353
https://github.com/broadinstitute/gatk/pull/3353:99,Testability,test,tests,99,Also reduced the overall number of tests to execute by eliminating non-docker unit and integration tests from the travis.yml. I would suggest checking that the docker tests in this travis execution actually uploaded to where it says they do on gcloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3353
https://github.com/broadinstitute/gatk/pull/3353:167,Testability,test,tests,167,Also reduced the overall number of tests to execute by eliminating non-docker unit and integration tests from the travis.yml. I would suggest checking that the docker tests in this travis execution actually uploaded to where it says they do on gcloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3353
https://github.com/broadinstitute/gatk/pull/3354:138,Availability,mask,masks,138,Adds additional filtering steps to the PathSeq filter to 1) trim adapter sequences and 2) mimic a simple filter used in RepeatMasker that masks windows with excessive A/T or G/C content.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354
https://github.com/broadinstitute/gatk/pull/3354:65,Energy Efficiency,adapt,adapter,65,Adds additional filtering steps to the PathSeq filter to 1) trim adapter sequences and 2) mimic a simple filter used in RepeatMasker that masks windows with excessive A/T or G/C content.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354
https://github.com/broadinstitute/gatk/pull/3354:65,Integrability,adapter,adapter,65,Adds additional filtering steps to the PathSeq filter to 1) trim adapter sequences and 2) mimic a simple filter used in RepeatMasker that masks windows with excessive A/T or G/C content.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354
https://github.com/broadinstitute/gatk/pull/3354:65,Modifiability,adapt,adapter,65,Adds additional filtering steps to the PathSeq filter to 1) trim adapter sequences and 2) mimic a simple filter used in RepeatMasker that masks windows with excessive A/T or G/C content.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354
https://github.com/broadinstitute/gatk/pull/3354:98,Usability,simpl,simple,98,Adds additional filtering steps to the PathSeq filter to 1) trim adapter sequences and 2) mimic a simple filter used in RepeatMasker that masks windows with excessive A/T or G/C content.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354
https://github.com/broadinstitute/gatk/issues/3357:933,Availability,error,error,933,"CNV workflows are unable to handle GRCh38 alternate and decoy contig names. This acts as an unintended safeguard against users including these contigs in a CNV analysis, which _they should not do for somatic analyses_. . However, in principle, this inability to process data for a contig, albeit an alternate contig, is a bug that should be fixed. This may be relevant to someone's research as I explain to the forum user who brought this bug to our attention. My reply is shown below. ---; Hey @ameynert,. Use three ticks on an independent line to surround your code block. I see from:; ```; /exports/igmm/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-launch SparkGenomeReadCounts \; -I ../../bcbio/final/WW00247b/WW00247b-ready.bam \; -o WW00247b.prop_cov \; --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa ; ```; that you are using GRCh38. So I think the `A*01` from the error message refers to any of the eleven HLA-A contigs:; ```; WMCF9-CB5:~ shlee$ cat ~/Documents/ref/hg38/Homo_sapiens_assembly38.dict | grep 'A\*01'; @SQ	SN:HLA-A*01:01:01:01	LN:3503	M5:01cd0df602495b044b2c214d69a60aa2	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:01:01:02N	LN:3291	M5:743d9f66c77fc21b964a681e0c6de2ad	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:01:38L	LN:3374	M5:dd27b7fe617e92bb77eea00fede6fd15	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:02	LN:3374	M5:3ba47a11a8a5b47ccb855308e26a2f4a	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:03	LN:3503	M5:554d43de8f2a97cae068169fe3d8462e	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:04N	LN:3136	M5:072ea3e53c79f3d00e1f1a7b492b0a8f	AS:38	UR:/seq/references/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357
https://github.com/broadinstitute/gatk/issues/3357:3222,Availability,error,error,3222,"a361a9181d69679af	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:11N	LN:3374	M5:b9ad3338cc73e2a99888a36e04c29f75	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:14	LN:3095	M5:0385be87eb49df4c59d7487495e3b1b4	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:16N	LN:2985	M5:10150ad21301a29f92e1521530fdd3f5	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:20	LN:3105	M5:05dc0384da2f751afe549a9bfdbc3037	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; ```. If the contig name is the issue, then this is a bug of sorts that, depending on your project, may or may not be an issue. If you expect all of your samples (PoN normals and tumor samples) to have coverage for specific alternate contigs and need to include such contigs in your analysis, then we can request a fix for this parsing error. I think this case would be unusual. If alternate contigs are not needed for your research, or you expect sporadic coverage for the contigs across samples, then you can move ahead by limiting your analysis to the [primary assembly](https://gatkforums.broadinstitute.org/gatk/discussion/7857/reference-genome-components). For a typical somatic CNV analysis, because of the way the PoN is pruned, when working with GRCh38 alignments, you want to be sure to limit your counting to the primary assembly. You want to use the `-L` argument with an intervals file that only lists the primary assembly and excludes alternate and decoy contigs. This is really important. Any apparent arm/contig level event with variable coverage across your samples will cause CreatePanelOfNormals to raise a _red flag_ for the sample. The tool considers the sample suspect, in that it interprets the arm/contig level ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357
https://github.com/broadinstitute/gatk/issues/3357:939,Integrability,message,message,939,"CNV workflows are unable to handle GRCh38 alternate and decoy contig names. This acts as an unintended safeguard against users including these contigs in a CNV analysis, which _they should not do for somatic analyses_. . However, in principle, this inability to process data for a contig, albeit an alternate contig, is a bug that should be fixed. This may be relevant to someone's research as I explain to the forum user who brought this bug to our attention. My reply is shown below. ---; Hey @ameynert,. Use three ticks on an independent line to surround your code block. I see from:; ```; /exports/igmm/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-launch SparkGenomeReadCounts \; -I ../../bcbio/final/WW00247b/WW00247b-ready.bam \; -o WW00247b.prop_cov \; --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa ; ```; that you are using GRCh38. So I think the `A*01` from the error message refers to any of the eleven HLA-A contigs:; ```; WMCF9-CB5:~ shlee$ cat ~/Documents/ref/hg38/Homo_sapiens_assembly38.dict | grep 'A\*01'; @SQ	SN:HLA-A*01:01:01:01	LN:3503	M5:01cd0df602495b044b2c214d69a60aa2	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:01:01:02N	LN:3291	M5:743d9f66c77fc21b964a681e0c6de2ad	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:01:38L	LN:3374	M5:dd27b7fe617e92bb77eea00fede6fd15	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:02	LN:3374	M5:3ba47a11a8a5b47ccb855308e26a2f4a	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:03	LN:3503	M5:554d43de8f2a97cae068169fe3d8462e	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:04N	LN:3136	M5:072ea3e53c79f3d00e1f1a7b492b0a8f	AS:38	UR:/seq/references/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357
https://github.com/broadinstitute/gatk/issues/3357:2959,Integrability,depend,depending,2959,"c79f3d00e1f1a7b492b0a8f	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:09	LN:3105	M5:68176666a98582ea361a9181d69679af	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:11N	LN:3374	M5:b9ad3338cc73e2a99888a36e04c29f75	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:14	LN:3095	M5:0385be87eb49df4c59d7487495e3b1b4	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:16N	LN:2985	M5:10150ad21301a29f92e1521530fdd3f5	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:20	LN:3105	M5:05dc0384da2f751afe549a9bfdbc3037	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; ```. If the contig name is the issue, then this is a bug of sorts that, depending on your project, may or may not be an issue. If you expect all of your samples (PoN normals and tumor samples) to have coverage for specific alternate contigs and need to include such contigs in your analysis, then we can request a fix for this parsing error. I think this case would be unusual. If alternate contigs are not needed for your research, or you expect sporadic coverage for the contigs across samples, then you can move ahead by limiting your analysis to the [primary assembly](https://gatkforums.broadinstitute.org/gatk/discussion/7857/reference-genome-components). For a typical somatic CNV analysis, because of the way the PoN is pruned, when working with GRCh38 alignments, you want to be sure to limit your counting to the primary assembly. You want to use the `-L` argument with an intervals file that only lists the primary assembly and excludes alternate and decoy contigs. This is really important. Any apparent arm/contig level event with variable coverage acr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357
https://github.com/broadinstitute/gatk/issues/3357:3931,Modifiability,variab,variable,3931,"@SQ	SN:HLA-A*01:16N	LN:2985	M5:10150ad21301a29f92e1521530fdd3f5	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:20	LN:3105	M5:05dc0384da2f751afe549a9bfdbc3037	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; ```. If the contig name is the issue, then this is a bug of sorts that, depending on your project, may or may not be an issue. If you expect all of your samples (PoN normals and tumor samples) to have coverage for specific alternate contigs and need to include such contigs in your analysis, then we can request a fix for this parsing error. I think this case would be unusual. If alternate contigs are not needed for your research, or you expect sporadic coverage for the contigs across samples, then you can move ahead by limiting your analysis to the [primary assembly](https://gatkforums.broadinstitute.org/gatk/discussion/7857/reference-genome-components). For a typical somatic CNV analysis, because of the way the PoN is pruned, when working with GRCh38 alignments, you want to be sure to limit your counting to the primary assembly. You want to use the `-L` argument with an intervals file that only lists the primary assembly and excludes alternate and decoy contigs. This is really important. Any apparent arm/contig level event with variable coverage across your samples will cause CreatePanelOfNormals to raise a _red flag_ for the sample. The tool considers the sample suspect, in that it interprets the arm/contig level event as somatic and in that it expects some amount of coverage for each contig for each normal sample. Suspect samples get tossed from the PoN. Because the workflow uses proportional counts, this means that for your tumor sample you must count over the same genomic intervals as the PoN normal samples. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40740#Comment_40740",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357
https://github.com/broadinstitute/gatk/issues/3357:103,Safety,safe,safeguard,103,"CNV workflows are unable to handle GRCh38 alternate and decoy contig names. This acts as an unintended safeguard against users including these contigs in a CNV analysis, which _they should not do for somatic analyses_. . However, in principle, this inability to process data for a contig, albeit an alternate contig, is a bug that should be fixed. This may be relevant to someone's research as I explain to the forum user who brought this bug to our attention. My reply is shown below. ---; Hey @ameynert,. Use three ticks on an independent line to surround your code block. I see from:; ```; /exports/igmm/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-launch SparkGenomeReadCounts \; -I ../../bcbio/final/WW00247b/WW00247b-ready.bam \; -o WW00247b.prop_cov \; --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa ; ```; that you are using GRCh38. So I think the `A*01` from the error message refers to any of the eleven HLA-A contigs:; ```; WMCF9-CB5:~ shlee$ cat ~/Documents/ref/hg38/Homo_sapiens_assembly38.dict | grep 'A\*01'; @SQ	SN:HLA-A*01:01:01:01	LN:3503	M5:01cd0df602495b044b2c214d69a60aa2	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:01:01:02N	LN:3291	M5:743d9f66c77fc21b964a681e0c6de2ad	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:01:38L	LN:3374	M5:dd27b7fe617e92bb77eea00fede6fd15	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:02	LN:3374	M5:3ba47a11a8a5b47ccb855308e26a2f4a	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:03	LN:3503	M5:554d43de8f2a97cae068169fe3d8462e	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:04N	LN:3136	M5:072ea3e53c79f3d00e1f1a7b492b0a8f	AS:38	UR:/seq/references/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357
https://github.com/broadinstitute/gatk/issues/3360:6363,Availability,down,down,6363,"adcast_0_piece0 stored as bytes in memory (estimated size 52.9 KB, free 15.8 GB); 17/07/21 16:52:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.41.105.80:34818 (size: 52.9 KB, free: 15.8 GB); 17/07/21 16:52:00 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:109; 17/07/21 16:52:00 INFO FileInputFormat: Total input paths to process : 1; 17/07/21 16:52:08 INFO SparkUI: Stopped Spark web UI at http://192.41.105.80:4040; 17/07/21 16:52:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/07/21 16:52:08 INFO MemoryStore: MemoryStore cleared; 17/07/21 16:52:08 INFO BlockManager: BlockManager stopped; 17/07/21 16:52:08 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/07/21 16:52:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/07/21 16:52:08 INFO SparkContext: Successfully stopped SparkContext; 16:52:08.357 INFO SparkGenomeReadCounts - Shutting down engine; [21 July 2017 16:52:08 BST] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=4171235328; java.lang.NumberFormatException: For input string: ""A*01""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at org.seqdoop.hadoop_bam.BAMInputFormat.getIntervals(BAMInputFormat.java:104); at org.seqdoop.hadoop_bam.BAMInputFormat.filterByInterval(BAMInputFormat.java:284); at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:158); at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:252); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:121); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:114,Deployability,update,update,114,spark things are falling over when they encounter the new hg38 contig names that include `:` and `-` . We need to update hadoop bam to understand these since they are an unfortunate fact of life. ```; [ameyner2@node2c15 read_counts]$ /exports/igmm/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-launch SparkGenomeReadCounts -I ../../bcbio/final/WW00247b/WW00247b-ready.bam -o WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa; Using GATK jar /gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar SparkGenomeReadCounts -I ../../bcbio/final/WW00247b/WW00247b-ready.bam -o WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa; 16:51:57.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; [21 July 2017 16:51:57 BST] SparkGenomeReadCounts --outputFile WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa --input ../../bcbio/final/WW00247b/WW00247b-ready.bam --keepXYMT false --binsize 10000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [21 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:10162,Energy Efficiency,reduce,reduceByKey,10162,rk.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:327); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:371); at org.apache.spark.rdd.RDD$$anonfun$countByVal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:10261,Energy Efficiency,reduce,reduceByKey,10261,ons$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:327); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:371); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1175); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:117,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:10571,Energy Efficiency,reduce,reduceByKey,10571,$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:327); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:371); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1175); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1175); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1174); at org.apache.spark.api.java.JavaRDDLike$class.co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:1139,Performance,Load,Loading,1139,derstand these since they are an unfortunate fact of life. ```; [ameyner2@node2c15 read_counts]$ /exports/igmm/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-launch SparkGenomeReadCounts -I ../../bcbio/final/WW00247b/WW00247b-ready.bam -o WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa; Using GATK jar /gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar SparkGenomeReadCounts -I ../../bcbio/final/WW00247b/WW00247b-ready.bam -o WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa; 16:51:57.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; [21 July 2017 16:51:57 BST] SparkGenomeReadCounts --outputFile WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa --input ../../bcbio/final/WW00247b/WW00247b-ready.bam --keepXYMT false --binsize 10000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [21 July 2017 16:51:57 BST] Executing as ameyner2@node2c15.ecdf.ed.ac.uk on Linux 3.10.0-327.36.3.el7.x86_64 amd64; OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:3010,Performance,load,load,3010, 2017 16:51:57 BST] Executing as ameyner2@node2c15.ecdf.ed.ac.uk on Linux 3.10.0-327.36.3.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.2; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:3126,Security,Secur,SecurityManager,3126,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d1; 17/07/21 16:51:58 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/07/21 16:51:58 INFO SparkEnv: Registering OutputCommitCoordinator; 17/07/21 16:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/07/21 16:51:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.41.105.80:4040; 17/07/21 16:51:58 INFO Executor: Starting executor ID driver o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:3199,Security,Secur,SecurityManager,3199,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d1; 17/07/21 16:51:58 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/07/21 16:51:58 INFO SparkEnv: Registering OutputCommitCoordinator; 17/07/21 16:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/07/21 16:51:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.41.105.80:4040; 17/07/21 16:51:58 INFO Executor: Starting executor ID driver o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:3274,Security,Secur,SecurityManager,3274,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d1; 17/07/21 16:51:58 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/07/21 16:51:58 INFO SparkEnv: Registering OutputCommitCoordinator; 17/07/21 16:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/07/21 16:51:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.41.105.80:4040; 17/07/21 16:51:58 INFO Executor: Starting executor ID driver o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:3346,Security,Secur,SecurityManager,3346,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d1; 17/07/21 16:51:58 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/07/21 16:51:58 INFO SparkEnv: Registering OutputCommitCoordinator; 17/07/21 16:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/07/21 16:51:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.41.105.80:4040; 17/07/21 16:51:58 INFO Executor: Starting executor ID driver o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:3420,Security,Secur,SecurityManager,3420,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d1; 17/07/21 16:51:58 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/07/21 16:51:58 INFO SparkEnv: Registering OutputCommitCoordinator; 17/07/21 16:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/07/21 16:51:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.41.105.80:4040; 17/07/21 16:51:58 INFO Executor: Starting executor ID driver o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:3437,Security,Secur,SecurityManager,3437,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d1; 17/07/21 16:51:58 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/07/21 16:51:58 INFO SparkEnv: Registering OutputCommitCoordinator; 17/07/21 16:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/07/21 16:51:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.41.105.80:4040; 17/07/21 16:51:58 INFO Executor: Starting executor ID driver o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:3454,Security,authenticat,authentication,3454,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d1; 17/07/21 16:51:58 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 17/07/21 16:51:58 INFO SparkEnv: Registering OutputCommitCoordinator; 17/07/21 16:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/07/21 16:51:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.41.105.80:4040; 17/07/21 16:51:58 INFO Executor: Starting executor ID driver o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3360:5987,Usability,clear,cleared,5987,"opping non-autosomes, as requested...; 16:51:59.672 INFO SparkGenomeReadCounts - Starting Spark coverage collection...; 17/07/21 16:52:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 463.4 KB, free 15.8 GB); 17/07/21 16:52:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 52.9 KB, free 15.8 GB); 17/07/21 16:52:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.41.105.80:34818 (size: 52.9 KB, free: 15.8 GB); 17/07/21 16:52:00 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:109; 17/07/21 16:52:00 INFO FileInputFormat: Total input paths to process : 1; 17/07/21 16:52:08 INFO SparkUI: Stopped Spark web UI at http://192.41.105.80:4040; 17/07/21 16:52:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/07/21 16:52:08 INFO MemoryStore: MemoryStore cleared; 17/07/21 16:52:08 INFO BlockManager: BlockManager stopped; 17/07/21 16:52:08 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/07/21 16:52:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/07/21 16:52:08 INFO SparkContext: Successfully stopped SparkContext; 16:52:08.357 INFO SparkGenomeReadCounts - Shutting down engine; [21 July 2017 16:52:08 BST] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=4171235328; java.lang.NumberFormatException: For input string: ""A*01""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at org.seqdoop.hadoop_bam.BAMInputFormat.getIntervals(BAMInputFormat.java:104); at org.seqdoop.hadoop_bam.BAMInputFormat.filterByInterval(BAMInputFormat.java:284); at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:158); at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360
https://github.com/broadinstitute/gatk/issues/3362:637,Availability,down,down,637,"I think CollectSequencingArtifactMetrics requires a reference, but the documentation lists it as optional. Without reference, I get an instant crash. I also get NullPointer exceptions when proving --DB_SNP. . ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /home/riestma1/gatk-4.beta.3/gatk-package-4.beta.3-local.jar CollectSequencingArtifactMetrics --input sample1.bam --output sample1_pre_adapter_detail_metrics; ...; 18:06:26.220 INFO CollectSequencingArtifactMetrics - Shutting down engine; [July 26, 2017 6:06:26 PM EDT] org.broadinstitute.hellbender.tools.picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1517289472; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.CollectSequencingArtifactMetrics.acceptRead(CollectSequencingArtifactMetrics.java:214); 	at org.broadinstitute.hellbender.tools.picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:114); 	at org.broadinstitute.hellbender.tools.picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:53); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:173); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3362
https://github.com/broadinstitute/gatk/pull/3369:116,Deployability,release,release,116,"Not ready for merge as this relies on https://github.com/HadoopGenomics/Hadoop-BAM/pull/136, hence a new Hadoop-BAM release. Addresses https://github.com/broadinstitute/gatk/issues/2572 and https://github.com/broadinstitute/gatk/issues/2571",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3369
https://github.com/broadinstitute/gatk/pull/3370:324,Deployability,pipeline,pipeline,324,"Add new scripts to gatk/scripts/sv/ folder, and alter action (but not; passed parameters) of older scripts to make running sv spark jobs; more convenient.; Added:; -copy_sv_results.sh: copy files to time and git-stamped folder on; google cloud storage; -> results folder on cluster; -> command line arguments to SV discover pipeline; -> console log file (if present). -manage_sv_pipeline.sh: create cluster, run job, copy results, and; delete cluster. Manage cluster naming, time and git-stamping,; and log file production. Altered:; -create_cluster.sh: control GCS zone and numbers of workers via; environmental variables. Defaults to previous hard-coded values. -runWholePipeline.sh: accept command-line arguments for sv; discovery pipeline, work with clusters having NUM_WORKERS != 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370
https://github.com/broadinstitute/gatk/pull/3370:734,Deployability,pipeline,pipeline,734,"Add new scripts to gatk/scripts/sv/ folder, and alter action (but not; passed parameters) of older scripts to make running sv spark jobs; more convenient.; Added:; -copy_sv_results.sh: copy files to time and git-stamped folder on; google cloud storage; -> results folder on cluster; -> command line arguments to SV discover pipeline; -> console log file (if present). -manage_sv_pipeline.sh: create cluster, run job, copy results, and; delete cluster. Manage cluster naming, time and git-stamping,; and log file production. Altered:; -create_cluster.sh: control GCS zone and numbers of workers via; environmental variables. Defaults to previous hard-coded values. -runWholePipeline.sh: accept command-line arguments for sv; discovery pipeline, work with clusters having NUM_WORKERS != 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370
https://github.com/broadinstitute/gatk/pull/3370:613,Modifiability,variab,variables,613,"Add new scripts to gatk/scripts/sv/ folder, and alter action (but not; passed parameters) of older scripts to make running sv spark jobs; more convenient.; Added:; -copy_sv_results.sh: copy files to time and git-stamped folder on; google cloud storage; -> results folder on cluster; -> command line arguments to SV discover pipeline; -> console log file (if present). -manage_sv_pipeline.sh: create cluster, run job, copy results, and; delete cluster. Manage cluster naming, time and git-stamping,; and log file production. Altered:; -create_cluster.sh: control GCS zone and numbers of workers via; environmental variables. Defaults to previous hard-coded values. -runWholePipeline.sh: accept command-line arguments for sv; discovery pipeline, work with clusters having NUM_WORKERS != 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370
https://github.com/broadinstitute/gatk/pull/3370:345,Testability,log,log,345,"Add new scripts to gatk/scripts/sv/ folder, and alter action (but not; passed parameters) of older scripts to make running sv spark jobs; more convenient.; Added:; -copy_sv_results.sh: copy files to time and git-stamped folder on; google cloud storage; -> results folder on cluster; -> command line arguments to SV discover pipeline; -> console log file (if present). -manage_sv_pipeline.sh: create cluster, run job, copy results, and; delete cluster. Manage cluster naming, time and git-stamping,; and log file production. Altered:; -create_cluster.sh: control GCS zone and numbers of workers via; environmental variables. Defaults to previous hard-coded values. -runWholePipeline.sh: accept command-line arguments for sv; discovery pipeline, work with clusters having NUM_WORKERS != 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370
https://github.com/broadinstitute/gatk/pull/3370:503,Testability,log,log,503,"Add new scripts to gatk/scripts/sv/ folder, and alter action (but not; passed parameters) of older scripts to make running sv spark jobs; more convenient.; Added:; -copy_sv_results.sh: copy files to time and git-stamped folder on; google cloud storage; -> results folder on cluster; -> command line arguments to SV discover pipeline; -> console log file (if present). -manage_sv_pipeline.sh: create cluster, run job, copy results, and; delete cluster. Manage cluster naming, time and git-stamping,; and log file production. Altered:; -create_cluster.sh: control GCS zone and numbers of workers via; environmental variables. Defaults to previous hard-coded values. -runWholePipeline.sh: accept command-line arguments for sv; discovery pipeline, work with clusters having NUM_WORKERS != 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370
https://github.com/broadinstitute/gatk/pull/3373:194,Availability,error,errors,194,"This includes the fix for the position overflowing in CloudStorageReadChannel; (https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2283), as well; as the fix for the intermittent 503 errors we've already been depending on; (https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3373
https://github.com/broadinstitute/gatk/pull/3373:220,Integrability,depend,depending,220,"This includes the fix for the position overflowing in CloudStorageReadChannel; (https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2283), as well; as the fix for the intermittent 503 errors we've already been depending on; (https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3373
https://github.com/broadinstitute/gatk/pull/3378:7,Deployability,patch,patch,7,"Just a patch to make the change suggested in the review of broadinstitute/gatk-protected#1001. @asmirnov239 your comment was correct, not sure what I was thinking back then!. Closes #3372.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3378
https://github.com/broadinstitute/gatk/pull/3379:374,Security,validat,validation,374,"@takutosato Since this is an unsupported script that I have already tested to make sure results are the same, don't spend much time on it. Here's the summary:. * Put sub-sampling of hapmap (the most expensive part and a one-time cost because the samples are the same every time) into its own wdl.; * Put the rest of generating the truth into the same wdl as the sensitivity validation. This will make things simpler for the TAG team.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3379
https://github.com/broadinstitute/gatk/pull/3379:68,Testability,test,tested,68,"@takutosato Since this is an unsupported script that I have already tested to make sure results are the same, don't spend much time on it. Here's the summary:. * Put sub-sampling of hapmap (the most expensive part and a one-time cost because the samples are the same every time) into its own wdl.; * Put the rest of generating the truth into the same wdl as the sensitivity validation. This will make things simpler for the TAG team.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3379
https://github.com/broadinstitute/gatk/pull/3379:408,Usability,simpl,simpler,408,"@takutosato Since this is an unsupported script that I have already tested to make sure results are the same, don't spend much time on it. Here's the summary:. * Put sub-sampling of hapmap (the most expensive part and a one-time cost because the samples are the same every time) into its own wdl.; * Put the rest of generating the truth into the same wdl as the sensitivity validation. This will make things simpler for the TAG team.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3379
https://github.com/broadinstitute/gatk/pull/3383:263,Deployability,pipeline,pipeline,263,"tion plots). Should prevent VariantRecalibrator from failing in a docker without R. I tested by building a new docker from the image with the NIO fix, adding a jar from this branch, then running the SNPSVariantRecalibratorCreateModel task from the joint calling pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3383
https://github.com/broadinstitute/gatk/pull/3383:87,Testability,test,tested,87,"tion plots). Should prevent VariantRecalibrator from failing in a docker without R. I tested by building a new docker from the image with the NIO fix, adding a jar from this branch, then running the SNPSVariantRecalibratorCreateModel task from the joint calling pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3383
https://github.com/broadinstitute/gatk/pull/3385:35,Availability,error,error,35,"@takutosato This corrects the math error you pointed out in your code review of the docs. While we're at it, it also outputs the error bars according to the formula given in the docs. I have tested it on an in silico contamination series and it improves results slightly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3385
https://github.com/broadinstitute/gatk/pull/3385:129,Availability,error,error,129,"@takutosato This corrects the math error you pointed out in your code review of the docs. While we're at it, it also outputs the error bars according to the formula given in the docs. I have tested it on an in silico contamination series and it improves results slightly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3385
https://github.com/broadinstitute/gatk/pull/3385:191,Testability,test,tested,191,"@takutosato This corrects the math error you pointed out in your code review of the docs. While we're at it, it also outputs the error bars according to the formula given in the docs. I have tested it on an in silico contamination series and it improves results slightly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3385
https://github.com/broadinstitute/gatk/pull/3386:11,Testability,Test,Tested,11,@LeeTL1220 Tested. Works.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3386
https://github.com/broadinstitute/gatk/pull/3388:335,Testability,test,test,335,"The earlier version of BaseQualityClipReadTransformer clipped bases by just removing them from the bases and base qualities arrays, but did not adjust the cigar, coordinate, etc. (thanks to @SHuang-Broad for catching this in #3354). This commit fixes this behavior by invoking the ReadClipper class and adds cigar checking to the unit test. It also makes some minor style changes to the transformer and test code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3388
https://github.com/broadinstitute/gatk/pull/3388:403,Testability,test,test,403,"The earlier version of BaseQualityClipReadTransformer clipped bases by just removing them from the bases and base qualities arrays, but did not adjust the cigar, coordinate, etc. (thanks to @SHuang-Broad for catching this in #3354). This commit fixes this behavior by invoking the ReadClipper class and adds cigar checking to the unit test. It also makes some minor style changes to the transformer and test code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3388
https://github.com/broadinstitute/gatk/issues/3389:238,Performance,load,load,238,"We got a bug report (broadinstitute/picard#886) in Picard about the Intel inflater / deflater causing CLPs to hang on CentOS. I wasn't able to replicate the hang on our on-prem CentOS server, but I did see the GKL consistently failing to load there. The problem was a known bug in GKL v0.5.2, fixed in v0.5.3: Intel-HLS/GKL#33. Bumping the version got the library to load for me on CentOS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3389
https://github.com/broadinstitute/gatk/issues/3389:367,Performance,load,load,367,"We got a bug report (broadinstitute/picard#886) in Picard about the Intel inflater / deflater causing CLPs to hang on CentOS. I wasn't able to replicate the hang on our on-prem CentOS server, but I did see the GKL consistently failing to load there. The problem was a known bug in GKL v0.5.2, fixed in v0.5.3: Intel-HLS/GKL#33. Bumping the version got the library to load for me on CentOS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3389
https://github.com/broadinstitute/gatk/pull/3392:5,Deployability,release,release,5,This release fixes (among other issues) a bug that could cause the; AVX compatibility check to hang on CentOS machines.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3392
https://github.com/broadinstitute/gatk/issues/3396:77,Performance,race condition,race condition,77,The current method of polling the travis build status for master is prone to race condition issues. Since there is really no systematic reason we don't want to run all the tests with every nightly build we should use new travis features to upload the tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3396
https://github.com/broadinstitute/gatk/issues/3396:172,Testability,test,tests,172,The current method of polling the travis build status for master is prone to race condition issues. Since there is really no systematic reason we don't want to run all the tests with every nightly build we should use new travis features to upload the tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3396
https://github.com/broadinstitute/gatk/issues/3396:251,Testability,test,tests,251,The current method of polling the travis build status for master is prone to race condition issues. Since there is really no systematic reason we don't want to run all the tests with every nightly build we should use new travis features to upload the tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3396
https://github.com/broadinstitute/gatk/pull/3401:99,Availability,fault,fault,99,GKL 0.5.5 fixes a critical bug in the FPGA PairHMM implementation. The bug produces a segmentation fault when FPGA PairHMM is invoked in 'private' mode. GKL 0.5.5 is otherwise identical to 0.5.3.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3401
https://github.com/broadinstitute/gatk/pull/3402:44,Performance,race condition,race condition,44,"The nightly builds weren't working due to a race condition with the travis build status indicator, so the checks were removed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3402
https://github.com/broadinstitute/gatk/pull/3403:537,Availability,failure,failure,537,"TargetCodec is dependent on the old htsjdk indexing scheme whereby the indexer called readActualHeader on the codec first (for the side effect of initializing the codec header state), and then manually processed the feature file contents by:. - re-creating the input SOURCE/stream a second time; - NOT calling readActualHeader; - extracting and passing the features one at a time to the codec's decode method, using the stream position to find the feature file offsets. Although this scheme worked with TargetCodec, it had several other failure modes (see https://github.com/samtools/htsjdk/pull/906). With https://github.com/samtools/htsjdk/pull/906, the SOURCE/stream is only opened once for indexing. However, TargetCodec uses an underlying CSVReader that automatically buffers input, which confounds the indexer. This PR works around that issue for indexing. Note: this won't compile until there is an htsjdk snapshot available with https://github.com/samtools/htsjdk/pull/906.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3403
https://github.com/broadinstitute/gatk/pull/3403:922,Availability,avail,available,922,"TargetCodec is dependent on the old htsjdk indexing scheme whereby the indexer called readActualHeader on the codec first (for the side effect of initializing the codec header state), and then manually processed the feature file contents by:. - re-creating the input SOURCE/stream a second time; - NOT calling readActualHeader; - extracting and passing the features one at a time to the codec's decode method, using the stream position to find the feature file offsets. Although this scheme worked with TargetCodec, it had several other failure modes (see https://github.com/samtools/htsjdk/pull/906). With https://github.com/samtools/htsjdk/pull/906, the SOURCE/stream is only opened once for indexing. However, TargetCodec uses an underlying CSVReader that automatically buffers input, which confounds the indexer. This PR works around that issue for indexing. Note: this won't compile until there is an htsjdk snapshot available with https://github.com/samtools/htsjdk/pull/906.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3403
https://github.com/broadinstitute/gatk/pull/3403:15,Integrability,depend,dependent,15,"TargetCodec is dependent on the old htsjdk indexing scheme whereby the indexer called readActualHeader on the codec first (for the side effect of initializing the codec header state), and then manually processed the feature file contents by:. - re-creating the input SOURCE/stream a second time; - NOT calling readActualHeader; - extracting and passing the features one at a time to the codec's decode method, using the stream position to find the feature file offsets. Although this scheme worked with TargetCodec, it had several other failure modes (see https://github.com/samtools/htsjdk/pull/906). With https://github.com/samtools/htsjdk/pull/906, the SOURCE/stream is only opened once for indexing. However, TargetCodec uses an underlying CSVReader that automatically buffers input, which confounds the indexer. This PR works around that issue for indexing. Note: this won't compile until there is an htsjdk snapshot available with https://github.com/samtools/htsjdk/pull/906.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3403
https://github.com/broadinstitute/gatk/issues/3404:102,Deployability,update,updated,102,This was also a GATK3 issue: https://github.com/broadinstitute/gsa-unstable/issues/1624. GQ should be updated to reflect the subset PLs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3404
https://github.com/broadinstitute/gatk/pull/3405:104,Performance,load,load,104,"The PathSeq filter repartitioning the reads directly before the BWA host filtering step to even out the load on each partition. This is helpful for typical samples with low pathogen abundance, when ~90% of the reads are filtered before this step. . However, in some sample types such as stool, saliva, or environmental samples, one can have a large number of reads (i.e. ~10M) at this stage so the cost of the repartition shuffle outweighs the benefit of load balancing for the slow BWA step. . This PR exposes a tool argument to skip the repartitioning.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3405
https://github.com/broadinstitute/gatk/pull/3405:455,Performance,load,load,455,"The PathSeq filter repartitioning the reads directly before the BWA host filtering step to even out the load on each partition. This is helpful for typical samples with low pathogen abundance, when ~90% of the reads are filtered before this step. . However, in some sample types such as stool, saliva, or environmental samples, one can have a large number of reads (i.e. ~10M) at this stage so the cost of the repartition shuffle outweighs the benefit of load balancing for the slow BWA step. . This PR exposes a tool argument to skip the repartitioning.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3405
