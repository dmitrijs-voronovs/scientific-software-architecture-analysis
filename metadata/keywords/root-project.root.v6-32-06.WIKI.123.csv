id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://root.cern/doc/master/TMVARegression_8C_source.html:4541,Performance,perform,performance,4541,"mlist = gTools().SplitString( myMethodList, ',' );; 115 for (UInt_t i=0; i<mlist.size(); i++) {; 116 std::string regMethod(mlist[i].Data());; 117 ; 118 if (Use.find(regMethod) == Use.end()) {; 119 std::cout << ""Method \"""" << regMethod << ""\"" not known in TMVA under this name. Choose among the following:"" << std::endl;; 120 for (std::map<std::string,int>::iterator it = Use.begin(); it != Use.end(); it++) std::cout << it->first << "" "";; 121 std::cout << std::endl;; 122 return;; 123 }; 124 Use[regMethod] = 1;; 125 }; 126 }; 127 ; 128 // --------------------------------------------------------------------------------------------------; 129 ; 130 // Here the preparation phase begins; 131 ; 132 // Create a new root output file; 133 TString outfileName( ""TMVAReg.root"" );; 134 TFile* outputFile = TFile::Open( outfileName, ""RECREATE"" );; 135 ; 136 // Create the factory object. Later you can choose the methods; 137 // whose performance you'd like to investigate. The factory will; 138 // then run the performance analysis for you.; 139 //; 140 // The first argument is the base of the name of all the; 141 // weightfiles in the directory weight/; 142 //; 143 // The second argument is the output file for the training results; 144 // All TMVA output can be suppressed by removing the ""!"" (not) in; 145 // front of the ""Silent"" argument in the option string; 146 TMVA::Factory *factory = new TMVA::Factory( ""TMVARegression"", outputFile,; 147 ""!V:!Silent:Color:DrawProgressBar:AnalysisType=Regression"" );; 148 ; 149 ; 150 TMVA::DataLoader *dataloader=new TMVA::DataLoader(""datasetreg"");; 151 // If you wish to modify default settings; 152 // (please check ""src/Config.h"" to see all available global options); 153 //; 154 // (TMVA::gConfig().GetVariablePlotting()).fTimesRMS = 8.0;; 155 // (TMVA::gConfig().GetIONames()).fWeightFileDir = ""myWeightDirectory"";; 156 ; 157 // Define the input variables that shall be used for the MVA training; 158 // note that you may also use variable expressions, su",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:6641,Performance,load,load,6641,"es()).fWeightFileDir = ""myWeightDirectory"";; 156 ; 157 // Define the input variables that shall be used for the MVA training; 158 // note that you may also use variable expressions, such as: ""3*var1/var2*abs(var3)""; 159 // [all types of expressions that can also be parsed by TTree::Draw( ""expression"" )]; 160 dataloader->AddVariable( ""var1"", ""Variable 1"", ""units"", 'F' );; 161 dataloader->AddVariable( ""var2"", ""Variable 2"", ""units"", 'F' );; 162 ; 163 // You can add so-called ""Spectator variables"", which are not used in the MVA training,; 164 // but will appear in the final ""TestTree"" produced by TMVA. This TestTree will contain the; 165 // input variables, the response values of all trained MVAs, and the spectator variables; 166 dataloader->AddSpectator( ""spec1:=var1*2"", ""Spectator 1"", ""units"", 'F' );; 167 dataloader->AddSpectator( ""spec2:=var1*3"", ""Spectator 2"", ""units"", 'F' );; 168 ; 169 // Add the variable carrying the regression target; 170 dataloader->AddTarget( ""fvalue"" );; 171 ; 172 // It is also possible to declare additional targets for multi-dimensional regression, ie:; 173 // factory->AddTarget( ""fvalue2"" );; 174 // BUT: this is currently ONLY implemented for MLP; 175 ; 176 // Read training and test data (see TMVAClassification for reading ASCII files); 177 // load the signal and background event samples from ROOT trees; 178 TFile *input(0);; 179 TString fname = ""./tmva_reg_example.root"";; 180 if (!gSystem->AccessPathName( fname )) {; 181 input = TFile::Open( fname ); // check if file in local directory exists; 182 }; 183 else {; 184 TFile::SetCacheFileDir(""."");; 185 input = TFile::Open(""http://root.cern/files/tmva_reg_example.root"", ""CACHEREAD""); // if not: download from ROOT server; 186 }; 187 if (!input) {; 188 std::cout << ""ERROR: could not open data file"" << std::endl;; 189 exit(1);; 190 }; 191 std::cout << ""--- TMVARegression : Using input file: "" << input->GetName() << std::endl;; 192 ; 193 // Register the regression tree; 194 ; 195 TTree *regTree = (",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:13880,Performance,perform,performance,13880,"ataloader, TMVA::Types::kBDT, ""BDT"",; 305 ""!H:!V:NTrees=100:MinNodeSize=1.0%:BoostType=AdaBoostR2:SeparationType=RegressionVariance:nCuts=20:PruneMethod=CostComplexity:PruneStrength=30"" );; 306 ; 307 if (Use[""BDTG""]); 308 factory->BookMethod( dataloader, TMVA::Types::kBDT, ""BDTG"",; 309 ""!H:!V:NTrees=2000::BoostType=Grad:Shrinkage=0.1:UseBaggedBoost:BaggedSampleFraction=0.5:nCuts=20:MaxDepth=3:MaxDepth=4"" );; 310 // --------------------------------------------------------------------------------------------------; 311 ; 312 // Now you can tell the factory to train, test, and evaluate the MVAs; 313 ; 314 // Train MVAs using the set of training events; 315 factory->TrainAllMethods();; 316 ; 317 // Evaluate all MVAs using the set of test events; 318 factory->TestAllMethods();; 319 ; 320 // Evaluate and compare performance of all configured MVAs; 321 factory->EvaluateAllMethods();; 322 ; 323 // --------------------------------------------------------------; 324 ; 325 // Save the output; 326 outputFile->Close();; 327 ; 328 std::cout << ""==> Wrote root file: "" << outputFile->GetName() << std::endl;; 329 std::cout << ""==> TMVARegression is done!"" << std::endl;; 330 ; 331 delete factory;; 332 delete dataloader;; 333 ; 334 // Launch the GUI for the root macros; 335 if (!gROOT->IsBatch()) TMVA::TMVARegGui( outfileName );; 336}; 337 ; 338int main( int argc, char** argv ); 339{; 340 // Select methods (don't look at this code - not of interest); 341 TString methodList;; 342 for (int i=1; i<argc; i++) {; 343 TString regMethod(argv[i]);; 344 if(regMethod==""-b"" || regMethod==""--batch"") continue;; 345 if (!methodList.IsNull()) methodList += TString("","");; 346 methodList += regMethod;; 347 }; 348 TMVARegression(methodList);; 349 return 0;; 350}; DataLoader.h; mainint main()Definition Prototype.cxx:12; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; Double_tdouble Double_tDefinition RtypesCore.h:59; TChain.h; TFile.h; inputOption_t Option_t TPoint TPoint const char GetTextMagnitude",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:15897,Performance,cache,cacheDir,15897,":46; Double_tdouble Double_tDefinition RtypesCore.h:59; TChain.h; TFile.h; inputOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void inputDefinition TGWin32VirtualXProxy.cxx:142; TMVARegGui.h; TObjString.h; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; TString.h; TSystem.h; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TTree.h; Tools.h; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TFile::Closevoid Close(Option_t *option="""") overrideClose a file.Definition TFile.cxx:950; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::AddRegressionTreevoid AddRegressionTree(TTree *tree, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)Definition DataLoader.h:103; TMVA::DataLoader::SetWeightExpressionvoid SetWeightExpression(const TString &variable, const TString &className="""")Definition DataLoader.cxx:563; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefini",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:16012,Performance,cache,cache,16012,":46; Double_tdouble Double_tDefinition RtypesCore.h:59; TChain.h; TFile.h; inputOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void inputDefinition TGWin32VirtualXProxy.cxx:142; TMVARegGui.h; TObjString.h; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; TString.h; TSystem.h; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TTree.h; Tools.h; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TFile::Closevoid Close(Option_t *option="""") overrideClose a file.Definition TFile.cxx:950; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::AddRegressionTreevoid AddRegressionTree(TTree *tree, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)Definition DataLoader.h:103; TMVA::DataLoader::SetWeightExpressionvoid SetWeightExpression(const TString &variable, const TString &className="""")Definition DataLoader.cxx:563; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefini",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:17683,Performance,load,loader,17683,"tion DataLoader.cxx:563; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddTargetvoid AddTarget(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:512; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Types::kFDA@ kFDADefinition Types.h:92; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kPDERS@ kPDERSDefinition Types.h:80; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMVA::Types::kPDEFoam@ kPDEFoamDefiniti",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:19239,Security,access,access,19239,", TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Types::kFDA@ kFDADefinition Types.h:92; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kPDERS@ kPDERSDefinition Types.h:80; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMVA::Types::kPDEFoam@ kPDEFoamDefinition Types.h:94; TMVA::Types::kLD@ kLDDefinition Types.h:95; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMVA::Types::kKNN@ kKNNDefinition Types.h:83; TMVA::Types::kMLP@ kMLPDefinition Types.h:90; TNamed::GetNameconst char * GetName() const overrideReturns name of object.Definition TNamed.h:47; TStringBasic string class.Definition TString.h:139; TString::IsNullBool_t IsNull() constDefinition TString.h:414; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::TMVARegGuivoid TMVARegGui(const char *fName=""TMVAReg.root"", TString dataset=""""); TMVA::gToolsTools & gTools(); Factory.h. tutorialstmvaTMVARegression.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:11 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:19273,Security,access,access,19273,", TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Types::kFDA@ kFDADefinition Types.h:92; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kPDERS@ kPDERSDefinition Types.h:80; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMVA::Types::kPDEFoam@ kPDEFoamDefinition Types.h:94; TMVA::Types::kLD@ kLDDefinition Types.h:95; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMVA::Types::kKNN@ kKNNDefinition Types.h:83; TMVA::Types::kMLP@ kMLPDefinition Types.h:90; TNamed::GetNameconst char * GetName() const overrideReturns name of object.Definition TNamed.h:47; TStringBasic string class.Definition TString.h:139; TString::IsNullBool_t IsNull() constDefinition TString.h:414; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; TMVA::TMVARegGuivoid TMVARegGui(const char *fName=""TMVAReg.root"", TString dataset=""""); TMVA::gToolsTools & gTools(); Factory.h. tutorialstmvaTMVARegression.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:11 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:310,Testability,test,testing,310,". ROOT: tutorials/tmva/TMVARegression.C Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVARegression.C. Go to the documentation of this file. 1/// \file; 2/// \ingroup tutorial_tmva; 3/// \notebook -nodraw; 4/// This macro provides examples for the training and testing of the; 5/// TMVA classifiers.; 6///; 7/// As input data is used a toy-MC sample consisting of four Gaussian-distributed; 8/// and linearly correlated input variables.; 9///; 10/// The methods to be used can be switched on and off by means of booleans, or; 11/// via the prompt command, for example:; 12///; 13/// root -l TMVARegression.C\‍(\""LD,MLP\""\‍); 14///; 15/// (note that the backslashes are mandatory); 16/// If no method given, a default set is used.; 17///; 18/// The output file ""TMVAReg.root"" can be analysed with the use of dedicated; 19/// macros (simply say: root -l <macro.C>), which can be conveniently; 20/// invoked through a GUI that will appear at the end of the run of this macro.; 21/// - Project : TMVA - a Root-integrated toolkit for multivariate data analysis; 22/// - Package : TMVA; 23/// - Root Macro: TMVARegression; 24///; 25/// \macro_output; 26/// \macro_code; 27/// \author Andreas Hoecker; 28 ; 29#include <cstdlib>; 30#include <iostream>; 31#include <map>; 32#include <string>; 33 ; 34#include ""TChain.h""; 35#include ""TFile.h""; 36#include ""TTree.h""; 37#include ""TString.h""; 38#include ""TObjString.h""; 39#include ""TSystem.h""; 40#include ""TROOT.h""; 41 ; 42#include ""TMVA/Tools.h""; 43#include ""TMVA/Factory.h""; 44#include ""TMVA/DataLoader.h""; 45#include ""TMVA/TMVARegGui.h""; 46 ; 47 ; 48using namespace TMVA;; 49 ; 50void TMVARegression( TString myMethodList = """" ); 51{; 52 // The explicit loading of the shared libTMVA is done in TMVAlogon.C, defined in .rootrc; 53 // if you use your private .rootrc, or run from a different directory, please copy the; 54 // corresponding lines from .rootrc; 55 ; 56 // methods to be processed can be given as an",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:2311,Testability,test,tested,2311,"d libTMVA is done in TMVAlogon.C, defined in .rootrc; 53 // if you use your private .rootrc, or run from a different directory, please copy the; 54 // corresponding lines from .rootrc; 55 ; 56 // methods to be processed can be given as an argument; use format:; 57 //; 58 // mylinux~> root -l TMVARegression.C\‍(\""myMethod1,myMethod2,myMethod3\""\‍); 59 //; 60 ; 61 //---------------------------------------------------------------; 62 // This loads the library; 63 TMVA::Tools::Instance();; 64 ; 65 ; 66 ; 67 // Default MVA methods to be trained + tested; 68 std::map<std::string,int> Use;; 69 ; 70 // Mutidimensional likelihood and Nearest-Neighbour methods; 71 Use[""PDERS""] = 0;; 72 Use[""PDEFoam""] = 1;; 73 Use[""KNN""] = 1;; 74 //; 75 // Linear Discriminant Analysis; 76 Use[""LD""] = 1;; 77 //; 78 // Function Discriminant analysis; 79 Use[""FDA_GA""] = 0;; 80 Use[""FDA_MC""] = 0;; 81 Use[""FDA_MT""] = 0;; 82 Use[""FDA_GAMT""] = 0;; 83 //; 84 // Neural Network; 85 Use[""MLP""] = 0;; 86 // Deep neural network (with CPU or GPU); 87#ifdef R__HAS_TMVAGPU; 88 Use[""DNN_GPU""] = 1;; 89 Use[""DNN_CPU""] = 0;; 90#else; 91 Use[""DNN_GPU""] = 0;; 92#ifdef R__HAS_TMVACPU; 93 Use[""DNN_CPU""] = 1;; 94#else; 95 Use[""DNN_CPU""] = 0;; 96#endif; 97#endif; 98 //; 99 // Support Vector Machine; 100 Use[""SVM""] = 0;; 101 //; 102 // Boosted Decision Trees; 103 Use[""BDT""] = 0;; 104 Use[""BDTG""] = 1;; 105 // ---------------------------------------------------------------; 106 ; 107 std::cout << std::endl;; 108 std::cout << ""==> Start TMVARegression"" << std::endl;; 109 ; 110 // Select methods (don't look at this code - not of interest); 111 if (myMethodList != """") {; 112 for (std::map<std::string,int>::iterator it = Use.begin(); it != Use.end(); it++) it->second = 0;; 113 ; 114 std::vector<TString> mlist = gTools().SplitString( myMethodList, ',' );; 115 for (UInt_t i=0; i<mlist.size(); i++) {; 116 std::string regMethod(mlist[i].Data());; 117 ; 118 if (Use.find(regMethod) == Use.end()) {; 119 std::cout << ""Method \"""" << reg",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:6574,Testability,test,test,6574,"es()).fWeightFileDir = ""myWeightDirectory"";; 156 ; 157 // Define the input variables that shall be used for the MVA training; 158 // note that you may also use variable expressions, such as: ""3*var1/var2*abs(var3)""; 159 // [all types of expressions that can also be parsed by TTree::Draw( ""expression"" )]; 160 dataloader->AddVariable( ""var1"", ""Variable 1"", ""units"", 'F' );; 161 dataloader->AddVariable( ""var2"", ""Variable 2"", ""units"", 'F' );; 162 ; 163 // You can add so-called ""Spectator variables"", which are not used in the MVA training,; 164 // but will appear in the final ""TestTree"" produced by TMVA. This TestTree will contain the; 165 // input variables, the response values of all trained MVAs, and the spectator variables; 166 dataloader->AddSpectator( ""spec1:=var1*2"", ""Spectator 1"", ""units"", 'F' );; 167 dataloader->AddSpectator( ""spec2:=var1*3"", ""Spectator 2"", ""units"", 'F' );; 168 ; 169 // Add the variable carrying the regression target; 170 dataloader->AddTarget( ""fvalue"" );; 171 ; 172 // It is also possible to declare additional targets for multi-dimensional regression, ie:; 173 // factory->AddTarget( ""fvalue2"" );; 174 // BUT: this is currently ONLY implemented for MLP; 175 ; 176 // Read training and test data (see TMVAClassification for reading ASCII files); 177 // load the signal and background event samples from ROOT trees; 178 TFile *input(0);; 179 TString fname = ""./tmva_reg_example.root"";; 180 if (!gSystem->AccessPathName( fname )) {; 181 input = TFile::Open( fname ); // check if file in local directory exists; 182 }; 183 else {; 184 TFile::SetCacheFileDir(""."");; 185 input = TFile::Open(""http://root.cern/files/tmva_reg_example.root"", ""CACHEREAD""); // if not: download from ROOT server; 186 }; 187 if (!input) {; 188 std::cout << ""ERROR: could not open data file"" << std::endl;; 189 exit(1);; 190 }; 191 std::cout << ""--- TMVARegression : Using input file: "" << input->GetName() << std::endl;; 192 ; 193 // Register the regression tree; 194 ; 195 TTree *regTree = (",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:8097,Testability,test,testing,8097,"lobal event weights per tree (see below for setting event-wise weights); 198 Double_t regWeight = 1.0;; 199 ; 200 // You can add an arbitrary number of regression trees; 201 dataloader->AddRegressionTree( regTree, regWeight );; 202 ; 203 // This would set individual event weights (the variables defined in the; 204 // expression need to exist in the original TTree); 205 dataloader->SetWeightExpression( ""var1"", ""Regression"" );; 206 ; 207 // Apply additional cuts on the signal and background samples (can be different); 208 TCut mycut = """"; // for example: TCut mycut = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; 209 ; 210 // tell the DataLoader to use all remaining events in the trees after training for testing:; 211 dataloader->PrepareTrainingAndTestTree( mycut,; 212 ""nTrain_Regression=1000:nTest_Regression=0:SplitMode=Random:NormMode=NumEvents:!V"" );; 213 //; 214 // dataloader->PrepareTrainingAndTestTree( mycut,; 215 // ""nTrain_Regression=0:nTest_Regression=0:SplitMode=Random:NormMode=NumEvents:!V"" );; 216 ; 217 // If no numbers of events are given, half of the events in the tree are used; 218 // for training, and the other half for testing:; 219 //; 220 // dataloader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; 221 ; 222 // Book MVA methods; 223 //; 224 // Please lookup the various method configuration options in the corresponding cxx files, eg:; 225 // src/MethoCuts.cxx, etc, or here: http://tmva.sourceforge.net/old_site/optionRef.html; 226 // it is possible to preset ranges in the option string in which the cut optimisation should be done:; 227 // ""...:CutRangeMin[2]=-1:CutRangeMax[2]=1""..."", where [2] is the third input variable; 228 ; 229 // PDE - RS method; 230 if (Use[""PDERS""]); 231 factory->BookMethod( dataloader, TMVA::Types::kPDERS, ""PDERS"",; 232 ""!H:!V:NormTree=T:VolumeRangeMode=Adaptive:KernelEstimator=Gauss:GaussSigma=0.3:NEventsMin=40:NEventsMax=60:VarTransform=None"" );; 233 // And the options strings for the MinMax and RMS methods, respectivel",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:8537,Testability,test,testing,8537,"lobal event weights per tree (see below for setting event-wise weights); 198 Double_t regWeight = 1.0;; 199 ; 200 // You can add an arbitrary number of regression trees; 201 dataloader->AddRegressionTree( regTree, regWeight );; 202 ; 203 // This would set individual event weights (the variables defined in the; 204 // expression need to exist in the original TTree); 205 dataloader->SetWeightExpression( ""var1"", ""Regression"" );; 206 ; 207 // Apply additional cuts on the signal and background samples (can be different); 208 TCut mycut = """"; // for example: TCut mycut = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; 209 ; 210 // tell the DataLoader to use all remaining events in the trees after training for testing:; 211 dataloader->PrepareTrainingAndTestTree( mycut,; 212 ""nTrain_Regression=1000:nTest_Regression=0:SplitMode=Random:NormMode=NumEvents:!V"" );; 213 //; 214 // dataloader->PrepareTrainingAndTestTree( mycut,; 215 // ""nTrain_Regression=0:nTest_Regression=0:SplitMode=Random:NormMode=NumEvents:!V"" );; 216 ; 217 // If no numbers of events are given, half of the events in the tree are used; 218 // for training, and the other half for testing:; 219 //; 220 // dataloader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; 221 ; 222 // Book MVA methods; 223 //; 224 // Please lookup the various method configuration options in the corresponding cxx files, eg:; 225 // src/MethoCuts.cxx, etc, or here: http://tmva.sourceforge.net/old_site/optionRef.html; 226 // it is possible to preset ranges in the option string in which the cut optimisation should be done:; 227 // ""...:CutRangeMin[2]=-1:CutRangeMax[2]=1""..."", where [2] is the third input variable; 228 ; 229 // PDE - RS method; 230 if (Use[""PDERS""]); 231 factory->BookMethod( dataloader, TMVA::Types::kPDERS, ""PDERS"",; 232 ""!H:!V:NormTree=T:VolumeRangeMode=Adaptive:KernelEstimator=Gauss:GaussSigma=0.3:NEventsMin=40:NEventsMax=60:VarTransform=None"" );; 233 // And the options strings for the MinMax and RMS methods, respectivel",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:10294,Testability,test,test,10294,"orm=None"" );; 233 // And the options strings for the MinMax and RMS methods, respectively:; 234 //; 235 // ""!H:!V:VolumeRangeMode=MinMax:DeltaFrac=0.2:KernelEstimator=Gauss:GaussSigma=0.3"" );; 236 // ""!H:!V:VolumeRangeMode=RMS:DeltaFrac=3:KernelEstimator=Gauss:GaussSigma=0.3"" );; 237 ; 238 if (Use[""PDEFoam""]); 239 factory->BookMethod( dataloader, TMVA::Types::kPDEFoam, ""PDEFoam"",; 240 ""!H:!V:MultiTargetRegression=F:TargetSelection=Mpv:TailCut=0.001:VolFrac=0.0666:nActiveCells=500:nSampl=2000:nBin=5:Compress=T:Kernel=None:Nmin=10:VarTransform=None"" );; 241 ; 242 // K-Nearest Neighbour classifier (KNN); 243 if (Use[""KNN""]); 244 factory->BookMethod( dataloader, TMVA::Types::kKNN, ""KNN"",; 245 ""nkNN=20:ScaleFrac=0.8:SigmaFact=1.0:Kernel=Gaus:UseKernel=F:UseWeight=T:!Trim"" );; 246 ; 247 // Linear discriminant; 248 if (Use[""LD""]); 249 factory->BookMethod( dataloader, TMVA::Types::kLD, ""LD"",; 250 ""!H:!V:VarTransform=None"" );; 251 ; 252 // Function discrimination analysis (FDA) -- test of various fitters - the recommended one is Minuit (or GA or SA); 253 if (Use[""FDA_MC""]); 254 factory->BookMethod( dataloader, TMVA::Types::kFDA, ""FDA_MC"",; 255 ""!H:!V:Formula=(0)+(1)*x0+(2)*x1:ParRanges=(-100,100);(-100,100);(-100,100):FitMethod=MC:SampleSize=100000:Sigma=0.1:VarTransform=D"" );; 256 ; 257 if (Use[""FDA_GA""]) // can also use Simulated Annealing (SA) algorithm (see Cuts_SA options) .. the formula of this example is good for parabolas; 258 factory->BookMethod( dataloader, TMVA::Types::kFDA, ""FDA_GA"",; 259 ""!H:!V:Formula=(0)+(1)*x0+(2)*x1:ParRanges=(-100,100);(-100,100);(-100,100):FitMethod=GA:PopSize=100:Cycles=3:Steps=30:Trim=True:SaveBestGen=1:VarTransform=Norm"" );; 260 ; 261 if (Use[""FDA_MT""]); 262 factory->BookMethod( dataloader, TMVA::Types::kFDA, ""FDA_MT"",; 263 ""!H:!V:Formula=(0)+(1)*x0+(2)*x1:ParRanges=(-100,100);(-100,100);(-100,100);(-10,10):FitMethod=MINUIT:ErrorLevel=1:PrintLevel=-1:FitStrategy=2:UseImprove:UseMinos:SetBatch"" );; 264 ; 265 if (Use[""FDA_GAMT""]); 266 fact",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:13633,Testability,test,test,13633,"ataloader, TMVA::Types::kBDT, ""BDT"",; 305 ""!H:!V:NTrees=100:MinNodeSize=1.0%:BoostType=AdaBoostR2:SeparationType=RegressionVariance:nCuts=20:PruneMethod=CostComplexity:PruneStrength=30"" );; 306 ; 307 if (Use[""BDTG""]); 308 factory->BookMethod( dataloader, TMVA::Types::kBDT, ""BDTG"",; 309 ""!H:!V:NTrees=2000::BoostType=Grad:Shrinkage=0.1:UseBaggedBoost:BaggedSampleFraction=0.5:nCuts=20:MaxDepth=3:MaxDepth=4"" );; 310 // --------------------------------------------------------------------------------------------------; 311 ; 312 // Now you can tell the factory to train, test, and evaluate the MVAs; 313 ; 314 // Train MVAs using the set of training events; 315 factory->TrainAllMethods();; 316 ; 317 // Evaluate all MVAs using the set of test events; 318 factory->TestAllMethods();; 319 ; 320 // Evaluate and compare performance of all configured MVAs; 321 factory->EvaluateAllMethods();; 322 ; 323 // --------------------------------------------------------------; 324 ; 325 // Save the output; 326 outputFile->Close();; 327 ; 328 std::cout << ""==> Wrote root file: "" << outputFile->GetName() << std::endl;; 329 std::cout << ""==> TMVARegression is done!"" << std::endl;; 330 ; 331 delete factory;; 332 delete dataloader;; 333 ; 334 // Launch the GUI for the root macros; 335 if (!gROOT->IsBatch()) TMVA::TMVARegGui( outfileName );; 336}; 337 ; 338int main( int argc, char** argv ); 339{; 340 // Select methods (don't look at this code - not of interest); 341 TString methodList;; 342 for (int i=1; i<argc; i++) {; 343 TString regMethod(argv[i]);; 344 if(regMethod==""-b"" || regMethod==""--batch"") continue;; 345 if (!methodList.IsNull()) methodList += TString("","");; 346 methodList += regMethod;; 347 }; 348 TMVARegression(methodList);; 349 return 0;; 350}; DataLoader.h; mainint main()Definition Prototype.cxx:12; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; Double_tdouble Double_tDefinition RtypesCore.h:59; TChain.h; TFile.h; inputOption_t Option_t TPoint TPoint const char GetTextMagnitude",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:13801,Testability,test,test,13801,"ataloader, TMVA::Types::kBDT, ""BDT"",; 305 ""!H:!V:NTrees=100:MinNodeSize=1.0%:BoostType=AdaBoostR2:SeparationType=RegressionVariance:nCuts=20:PruneMethod=CostComplexity:PruneStrength=30"" );; 306 ; 307 if (Use[""BDTG""]); 308 factory->BookMethod( dataloader, TMVA::Types::kBDT, ""BDTG"",; 309 ""!H:!V:NTrees=2000::BoostType=Grad:Shrinkage=0.1:UseBaggedBoost:BaggedSampleFraction=0.5:nCuts=20:MaxDepth=3:MaxDepth=4"" );; 310 // --------------------------------------------------------------------------------------------------; 311 ; 312 // Now you can tell the factory to train, test, and evaluate the MVAs; 313 ; 314 // Train MVAs using the set of training events; 315 factory->TrainAllMethods();; 316 ; 317 // Evaluate all MVAs using the set of test events; 318 factory->TestAllMethods();; 319 ; 320 // Evaluate and compare performance of all configured MVAs; 321 factory->EvaluateAllMethods();; 322 ; 323 // --------------------------------------------------------------; 324 ; 325 // Save the output; 326 outputFile->Close();; 327 ; 328 std::cout << ""==> Wrote root file: "" << outputFile->GetName() << std::endl;; 329 std::cout << ""==> TMVARegression is done!"" << std::endl;; 330 ; 331 delete factory;; 332 delete dataloader;; 333 ; 334 // Launch the GUI for the root macros; 335 if (!gROOT->IsBatch()) TMVA::TMVARegGui( outfileName );; 336}; 337 ; 338int main( int argc, char** argv ); 339{; 340 // Select methods (don't look at this code - not of interest); 341 TString methodList;; 342 for (int i=1; i<argc; i++) {; 343 TString regMethod(argv[i]);; 344 if(regMethod==""-b"" || regMethod==""--batch"") continue;; 345 if (!methodList.IsNull()) methodList += TString("","");; 346 methodList += regMethod;; 347 }; 348 TMVARegression(methodList);; 349 return 0;; 350}; DataLoader.h; mainint main()Definition Prototype.cxx:12; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; Double_tdouble Double_tDefinition RtypesCore.h:59; TChain.h; TFile.h; inputOption_t Option_t TPoint TPoint const char GetTextMagnitude",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:16870,Testability,test,test,16870,"ile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TFile::Closevoid Close(Option_t *option="""") overrideClose a file.Definition TFile.cxx:950; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::AddRegressionTreevoid AddRegressionTree(TTree *tree, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)Definition DataLoader.h:103; TMVA::DataLoader::SetWeightExpressionvoid SetWeightExpression(const TString &variable, const TString &className="""")Definition DataLoader.cxx:563; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddTargetvoid AddTarget(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:512; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::F",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:17909,Testability,test,testing,17909,"for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddTargetvoid AddTarget(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:512; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Types::kFDA@ kFDADefinition Types.h:92; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kPDERS@ kPDERSDefinition Types.h:80; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMVA::Types::kPDEFoam@ kPDEFoamDefinition Types.h:94; TMVA::Types::kLD@ kLDDefinition Types.h:95; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMVA::Types::kKNN@ kKNNDefinition Types.h:83; TMVA::Types::kMLP@ kMLPDefinition Types.",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/TMVARegression_8C_source.html:881,Usability,simpl,simply,881,". ROOT: tutorials/tmva/TMVARegression.C Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVARegression.C. Go to the documentation of this file. 1/// \file; 2/// \ingroup tutorial_tmva; 3/// \notebook -nodraw; 4/// This macro provides examples for the training and testing of the; 5/// TMVA classifiers.; 6///; 7/// As input data is used a toy-MC sample consisting of four Gaussian-distributed; 8/// and linearly correlated input variables.; 9///; 10/// The methods to be used can be switched on and off by means of booleans, or; 11/// via the prompt command, for example:; 12///; 13/// root -l TMVARegression.C\‍(\""LD,MLP\""\‍); 14///; 15/// (note that the backslashes are mandatory); 16/// If no method given, a default set is used.; 17///; 18/// The output file ""TMVAReg.root"" can be analysed with the use of dedicated; 19/// macros (simply say: root -l <macro.C>), which can be conveniently; 20/// invoked through a GUI that will appear at the end of the run of this macro.; 21/// - Project : TMVA - a Root-integrated toolkit for multivariate data analysis; 22/// - Package : TMVA; 23/// - Root Macro: TMVARegression; 24///; 25/// \macro_output; 26/// \macro_code; 27/// \author Andreas Hoecker; 28 ; 29#include <cstdlib>; 30#include <iostream>; 31#include <map>; 32#include <string>; 33 ; 34#include ""TChain.h""; 35#include ""TFile.h""; 36#include ""TTree.h""; 37#include ""TString.h""; 38#include ""TObjString.h""; 39#include ""TSystem.h""; 40#include ""TROOT.h""; 41 ; 42#include ""TMVA/Tools.h""; 43#include ""TMVA/Factory.h""; 44#include ""TMVA/DataLoader.h""; 45#include ""TMVA/TMVARegGui.h""; 46 ; 47 ; 48using namespace TMVA;; 49 ; 50void TMVARegression( TString myMethodList = """" ); 51{; 52 // The explicit loading of the shared libTMVA is done in TMVAlogon.C, defined in .rootrc; 53 // if you use your private .rootrc, or run from a different directory, please copy the; 54 // corresponding lines from .rootrc; 55 ; 56 // methods to be processed can be given as an",MatchSource.WIKI,doc/master/TMVARegression_8C_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVARegression_8C_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html:12507,Availability,error,error,12507," used for training.Definition Functions.h:82; TMVA::DNN::EOptimizer::kAdam@ kAdam; TMVA::DNN::EOptimizer::kRMSProp@ kRMSProp; TMVA::DNN::EOptimizer::kAdadelta@ kAdadelta; TMVA::DNN::EOptimizer::kSGD@ kSGD; TMVA::DNN::EOptimizer::kAdagrad@ kAdagrad; TMVA::DNN::evaluateMatrixvoid evaluateMatrix(typename Architecture_t::Matrix_t &A, EActivationFunction f)Definition Functions.h:152; TMVA::DNN::addRegularizationGradientsvoid addRegularizationGradients(typename Architecture_t::Matrix_t &A, const typename Architecture_t::Matrix_t &W, typename Architecture_t::Scalar_t weightDecay, ERegularization R)Add the regularization gradient corresponding to weight matrix W, to the matrix A.Definition Functions.h:258; TMVA::DNN::EOutputFunctionEOutputFunctionEnum that represents output functions.Definition Functions.h:46; TMVA::DNN::EOutputFunction::kSoftmax@ kSoftmax; TMVA::DNN::EOutputFunction::kSigmoid@ kSigmoid; TMVA::DNN::EOutputFunction::kIdentity@ kIdentity; TMVA::DNN::weightDecaydouble weightDecay(double error, ItWeight itWeight, ItWeight itWeightEnd, double factorWeightDecay, EnumRegularization eRegularization)compute the weight decay for regularization (L1 or L2)Definition NeuralNet.icc:498; TMVA::DNN::evaluatevoid evaluate(typename Architecture_t::Tensor_t &A, EActivationFunction f)Apply the given activation function to each value in the given tensor A.Definition Functions.h:98; TMVA::DNN::regularizationauto regularization(const typename Architecture_t::Matrix_t &A, ERegularization R) -> decltype(Architecture_t::L1Regularization(A))Evaluate the regularization functional for a given weight matrix.Definition Functions.h:238; TMVA::DNN::ERegularizationERegularizationEnum representing the regularization type applied for a given layer.Definition Functions.h:65; TMVA::DNN::ERegularization::kL2@ kL2; TMVA::DNN::ERegularization::kL1@ kL1; TMVA::DNN::ERegularization::kNone@ kNone; TMVA::DNN::EActivationFunctionEActivationFunctionEnum that represents layer activation functions.Definit",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html:15041,Modifiability,variab,variable,15041,"ation::kL2@ kL2; TMVA::DNN::ERegularization::kL1@ kL1; TMVA::DNN::ERegularization::kNone@ kNone; TMVA::DNN::EActivationFunctionEActivationFunctionEnum that represents layer activation functions.Definition Functions.h:32; TMVA::DNN::EActivationFunction::kRelu@ kRelu; TMVA::DNN::EActivationFunction::kGauss@ kGauss; TMVA::DNN::EActivationFunction::kTanh@ kTanh; TMVA::DNN::EActivationFunction::kFastTanh@ kFastTanh; TMVA::DNN::EActivationFunction::kSigmoid@ kSigmoid; TMVA::DNN::EActivationFunction::kIdentity@ kIdentity; TMVA::DNN::EActivationFunction::kSoftSign@ kSoftSign; TMVA::DNN::EActivationFunction::kSymmRelu@ kSymmRelu; TMVA::DNN::ELossFunctionELossFunctionEnum that represents objective functions for the net, i.e.Definition Functions.h:57; TMVA::DNN::ELossFunction::kCrossEntropy@ kCrossEntropy; TMVA::DNN::ELossFunction::kSoftmaxCrossEntropy@ kSoftmaxCrossEntropy; TMVA::DNN::ELossFunction::kMeanSquaredError@ kMeanSquaredError; TMVA::DNN::evaluateGradientsvoid evaluateGradients(typename Architecture_t::Matrix_t &dY, ELossFunction f, const typename Architecture_t::Matrix_t &Y, const typename Architecture_t::Matrix_t &output, const typename Architecture_t::Matrix_t &weights)Compute the gradient of the given output function f for given activations output of the output layer ...Definition Functions.h:215; TMVA::DNN::initializevoid initialize(typename Architecture_t::Matrix_t &A, EInitialization m)Definition Functions.h:282; TMVA::DNN::evaluateDerivativevoid evaluateDerivative(typename Architecture_t::Tensor_t &B, EActivationFunction f, const typename Architecture_t::Tensor_t &A)Compute the first partial derivative of the activation function for the values given in tensor A and ...Definition Functions.h:125; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; mTMarker mDefinition textangle.C:8; outputstatic void output(). tmvatmvaincTMVADNNFunctions.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:45 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html:2521,Performance,optimiz,optimizer,2521," kTanh = 3,; 37 kSymmRelu = 4,; 38 kSoftSign = 5,; 39 kGauss = 6,; 40 kFastTanh = 7; 41 ; 42};; 43 ; 44/*! Enum that represents output functions */; 45enum class EOutputFunction; 46{; 47 kIdentity = 'I',; 48 kSigmoid = 'S',; 49 kSoftmax = 'M'; 50};; 51 ; 52/*! Enum that represents objective functions for the net, i.e. functions; 53* that take the output from the last layer in the net together with the; 54* truths and return the objective function values that is to be minimized; 55* in the training process. */; 56enum class ELossFunction; 57{; 58 kCrossEntropy = 'C',; 59 kMeanSquaredError = 'R',; 60 kSoftmaxCrossEntropy = 'S'; 61};; 62 ; 63/*! Enum representing the regularization type applied for a given layer */; 64enum class ERegularization; 65{; 66 kNone = '0',; 67 kL1 = '1',; 68 kL2 = '2'; 69 };; 70 ; 71/* Enum representing the initialization method used for this layer. */; 72enum class EInitialization {; 73 kGauss = 'G',; 74 kUniform = 'U',; 75 kIdentity = 'I',; 76 kZero = 'Z',; 77 kGlorotNormal = 'X',; 78 kGlorotUniform = 'F',; 79};; 80 ; 81/// Enum representing the optimizer used for training.; 82enum class EOptimizer {; 83 kSGD = 0,; 84 kAdam = 1,; 85 kAdagrad = 2,; 86 kRMSProp = 3,; 87 kAdadelta = 4,; 88};; 89 ; 90//______________________________________________________________________________; 91//; 92// Activation Functions; 93//______________________________________________________________________________; 94 ; 95/*! Apply the given activation function to each value in the given; 96* tensor A. */; 97template<typename Architecture_t>; 98inline void evaluate(typename Architecture_t::Tensor_t &A,; 99 EActivationFunction f); 100{; 101 switch(f); 102 {; 103 case EActivationFunction::kIdentity : break;; 104 case EActivationFunction::kRelu : Architecture_t::Relu(A);; 105 break;; 106 case EActivationFunction::kSigmoid : Architecture_t::Sigmoid(A);; 107 break;; 108 case EActivationFunction::kTanh : Architecture_t::Tanh(A);; 109 break;; 110 case EActivationFunction:",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html:11490,Performance,optimiz,optimizer,11490,"rchitecture_t::InitializeZero(A);; 293 break;; 294 case EInitialization::kGlorotNormal : Architecture_t::InitializeGlorotNormal(A);; 295 break;; 296 case EInitialization::kGlorotUniform : Architecture_t::InitializeGlorotUniform(A);; 297 break;; 298 }; 299}; 300 ; 301} // namespace DNN; 302} // namespace TMVA; 303 ; 304#endif; f#define f(i)Definition RSha256.hxx:104; X#define X(type, name); R; TMVA::DNN::EInitializationEInitializationDefinition Functions.h:72; TMVA::DNN::EInitialization::kGauss@ kGauss; TMVA::DNN::EInitialization::kGlorotNormal@ kGlorotNormal; TMVA::DNN::EInitialization::kUniform@ kUniform; TMVA::DNN::EInitialization::kGlorotUniform@ kGlorotUniform; TMVA::DNN::EInitialization::kZero@ kZero; TMVA::DNN::EInitialization::kIdentity@ kIdentity; TMVA::DNN::evaluateDerivativeMatrixvoid evaluateDerivativeMatrix(typename Architecture_t::Matrix_t &B, EActivationFunction f, const typename Architecture_t::Matrix_t &A)Definition Functions.h:160; TMVA::DNN::EOptimizerEOptimizerEnum representing the optimizer used for training.Definition Functions.h:82; TMVA::DNN::EOptimizer::kAdam@ kAdam; TMVA::DNN::EOptimizer::kRMSProp@ kRMSProp; TMVA::DNN::EOptimizer::kAdadelta@ kAdadelta; TMVA::DNN::EOptimizer::kSGD@ kSGD; TMVA::DNN::EOptimizer::kAdagrad@ kAdagrad; TMVA::DNN::evaluateMatrixvoid evaluateMatrix(typename Architecture_t::Matrix_t &A, EActivationFunction f)Definition Functions.h:152; TMVA::DNN::addRegularizationGradientsvoid addRegularizationGradients(typename Architecture_t::Matrix_t &A, const typename Architecture_t::Matrix_t &W, typename Architecture_t::Scalar_t weightDecay, ERegularization R)Add the regularization gradient corresponding to weight matrix W, to the matrix A.Definition Functions.h:258; TMVA::DNN::EOutputFunctionEOutputFunctionEnum that represents output functions.Definition Functions.h:46; TMVA::DNN::EOutputFunction::kSoftmax@ kSoftmax; TMVA::DNN::EOutputFunction::kSigmoid@ kSigmoid; TMVA::DNN::EOutputFunction::kIdentity@ kIdentity; TMVA::DNN::weig",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h.html:330,Integrability,depend,dependency,330,". ROOT: tmva/tmva/inc/TMVA/Factory.h File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Namespaces ; Factory.h File Reference. #include <vector>; #include <map>; #include ""TCut.h""; #include ""TMVA/Configurable.h""; #include ""TMVA/Types.h""; #include ""TMVA/DataSet.h"". Include dependency graph for Factory.h:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. This graph shows which files directly or indirectly include this file:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Classes; class  TMVA::Factory;  This is the main MVA steering class. More...;  . Namespaces; namespace  TMVA;  create variable transformations ;  . tmvatmvaincTMVAFactory.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:21 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h.html:723,Modifiability,variab,variable,723,". ROOT: tmva/tmva/inc/TMVA/Factory.h File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classes |; Namespaces ; Factory.h File Reference. #include <vector>; #include <map>; #include ""TCut.h""; #include ""TMVA/Configurable.h""; #include ""TMVA/Types.h""; #include ""TMVA/DataSet.h"". Include dependency graph for Factory.h:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. This graph shows which files directly or indirectly include this file:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Classes; class  TMVA::Factory;  This is the main MVA steering class. More...;  . Namespaces; namespace  TMVA;  create variable transformations ;  . tmvatmvaincTMVAFactory.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:21 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:500,Deployability,integrat,integrated,500,". ROOT: tmva/tmva/inc/TMVA/Factory.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Factory.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Peter Speckmayer, Joerg Stelzer, Helge Voss, Kai Voss, Eckhard von Toerne, Jan Therhaag; 3// Updated by: Omar Zapata, Lorenzo Moneta, Sergei Gleyzer; 4 ; 5/**********************************************************************************; 6 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 7 * Package: TMVA *; 8 * Class : Factory *; 9 * *; 10 * *; 11 * Description: *; 12 * This is the main MVA steering class: it creates (books) all MVA methods, *; 13 * and guides them through the training, testing and evaluation phases. *; 14 * *; 15 * Authors (alphabetical): *; 16 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 17 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 18 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 19 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 20 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 21 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 22 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 23 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 24 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 25 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 26 * *; 27 * Copyright (c) 2005-2011: *; 28 * CERN, Switzerland *; 29 * U. of Victoria, Canada *; 30 * MPI-K Heidelberg, Germany *; 31 * U. of Bonn, Germany *; 32 * UdeA/ITM, Colombia *; 33 * U. of Florida, USA *; 34 * *; 35 * Redistribution and use in source and binary forms, with or without *; 36 * modification, are permitted according to the terms listed in LICENSE *; 37 * (see tmva/doc/LICENSE) *; 38 **********************************************************************************/; 39 ; 40#ifndef ROOT_T",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:5891,Deployability,configurat,configuration,5891,"104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration options tuning.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:500,Integrability,integrat,integrated,500,". ROOT: tmva/tmva/inc/TMVA/Factory.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Factory.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Peter Speckmayer, Joerg Stelzer, Helge Voss, Kai Voss, Eckhard von Toerne, Jan Therhaag; 3// Updated by: Omar Zapata, Lorenzo Moneta, Sergei Gleyzer; 4 ; 5/**********************************************************************************; 6 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 7 * Package: TMVA *; 8 * Class : Factory *; 9 * *; 10 * *; 11 * Description: *; 12 * This is the main MVA steering class: it creates (books) all MVA methods, *; 13 * and guides them through the training, testing and evaluation phases. *; 14 * *; 15 * Authors (alphabetical): *; 16 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 17 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 18 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 19 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 20 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 21 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 22 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 23 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 24 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 25 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 26 * *; 27 * Copyright (c) 2005-2011: *; 28 * CERN, Switzerland *; 29 * U. of Victoria, Canada *; 30 * MPI-K Heidelberg, Germany *; 31 * U. of Bonn, Germany *; 32 * UdeA/ITM, Colombia *; 33 * U. of Florida, USA *; 34 * *; 35 * Redistribution and use in source and binary forms, with or without *; 36 * modification, are permitted according to the terms listed in LICENSE *; 37 * (see tmva/doc/LICENSE) *; 38 **********************************************************************************/; 39 ; 40#ifndef ROOT_T",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:5629,Integrability,message,messages,5629,"104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration options tuning.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:5829,Integrability,message,messages,5829,"104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration options tuning.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:5966,Integrability,message,messages,5966,"asMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration options tuning.; 145 // If no classifier name is given, help messages for all booked; 146 // classifiers are printed; 147 void PrintHelpMessage(const TString& datasetname , const TString& methodTitle = """" ) const;; 148 ; 149 TDirectory* RootBaseDir() { return (TDirectory*)fgTargetFile; }; 150 ; 151 Bool_t IsSilentFile() const { return fSilentFile;}; 152 Bool_t IsModelPersistence() const { return fModelPersistence; }; 153 ; 154 Double_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass = 0,; 155 Types::ETreeType type = Types::kTesting);; 156 Double_t GetROCIntegral(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 157 Types::ETreeType type = Types::kTesting);; 158 ; 159 // Methods to get a TGraph for an indicated method in dataset.; 160 // Optional title and axis added with fLegend=kTRUE.; 161 // Argument iClass used in multiclass settings, otherwise ignored.; 162 TGraph *GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles = kTRUE, UInt_t iClass = 0,; 163 Types::ETreeType type = Types::kTesting);; 164 TGraph *GetROCCurve(TString datasetname, TString theMethodName, Bool_t setTitles = kTRUE, UInt_t iClass = 0,; 165 Types::ETreeType type = Types::kTesting);; 166 ; 167 // Methods to get a TMultiGraph for a given class and all methods in dataset.; 168 TMultiGraph *GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Ty",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:7816,Integrability,message,message,7816,,MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:12357,Integrability,message,message,12357,"unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; TCanvasThe Canvas class.Definition TCanvas.h:23; TDirectoryDescribe directory structure in memory.Definition TDirectory.h:45; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::CrossValidationClass to perform cross validation, splitting the dataloader into folds.Definition CrossValidation.h:124; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::PrintHelpMessagevoid PrintHelpMessage(const TString &datasetname, const TString &methodTitle="""") constPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fSilentFileBool_t fSilentFile! used in constructor without fileDefinition Factory.h:217; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::IsModelPersistenceBool_t IsModelPersistence() constDefinition Factory.h:152; TMVA::Factory::fOptionsTString fOptions! option string given by construction (presently only ""V"")Definition Factory.h:211; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::VerboseBool_t Verbose(void) constDefinition Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)Definition Factory.cxx:602; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoad",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:19385,Integrability,message,message,19385,"nAllMethodsForRegressionvoid TrainAllMethodsForRegression(void)Definition Factory.h:116; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::fDefaultTrfsstd::vector< TMVA::VariableTransformBase * > fDefaultTrfs! list of transformations on default DataSetDefinition Factory.h:208; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetname, const TString &title) constReturns pointer to MVA that corresponds to given method title.Definition Factory.cxx:566; TMVA::Factory::DeleteAllMethodsvoid DeleteAllMethods(void)Delete methods.Definition Factory.cxx:324; TMVA::Factory::fTransformationsTString fTransformations! list of transformations to testDefinition Factory.h:212; TMVA::Factory::Greetingsvoid Greetings()Print welcome message.Definition Factory.cxx:295; TMVA::IMethodInterface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::ROCCurveDefinition ROCCurve.h:46; TMVA::Types::EMVAEMVADefinition Types.h:76; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kTesting@ kTestingDefinition Types.h:144; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TStringBasic string class.Definition TString.h:139; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; bool; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; v@ vDefinition rootcling_impl.cxx:3699; Types.h. tmvatmvaincTMVAFactory.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:5891,Modifiability,config,configuration,5891,"104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration options tuning.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:7904,Modifiability,variab,variable,7904,,MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:8070,Modifiability,variab,variables,8070,,MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:14573,Modifiability,variab,variables,14573,"n the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::TrainAllMethodsForClassificationvoid TrainAllMethodsForClassification(void)Definition Factory.h:115; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::RootBaseDirTDirectory * RootBaseDir()Definition Factory.h:149; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::OptimizeAllMethodsForRegressionvoid OptimizeAllMethodsForRegression(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Definition Factory.h:111; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(Data",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:20089,Modifiability,variab,variable,20089,"nAllMethodsForRegressionvoid TrainAllMethodsForRegression(void)Definition Factory.h:116; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::fDefaultTrfsstd::vector< TMVA::VariableTransformBase * > fDefaultTrfs! list of transformations on default DataSetDefinition Factory.h:208; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetname, const TString &title) constReturns pointer to MVA that corresponds to given method title.Definition Factory.cxx:566; TMVA::Factory::DeleteAllMethodsvoid DeleteAllMethods(void)Delete methods.Definition Factory.cxx:324; TMVA::Factory::fTransformationsTString fTransformations! list of transformations to testDefinition Factory.h:212; TMVA::Factory::Greetingsvoid Greetings()Print welcome message.Definition Factory.cxx:295; TMVA::IMethodInterface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::ROCCurveDefinition ROCCurve.h:46; TMVA::Types::EMVAEMVADefinition Types.h:76; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kTesting@ kTestingDefinition Types.h:144; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TStringBasic string class.Definition TString.h:139; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; bool; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; v@ vDefinition rootcling_impl.cxx:3699; Types.h. tmvatmvaincTMVAFactory.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:3615,Performance,load,loader,3615,"s TFile;; 64class TGraph;; 65class TH1F;; 66class TMultiGraph;; 67class TTree;; 68namespace TMVA {; 69 ; 70 class IMethod;; 71 class MethodBase;; 72 class DataInputHandler;; 73 class DataSetInfo;; 74 class DataSetManager;; 75 class DataLoader;; 76 class ROCCurve;; 77 class VariableTransformBase;; 78 ; 79 ; 80 class Factory : public Configurable {; 81 friend class CrossValidation;; 82 public:; 83 ; 84 typedef std::vector<IMethod*> MVector;; 85 std::map<TString,MVector*> fMethodsMap;//all methods for every dataset with the same name; 86 ; 87 // no default constructor; 88 Factory( TString theJobName, TFile* theTargetFile, TString theOption = """" );; 89 ; 90 // constructor to work without file; 91 Factory( TString theJobName, TString theOption = """" );; 92 ; 93 // default destructor; 94 virtual ~Factory();; 95 ; 96 // use TName::GetName and define correct name in constructor; 97 //virtual const char* GetName() const { return ""Factory""; }; 98 ; 99 ; 100 MethodBase* BookMethod( DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption = """" );; 101 MethodBase* BookMethod( DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption = """" );; 102 MethodBase* BookMethod( DataLoader *, TMVA::Types::EMVA /*theMethod*/,; 103 TString /*methodTitle*/,; 104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsF",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:3734,Performance,load,loader,3734,"od;; 71 class MethodBase;; 72 class DataInputHandler;; 73 class DataSetInfo;; 74 class DataSetManager;; 75 class DataLoader;; 76 class ROCCurve;; 77 class VariableTransformBase;; 78 ; 79 ; 80 class Factory : public Configurable {; 81 friend class CrossValidation;; 82 public:; 83 ; 84 typedef std::vector<IMethod*> MVector;; 85 std::map<TString,MVector*> fMethodsMap;//all methods for every dataset with the same name; 86 ; 87 // no default constructor; 88 Factory( TString theJobName, TFile* theTargetFile, TString theOption = """" );; 89 ; 90 // constructor to work without file; 91 Factory( TString theJobName, TString theOption = """" );; 92 ; 93 // default destructor; 94 virtual ~Factory();; 95 ; 96 // use TName::GetName and define correct name in constructor; 97 //virtual const char* GetName() const { return ""Factory""; }; 98 ; 99 ; 100 MethodBase* BookMethod( DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption = """" );; 101 MethodBase* BookMethod( DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption = """" );; 102 MethodBase* BookMethod( DataLoader *, TMVA::Types::EMVA /*theMethod*/,; 103 TString /*methodTitle*/,; 104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); };",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:4064,Performance,optimiz,optimize,4064,"::map<TString,MVector*> fMethodsMap;//all methods for every dataset with the same name; 86 ; 87 // no default constructor; 88 Factory( TString theJobName, TFile* theTargetFile, TString theOption = """" );; 89 ; 90 // constructor to work without file; 91 Factory( TString theJobName, TString theOption = """" );; 92 ; 93 // default destructor; 94 virtual ~Factory();; 95 ; 96 // use TName::GetName and define correct name in constructor; 97 //virtual const char* GetName() const { return ""Factory""; }; 98 ; 99 ; 100 MethodBase* BookMethod( DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption = """" );; 101 MethodBase* BookMethod( DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption = """" );; 102 MethodBase* BookMethod( DataLoader *, TMVA::Types::EMVA /*theMethod*/,; 103 TString /*methodTitle*/,; 104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theO",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:4802,Performance,perform,performance,4802," );; 102 MethodBase* BookMethod( DataLoader *, TMVA::Types::EMVA /*theMethod*/,; 103 TString /*methodTitle*/,; 104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints clas",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:4906,Performance,load,loader,4906,"Title*/,; 104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration optio",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:4987,Performance,load,loader,4987,"104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration options tuning.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:6372,Performance,load,loader,6372,"asMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration options tuning.; 145 // If no classifier name is given, help messages for all booked; 146 // classifiers are printed; 147 void PrintHelpMessage(const TString& datasetname , const TString& methodTitle = """" ) const;; 148 ; 149 TDirectory* RootBaseDir() { return (TDirectory*)fgTargetFile; }; 150 ; 151 Bool_t IsSilentFile() const { return fSilentFile;}; 152 Bool_t IsModelPersistence() const { return fModelPersistence; }; 153 ; 154 Double_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass = 0,; 155 Types::ETreeType type = Types::kTesting);; 156 Double_t GetROCIntegral(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 157 Types::ETreeType type = Types::kTesting);; 158 ; 159 // Methods to get a TGraph for an indicated method in dataset.; 160 // Optional title and axis added with fLegend=kTRUE.; 161 // Argument iClass used in multiclass settings, otherwise ignored.; 162 TGraph *GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles = kTRUE, UInt_t iClass = 0,; 163 Types::ETreeType type = Types::kTesting);; 164 TGraph *GetROCCurve(TString datasetname, TString theMethodName, Bool_t setTitles = kTRUE, UInt_t iClass = 0,; 165 Types::ETreeType type = Types::kTesting);; 166 ; 167 // Methods to get a TMultiGraph for a given class and all methods in dataset.; 168 TMultiGraph *GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Ty",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:6849,Performance,load,loader,6849,"inted; 147 void PrintHelpMessage(const TString& datasetname , const TString& methodTitle = """" ) const;; 148 ; 149 TDirectory* RootBaseDir() { return (TDirectory*)fgTargetFile; }; 150 ; 151 Bool_t IsSilentFile() const { return fSilentFile;}; 152 Bool_t IsModelPersistence() const { return fModelPersistence; }; 153 ; 154 Double_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass = 0,; 155 Types::ETreeType type = Types::kTesting);; 156 Double_t GetROCIntegral(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 157 Types::ETreeType type = Types::kTesting);; 158 ; 159 // Methods to get a TGraph for an indicated method in dataset.; 160 // Optional title and axis added with fLegend=kTRUE.; 161 // Argument iClass used in multiclass settings, otherwise ignored.; 162 TGraph *GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles = kTRUE, UInt_t iClass = 0,; 163 Types::ETreeType type = Types::kTesting);; 164 TGraph *GetROCCurve(TString datasetname, TString theMethodName, Bool_t setTitles = kTRUE, UInt_t iClass = 0,; 165 Types::ETreeType type = Types::kTesting);; 166 ; 167 // Methods to get a TMultiGraph for a given class and all methods in dataset.; 168 TMultiGraph *GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type = Types::kTesting);; 169 TMultiGraph *GetROCCurveAsMultiGraph(TString datasetname, UInt_t iClass, Types::ETreeType type = Types::kTesting);; 170 ; 171 // Draw all ROC curves of a given class for all methods in the dataset.; 172 TCanvas *GetROCCurve(DataLoader *loader, UInt_t iClass = 0, Types::ETreeType type = Types::kTesting);; 173 TCanvas *GetROCCurve(TString datasetname, UInt_t iClass = 0, Types::ETreeType type = Types::kTesting);; 174 ; 175 private:; 176 ; 177 // the beautiful greeting message; 178 void Greetings();; 179 ; 180 //evaluate the simple case that is removing 1 variable at time; 181 TH1F* EvaluateImportanceShort( DataLoader *loader,Types::EMVA theMethod, TString methodTit",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:7276,Performance,load,loader,7276,"tring theMethodName, UInt_t iClass = 0,; 155 Types::ETreeType type = Types::kTesting);; 156 Double_t GetROCIntegral(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 157 Types::ETreeType type = Types::kTesting);; 158 ; 159 // Methods to get a TGraph for an indicated method in dataset.; 160 // Optional title and axis added with fLegend=kTRUE.; 161 // Argument iClass used in multiclass settings, otherwise ignored.; 162 TGraph *GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles = kTRUE, UInt_t iClass = 0,; 163 Types::ETreeType type = Types::kTesting);; 164 TGraph *GetROCCurve(TString datasetname, TString theMethodName, Bool_t setTitles = kTRUE, UInt_t iClass = 0,; 165 Types::ETreeType type = Types::kTesting);; 166 ; 167 // Methods to get a TMultiGraph for a given class and all methods in dataset.; 168 TMultiGraph *GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type = Types::kTesting);; 169 TMultiGraph *GetROCCurveAsMultiGraph(TString datasetname, UInt_t iClass, Types::ETreeType type = Types::kTesting);; 170 ; 171 // Draw all ROC curves of a given class for all methods in the dataset.; 172 TCanvas *GetROCCurve(DataLoader *loader, UInt_t iClass = 0, Types::ETreeType type = Types::kTesting);; 173 TCanvas *GetROCCurve(TString datasetname, UInt_t iClass = 0, Types::ETreeType type = Types::kTesting);; 174 ; 175 private:; 176 ; 177 // the beautiful greeting message; 178 void Greetings();; 179 ; 180 //evaluate the simple case that is removing 1 variable at time; 181 TH1F* EvaluateImportanceShort( DataLoader *loader,Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 182 //evaluate all variables combinations; 183 TH1F* EvaluateImportanceAll( DataLoader *loader,Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 184 //evaluate randomly given a number of seeds; 185 TH1F* EvaluateImportanceRandom( DataLoader *loader,UInt_t nseeds, Types::EMVA theMethod, TString methodTitle",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:7582,Performance,load,loader,7582,,MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:7969,Performance,load,loader,7969,,MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:8139,Performance,load,loader,8139,,MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:8318,Performance,load,loader,8318,,MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:8613,Performance,load,loader,8613,"ETreeType type = Types::kTesting);; 173 TCanvas *GetROCCurve(TString datasetname, UInt_t iClass = 0, Types::ETreeType type = Types::kTesting);; 174 ; 175 private:; 176 ; 177 // the beautiful greeting message; 178 void Greetings();; 179 ; 180 //evaluate the simple case that is removing 1 variable at time; 181 TH1F* EvaluateImportanceShort( DataLoader *loader,Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 182 //evaluate all variables combinations; 183 TH1F* EvaluateImportanceAll( DataLoader *loader,Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 184 //evaluate randomly given a number of seeds; 185 TH1F* EvaluateImportanceRandom( DataLoader *loader,UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 186 ; 187 TH1F* GetImportance(const int nbits,std::vector<Double_t> importances,std::vector<TString> varNames);; 188 ; 189 // Helpers for public facing ROC methods; 190 ROCCurve *GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass = 0,; 191 Types::ETreeType type = Types::kTesting);; 192 ROCCurve *GetROC(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 193 Types::ETreeType type = Types::kTesting);; 194 ; 195 void WriteDataInformation(DataSetInfo& fDataSetInfo);; 196 ; 197 void SetInputTreesFromEventAssignTrees();; 198 ; 199 MethodBase* BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile);; 200 ; 201 private:; 202 ; 203 // data members; 204 ; 205 TFile* fgTargetFile; ///<! ROOT output file; 206 ; 207 ; 208 std::vector<TMVA::VariableTransformBase*> fDefaultTrfs; ///<! list of transformations on default DataSet; 209 ; 210 // cd to local directory; 211 TString fOptions; ///<! option string given by construction (presently only ""V""); 212 TString fTransformations; ///<! list of transformations to test; 213 Bool_t fVerbose; ///<! verbose mode; 214 TString fVerboseLevel; ///<! verbosity level, controls granularity of ",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:10172,Performance,perform,performs,10172,"1F* EvaluateImportanceRandom( DataLoader *loader,UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 186 ; 187 TH1F* GetImportance(const int nbits,std::vector<Double_t> importances,std::vector<TString> varNames);; 188 ; 189 // Helpers for public facing ROC methods; 190 ROCCurve *GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass = 0,; 191 Types::ETreeType type = Types::kTesting);; 192 ROCCurve *GetROC(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 193 Types::ETreeType type = Types::kTesting);; 194 ; 195 void WriteDataInformation(DataSetInfo& fDataSetInfo);; 196 ; 197 void SetInputTreesFromEventAssignTrees();; 198 ; 199 MethodBase* BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile);; 200 ; 201 private:; 202 ; 203 // data members; 204 ; 205 TFile* fgTargetFile; ///<! ROOT output file; 206 ; 207 ; 208 std::vector<TMVA::VariableTransformBase*> fDefaultTrfs; ///<! list of transformations on default DataSet; 209 ; 210 // cd to local directory; 211 TString fOptions; ///<! option string given by construction (presently only ""V""); 212 TString fTransformations; ///<! list of transformations to test; 213 Bool_t fVerbose; ///<! verbose mode; 214 TString fVerboseLevel; ///<! verbosity level, controls granularity of logging; 215 Bool_t fCorrelations; ///<! enable to calculate correlations; 216 Bool_t fROC; ///<! enable to calculate ROC values; 217 Bool_t fSilentFile; ///<! used in constructor without file; 218 ; 219 TString fJobName; ///<! jobname, used as extension in weight file names; 220 ; 221 Types::EAnalysisType fAnalysisType; ///<! the training type; 222 Bool_t fModelPersistence;///<! option to save the trained model in xml file or using serialization; 223 ; 224 ; 225 protected:; 226 ; 227 ClassDef(Factory,0); // The factory creates all MVA methods, and performs their training and testing; 228 };; 229 ; 230} // namespace TMVA; 231 ; 232#endif; Configurable.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:11913,Performance,perform,perform,11913,"lormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; TCanvasThe Canvas class.Definition TCanvas.h:23; TDirectoryDescribe directory structure in memory.Definition TDirectory.h:45; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::CrossValidationClass to perform cross validation, splitting the dataloader into folds.Definition CrossValidation.h:124; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::PrintHelpMessagevoid PrintHelpMessage(const TString &datasetname, const TString &methodTitle="""") constPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fSilentFileBool_t fSilentFile! used in constructor without fileDefinition Factory.h:217; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::IsModelPersistenceBool_t IsModelPersistence() constDefinition Factory.h:152; TMVA::Factory::fOptionsTString fOptions! option string given by construction (presently only ""V"")Definition Factory.h:211; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factor",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:13300,Performance,load,loader,13300,"itle="""") constPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fSilentFileBool_t fSilentFile! used in constructor without fileDefinition Factory.h:217; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::IsModelPersistenceBool_t IsModelPersistence() constDefinition Factory.h:152; TMVA::Factory::fOptionsTString fOptions! option string given by construction (presently only ""V"")Definition Factory.h:211; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::VerboseBool_t Verbose(void) constDefinition Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)Definition Factory.cxx:602; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::TrainAllMethodsForClassificationvoid TrainAllMethodsForClassification(void)Definition Factory.h:115; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > va",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:14072,Performance,load,loader,14072,"rbose(void) constDefinition Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)Definition Factory.cxx:602; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::TrainAllMethodsForClassificationvoid TrainAllMethodsForClassification(void)Definition Factory.h:115; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::RootBaseDirTDirectory * RootBaseDir()Definition Factory.h:149; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMV",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:14518,Performance,load,loader,14518,"n the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::TrainAllMethodsForClassificationvoid TrainAllMethodsForClassification(void)Definition Factory.h:115; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::RootBaseDirTDirectory * RootBaseDir()Definition Factory.h:149; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::OptimizeAllMethodsForRegressionvoid OptimizeAllMethodsForRegression(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Definition Factory.h:111; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(Data",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:14922,Performance,load,loader,14922,"ir evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::RootBaseDirTDirectory * RootBaseDir()Definition Factory.h:149; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::OptimizeAllMethodsForRegressionvoid OptimizeAllMethodsForRegression(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Definition Factory.h:111; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::fMethodsMapstd::map< TString, MVector * > fMethodsMapDefinition Factory.h:85; TMVA::Factory::SetInputTreesFromEventAssignTreesvoid SetInputTreesFromEventAssignTrees(); TMVA::Factory::~Factoryvirtua",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:15149,Performance,load,loader,15149,"ition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::RootBaseDirTDirectory * RootBaseDir()Definition Factory.h:149; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::OptimizeAllMethodsForRegressionvoid OptimizeAllMethodsForRegression(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Definition Factory.h:111; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::fMethodsMapstd::map< TString, MVector * > fMethodsMapDefinition Factory.h:85; TMVA::Factory::SetInputTreesFromEventAssignTreesvoid SetInputTreesFromEventAssignTrees(); TMVA::Factory::~Factoryvirtual ~Factory()Destructor.Definition Factory.cxx:306; TMVA::Factory::MakeClassvirtual void MakeClass(const TString &datasetname, const TString &methodTitle="""") constDefinition Factory.cxx:1305; TMVA::Factory::BookMethodWeightfileMetho",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:15528,Performance,load,loader,15528,"iables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::RootBaseDirTDirectory * RootBaseDir()Definition Factory.h:149; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::OptimizeAllMethodsForRegressionvoid OptimizeAllMethodsForRegression(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Definition Factory.h:111; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::fMethodsMapstd::map< TString, MVector * > fMethodsMapDefinition Factory.h:85; TMVA::Factory::SetInputTreesFromEventAssignTreesvoid SetInputTreesFromEventAssignTrees(); TMVA::Factory::~Factoryvirtual ~Factory()Destructor.Definition Factory.cxx:306; TMVA::Factory::MakeClassvirtual void MakeClass(const TString &datasetname, const TString &methodTitle="""") constDefinition Factory.cxx:1305; TMVA::Factory::BookMethodWeightfileMethodBase * BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile)Adds an already constructed method to be managed by this factory.Definition Factory.cxx:501; TMVA::Factory::fModelPersistenceBool_t fModelPersistence! option to save the trained model in xml file or using serializationDefinition Factory.h:222; TMVA::Factory::OptimizeAllMethodsstd::map< TString, Do",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:17013,Performance,load,loader,17013,"ame, const TString &methodTitle="""") constDefinition Factory.cxx:1305; TMVA::Factory::BookMethodWeightfileMethodBase * BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile)Adds an already constructed method to be managed by this factory.Definition Factory.cxx:501; TMVA::Factory::fModelPersistenceBool_t fModelPersistence! option to save the trained model in xml file or using serializationDefinition Factory.h:222; TMVA::Factory::OptimizeAllMethodsstd::map< TString, Double_t > OptimizeAllMethods(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Iterates through all booked methods and sees if they use parameter tuning and if so does just that,...Definition Factory.cxx:701; TMVA::Factory::OptimizeAllMethodsForClassificationvoid OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Definition Factory.h:110; TMVA::Factory::GetROCROCCurve * GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Private method to generate a ROCCurve instance for a given method.Definition Factory.cxx:749; TMVA::Factory::IsSilentFileBool_t IsSilentFile() constDefinition Factory.h:151; TMVA::Factory::EvaluateImportanceShortTH1F * EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2358; TMVA::Factory::fAnalysisTypeTypes::EAnalysisType fAnalysisType! the training typeDefinition Factory.h:221; TMVA::Factory::fJobNameTString fJobName! jobname, used as extension in weight file namesDefinition Factory.h:219; TMVA::Factory::HasMethodBool_t HasMethod(const TString &datasetname, const TString &title) constChecks whether a given method name is defined for a given dataset.Definition Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass spec",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:17354,Performance,load,loader,17354,"tory.cxx:501; TMVA::Factory::fModelPersistenceBool_t fModelPersistence! option to save the trained model in xml file or using serializationDefinition Factory.h:222; TMVA::Factory::OptimizeAllMethodsstd::map< TString, Double_t > OptimizeAllMethods(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Iterates through all booked methods and sees if they use parameter tuning and if so does just that,...Definition Factory.cxx:701; TMVA::Factory::OptimizeAllMethodsForClassificationvoid OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Definition Factory.h:110; TMVA::Factory::GetROCROCCurve * GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Private method to generate a ROCCurve instance for a given method.Definition Factory.cxx:749; TMVA::Factory::IsSilentFileBool_t IsSilentFile() constDefinition Factory.h:151; TMVA::Factory::EvaluateImportanceShortTH1F * EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2358; TMVA::Factory::fAnalysisTypeTypes::EAnalysisType fAnalysisType! the training typeDefinition Factory.h:221; TMVA::Factory::fJobNameTString fJobName! jobname, used as extension in weight file namesDefinition Factory.h:219; TMVA::Factory::HasMethodBool_t HasMethod(const TString &datasetname, const TString &title) constChecks whether a given method name is defined for a given dataset.Definition Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *, TMVA::Types::EMVA, TString, TString, TMVA::Types::EMVA, TString)Definition Factory.h:102; TMVA::Factory::TrainAllMethodsForRegressionvoid Tra",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:17930,Performance,load,loader,17930,"GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Private method to generate a ROCCurve instance for a given method.Definition Factory.cxx:749; TMVA::Factory::IsSilentFileBool_t IsSilentFile() constDefinition Factory.h:151; TMVA::Factory::EvaluateImportanceShortTH1F * EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2358; TMVA::Factory::fAnalysisTypeTypes::EAnalysisType fAnalysisType! the training typeDefinition Factory.h:221; TMVA::Factory::fJobNameTString fJobName! jobname, used as extension in weight file namesDefinition Factory.h:219; TMVA::Factory::HasMethodBool_t HasMethod(const TString &datasetname, const TString &title) constChecks whether a given method name is defined for a given dataset.Definition Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *, TMVA::Types::EMVA, TString, TString, TMVA::Types::EMVA, TString)Definition Factory.h:102; TMVA::Factory::TrainAllMethodsForRegressionvoid TrainAllMethodsForRegression(void)Definition Factory.h:116; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::fDefaultTrfsstd::vector< TMVA::VariableTransformBase * > fDefaultTrfs! list of transformations on default DataSetDefinition Factory.h:208; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetn",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:18492,Performance,load,loader,18492,"alysisType fAnalysisType! the training typeDefinition Factory.h:221; TMVA::Factory::fJobNameTString fJobName! jobname, used as extension in weight file namesDefinition Factory.h:219; TMVA::Factory::HasMethodBool_t HasMethod(const TString &datasetname, const TString &title) constChecks whether a given method name is defined for a given dataset.Definition Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *, TMVA::Types::EMVA, TString, TString, TMVA::Types::EMVA, TString)Definition Factory.h:102; TMVA::Factory::TrainAllMethodsForRegressionvoid TrainAllMethodsForRegression(void)Definition Factory.h:116; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::fDefaultTrfsstd::vector< TMVA::VariableTransformBase * > fDefaultTrfs! list of transformations on default DataSetDefinition Factory.h:208; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetname, const TString &title) constReturns pointer to MVA that corresponds to given method title.Definition Factory.cxx:566; TMVA::Factory::DeleteAllMethodsvoid DeleteAllMethods(void)Delete methods.Definition Factory.cxx:324; TMVA::Factory::fTransformationsTString fTransformations! list of transformations to testDefinition Factory.h:212; TMVA::Factory::Greetingsvoid Greetings()Print welcome message.Definition Factory.cxx:295; TMVA::IMethodInterface for all concrete MVA method implementations.Definition",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:5194,Security,access,accessors,5194,"104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString& methodTitle = """" ) const;; 142 ; 143 // prints classifier-specific help messages, dedicated to; 144 // help with the optimisation and configuration options tuning.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:11927,Security,validat,validation,11927,"lormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t typeDefinition TGWin32VirtualXProxy.cxx:249; TCanvasThe Canvas class.Definition TCanvas.h:23; TDirectoryDescribe directory structure in memory.Definition TDirectory.h:45; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TGraphA TGraph is an object made of two arrays X and Y with npoints each.Definition TGraph.h:41; TH1F1-D histogram with a float per channel (see TH1 documentation)Definition TH1.h:622; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::CrossValidationClass to perform cross validation, splitting the dataloader into folds.Definition CrossValidation.h:124; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::PrintHelpMessagevoid PrintHelpMessage(const TString &datasetname, const TString &methodTitle="""") constPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fSilentFileBool_t fSilentFile! used in constructor without fileDefinition Factory.h:217; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::IsModelPersistenceBool_t IsModelPersistence() constDefinition Factory.h:152; TMVA::Factory::fOptionsTString fOptions! option string given by construction (presently only ""V"")Definition Factory.h:211; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factor",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:757,Testability,test,testing,757,". ROOT: tmva/tmva/inc/TMVA/Factory.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Factory.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Peter Speckmayer, Joerg Stelzer, Helge Voss, Kai Voss, Eckhard von Toerne, Jan Therhaag; 3// Updated by: Omar Zapata, Lorenzo Moneta, Sergei Gleyzer; 4 ; 5/**********************************************************************************; 6 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 7 * Package: TMVA *; 8 * Class : Factory *; 9 * *; 10 * *; 11 * Description: *; 12 * This is the main MVA steering class: it creates (books) all MVA methods, *; 13 * and guides them through the training, testing and evaluation phases. *; 14 * *; 15 * Authors (alphabetical): *; 16 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 17 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 18 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 19 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 20 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 21 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 22 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 23 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 24 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 25 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 26 * *; 27 * Copyright (c) 2005-2011: *; 28 * CERN, Switzerland *; 29 * U. of Victoria, Canada *; 30 * MPI-K Heidelberg, Germany *; 31 * U. of Bonn, Germany *; 32 * UdeA/ITM, Colombia *; 33 * U. of Florida, USA *; 34 * *; 35 * Redistribution and use in source and binary forms, with or without *; 36 * modification, are permitted according to the terms listed in LICENSE *; 37 * (see tmva/doc/LICENSE) *; 38 **********************************************************************************/; 39 ; 40#ifndef ROOT_T",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:2278,Testability,test,testing,2278," Bonn, Germany *; 20 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 21 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 22 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 23 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 24 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 25 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 26 * *; 27 * Copyright (c) 2005-2011: *; 28 * CERN, Switzerland *; 29 * U. of Victoria, Canada *; 30 * MPI-K Heidelberg, Germany *; 31 * U. of Bonn, Germany *; 32 * UdeA/ITM, Colombia *; 33 * U. of Florida, USA *; 34 * *; 35 * Redistribution and use in source and binary forms, with or without *; 36 * modification, are permitted according to the terms listed in LICENSE *; 37 * (see tmva/doc/LICENSE) *; 38 **********************************************************************************/; 39 ; 40#ifndef ROOT_TMVA_Factory; 41#define ROOT_TMVA_Factory; 42 ; 43//////////////////////////////////////////////////////////////////////////; 44// //; 45// Factory //; 46// //; 47// This is the main MVA steering class: it creates all MVA methods, //; 48// and guides them through the training, testing and evaluation //; 49// phases //; 50// //; 51//////////////////////////////////////////////////////////////////////////; 52 ; 53#include <vector>; 54#include <map>; 55#include ""TCut.h""; 56 ; 57#include ""TMVA/Configurable.h""; 58#include ""TMVA/Types.h""; 59#include ""TMVA/DataSet.h""; 60 ; 61class TCanvas;; 62class TDirectory;; 63class TFile;; 64class TGraph;; 65class TH1F;; 66class TMultiGraph;; 67class TTree;; 68namespace TMVA {; 69 ; 70 class IMethod;; 71 class MethodBase;; 72 class DataInputHandler;; 73 class DataSetInfo;; 74 class DataSetManager;; 75 class DataLoader;; 76 class ROCCurve;; 77 class VariableTransformBase;; 78 ; 79 ; 80 class Factory : public Configurable {; 81 friend class CrossValidation;; 82 public:; 83 ; 84 typedef std::vector<IMethod*> MVector;; 85 std::map<T",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:4752,Testability,test,testing,4752,"eMethod, TString methodTitle, TString theOption = """" );; 102 MethodBase* BookMethod( DataLoader *, TMVA::Types::EMVA /*theMethod*/,; 103 TString /*methodTitle*/,; 104 TString /*methodOption*/,; 105 TMVA::Types::EMVA /*theComposite*/,; 106 TString /*compositeOption = """"*/ ) { return nullptr; }; 107 ; 108 // optimize all booked methods (well, if desired by the method); 109 std::map<TString,Double_t> OptimizeAllMethods (TString fomType=""ROCIntegral"", TString fitType=""FitGA"");; 110 void OptimizeAllMethodsForClassification(TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 111 void OptimizeAllMethodsForRegression (TString fomType=""ROCIntegral"", TString fitType=""FitGA"") { OptimizeAllMethods(fomType,fitType); }; 112 ; 113 // training for all booked methods; 114 void TrainAllMethods ();; 115 void TrainAllMethodsForClassification( void ) { TrainAllMethods(); }; 116 void TrainAllMethodsForRegression ( void ) { TrainAllMethods(); }; 117 ; 118 // testing; 119 void TestAllMethods();; 120 ; 121 // performance evaluation; 122 void EvaluateAllMethods( void );; 123 void EvaluateAllVariables(DataLoader *loader, TString options = """" );; 124 ; 125 TH1F* EvaluateImportance( DataLoader *loader,VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 126 ; 127 // delete all methods and reset the method vector; 128 void DeleteAllMethods( void );; 129 ; 130 // accessors; 131 IMethod* GetMethod( const TString& datasetname, const TString& title ) const;; 132 Bool_t HasMethod( const TString& datasetname, const TString& title ) const;; 133 ; 134 Bool_t Verbose( void ) const { return fVerbose; }; 135 void SetVerbose( Bool_t v=kTRUE );; 136 ; 137 // make ROOT-independent C++ class for classifier response; 138 // (classifier-specific implementation); 139 // If no classifier name is given, help messages for all booked; 140 // classifiers are printed; 141 virtual void MakeClass(const TString& datasetname , const TString&",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:9496,Testability,test,test,9496,"1F* EvaluateImportanceRandom( DataLoader *loader,UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 186 ; 187 TH1F* GetImportance(const int nbits,std::vector<Double_t> importances,std::vector<TString> varNames);; 188 ; 189 // Helpers for public facing ROC methods; 190 ROCCurve *GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass = 0,; 191 Types::ETreeType type = Types::kTesting);; 192 ROCCurve *GetROC(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 193 Types::ETreeType type = Types::kTesting);; 194 ; 195 void WriteDataInformation(DataSetInfo& fDataSetInfo);; 196 ; 197 void SetInputTreesFromEventAssignTrees();; 198 ; 199 MethodBase* BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile);; 200 ; 201 private:; 202 ; 203 // data members; 204 ; 205 TFile* fgTargetFile; ///<! ROOT output file; 206 ; 207 ; 208 std::vector<TMVA::VariableTransformBase*> fDefaultTrfs; ///<! list of transformations on default DataSet; 209 ; 210 // cd to local directory; 211 TString fOptions; ///<! option string given by construction (presently only ""V""); 212 TString fTransformations; ///<! list of transformations to test; 213 Bool_t fVerbose; ///<! verbose mode; 214 TString fVerboseLevel; ///<! verbosity level, controls granularity of logging; 215 Bool_t fCorrelations; ///<! enable to calculate correlations; 216 Bool_t fROC; ///<! enable to calculate ROC values; 217 Bool_t fSilentFile; ///<! used in constructor without file; 218 ; 219 TString fJobName; ///<! jobname, used as extension in weight file names; 220 ; 221 Types::EAnalysisType fAnalysisType; ///<! the training type; 222 Bool_t fModelPersistence;///<! option to save the trained model in xml file or using serialization; 223 ; 224 ; 225 protected:; 226 ; 227 ClassDef(Factory,0); // The factory creates all MVA methods, and performs their training and testing; 228 };; 229 ; 230} // namespace TMVA; 231 ; 232#endif; Configurable.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:9617,Testability,log,logging,9617,"1F* EvaluateImportanceRandom( DataLoader *loader,UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 186 ; 187 TH1F* GetImportance(const int nbits,std::vector<Double_t> importances,std::vector<TString> varNames);; 188 ; 189 // Helpers for public facing ROC methods; 190 ROCCurve *GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass = 0,; 191 Types::ETreeType type = Types::kTesting);; 192 ROCCurve *GetROC(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 193 Types::ETreeType type = Types::kTesting);; 194 ; 195 void WriteDataInformation(DataSetInfo& fDataSetInfo);; 196 ; 197 void SetInputTreesFromEventAssignTrees();; 198 ; 199 MethodBase* BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile);; 200 ; 201 private:; 202 ; 203 // data members; 204 ; 205 TFile* fgTargetFile; ///<! ROOT output file; 206 ; 207 ; 208 std::vector<TMVA::VariableTransformBase*> fDefaultTrfs; ///<! list of transformations on default DataSet; 209 ; 210 // cd to local directory; 211 TString fOptions; ///<! option string given by construction (presently only ""V""); 212 TString fTransformations; ///<! list of transformations to test; 213 Bool_t fVerbose; ///<! verbose mode; 214 TString fVerboseLevel; ///<! verbosity level, controls granularity of logging; 215 Bool_t fCorrelations; ///<! enable to calculate correlations; 216 Bool_t fROC; ///<! enable to calculate ROC values; 217 Bool_t fSilentFile; ///<! used in constructor without file; 218 ; 219 TString fJobName; ///<! jobname, used as extension in weight file names; 220 ; 221 Types::EAnalysisType fAnalysisType; ///<! the training type; 222 Bool_t fModelPersistence;///<! option to save the trained model in xml file or using serialization; 223 ; 224 ; 225 protected:; 226 ; 227 ClassDef(Factory,0); // The factory creates all MVA methods, and performs their training and testing; 228 };; 229 ; 230} // namespace TMVA; 231 ; 232#endif; Configurable.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:10200,Testability,test,testing,10200,"1F* EvaluateImportanceRandom( DataLoader *loader,UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption = """" );; 186 ; 187 TH1F* GetImportance(const int nbits,std::vector<Double_t> importances,std::vector<TString> varNames);; 188 ; 189 // Helpers for public facing ROC methods; 190 ROCCurve *GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass = 0,; 191 Types::ETreeType type = Types::kTesting);; 192 ROCCurve *GetROC(TString datasetname, TString theMethodName, UInt_t iClass = 0,; 193 Types::ETreeType type = Types::kTesting);; 194 ; 195 void WriteDataInformation(DataSetInfo& fDataSetInfo);; 196 ; 197 void SetInputTreesFromEventAssignTrees();; 198 ; 199 MethodBase* BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile);; 200 ; 201 private:; 202 ; 203 // data members; 204 ; 205 TFile* fgTargetFile; ///<! ROOT output file; 206 ; 207 ; 208 std::vector<TMVA::VariableTransformBase*> fDefaultTrfs; ///<! list of transformations on default DataSet; 209 ; 210 // cd to local directory; 211 TString fOptions; ///<! option string given by construction (presently only ""V""); 212 TString fTransformations; ///<! list of transformations to test; 213 Bool_t fVerbose; ///<! verbose mode; 214 TString fVerboseLevel; ///<! verbosity level, controls granularity of logging; 215 Bool_t fCorrelations; ///<! enable to calculate correlations; 216 Bool_t fROC; ///<! enable to calculate ROC values; 217 Bool_t fSilentFile; ///<! used in constructor without file; 218 ; 219 TString fJobName; ///<! jobname, used as extension in weight file names; 220 ; 221 Types::EAnalysisType fAnalysisType; ///<! the training type; 222 Bool_t fModelPersistence;///<! option to save the trained model in xml file or using serialization; 223 ; 224 ; 225 protected:; 226 ; 227 ClassDef(Factory,0); // The factory creates all MVA methods, and performs their training and testing; 228 };; 229 ; 230} // namespace TMVA; 231 ; 232#endif; Configurable.",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:13526,Testability,test,testing,13526," TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::IsModelPersistenceBool_t IsModelPersistence() constDefinition Factory.h:152; TMVA::Factory::fOptionsTString fOptions! option string given by construction (presently only ""V"")Definition Factory.h:211; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::VerboseBool_t Verbose(void) constDefinition Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)Definition Factory.cxx:602; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::TrainAllMethodsForClassificationvoid TrainAllMethodsForClassification(void)Definition Factory.h:115; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLo",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:14801,Testability,log,loggingDefinition,14801,"ol_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::RootBaseDirTDirectory * RootBaseDir()Definition Factory.h:149; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::OptimizeAllMethodsForRegressionvoid OptimizeAllMethodsForRegression(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Definition Factory.h:111; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::fMethodsMapstd::map< TS",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:19301,Testability,test,testDefinition,19301,":EMVA, TString)Definition Factory.h:102; TMVA::Factory::TrainAllMethodsForRegressionvoid TrainAllMethodsForRegression(void)Definition Factory.h:116; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::fDefaultTrfsstd::vector< TMVA::VariableTransformBase * > fDefaultTrfs! list of transformations on default DataSetDefinition Factory.h:208; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetname, const TString &title) constReturns pointer to MVA that corresponds to given method title.Definition Factory.cxx:566; TMVA::Factory::DeleteAllMethodsvoid DeleteAllMethods(void)Delete methods.Definition Factory.cxx:324; TMVA::Factory::fTransformationsTString fTransformations! list of transformations to testDefinition Factory.h:212; TMVA::Factory::Greetingsvoid Greetings()Print welcome message.Definition Factory.cxx:295; TMVA::IMethodInterface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::ROCCurveDefinition ROCCurve.h:46; TMVA::Types::EMVAEMVADefinition Types.h:76; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kTesting@ kTestingDefinition Types.h:144; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TStringBasic string class.Definition TString.h:139; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; bool; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22; v@ vDefinition rootcling_impl.cxx:3699; Types.h. tmvatmvaincTMVAFactory.h. ROOT master - Reference Guide Generated on T",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:723,Usability,guid,guides,723,". ROOT: tmva/tmva/inc/TMVA/Factory.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Factory.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Peter Speckmayer, Joerg Stelzer, Helge Voss, Kai Voss, Eckhard von Toerne, Jan Therhaag; 3// Updated by: Omar Zapata, Lorenzo Moneta, Sergei Gleyzer; 4 ; 5/**********************************************************************************; 6 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 7 * Package: TMVA *; 8 * Class : Factory *; 9 * *; 10 * *; 11 * Description: *; 12 * This is the main MVA steering class: it creates (books) all MVA methods, *; 13 * and guides them through the training, testing and evaluation phases. *; 14 * *; 15 * Authors (alphabetical): *; 16 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 17 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 18 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 19 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 20 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 21 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 22 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 23 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 24 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 25 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 26 * *; 27 * Copyright (c) 2005-2011: *; 28 * CERN, Switzerland *; 29 * U. of Victoria, Canada *; 30 * MPI-K Heidelberg, Germany *; 31 * U. of Bonn, Germany *; 32 * UdeA/ITM, Colombia *; 33 * U. of Florida, USA *; 34 * *; 35 * Redistribution and use in source and binary forms, with or without *; 36 * modification, are permitted according to the terms listed in LICENSE *; 37 * (see tmva/doc/LICENSE) *; 38 **********************************************************************************/; 39 ; 40#ifndef ROOT_T",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:2244,Usability,guid,guides,2244," Bonn, Germany *; 20 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 21 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 22 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 23 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 24 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 25 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 26 * *; 27 * Copyright (c) 2005-2011: *; 28 * CERN, Switzerland *; 29 * U. of Victoria, Canada *; 30 * MPI-K Heidelberg, Germany *; 31 * U. of Bonn, Germany *; 32 * UdeA/ITM, Colombia *; 33 * U. of Florida, USA *; 34 * *; 35 * Redistribution and use in source and binary forms, with or without *; 36 * modification, are permitted according to the terms listed in LICENSE *; 37 * (see tmva/doc/LICENSE) *; 38 **********************************************************************************/; 39 ; 40#ifndef ROOT_TMVA_Factory; 41#define ROOT_TMVA_Factory; 42 ; 43//////////////////////////////////////////////////////////////////////////; 44// //; 45// Factory //; 46// //; 47// This is the main MVA steering class: it creates all MVA methods, //; 48// and guides them through the training, testing and evaluation //; 49// phases //; 50// //; 51//////////////////////////////////////////////////////////////////////////; 52 ; 53#include <vector>; 54#include <map>; 55#include ""TCut.h""; 56 ; 57#include ""TMVA/Configurable.h""; 58#include ""TMVA/Types.h""; 59#include ""TMVA/DataSet.h""; 60 ; 61class TCanvas;; 62class TDirectory;; 63class TFile;; 64class TGraph;; 65class TH1F;; 66class TMultiGraph;; 67class TTree;; 68namespace TMVA {; 69 ; 70 class IMethod;; 71 class MethodBase;; 72 class DataInputHandler;; 73 class DataSetInfo;; 74 class DataSetManager;; 75 class DataLoader;; 76 class ROCCurve;; 77 class VariableTransformBase;; 78 ; 79 ; 80 class Factory : public Configurable {; 81 friend class CrossValidation;; 82 public:; 83 ; 84 typedef std::vector<IMethod*> MVector;; 85 std::map<T",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html:7873,Usability,simpl,simple,7873,,MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Factory_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:2272,Availability,avail,available,2272," <atomic>; 42 ; 43#include ""RtypesCore.h""; 44 ; 45#include ""TString.h""; 46 ; 47namespace TMVA {; 48 ; 49 typedef UInt_t TMVAVersion_t;; 50 ; 51 class MsgLogger;; 52 ; 53 // message types for MsgLogger; 54 // define outside of Types class to facilite access; 55 enum EMsgType {; 56 kDEBUG = 1,; 57 kVERBOSE = 2,; 58 kINFO = 3,; 59 kWARNING = 4,; 60 kERROR = 5,; 61 kFATAL = 6,; 62 kSILENT = 7,; 63 kHEADER = 8; 64 };; 65 ; 66 enum HistType { kMVAType = 0, kProbaType = 1, kRarityType = 2, kCompareType = 3 };; 67 ; 68 //Variable Importance type; 69 enum VIType {kShort=0,kAll=1,kRandom=2};; 70 ; 71 class Types {; 72 ; 73 public:; 74 ; 75 // available MVA methods; 76 enum EMVA {; 77 kVariable = 0,; 78 kCuts ,; 79 kLikelihood ,; 80 kPDERS ,; 81 kHMatrix ,; 82 kFisher ,; 83 kKNN ,; 84 kCFMlpANN ,; 85 kTMlpANN ,; 86 kBDT ,; 87 kDT ,; 88 kRuleFit ,; 89 kSVM ,; 90 kMLP ,; 91 kBayesClassifier,; 92 kFDA ,; 93 kBoost ,; 94 kPDEFoam ,; 95 kLD ,; 96 kPlugins ,; 97 kCategory ,; 98 kDNN ,; 99 kDL ,; 100 kPyRandomForest ,; 101 kPyAdaBoost ,; 102 kPyGTB ,; 103 kPyKeras ,; 104 kPyTorch ,; 105 kC50 ,; 106 kRSNNS ,; 107 kRSVM ,; 108 kRXGB ,; 109 kCrossValidation,; 110 kMaxMethod; 111 };; 112 ; 113 // available variable transformations; 114 enum EVariableTransform {; 115 kIdentity = 0,; 116 kDecorrelated,; 117 kNormalized,; 118 kPCA,; 119 kRearranged,; 120 kGauss,; 121 kUniform,; 122 kMaxVariableTransform; 123 };; 124 ; 125 // type of analysis; 126 enum EAnalysisType {; 127 kClassification = 0,; 128 kRegression,; 129 kMulticlass,; 130 kNoAnalysisType,; 131 kMaxAnalysisType; 132 };; 133 ; 134 enum ESBType {; 135 kSignal = 0, ///< Never change this number - it is elsewhere assumed to be zero !; 136 kBackground,; 137 kSBBoth,; 138 kMaxSBType,; 139 kTrueType; 140 };; 141 ; 142 enum ETreeType {; 143 kTraining = 0,; 144 kTesting,; 145 kMaxTreeType, ///< also used as temporary storage for trees not yet assigned for testing;training...; 146 kValidation, ///< these are placeholders... currently not us",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:2825,Availability,avail,available,2825," <atomic>; 42 ; 43#include ""RtypesCore.h""; 44 ; 45#include ""TString.h""; 46 ; 47namespace TMVA {; 48 ; 49 typedef UInt_t TMVAVersion_t;; 50 ; 51 class MsgLogger;; 52 ; 53 // message types for MsgLogger; 54 // define outside of Types class to facilite access; 55 enum EMsgType {; 56 kDEBUG = 1,; 57 kVERBOSE = 2,; 58 kINFO = 3,; 59 kWARNING = 4,; 60 kERROR = 5,; 61 kFATAL = 6,; 62 kSILENT = 7,; 63 kHEADER = 8; 64 };; 65 ; 66 enum HistType { kMVAType = 0, kProbaType = 1, kRarityType = 2, kCompareType = 3 };; 67 ; 68 //Variable Importance type; 69 enum VIType {kShort=0,kAll=1,kRandom=2};; 70 ; 71 class Types {; 72 ; 73 public:; 74 ; 75 // available MVA methods; 76 enum EMVA {; 77 kVariable = 0,; 78 kCuts ,; 79 kLikelihood ,; 80 kPDERS ,; 81 kHMatrix ,; 82 kFisher ,; 83 kKNN ,; 84 kCFMlpANN ,; 85 kTMlpANN ,; 86 kBDT ,; 87 kDT ,; 88 kRuleFit ,; 89 kSVM ,; 90 kMLP ,; 91 kBayesClassifier,; 92 kFDA ,; 93 kBoost ,; 94 kPDEFoam ,; 95 kLD ,; 96 kPlugins ,; 97 kCategory ,; 98 kDNN ,; 99 kDL ,; 100 kPyRandomForest ,; 101 kPyAdaBoost ,; 102 kPyGTB ,; 103 kPyKeras ,; 104 kPyTorch ,; 105 kC50 ,; 106 kRSNNS ,; 107 kRSVM ,; 108 kRXGB ,; 109 kCrossValidation,; 110 kMaxMethod; 111 };; 112 ; 113 // available variable transformations; 114 enum EVariableTransform {; 115 kIdentity = 0,; 116 kDecorrelated,; 117 kNormalized,; 118 kPCA,; 119 kRearranged,; 120 kGauss,; 121 kUniform,; 122 kMaxVariableTransform; 123 };; 124 ; 125 // type of analysis; 126 enum EAnalysisType {; 127 kClassification = 0,; 128 kRegression,; 129 kMulticlass,; 130 kNoAnalysisType,; 131 kMaxAnalysisType; 132 };; 133 ; 134 enum ESBType {; 135 kSignal = 0, ///< Never change this number - it is elsewhere assumed to be zero !; 136 kBackground,; 137 kSBBoth,; 138 kMaxSBType,; 139 kTrueType; 140 };; 141 ; 142 enum ETreeType {; 143 kTraining = 0,; 144 kTesting,; 145 kMaxTreeType, ///< also used as temporary storage for trees not yet assigned for testing;training...; 146 kValidation, ///< these are placeholders... currently not us",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:373,Deployability,integrat,integrated,373,". ROOT: tmva/tmva/inc/TMVA/Types.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Types.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Joerg Stelzer, Helge Voss; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : Types *; 8 * *; 9 * *; 10 * Description: *; 11 * GLobal types (singleton class) *; 12 * *; 13 * Authors (alphabetical): *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Peter Speckmayer <Peter.Speckmayer@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <Joerg.Stelzer@cern.ch> - CERN, Switzerland *; 17 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 18 * *; 19 * Copyright (c) 2005: *; 20 * CERN, Switzerland *; 21 * U. of Victoria, Canada *; 22 * MPI-K Heidelberg, Germany *; 23 * *; 24 * Redistribution and use in source and binary forms, with or without *; 25 * modification, are permitted according to the terms listed in LICENSE *; 26 * (http://mva.sourceforge.net/license.txt) *; 27 **********************************************************************************/; 28 ; 29#ifndef ROOT_TMVA_Types; 30#define ROOT_TMVA_Types; 31 ; 32//////////////////////////////////////////////////////////////////////////; 33// //; 34// Types //; 35// //; 36// Singleton class for Global types used by TMVA //; 37// //; 38//////////////////////////////////////////////////////////////////////////; 39 ; 40#include <map>; 41#include <atomic>; 42 ; 43#include ""RtypesCore.h""; 44 ; 45#include ""TString.h""; 46 ; 47namespace TMVA {; 48 ; 49 typedef UInt_t TMVAVersion_t;; 50 ; 51 class MsgLogger;; 52 ; 53 // message types for MsgLogger; 54 // define outside of Types class to facilite access; 55 enum EMsgType {; 56 kDEBUG = 1,; 57 kVERBOSE = 2,; 58 kINFO = 3,; 59 kWARNING = 4,; 60 kERROR = 5,; 61 kFATAL",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:373,Integrability,integrat,integrated,373,". ROOT: tmva/tmva/inc/TMVA/Types.h Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Types.h. Go to the documentation of this file. 1// @(#)root/tmva $Id$; 2// Author: Andreas Hoecker, Joerg Stelzer, Helge Voss; 3 ; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : Types *; 8 * *; 9 * *; 10 * Description: *; 11 * GLobal types (singleton class) *; 12 * *; 13 * Authors (alphabetical): *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Peter Speckmayer <Peter.Speckmayer@cern.ch> - CERN, Switzerland *; 16 * Joerg Stelzer <Joerg.Stelzer@cern.ch> - CERN, Switzerland *; 17 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 18 * *; 19 * Copyright (c) 2005: *; 20 * CERN, Switzerland *; 21 * U. of Victoria, Canada *; 22 * MPI-K Heidelberg, Germany *; 23 * *; 24 * Redistribution and use in source and binary forms, with or without *; 25 * modification, are permitted according to the terms listed in LICENSE *; 26 * (http://mva.sourceforge.net/license.txt) *; 27 **********************************************************************************/; 28 ; 29#ifndef ROOT_TMVA_Types; 30#define ROOT_TMVA_Types; 31 ; 32//////////////////////////////////////////////////////////////////////////; 33// //; 34// Types //; 35// //; 36// Singleton class for Global types used by TMVA //; 37// //; 38//////////////////////////////////////////////////////////////////////////; 39 ; 40#include <map>; 41#include <atomic>; 42 ; 43#include ""RtypesCore.h""; 44 ; 45#include ""TString.h""; 46 ; 47namespace TMVA {; 48 ; 49 typedef UInt_t TMVAVersion_t;; 50 ; 51 class MsgLogger;; 52 ; 53 // message types for MsgLogger; 54 // define outside of Types class to facilite access; 55 enum EMsgType {; 56 kDEBUG = 1,; 57 kVERBOSE = 2,; 58 kINFO = 3,; 59 kWARNING = 4,; 60 kERROR = 5,; 61 kFATAL",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:1804,Integrability,message,message,1804," <atomic>; 42 ; 43#include ""RtypesCore.h""; 44 ; 45#include ""TString.h""; 46 ; 47namespace TMVA {; 48 ; 49 typedef UInt_t TMVAVersion_t;; 50 ; 51 class MsgLogger;; 52 ; 53 // message types for MsgLogger; 54 // define outside of Types class to facilite access; 55 enum EMsgType {; 56 kDEBUG = 1,; 57 kVERBOSE = 2,; 58 kINFO = 3,; 59 kWARNING = 4,; 60 kERROR = 5,; 61 kFATAL = 6,; 62 kSILENT = 7,; 63 kHEADER = 8; 64 };; 65 ; 66 enum HistType { kMVAType = 0, kProbaType = 1, kRarityType = 2, kCompareType = 3 };; 67 ; 68 //Variable Importance type; 69 enum VIType {kShort=0,kAll=1,kRandom=2};; 70 ; 71 class Types {; 72 ; 73 public:; 74 ; 75 // available MVA methods; 76 enum EMVA {; 77 kVariable = 0,; 78 kCuts ,; 79 kLikelihood ,; 80 kPDERS ,; 81 kHMatrix ,; 82 kFisher ,; 83 kKNN ,; 84 kCFMlpANN ,; 85 kTMlpANN ,; 86 kBDT ,; 87 kDT ,; 88 kRuleFit ,; 89 kSVM ,; 90 kMLP ,; 91 kBayesClassifier,; 92 kFDA ,; 93 kBoost ,; 94 kPDEFoam ,; 95 kLD ,; 96 kPlugins ,; 97 kCategory ,; 98 kDNN ,; 99 kDL ,; 100 kPyRandomForest ,; 101 kPyAdaBoost ,; 102 kPyGTB ,; 103 kPyKeras ,; 104 kPyTorch ,; 105 kC50 ,; 106 kRSNNS ,; 107 kRSVM ,; 108 kRXGB ,; 109 kCrossValidation,; 110 kMaxMethod; 111 };; 112 ; 113 // available variable transformations; 114 enum EVariableTransform {; 115 kIdentity = 0,; 116 kDecorrelated,; 117 kNormalized,; 118 kPCA,; 119 kRearranged,; 120 kGauss,; 121 kUniform,; 122 kMaxVariableTransform; 123 };; 124 ; 125 // type of analysis; 126 enum EAnalysisType {; 127 kClassification = 0,; 128 kRegression,; 129 kMulticlass,; 130 kNoAnalysisType,; 131 kMaxAnalysisType; 132 };; 133 ; 134 enum ESBType {; 135 kSignal = 0, ///< Never change this number - it is elsewhere assumed to be zero !; 136 kBackground,; 137 kSBBoth,; 138 kMaxSBType,; 139 kTrueType; 140 };; 141 ; 142 enum ETreeType {; 143 kTraining = 0,; 144 kTesting,; 145 kMaxTreeType, ///< also used as temporary storage for trees not yet assigned for testing;training...; 146 kValidation, ///< these are placeholders... currently not us",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:4477,Integrability,message,message,4477," kClassification = 0,; 128 kRegression,; 129 kMulticlass,; 130 kNoAnalysisType,; 131 kMaxAnalysisType; 132 };; 133 ; 134 enum ESBType {; 135 kSignal = 0, ///< Never change this number - it is elsewhere assumed to be zero !; 136 kBackground,; 137 kSBBoth,; 138 kMaxSBType,; 139 kTrueType; 140 };; 141 ; 142 enum ETreeType {; 143 kTraining = 0,; 144 kTesting,; 145 kMaxTreeType, ///< also used as temporary storage for trees not yet assigned for testing;training...; 146 kValidation, ///< these are placeholders... currently not used, but could be moved ""forward"" if; 147 kTrainingOriginal ///< ever needed; 148 };; 149 ; 150 enum EBoostStage {; 151 kBoostProcBegin=0,; 152 kBeforeTraining,; 153 kBeforeBoosting,; 154 kAfterBoosting,; 155 kBoostProcEnd; 156 };; 157 ; 158 public:; 159 ; 160 static Types& Instance();; 161 static void DestroyInstance();; 162 ~Types();; 163 ; 164 Types::EMVA GetMethodType( const TString& method ) const;; 165 TString GetMethodName( Types::EMVA method ) const;; 166 ; 167 Bool_t AddTypeMapping(Types::EMVA method, const TString& methodname);; 168 ; 169 private:; 170 ; 171 Types();; 172#if !defined _MSC_VER; 173 static std::atomic<Types*> fgTypesPtr;; 174#else; 175 static Types* fgTypesPtr;; 176#endif; 177 ; 178 private:; 179 ; 180 std::map<TString, TMVA::Types::EMVA> fStr2type; ///< types-to-text map; 181 mutable MsgLogger* fLogger; ///<! message logger; 182 MsgLogger& Log() const { return *fLogger; }; 183 ; 184 };; 185}; 186 ; 187#endif; RtypesCore.h; Bool_tbool Bool_tDefinition RtypesCore.h:63; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; TString.h; TMVA::MsgLoggerostringstream derivative to redirect and format outputDefinition MsgLogger.h:57; TMVA::TypesSingleton class for Global types used by TMVA.Definition Types.h:71; TMVA::Types::DestroyInstancestatic void DestroyInstance()""destructor"" of the single instanceDefinition Types.cxx:90; TMVA::Types::EBoostStageEBoostStageDefinition Types.h:150; TMVA::Types::kBoostProcBegin@ kBoostProcBeginDef",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:9394,Integrability,message,message,9394,"@ kUniformDefinition Types.h:121; TMVA::Types::kNormalized@ kNormalizedDefinition Types.h:117; TMVA::Types::kRearranged@ kRearrangedDefinition Types.h:119; TMVA::Types::kGauss@ kGaussDefinition Types.h:120; TMVA::Types::kIdentity@ kIdentityDefinition Types.h:115; TMVA::Types::kMaxVariableTransform@ kMaxVariableTransformDefinition Types.h:122; TMVA::Types::kDecorrelated@ kDecorrelatedDefinition Types.h:116; TMVA::Types::GetMethodTypeTypes::EMVA GetMethodType(const TString &method) constreturns the method type (enum) for a given method (string)Definition Types.cxx:121; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::kMulticlass@ kMulticlassDefinition Types.h:129; TMVA::Types::kNoAnalysisType@ kNoAnalysisTypeDefinition Types.h:130; TMVA::Types::kClassification@ kClassificationDefinition Types.h:127; TMVA::Types::kMaxAnalysisType@ kMaxAnalysisTypeDefinition Types.h:131; TMVA::Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::fLoggerMsgLogger * fLogger! message loggerDefinition Types.h:181; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kMaxTreeType@ kMaxTreeTypealso used as temporary storage for trees not yet assigned for testing;training...Definition Types.h:145; TMVA::Types::kTrainingOriginal@ kTrainingOriginalever neededDefinition Types.h:147; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::Types::kValidation@ kValidationthese are placeholders... currently not used, but could be moved ""forward"" ifDefinition Types.h:146; TMVA::Types::kTesting@ kTestingDefinition Types.h:144; TMVA::Types::LogMsgLogger & Log() constDefinition Types.h:182; TMVA::Types::TypesTypes()constructorDefinition Types.cxx:56; TMVA::VITypeVITypeDefinition Types.h:69; TMVA::kRandom@ kRandomDefinition Types.h:69; TMVA::kAll@ kAllDefinition Types.h:69; TMVA::kShort@ kShortDefinition Types.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TMVA::kSILENT@ kSILENTDefinition Types.h:62; TMVA::kDEBUG@ kDEBUGDefinition Types.h:",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:2835,Modifiability,variab,variable,2835," <atomic>; 42 ; 43#include ""RtypesCore.h""; 44 ; 45#include ""TString.h""; 46 ; 47namespace TMVA {; 48 ; 49 typedef UInt_t TMVAVersion_t;; 50 ; 51 class MsgLogger;; 52 ; 53 // message types for MsgLogger; 54 // define outside of Types class to facilite access; 55 enum EMsgType {; 56 kDEBUG = 1,; 57 kVERBOSE = 2,; 58 kINFO = 3,; 59 kWARNING = 4,; 60 kERROR = 5,; 61 kFATAL = 6,; 62 kSILENT = 7,; 63 kHEADER = 8; 64 };; 65 ; 66 enum HistType { kMVAType = 0, kProbaType = 1, kRarityType = 2, kCompareType = 3 };; 67 ; 68 //Variable Importance type; 69 enum VIType {kShort=0,kAll=1,kRandom=2};; 70 ; 71 class Types {; 72 ; 73 public:; 74 ; 75 // available MVA methods; 76 enum EMVA {; 77 kVariable = 0,; 78 kCuts ,; 79 kLikelihood ,; 80 kPDERS ,; 81 kHMatrix ,; 82 kFisher ,; 83 kKNN ,; 84 kCFMlpANN ,; 85 kTMlpANN ,; 86 kBDT ,; 87 kDT ,; 88 kRuleFit ,; 89 kSVM ,; 90 kMLP ,; 91 kBayesClassifier,; 92 kFDA ,; 93 kBoost ,; 94 kPDEFoam ,; 95 kLD ,; 96 kPlugins ,; 97 kCategory ,; 98 kDNN ,; 99 kDL ,; 100 kPyRandomForest ,; 101 kPyAdaBoost ,; 102 kPyGTB ,; 103 kPyKeras ,; 104 kPyTorch ,; 105 kC50 ,; 106 kRSNNS ,; 107 kRSVM ,; 108 kRXGB ,; 109 kCrossValidation,; 110 kMaxMethod; 111 };; 112 ; 113 // available variable transformations; 114 enum EVariableTransform {; 115 kIdentity = 0,; 116 kDecorrelated,; 117 kNormalized,; 118 kPCA,; 119 kRearranged,; 120 kGauss,; 121 kUniform,; 122 kMaxVariableTransform; 123 };; 124 ; 125 // type of analysis; 126 enum EAnalysisType {; 127 kClassification = 0,; 128 kRegression,; 129 kMulticlass,; 130 kNoAnalysisType,; 131 kMaxAnalysisType; 132 };; 133 ; 134 enum ESBType {; 135 kSignal = 0, ///< Never change this number - it is elsewhere assumed to be zero !; 136 kBackground,; 137 kSBBoth,; 138 kMaxSBType,; 139 kTrueType; 140 };; 141 ; 142 enum ETreeType {; 143 kTraining = 0,; 144 kTesting,; 145 kMaxTreeType, ///< also used as temporary storage for trees not yet assigned for testing;training...; 146 kValidation, ///< these are placeholders... currently not us",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:11032,Modifiability,variab,variable,11032,"7; TMVA::Types::kMaxAnalysisType@ kMaxAnalysisTypeDefinition Types.h:131; TMVA::Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::fLoggerMsgLogger * fLogger! message loggerDefinition Types.h:181; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kMaxTreeType@ kMaxTreeTypealso used as temporary storage for trees not yet assigned for testing;training...Definition Types.h:145; TMVA::Types::kTrainingOriginal@ kTrainingOriginalever neededDefinition Types.h:147; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::Types::kValidation@ kValidationthese are placeholders... currently not used, but could be moved ""forward"" ifDefinition Types.h:146; TMVA::Types::kTesting@ kTestingDefinition Types.h:144; TMVA::Types::LogMsgLogger & Log() constDefinition Types.h:182; TMVA::Types::TypesTypes()constructorDefinition Types.cxx:56; TMVA::VITypeVITypeDefinition Types.h:69; TMVA::kRandom@ kRandomDefinition Types.h:69; TMVA::kAll@ kAllDefinition Types.h:69; TMVA::kShort@ kShortDefinition Types.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TMVA::kSILENT@ kSILENTDefinition Types.h:62; TMVA::kDEBUG@ kDEBUGDefinition Types.h:56; TMVA::kVERBOSE@ kVERBOSEDefinition Types.h:57; TMVA::kHEADER@ kHEADERDefinition Types.h:63; TMVA::kERROR@ kERRORDefinition Types.h:60; TMVA::kINFO@ kINFODefinition Types.h:58; TMVA::kWARNING@ kWARNINGDefinition Types.h:59; TMVA::kFATAL@ kFATALDefinition Types.h:61; TMVA::HistTypeHistTypeDefinition Types.h:66; TMVA::kRarityType@ kRarityTypeDefinition Types.h:66; TMVA::kCompareType@ kCompareTypeDefinition Types.h:66; TMVA::kProbaType@ kProbaTypeDefinition Types.h:66; TMVA::kMVAType@ kMVATypeDefinition Types.h:66; TMVA::TMVAVersion_tUInt_t TMVAVersion_tDefinition Types.h:49; TStringBasic string class.Definition TString.h:139; TMVAcreate variable transformationsDefinition GeneticMinimizer.h:22. tmvatmvaincTMVATypes.h. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:1881,Security,access,access,1881," <atomic>; 42 ; 43#include ""RtypesCore.h""; 44 ; 45#include ""TString.h""; 46 ; 47namespace TMVA {; 48 ; 49 typedef UInt_t TMVAVersion_t;; 50 ; 51 class MsgLogger;; 52 ; 53 // message types for MsgLogger; 54 // define outside of Types class to facilite access; 55 enum EMsgType {; 56 kDEBUG = 1,; 57 kVERBOSE = 2,; 58 kINFO = 3,; 59 kWARNING = 4,; 60 kERROR = 5,; 61 kFATAL = 6,; 62 kSILENT = 7,; 63 kHEADER = 8; 64 };; 65 ; 66 enum HistType { kMVAType = 0, kProbaType = 1, kRarityType = 2, kCompareType = 3 };; 67 ; 68 //Variable Importance type; 69 enum VIType {kShort=0,kAll=1,kRandom=2};; 70 ; 71 class Types {; 72 ; 73 public:; 74 ; 75 // available MVA methods; 76 enum EMVA {; 77 kVariable = 0,; 78 kCuts ,; 79 kLikelihood ,; 80 kPDERS ,; 81 kHMatrix ,; 82 kFisher ,; 83 kKNN ,; 84 kCFMlpANN ,; 85 kTMlpANN ,; 86 kBDT ,; 87 kDT ,; 88 kRuleFit ,; 89 kSVM ,; 90 kMLP ,; 91 kBayesClassifier,; 92 kFDA ,; 93 kBoost ,; 94 kPDEFoam ,; 95 kLD ,; 96 kPlugins ,; 97 kCategory ,; 98 kDNN ,; 99 kDL ,; 100 kPyRandomForest ,; 101 kPyAdaBoost ,; 102 kPyGTB ,; 103 kPyKeras ,; 104 kPyTorch ,; 105 kC50 ,; 106 kRSNNS ,; 107 kRSVM ,; 108 kRXGB ,; 109 kCrossValidation,; 110 kMaxMethod; 111 };; 112 ; 113 // available variable transformations; 114 enum EVariableTransform {; 115 kIdentity = 0,; 116 kDecorrelated,; 117 kNormalized,; 118 kPCA,; 119 kRearranged,; 120 kGauss,; 121 kUniform,; 122 kMaxVariableTransform; 123 };; 124 ; 125 // type of analysis; 126 enum EAnalysisType {; 127 kClassification = 0,; 128 kRegression,; 129 kMulticlass,; 130 kNoAnalysisType,; 131 kMaxAnalysisType; 132 };; 133 ; 134 enum ESBType {; 135 kSignal = 0, ///< Never change this number - it is elsewhere assumed to be zero !; 136 kBackground,; 137 kSBBoth,; 138 kMaxSBType,; 139 kTrueType; 140 };; 141 ; 142 enum ETreeType {; 143 kTraining = 0,; 144 kTesting,; 145 kMaxTreeType, ///< also used as temporary storage for trees not yet assigned for testing;training...; 146 kValidation, ///< these are placeholders... currently not us",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:3546,Testability,test,testing,3546," <atomic>; 42 ; 43#include ""RtypesCore.h""; 44 ; 45#include ""TString.h""; 46 ; 47namespace TMVA {; 48 ; 49 typedef UInt_t TMVAVersion_t;; 50 ; 51 class MsgLogger;; 52 ; 53 // message types for MsgLogger; 54 // define outside of Types class to facilite access; 55 enum EMsgType {; 56 kDEBUG = 1,; 57 kVERBOSE = 2,; 58 kINFO = 3,; 59 kWARNING = 4,; 60 kERROR = 5,; 61 kFATAL = 6,; 62 kSILENT = 7,; 63 kHEADER = 8; 64 };; 65 ; 66 enum HistType { kMVAType = 0, kProbaType = 1, kRarityType = 2, kCompareType = 3 };; 67 ; 68 //Variable Importance type; 69 enum VIType {kShort=0,kAll=1,kRandom=2};; 70 ; 71 class Types {; 72 ; 73 public:; 74 ; 75 // available MVA methods; 76 enum EMVA {; 77 kVariable = 0,; 78 kCuts ,; 79 kLikelihood ,; 80 kPDERS ,; 81 kHMatrix ,; 82 kFisher ,; 83 kKNN ,; 84 kCFMlpANN ,; 85 kTMlpANN ,; 86 kBDT ,; 87 kDT ,; 88 kRuleFit ,; 89 kSVM ,; 90 kMLP ,; 91 kBayesClassifier,; 92 kFDA ,; 93 kBoost ,; 94 kPDEFoam ,; 95 kLD ,; 96 kPlugins ,; 97 kCategory ,; 98 kDNN ,; 99 kDL ,; 100 kPyRandomForest ,; 101 kPyAdaBoost ,; 102 kPyGTB ,; 103 kPyKeras ,; 104 kPyTorch ,; 105 kC50 ,; 106 kRSNNS ,; 107 kRSVM ,; 108 kRXGB ,; 109 kCrossValidation,; 110 kMaxMethod; 111 };; 112 ; 113 // available variable transformations; 114 enum EVariableTransform {; 115 kIdentity = 0,; 116 kDecorrelated,; 117 kNormalized,; 118 kPCA,; 119 kRearranged,; 120 kGauss,; 121 kUniform,; 122 kMaxVariableTransform; 123 };; 124 ; 125 // type of analysis; 126 enum EAnalysisType {; 127 kClassification = 0,; 128 kRegression,; 129 kMulticlass,; 130 kNoAnalysisType,; 131 kMaxAnalysisType; 132 };; 133 ; 134 enum ESBType {; 135 kSignal = 0, ///< Never change this number - it is elsewhere assumed to be zero !; 136 kBackground,; 137 kSBBoth,; 138 kMaxSBType,; 139 kTrueType; 140 };; 141 ; 142 enum ETreeType {; 143 kTraining = 0,; 144 kTesting,; 145 kMaxTreeType, ///< also used as temporary storage for trees not yet assigned for testing;training...; 146 kValidation, ///< these are placeholders... currently not us",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:4485,Testability,log,logger,4485," kClassification = 0,; 128 kRegression,; 129 kMulticlass,; 130 kNoAnalysisType,; 131 kMaxAnalysisType; 132 };; 133 ; 134 enum ESBType {; 135 kSignal = 0, ///< Never change this number - it is elsewhere assumed to be zero !; 136 kBackground,; 137 kSBBoth,; 138 kMaxSBType,; 139 kTrueType; 140 };; 141 ; 142 enum ETreeType {; 143 kTraining = 0,; 144 kTesting,; 145 kMaxTreeType, ///< also used as temporary storage for trees not yet assigned for testing;training...; 146 kValidation, ///< these are placeholders... currently not used, but could be moved ""forward"" if; 147 kTrainingOriginal ///< ever needed; 148 };; 149 ; 150 enum EBoostStage {; 151 kBoostProcBegin=0,; 152 kBeforeTraining,; 153 kBeforeBoosting,; 154 kAfterBoosting,; 155 kBoostProcEnd; 156 };; 157 ; 158 public:; 159 ; 160 static Types& Instance();; 161 static void DestroyInstance();; 162 ~Types();; 163 ; 164 Types::EMVA GetMethodType( const TString& method ) const;; 165 TString GetMethodName( Types::EMVA method ) const;; 166 ; 167 Bool_t AddTypeMapping(Types::EMVA method, const TString& methodname);; 168 ; 169 private:; 170 ; 171 Types();; 172#if !defined _MSC_VER; 173 static std::atomic<Types*> fgTypesPtr;; 174#else; 175 static Types* fgTypesPtr;; 176#endif; 177 ; 178 private:; 179 ; 180 std::map<TString, TMVA::Types::EMVA> fStr2type; ///< types-to-text map; 181 mutable MsgLogger* fLogger; ///<! message logger; 182 MsgLogger& Log() const { return *fLogger; }; 183 ; 184 };; 185}; 186 ; 187#endif; RtypesCore.h; Bool_tbool Bool_tDefinition RtypesCore.h:63; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; TString.h; TMVA::MsgLoggerostringstream derivative to redirect and format outputDefinition MsgLogger.h:57; TMVA::TypesSingleton class for Global types used by TMVA.Definition Types.h:71; TMVA::Types::DestroyInstancestatic void DestroyInstance()""destructor"" of the single instanceDefinition Types.cxx:90; TMVA::Types::EBoostStageEBoostStageDefinition Types.h:150; TMVA::Types::kBoostProcBegin@ kBoostProcBeginDef",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:9402,Testability,log,loggerDefinition,9402,"@ kUniformDefinition Types.h:121; TMVA::Types::kNormalized@ kNormalizedDefinition Types.h:117; TMVA::Types::kRearranged@ kRearrangedDefinition Types.h:119; TMVA::Types::kGauss@ kGaussDefinition Types.h:120; TMVA::Types::kIdentity@ kIdentityDefinition Types.h:115; TMVA::Types::kMaxVariableTransform@ kMaxVariableTransformDefinition Types.h:122; TMVA::Types::kDecorrelated@ kDecorrelatedDefinition Types.h:116; TMVA::Types::GetMethodTypeTypes::EMVA GetMethodType(const TString &method) constreturns the method type (enum) for a given method (string)Definition Types.cxx:121; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::kMulticlass@ kMulticlassDefinition Types.h:129; TMVA::Types::kNoAnalysisType@ kNoAnalysisTypeDefinition Types.h:130; TMVA::Types::kClassification@ kClassificationDefinition Types.h:127; TMVA::Types::kMaxAnalysisType@ kMaxAnalysisTypeDefinition Types.h:131; TMVA::Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::fLoggerMsgLogger * fLogger! message loggerDefinition Types.h:181; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kMaxTreeType@ kMaxTreeTypealso used as temporary storage for trees not yet assigned for testing;training...Definition Types.h:145; TMVA::Types::kTrainingOriginal@ kTrainingOriginalever neededDefinition Types.h:147; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::Types::kValidation@ kValidationthese are placeholders... currently not used, but could be moved ""forward"" ifDefinition Types.h:146; TMVA::Types::kTesting@ kTestingDefinition Types.h:144; TMVA::Types::LogMsgLogger & Log() constDefinition Types.h:182; TMVA::Types::TypesTypes()constructorDefinition Types.cxx:56; TMVA::VITypeVITypeDefinition Types.h:69; TMVA::kRandom@ kRandomDefinition Types.h:69; TMVA::kAll@ kAllDefinition Types.h:69; TMVA::kShort@ kShortDefinition Types.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TMVA::kSILENT@ kSILENTDefinition Types.h:62; TMVA::kDEBUG@ kDEBUGDefinition Types.h:",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html:9588,Testability,test,testing,9588,"MVA::Types::kGauss@ kGaussDefinition Types.h:120; TMVA::Types::kIdentity@ kIdentityDefinition Types.h:115; TMVA::Types::kMaxVariableTransform@ kMaxVariableTransformDefinition Types.h:122; TMVA::Types::kDecorrelated@ kDecorrelatedDefinition Types.h:116; TMVA::Types::GetMethodTypeTypes::EMVA GetMethodType(const TString &method) constreturns the method type (enum) for a given method (string)Definition Types.cxx:121; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::kMulticlass@ kMulticlassDefinition Types.h:129; TMVA::Types::kNoAnalysisType@ kNoAnalysisTypeDefinition Types.h:130; TMVA::Types::kClassification@ kClassificationDefinition Types.h:127; TMVA::Types::kMaxAnalysisType@ kMaxAnalysisTypeDefinition Types.h:131; TMVA::Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::fLoggerMsgLogger * fLogger! message loggerDefinition Types.h:181; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kMaxTreeType@ kMaxTreeTypealso used as temporary storage for trees not yet assigned for testing;training...Definition Types.h:145; TMVA::Types::kTrainingOriginal@ kTrainingOriginalever neededDefinition Types.h:147; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::Types::kValidation@ kValidationthese are placeholders... currently not used, but could be moved ""forward"" ifDefinition Types.h:146; TMVA::Types::kTesting@ kTestingDefinition Types.h:144; TMVA::Types::LogMsgLogger & Log() constDefinition Types.h:182; TMVA::Types::TypesTypes()constructorDefinition Types.cxx:56; TMVA::VITypeVITypeDefinition Types.h:69; TMVA::kRandom@ kRandomDefinition Types.h:69; TMVA::kAll@ kAllDefinition Types.h:69; TMVA::kShort@ kShortDefinition Types.h:69; TMVA::EMsgTypeEMsgTypeDefinition Types.h:55; TMVA::kSILENT@ kSILENTDefinition Types.h:62; TMVA::kDEBUG@ kDEBUGDefinition Types.h:56; TMVA::kVERBOSE@ kVERBOSEDefinition Types.h:57; TMVA::kHEADER@ kHEADERDefinition Types.h:63; TMVA::kERROR@ kERRORDefinition Types.h:60; TMVA::kINFO@ kINFO",MatchSource.WIKI,doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2inc_2TMVA_2Types_8h_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx.html:1351,Integrability,depend,dependency,1351,"de ""TMVA/Ranking.h""; #include ""TMVA/DataSet.h""; #include ""TMVA/IMethod.h""; #include ""TMVA/MethodBase.h""; #include ""TMVA/DataInputHandler.h""; #include ""TMVA/DataSetManager.h""; #include ""TMVA/DataSetInfo.h""; #include ""TMVA/DataLoader.h""; #include ""TMVA/MethodBoost.h""; #include ""TMVA/MethodCategory.h""; #include ""TMVA/ROCCalc.h""; #include ""TMVA/ROCCurve.h""; #include ""TMVA/MsgLogger.h""; #include ""TMVA/VariableInfo.h""; #include ""TMVA/VariableTransform.h""; #include ""TMVA/Results.h""; #include ""TMVA/ResultsClassification.h""; #include ""TMVA/ResultsRegression.h""; #include ""TMVA/ResultsMulticlass.h""; #include <list>; #include <bitset>; #include <set>; #include ""TMVA/Types.h""; #include ""TROOT.h""; #include ""TFile.h""; #include ""TLeaf.h""; #include ""TEventList.h""; #include ""TH2.h""; #include ""TGraph.h""; #include ""TStyle.h""; #include ""TMatrixF.h""; #include ""TMatrixDSym.h""; #include ""TMultiGraph.h""; #include ""TPrincipal.h""; #include ""TMath.h""; #include ""TSystem.h""; #include ""TCanvas.h"". Include dependency graph for Factory.cxx:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead. Namespaces; namespace  ROOT;  tbb::task_arena is an alias of tbb::interface7::task_arena, which doesn't allow to forward declare tbb::task_arena without forward declaring tbb::interface7 ;  . Macros; #define READXML   kTRUE;  ; #define VIBITS   32;  . Functions; static uint64_t sum (uint64_t i);  . Variables; const Int_t MinNoTrainingEvents = 10;  . Macro Definition Documentation. ◆ READXML. #define READXML   kTRUE. Definition at line 100 of file Factory.cxx. ◆ VIBITS. #define VIBITS   32. Definition at line 103 of file Factory.cxx. Function Documentation. ◆ sum(). static uint64_t sum ; (; uint64_t ; i). static . Definition at line 2345 of file Factory.cxx. Variable Documentation. ◆ MinNoTrainingEvents. const Int_t MinNoTrainingEvents = 10. Definition at line 95 of file Factory.cxx. tmvatmvasrcFactory.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:21 (GVA ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:36515,Availability,error,error,36515," 876 if (allowedAnalysisTypes.count(this->fAnalysisType) == 0) {; 877 Log() << kERROR << Form(""Can only generate ROC integral for analysis type kClassification. and kMulticlass.""); 878 << Endl;; 879 return 0;; 880 }; 881 ; 882 TMVA::ROCCurve *rocCurve = GetROC(datasetname, theMethodName, iClass, type);; 883 if (!rocCurve) {; 884 Log() << kFATAL; 885 << Form(""ROCCurve object was not created in Method = %s not found with Dataset = %s "", theMethodName.Data(),; 886 datasetname.Data()); 887 << Endl;; 888 return 0;; 889 }; 890 ; 891 Int_t npoints = TMVA::gConfig().fVariablePlotting.fNbinsXOfROCCurve + 1;; 892 Double_t rocIntegral = rocCurve->GetROCIntegral(npoints);; 893 delete rocCurve;; 894 ; 895 return rocIntegral;; 896}; 897 ; 898////////////////////////////////////////////////////////////////////////////////; 899/// Argument iClass specifies the class to generate the ROC curve in a; 900/// multiclass setting. It is ignored for binary classification.; 901///; 902/// Returns a ROC graph for a given method, or nullptr on error.; 903///; 904/// Note: Evaluation of the given method must have been run prior to ROC; 905/// generation through Factory::EvaluateAllMetods.; 906///; 907/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 908/// and the others considered background. This is ok in binary classification; 909/// but in in multi class classification, the ROC surface is an N dimensional; 910/// shape, where N is number of classes - 1.; 911 ; 912TGraph *TMVA::Factory::GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles, UInt_t iClass,; 913 Types::ETreeType type); 914{; 915 return GetROCCurve((TString)loader->GetName(), theMethodName, setTitles, iClass, type);; 916}; 917 ; 918////////////////////////////////////////////////////////////////////////////////; 919/// Argument iClass specifies the class to generate the ROC curve in a; 920/// multiclass setting. It is ignored for binary classification.; 921///; 922/// Returns a R",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:37526,Availability,error,error,37526,"method, or nullptr on error.; 903///; 904/// Note: Evaluation of the given method must have been run prior to ROC; 905/// generation through Factory::EvaluateAllMetods.; 906///; 907/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 908/// and the others considered background. This is ok in binary classification; 909/// but in in multi class classification, the ROC surface is an N dimensional; 910/// shape, where N is number of classes - 1.; 911 ; 912TGraph *TMVA::Factory::GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles, UInt_t iClass,; 913 Types::ETreeType type); 914{; 915 return GetROCCurve((TString)loader->GetName(), theMethodName, setTitles, iClass, type);; 916}; 917 ; 918////////////////////////////////////////////////////////////////////////////////; 919/// Argument iClass specifies the class to generate the ROC curve in a; 920/// multiclass setting. It is ignored for binary classification.; 921///; 922/// Returns a ROC graph for a given method, or nullptr on error.; 923///; 924/// Note: Evaluation of the given method must have been run prior to ROC; 925/// generation through Factory::EvaluateAllMetods.; 926///; 927/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 928/// and the others considered background. This is ok in binary classification; 929/// but in in multi class classification, the ROC surface is an N dimensional; 930/// shape, where N is number of classes - 1.; 931 ; 932TGraph *TMVA::Factory::GetROCCurve(TString datasetname, TString theMethodName, Bool_t setTitles, UInt_t iClass,; 933 Types::ETreeType type); 934{; 935 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 936 Log() << kERROR << Form(""DataSet = %s not found in methods map."", datasetname.Data()) << Endl;; 937 return nullptr;; 938 }; 939 ; 940 if (!this->HasMethod(datasetname, theMethodName)) {; 941 Log() << kERROR << Form(""Method = %s not found with Dataset = %s "", theMethodName.Data(), datasetname",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:88567,Availability,error,error,88567,"g() << kINFO << ""---------------------------------------------------"" << Endl;; 2076 printMatrix(multiclass_testConfusionEffB30[iMethod], multiclass_trainConfusionEffB30[iMethod], classnames,; 2077 numClasses, Log());; 2078 Log() << kINFO << Endl;; 2079 }; 2080 Log() << kINFO << hLine << Endl;; 2081 Log() << kINFO << Endl;; 2082 ; 2083 } else {; 2084 // Binary classification; 2085 if (fROC) {; 2086 Log().EnableOutput();; 2087 gConfig().SetSilent(kFALSE);; 2088 Log() << Endl;; 2089 TString hLine = ""------------------------------------------------------------------------------------------""; 2090 ""-------------------------"";; 2091 Log() << kINFO << ""Evaluation results ranked by best signal efficiency and purity (area)"" << Endl;; 2092 Log() << kINFO << hLine << Endl;; 2093 Log() << kINFO << ""DataSet MVA "" << Endl;; 2094 Log() << kINFO << ""Name: Method: ROC-integ"" << Endl;; 2095 ; 2096 // Log() << kDEBUG << ""DataSet MVA Signal efficiency at bkg eff.(error):; 2097 // | Sepa- Signifi- "" << Endl; Log() << kDEBUG << ""Name: Method: @B=0.01; 2098 // @B=0.10 @B=0.30 ROC-integ ROCCurve| ration: cance: "" << Endl;; 2099 Log() << kDEBUG << hLine << Endl;; 2100 for (Int_t k = 0; k < 2; k++) {; 2101 if (k == 1 && nmeth_used[k] > 0) {; 2102 Log() << kINFO << hLine << Endl;; 2103 Log() << kINFO << ""Input Variables: "" << Endl << hLine << Endl;; 2104 }; 2105 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 2106 TString datasetName = itrMap->first;; 2107 TString methodName = mname[k][i];; 2108 ; 2109 if (k == 1) {; 2110 methodName.ReplaceAll(""Variable_"", """");; 2111 }; 2112 ; 2113 MethodBase *theMethod = dynamic_cast<MethodBase *>(GetMethod(datasetName, methodName));; 2114 if (theMethod == 0) {; 2115 continue;; 2116 }; 2117 ; 2118 TMVA::DataSet *dataset = theMethod->Data();; 2119 TMVA::Results *results = dataset->GetResults(methodName, Types::kTesting, this->fAnalysisType);; 2120 std::vector<Bool_t> *mvaResType =; 2121 dynamic_cast<ResultsClassification *>(results)->GetValueVectorTypes();; 2122",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:94147,Availability,error,error,94147,"ences for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo(",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:479,Deployability,integrat,integrated,479,". ROOT: tmva/tmva/src/Factory.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Factory.cxx. Go to the documentation of this file. 1// @(#)Root/tmva $Id$; 2// Author: Andreas Hoecker, Peter Speckmayer, Joerg Stelzer, Helge Voss, Kai Voss, Eckhard von Toerne, Jan Therhaag; 3// Updated by: Omar Zapata, Kim Albertsson; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : Factory *; 8 * *; 9 * *; 10 * Description: *; 11 * Implementation (see header for description) *; 12 * *; 13 * Authors : *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 16 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 17 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 18 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 22 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 23 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 24 * Kim Albertsson <kim.albertsson@cern.ch> - LTU & CERN *; 25 * *; 26 * Copyright (c) 2005-2015: *; 27 * CERN, Switzerland *; 28 * U. of Victoria, Canada *; 29 * MPI-K Heidelberg, Germany *; 30 * U. of Bonn, Germany *; 31 * UdeA/ITM, Colombia *; 32 * U. of Florida, USA *; 33 * *; 34 * Redistribution and use in source and binary forms, with or without *; 35 * modification, are permitted according to the terms listed in LICENSE *; 36 * (see tmva/doc/LICENSE) *; 37 **********************************************************************************/; 38 ; 39/*! \class TMVA::Factory; 40\ingroup TMVA; 41 ; 42This is the main MVA steering class.; 43It cr",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:19758,Deployability,configurat,configuration,19758,"e); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = Classifie",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:19988,Deployability,configurat,configurations,19988,"e); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = Classifie",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:133368,Deployability,release,release,133368,"8; TMVA::ROCCurve::GetROCCurveTGraph * GetROCCurve(const UInt_t points=100)Returns a new TGraph containing the ROC curve.Definition ROCCurve.cxx:274; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Ranking::Printvirtual void Print() constget maximum length of variable namesDefinition Ranking.cxx:111; TMVA::ResultsClassificationClass that is the base-class for a vector of result.Definition ResultsClassification.h:48; TMVA::ResultsMulticlassClass which takes the results of a multiclass classification.Definition ResultsMulticlass.h:55; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::Tools::FormattedOutputvoid FormattedOutput(const std::vector< Double_t > &, const std::vector< TString > &, const TString titleVars, const TString titleValues, MsgLogger &logger, TString format=""%+1.3f"")formatted output of simple tableDefinition Tools.cxx:887; TMVA::Tools::ROOTVersionMessagevoid ROOTVersionMessage(MsgLogger &logger)prints the ROOT release number and dateDefinition Tools.cxx:1325; TMVA::Tools::UsefulSortDescendingvoid UsefulSortDescending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:564; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::GetCorrelationMatrixconst TMatrixD * GetCorrelationMatrix(const TMatrixD *covMat)turns covariance into correlation matrixDefinition Tools.cxx:324; TMVA::Tools::kHtmlLink@ kHtmlLinkDefinition Tools.h:212; TMVA::Tools::UsefulSortAscendingvoid UsefulSortAscending(std::vector< std::vector< Double_t > > &,",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:134734,Deployability,release,release,134734,"ringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::GetCorrelationMatrixconst TMatrixD * GetCorrelationMatrix(const TMatrixD *covMat)turns covariance into correlation matrixDefinition Tools.cxx:324; TMVA::Tools::kHtmlLink@ kHtmlLinkDefinition Tools.h:212; TMVA::Tools::UsefulSortAscendingvoid UsefulSortAscending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:538; TMVA::Tools::TMVACitationvoid TMVACitation(MsgLogger &logger, ECitation citType=kPlainText)kinds of TMVA citationDefinition Tools.cxx:1440; TMVA::Tools::TMVAVersionMessagevoid TMVAVersionMessage(MsgLogger &logger)prints the TMVA release number and dateDefinition Tools.cxx:1316; TMVA::Tools::TMVAWelcomeMessagevoid TMVAWelcomeMessage()direct output, eg, when starting ROOT session -> no use of Logger hereDefinition Tools.cxx:1302; TMVA::TransformationHandlerClass that contains all the data information.Definition TransformationHandler.h:56; TMVA::TransformationHandler::PrintVariableRankingvoid PrintVariableRanking() constprints ranking of input variablesDefinition TransformationHandler.cxx:926; TMVA::TypesSingleton class for Global types used by TMVA.Definition Types.h:71; TMVA::Types::Instancestatic Types & Instance()The single instance of ""Types"" if existing already, or create it (Singleton)Definition Types.cxx:70; TMVA::Types::EMVAEMVADefinition Types.h:76; TMVA::Types::kCategory@ kCategoryDefinition Types.h:97; TMVA::Types::kCuts@ kCutsDefinition Types.h:78; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::kMulticlass@ kMulticlassDefinition Types.h:129; T",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:138292,Deployability,update,update,138292," the TNamed.Definition TNamed.cxx:164; TNamed::GetNameconst char * GetName() const overrideReturns name of object.Definition TNamed.h:47; TNamed::fNameTString fNameDefinition TNamed.h:32; TObject::kOverwrite@ kOverwriteoverwrite existing object with same nameDefinition TObject.h:92; TObject::GetNamevirtual const char * GetName() constReturns name of object.Definition TObject.cxx:444; TObject::Writevirtual Int_t Write(const char *name=nullptr, Int_t option=0, Int_t bufsize=0)Write this object to the current directory.Definition TObject.cxx:886; TPad::SetGridvoid SetGrid(Int_t valuex=1, Int_t valuey=1) overrideDefinition TPad.h:335; TPad::BuildLegendTLegend * BuildLegend(Double_t x1=0.3, Double_t y1=0.21, Double_t x2=0.3, Double_t y2=0.21, const char *title="""", Option_t *option="""") overrideBuild a legend from the graphical objects in the pad.Definition TPad.cxx:555; TPrincipalPrincipal Components Analysis (PCA)Definition TPrincipal.h:21; TPrincipal::AddRowvirtual void AddRow(const Double_t *x)Add a data point and update the covariance matrix.Definition TPrincipal.cxx:414; TPrincipal::GetCovarianceMatrixconst TMatrixD * GetCovarianceMatrix() constReturn the covariance matrix.Definition TPrincipal.h:60; TPrincipal::MakePrincipalsvirtual void MakePrincipals()Perform the principal components analysis.Definition TPrincipal.cxx:884; TRandom3Random number generator class based on M.Definition TRandom3.h:27; TRandom::Integervirtual UInt_t Integer(UInt_t imax)Returns a random integer uniformly distributed on the interval [ 0, imax-1 ].Definition TRandom.cxx:361; TStringBasic string class.Definition TString.h:139; TString::LengthSsiz_t Length() constDefinition TString.h:417; TString::ToLowervoid ToLower()Change string to lower-case.Definition TString.cxx:1182; TString::CompareToint CompareTo(const char *cs, ECaseCompare cmp=kExact) constCompare a string to char *cs2.Definition TString.cxx:457; TString::Dataconst char * Data() constDefinition TString.h:376; TString::ReplaceAllTS",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:6382,Energy Efficiency,schedul,schedule,6382,"fferent sequences..); 137 Bool_t color = kFALSE;; 138 Bool_t drawProgressBar = kFALSE;; 139#else; 140 Bool_t color = !gROOT->IsBatch();; 141 Bool_t drawProgressBar = kTRUE;; 142#endif; 143 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 144 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 145 AddPreDefVal(TString(""Debug""));; 146 AddPreDefVal(TString(""Verbose""));; 147 AddPreDefVal(TString(""Info""));; 148 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 149 DeclareOptionRef(; 150 fTransformations, ""Transformations"",; 151 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", for identity, ""; 152 ""decorrelation, PCA, Uniform and Gaussianisation followed by decorrelation transformations"");; 153 DeclareOptionRef(fCorrelations, ""Correlations"", ""boolean to show correlation in output"");; 154 DeclareOptionRef(fROC, ""ROC"", ""boolean to show ROC in output"");; 155 DeclareOptionRef(silent, ""Silent"",; 156 ""Batch mode: boolean silent flag inhibiting any output from TMVA after the creation of the factory ""; 157 ""class object (default: False)"");; 158 DeclareOptionRef(drawProgressBar, ""DrawProgressBar"",; 159 ""Draw progress bar to display training, testing and evaluation schedule (default: True)"");; 160 DeclareOptionRef(fModelPersistence, ""ModelPersistence"",; 161 ""Option to save the trained model in xml file or using serialization"");; 162 ; 163 TString analysisType(""Auto"");; 164 DeclareOptionRef(analysisType, ""AnalysisType"",; 165 ""Set the analysis type (Classification, Regression, Multiclass, Auto) (default: Auto)"");; 166 AddPreDefVal(TString(""Classification""));; 167 AddPreDefVal(TString(""Regression""));; 168 AddPreDefVal(TString(""Multiclass""));; 169 AddPreDefVal(TString(""Auto""));; 170 ; 171 ParseOptions();; 172 CheckForUnusedOptions();; 173 ; 174 if (Verbose()); 175 fLogger->SetMinType(kVERBOSE);; 176 if (fVerboseLevel.CompareTo(""Debug"") ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:10292,Energy Efficiency,schedul,schedule,10292,"fferent sequences..); 227 Bool_t color = kFALSE;; 228 Bool_t drawProgressBar = kFALSE;; 229#else; 230 Bool_t color = !gROOT->IsBatch();; 231 Bool_t drawProgressBar = kTRUE;; 232#endif; 233 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 234 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 235 AddPreDefVal(TString(""Debug""));; 236 AddPreDefVal(TString(""Verbose""));; 237 AddPreDefVal(TString(""Info""));; 238 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 239 DeclareOptionRef(; 240 fTransformations, ""Transformations"",; 241 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", for identity, ""; 242 ""decorrelation, PCA, Uniform and Gaussianisation followed by decorrelation transformations"");; 243 DeclareOptionRef(fCorrelations, ""Correlations"", ""boolean to show correlation in output"");; 244 DeclareOptionRef(fROC, ""ROC"", ""boolean to show ROC in output"");; 245 DeclareOptionRef(silent, ""Silent"",; 246 ""Batch mode: boolean silent flag inhibiting any output from TMVA after the creation of the factory ""; 247 ""class object (default: False)"");; 248 DeclareOptionRef(drawProgressBar, ""DrawProgressBar"",; 249 ""Draw progress bar to display training, testing and evaluation schedule (default: True)"");; 250 DeclareOptionRef(fModelPersistence, ""ModelPersistence"",; 251 ""Option to save the trained model in xml file or using serialization"");; 252 ; 253 TString analysisType(""Auto"");; 254 DeclareOptionRef(analysisType, ""AnalysisType"",; 255 ""Set the analysis type (Classification, Regression, Multiclass, Auto) (default: Auto)"");; 256 AddPreDefVal(TString(""Classification""));; 257 AddPreDefVal(TString(""Regression""));; 258 AddPreDefVal(TString(""Multiclass""));; 259 AddPreDefVal(TString(""Auto""));; 260 ; 261 ParseOptions();; 262 CheckForUnusedOptions();; 263 ; 264 if (Verbose()); 265 fLogger->SetMinType(kVERBOSE);; 266 if (fVerboseLevel.CompareTo(""Debug"") ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:479,Integrability,integrat,integrated,479,". ROOT: tmva/tmva/src/Factory.cxx Source File. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Factory.cxx. Go to the documentation of this file. 1// @(#)Root/tmva $Id$; 2// Author: Andreas Hoecker, Peter Speckmayer, Joerg Stelzer, Helge Voss, Kai Voss, Eckhard von Toerne, Jan Therhaag; 3// Updated by: Omar Zapata, Kim Albertsson; 4/**********************************************************************************; 5 * Project: TMVA - a Root-integrated toolkit for multivariate data analysis *; 6 * Package: TMVA *; 7 * Class : Factory *; 8 * *; 9 * *; 10 * Description: *; 11 * Implementation (see header for description) *; 12 * *; 13 * Authors : *; 14 * Andreas Hoecker <Andreas.Hocker@cern.ch> - CERN, Switzerland *; 15 * Joerg Stelzer <stelzer@cern.ch> - DESY, Germany *; 16 * Peter Speckmayer <peter.speckmayer@cern.ch> - CERN, Switzerland *; 17 * Jan Therhaag <Jan.Therhaag@cern.ch> - U of Bonn, Germany *; 18 * Eckhard v. Toerne <evt@uni-bonn.de> - U of Bonn, Germany *; 19 * Helge Voss <Helge.Voss@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 22 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 23 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 24 * Kim Albertsson <kim.albertsson@cern.ch> - LTU & CERN *; 25 * *; 26 * Copyright (c) 2005-2015: *; 27 * CERN, Switzerland *; 28 * U. of Victoria, Canada *; 29 * MPI-K Heidelberg, Germany *; 30 * U. of Bonn, Germany *; 31 * UdeA/ITM, Colombia *; 32 * U. of Florida, USA *; 33 * *; 34 * Redistribution and use in source and binary forms, with or without *; 35 * modification, are permitted according to the terms listed in LICENSE *; 36 * (see tmva/doc/LICENSE) *; 37 **********************************************************************************/; 38 ; 39/*! \class TMVA::Factory; 40\ingroup TMVA; 41 ; 42This is the main MVA steering class.; 43It cr",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:11848,Integrability,message,message,11848,"on, Multiclass, Auto) (default: Auto)"");; 256 AddPreDefVal(TString(""Classification""));; 257 AddPreDefVal(TString(""Regression""));; 258 AddPreDefVal(TString(""Multiclass""));; 259 AddPreDefVal(TString(""Auto""));; 260 ; 261 ParseOptions();; 262 CheckForUnusedOptions();; 263 ; 264 if (Verbose()); 265 fLogger->SetMinType(kVERBOSE);; 266 if (fVerboseLevel.CompareTo(""Debug"") == 0); 267 fLogger->SetMinType(kDEBUG);; 268 if (fVerboseLevel.CompareTo(""Verbose"") == 0); 269 fLogger->SetMinType(kVERBOSE);; 270 if (fVerboseLevel.CompareTo(""Info"") == 0); 271 fLogger->SetMinType(kINFO);; 272 ; 273 // global settings; 274 gConfig().SetUseColor(color);; 275 gConfig().SetSilent(silent);; 276 gConfig().SetDrawProgressBar(drawProgressBar);; 277 ; 278 analysisType.ToLower();; 279 if (analysisType == ""classification""); 280 fAnalysisType = Types::kClassification;; 281 else if (analysisType == ""regression""); 282 fAnalysisType = Types::kRegression;; 283 else if (analysisType == ""multiclass""); 284 fAnalysisType = Types::kMulticlass;; 285 else if (analysisType == ""auto""); 286 fAnalysisType = Types::kNoAnalysisType;; 287 ; 288 Greetings();; 289}; 290 ; 291////////////////////////////////////////////////////////////////////////////////; 292/// Print welcome message.; 293/// Options are: kLogoWelcomeMsg, kIsometricWelcomeMsg, kLeanWelcomeMsg; 294 ; 295void TMVA::Factory::Greetings(); 296{; 297 gTools().ROOTVersionMessage(Log());; 298 gTools().TMVAWelcomeMessage(Log(), gTools().kLogoWelcomeMsg);; 299 gTools().TMVAVersionMessage(Log());; 300 Log() << Endl;; 301}; 302 ; 303////////////////////////////////////////////////////////////////////////////////; 304/// Destructor.; 305 ; 306TMVA::Factory::~Factory(void); 307{; 308 std::vector<TMVA::VariableTransformBase *>::iterator trfIt = fDefaultTrfs.begin();; 309 for (; trfIt != fDefaultTrfs.end(); ++trfIt); 310 delete (*trfIt);; 311 ; 312 this->DeleteAllMethods();; 313 ; 314 // problem with call of REGISTER_METHOD macro ...; 315 // ClassifierFactory::Destroy",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:53712,Integrability,message,messages,53712,"lor(""reset"") << Endl;; 1274 ; 1275 // don't do anything if no method booked; 1276 if (fMethodsMap.empty()) {; 1277 Log() << kINFO << ""...nothing found to test"" << Endl;; 1278 return;; 1279 }; 1280 std::map<TString, MVector *>::iterator itrMap;; 1281 ; 1282 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1283 MVector *methods = itrMap->second;; 1284 MVector::iterator itrMethod;; 1285 ; 1286 // iterate over methods and test; 1287 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1288 Event::SetIsTraining(kFALSE);; 1289 MethodBase *mva = dynamic_cast<MethodBase *>(*itrMethod);; 1290 if (mva == 0); 1291 continue;; 1292 Types::EAnalysisType analysisType = mva->GetAnalysisType();; 1293 Log() << kHEADER << ""Test method: "" << mva->GetMethodName() << "" for ""; 1294 << (analysisType == Types::kRegression; 1295 ? ""Regression""; 1296 : (analysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 1297 << "" performance"" << Endl << Endl;; 1298 mva->AddOutput(Types::kTesting, analysisType);; 1299 }; 1300 }; 1301}; 1302 ; 1303////////////////////////////////////////////////////////////////////////////////; 1304 ; 1305void TMVA::Factory::MakeClass(const TString &datasetname, const TString &methodTitle) const; 1306{; 1307 if (methodTitle != """") {; 1308 IMethod *method = GetMethod(datasetname, methodTitle);; 1309 if (method); 1310 method->MakeClass();; 1311 else {; 1312 Log() << kWARNING << ""<MakeClass> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1313 }; 1314 } else {; 1315 ; 1316 // no classifier specified, print all help messages; 1317 MVector *methods = fMethodsMap.find(datasetname)->second;; 1318 MVector::const_iterator itrMethod;; 1319 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1320 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1321 if (method == 0); 1322 continue;; 1323 Log() << kINFO << ""Make response class for classi",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:54290,Integrability,message,message,54290,"lassification"")); 1297 << "" performance"" << Endl << Endl;; 1298 mva->AddOutput(Types::kTesting, analysisType);; 1299 }; 1300 }; 1301}; 1302 ; 1303////////////////////////////////////////////////////////////////////////////////; 1304 ; 1305void TMVA::Factory::MakeClass(const TString &datasetname, const TString &methodTitle) const; 1306{; 1307 if (methodTitle != """") {; 1308 IMethod *method = GetMethod(datasetname, methodTitle);; 1309 if (method); 1310 method->MakeClass();; 1311 else {; 1312 Log() << kWARNING << ""<MakeClass> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1313 }; 1314 } else {; 1315 ; 1316 // no classifier specified, print all help messages; 1317 MVector *methods = fMethodsMap.find(datasetname)->second;; 1318 MVector::const_iterator itrMethod;; 1319 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1320 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1321 if (method == 0); 1322 continue;; 1323 Log() << kINFO << ""Make response class for classifier: "" << method->GetMethodName() << Endl;; 1324 method->MakeClass();; 1325 }; 1326 }; 1327}; 1328 ; 1329////////////////////////////////////////////////////////////////////////////////; 1330/// Print predefined help message of classifier.; 1331/// Iterate over methods and test.; 1332 ; 1333void TMVA::Factory::PrintHelpMessage(const TString &datasetname, const TString &methodTitle) const; 1334{; 1335 if (methodTitle != """") {; 1336 IMethod *method = GetMethod(datasetname, methodTitle);; 1337 if (method); 1338 method->PrintHelpMessage();; 1339 else {; 1340 Log() << kWARNING << ""<PrintHelpMessage> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 Method",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:54824,Integrability,message,messages,54824,"""\"" in list"" << Endl;; 1313 }; 1314 } else {; 1315 ; 1316 // no classifier specified, print all help messages; 1317 MVector *methods = fMethodsMap.find(datasetname)->second;; 1318 MVector::const_iterator itrMethod;; 1319 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1320 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1321 if (method == 0); 1322 continue;; 1323 Log() << kINFO << ""Make response class for classifier: "" << method->GetMethodName() << Endl;; 1324 method->MakeClass();; 1325 }; 1326 }; 1327}; 1328 ; 1329////////////////////////////////////////////////////////////////////////////////; 1330/// Print predefined help message of classifier.; 1331/// Iterate over methods and test.; 1332 ; 1333void TMVA::Factory::PrintHelpMessage(const TString &datasetname, const TString &methodTitle) const; 1334{; 1335 if (methodTitle != """") {; 1336 IMethod *method = GetMethod(datasetname, methodTitle);; 1337 if (method); 1338 method->PrintHelpMessage();; 1339 else {; 1340 Log() << kWARNING << ""<PrintHelpMessage> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////////////////////////////////////////////////; 1358/// Iterates over all MVA input variables and evaluates them.; 1359 ; 1360void TMVA::Factory::EvaluateAllVariables(DataLoader *loader, TString options); 1361{; 1362 Log() << kINFO << ""Evaluating all variables..."" << Endl;; 1363 E",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:55165,Integrability,message,message,55165," }; 1327}; 1328 ; 1329////////////////////////////////////////////////////////////////////////////////; 1330/// Print predefined help message of classifier.; 1331/// Iterate over methods and test.; 1332 ; 1333void TMVA::Factory::PrintHelpMessage(const TString &datasetname, const TString &methodTitle) const; 1334{; 1335 if (methodTitle != """") {; 1336 IMethod *method = GetMethod(datasetname, methodTitle);; 1337 if (method); 1338 method->PrintHelpMessage();; 1339 else {; 1340 Log() << kWARNING << ""<PrintHelpMessage> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////////////////////////////////////////////////; 1358/// Iterates over all MVA input variables and evaluates them.; 1359 ; 1360void TMVA::Factory::EvaluateAllVariables(DataLoader *loader, TString options); 1361{; 1362 Log() << kINFO << ""Evaluating all variables..."" << Endl;; 1363 Event::SetIsTraining(kFALSE);; 1364 ; 1365 for (UInt_t i = 0; i < loader->GetDataSetInfo().GetNVariables(); i++) {; 1366 TString s = loader->GetDataSetInfo().GetVariableInfo(i).GetLabel();; 1367 if (options.Contains(""V"")); 1368 s += "":V"";; 1369 this->BookMethod(loader, ""Variable"", s);; 1370 }; 1371}; 1372 ; 1373////////////////////////////////////////////////////////////////////////////////; 1374/// Iterates over all MVAs that have been booked, and calls their evaluation methods.; 1375 ; 1376void TMVA::Factory::EvaluateAllMethods(void); 1377",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:115734,Integrability,message,message,115734,"fig::fVariablePlottingclass TMVA::Config::VariablePlotting fVariablePlotting; TMVA::Config::SetSilentvoid SetSilent(Bool_t s)Definition Config.h:63; TMVA::Config::GetIONamesIONames & GetIONames()Definition Config.h:98; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::Configurable::SetConfigDescriptionvoid SetConfigDescription(const char *d)Definition Configurable.h:64; TMVA::Configurable::DeclareOptionRefOptionBase * DeclareOptionRef(T &ref, const TString &name, const TString &desc=""""); TMVA::Configurable::AddPreDefValvoid AddPreDefVal(const T &)Definition Configurable.h:168; TMVA::Configurable::SetConfigNamevoid SetConfigName(const char *n)Definition Configurable.h:63; TMVA::Configurable::ParseOptionsvirtual void ParseOptions()options parserDefinition Configurable.cxx:124; TMVA::Configurable::GetOptionsconst TString & GetOptions() constDefinition Configurable.h:84; TMVA::Configurable::LogMsgLogger & Log() constDefinition Configurable.h:122; TMVA::Configurable::fLoggerMsgLogger * fLogger! message loggerDefinition Configurable.h:128; TMVA::Configurable::CheckForUnusedOptionsvoid CheckForUnusedOptions() constchecks for unused options in option stringDefinition Configurable.cxx:270; TMVA::DataInputHandler::GetEntriesUInt_t GetEntries(const TString &name) constDefinition DataInputHandler.h:100; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::DataSetInfoClass that contains all the data information.Definition DataSe",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:120183,Integrability,message,message,120183,")Definition DataSet.cxx:265; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::DataSet::SetCurrentTypevoid SetCurrentType(Types::ETreeType type) constDefinition DataSet.h:89; TMVA::DataSet::GetEventCollectionconst std::vector< Event * > & GetEventCollection(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:216; TMVA::DataSet::GetNEvtBkgdTestLong64_t GetNEvtBkgdTest()return number of background test events in datasetDefinition DataSet.cxx:435; TMVA::EventDefinition Event.h:51; TMVA::Event::GetValueFloat_t GetValue(UInt_t ivar) constreturn value of i'th variableDefinition Event.cxx:236; TMVA::Event::SetIsTrainingstatic void SetIsTraining(Bool_t)when this static function is called, it sets the flag whether events with negative event weight shoul...Definition Event.cxx:399; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::PrintHelpMessagevoid PrintHelpMessage(const TString &datasetname, const TString &methodTitle="""") constPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::VerboseBool_t Verbose(void) constDefinition Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)Definition Factory.cxx:602; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::FactoryFactory(TString theJobName, TFile *theTargetFile, TString theOption="""")Standard constructor.Definition Factory.cxx:113; TMVA::Factory::TestAllMethodsvoid",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:125712,Integrability,message,message,125712,"ng theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetname, const TString &title) constReturns pointer to MVA that corresponds to given method title.Definition Factory.cxx:566; TMVA::Factory::DeleteAllMethodsvoid DeleteAllMethods(void)Delete methods.Definition Factory.cxx:324; TMVA::Factory::fTransformationsTString fTransformations! list of transformations to testDefinition Factory.h:212; TMVA::Factory::Greetingsvoid Greetings()Print welcome message.Definition Factory.cxx:295; TMVA::IMethodInterface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::IMethod::PrintHelpMessagevirtual void PrintHelpMessage() const =0; TMVA::IMethod::HasAnalysisTypevirtual Bool_t HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets)=0; TMVA::IMethod::MakeClassvirtual void MakeClass(const TString &classFileName=TString("""")) const =0; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::MethodBase::GetSeparationvirtual Double_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodBase.cxx:2789; TMVA::MethodBase::SetSilentFilevoid SetSilentFile(Bool_t status)Definition MethodBase.h:378; TMVA::MethodBase::SetWeightFileDirvoid SetWeightFileDir(TString fileDir)set directory of weight fileDefinition MethodBase.cxx:2059; TMVA::MethodBase::TestRegressionvirtual void TestRegression(Double_t &bias, Double_t &b",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:4576,Modifiability,config,configurable,4576,"TargetFile : output ROOT file; the test tree and all evaluation plots; 110/// will be stored here; 111/// - theOption : option string; currently: ""V"" for verbose; 112 ; 113TMVA::Factory::Factory(TString jobName, TFile *theTargetFile, TString theOption); 114 : Configurable(theOption), fTransformations(""I""), fVerbose(kFALSE), fVerboseLevel(kINFO), fCorrelations(kFALSE),; 115 fROC(kTRUE), fSilentFile(theTargetFile == nullptr), fJobName(jobName), fAnalysisType(Types::kClassification),; 116 fModelPersistence(kTRUE); 117{; 118 fName = ""Factory"";; 119 fgTargetFile = theTargetFile;; 120 fLogger->SetSource(fName.Data());; 121 ; 122 // render silent; 123 if (gTools().CheckForSilentOption(GetOptions())); 124 Log().InhibitOutput(); // make sure is silent if wanted to; 125 ; 126 // init configurable; 127 SetConfigDescription(""Configuration options for Factory running"");; 128 SetConfigName(GetName());; 129 ; 130 // histograms are not automatically associated with the current; 131 // directory and hence don't go out of scope when closing the file; 132 // TH1::AddDirectory(kFALSE);; 133 Bool_t silent = kFALSE;; 134#ifdef WIN32; 135 // under Windows, switch progress bar and color off by default, as the typical windows shell doesn't handle these; 136 // (would need different sequences..); 137 Bool_t color = kFALSE;; 138 Bool_t drawProgressBar = kFALSE;; 139#else; 140 Bool_t color = !gROOT->IsBatch();; 141 Bool_t drawProgressBar = kTRUE;; 142#endif; 143 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 144 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 145 AddPreDefVal(TString(""Debug""));; 146 AddPreDefVal(TString(""Verbose""));; 147 AddPreDefVal(TString(""Info""));; 148 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 149 DeclareOptionRef(; 150 fTransformations, ""Transformations"",; 151 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:8489,Modifiability,config,configurable,8489,"ticlass;; 195 else if (analysisType == ""auto""); 196 fAnalysisType = Types::kNoAnalysisType;; 197 ; 198 // Greetings();; 199}; 200 ; 201////////////////////////////////////////////////////////////////////////////////; 202/// Constructor.; 203 ; 204TMVA::Factory::Factory(TString jobName, TString theOption); 205 : Configurable(theOption), fTransformations(""I""), fVerbose(kFALSE), fCorrelations(kFALSE), fROC(kTRUE),; 206 fSilentFile(kTRUE), fJobName(jobName), fAnalysisType(Types::kClassification), fModelPersistence(kTRUE); 207{; 208 fName = ""Factory"";; 209 fgTargetFile = nullptr;; 210 fLogger->SetSource(fName.Data());; 211 ; 212 // render silent; 213 if (gTools().CheckForSilentOption(GetOptions())); 214 Log().InhibitOutput(); // make sure is silent if wanted to; 215 ; 216 // init configurable; 217 SetConfigDescription(""Configuration options for Factory running"");; 218 SetConfigName(GetName());; 219 ; 220 // histograms are not automatically associated with the current; 221 // directory and hence don't go out of scope when closing the file; 222 TH1::AddDirectory(kFALSE);; 223 Bool_t silent = kFALSE;; 224#ifdef WIN32; 225 // under Windows, switch progress bar and color off by default, as the typical windows shell doesn't handle these; 226 // (would need different sequences..); 227 Bool_t color = kFALSE;; 228 Bool_t drawProgressBar = kFALSE;; 229#else; 230 Bool_t color = !gROOT->IsBatch();; 231 Bool_t drawProgressBar = kTRUE;; 232#endif; 233 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 234 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 235 AddPreDefVal(TString(""Debug""));; 236 AddPreDefVal(TString(""Verbose""));; 237 AddPreDefVal(TString(""Info""));; 238 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 239 DeclareOptionRef(; 240 fTransformations, ""Transformations"",; 241 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", f",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:19758,Modifiability,config,configuration,19758,"e); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = Classifie",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:19988,Modifiability,config,configurations,19988,"e); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = Classifie",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:27466,Modifiability,variab,variables,27466,";; 661 TransformationHandler *identityTrHandler = 0;; 662 ; 663 std::vector<TString> trfsDef = gTools().SplitString(processTrfs, ';');; 664 std::vector<TString>::iterator trfsDefIt = trfsDef.begin();; 665 for (; trfsDefIt != trfsDef.end(); ++trfsDefIt) {; 666 trfs.push_back(new TMVA::TransformationHandler(fDataSetInfo, ""Factory""));; 667 TString trfS = (*trfsDefIt);; 668 ; 669 // Log() << kINFO << Endl;; 670 Log() << kDEBUG << ""current transformation string: '"" << trfS.Data() << ""'"" << Endl;; 671 TMVA::CreateVariableTransforms(trfS, fDataSetInfo, *(trfs.back()), Log());; 672 ; 673 if (trfS.BeginsWith('I')); 674 identityTrHandler = trfs.back();; 675 }; 676 ; 677 const std::vector<Event *> &inputEvents = fDataSetInfo.GetDataSet()->GetEventCollection();; 678 ; 679 // apply all transformations; 680 std::vector<TMVA::TransformationHandler *>::iterator trfIt = trfs.begin();; 681 ; 682 for (; trfIt != trfs.end(); ++trfIt) {; 683 // setting a Root dir causes the variables distributions to be saved to the root file; 684 (*trfIt)->SetRootDir(RootBaseDir()->GetDirectory(fDataSetInfo.GetName())); // every dataloader have its own dir; 685 (*trfIt)->CalcTransformations(inputEvents);; 686 }; 687 if (identityTrHandler); 688 identityTrHandler->PrintVariableRanking();; 689 ; 690 // clean up; 691 for (trfIt = trfs.begin(); trfIt != trfs.end(); ++trfIt); 692 delete *trfIt;; 693}; 694 ; 695////////////////////////////////////////////////////////////////////////////////; 696/// Iterates through all booked methods and sees if they use parameter tuning and if so; 697/// does just that, i.e.\ calls ""Method::Train()"" for different parameter settings and; 698/// keeps in mind the ""optimal one""...\ and that's the one that will later on be used; 699/// in the main training loop.; 700 ; 701std::map<TString, Double_t> TMVA::Factory::OptimizeAllMethods(TString fomType, TString fitType); 702{; 703 ; 704 std::map<TString, MVector *>::iterator itrMap;; 705 std::map<TString, Double_t> TunedParameters;;",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:48473,Modifiability,variab,variable,48473,"input data for the training provided!"" << Endl;; 1150 }; 1151 ; 1152 if (fAnalysisType == Types::kRegression && mva->DataInfo().GetNTargets() < 1); 1153 Log() << kFATAL << ""You want to do regression training without specifying a target."" << Endl;; 1154 else if ((fAnalysisType == Types::kMulticlass || fAnalysisType == Types::kClassification) &&; 1155 mva->DataInfo().GetNClasses() < 2); 1156 Log() << kFATAL << ""You want to do classification training, but specified less than two classes."" << Endl;; 1157 ; 1158 // first print some information about the default dataset; 1159 if (!IsSilentFile()); 1160 WriteDataInformation(mva->fDataSetInfo);; 1161 ; 1162 if (mva->Data()->GetNTrainingEvents() < MinNoTrainingEvents) {; 1163 Log() << kWARNING << ""Method "" << mva->GetMethodName() << "" not trained (training tree has less entries [""; 1164 << mva->Data()->GetNTrainingEvents() << ""] than required ["" << MinNoTrainingEvents << ""]"" << Endl;; 1165 continue;; 1166 }; 1167 ; 1168 Log() << kHEADER << ""Train method: "" << mva->GetMethodName() << "" for ""; 1169 << (fAnalysisType == Types::kRegression; 1170 ? ""Regression""; 1171 : (fAnalysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 1172 << Endl << Endl;; 1173 mva->TrainMethod();; 1174 Log() << kHEADER << ""Training finished"" << Endl << Endl;; 1175 }; 1176 ; 1177 if (fAnalysisType != Types::kRegression) {; 1178 ; 1179 // variable ranking; 1180 // Log() << Endl;; 1181 Log() << kINFO << ""Ranking input variables (method specific)..."" << Endl;; 1182 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1183 MethodBase *mva = dynamic_cast<MethodBase *>(*itrMethod);; 1184 if (mva && mva->Data()->GetNTrainingEvents() >= MinNoTrainingEvents) {; 1185 ; 1186 // create and print ranking; 1187 const Ranking *ranking = (*itrMethod)->CreateRanking();; 1188 if (ranking != 0); 1189 ranking->Print();; 1190 else; 1191 Log() << kINFO << ""No variable ranking supplied by classifier: ""; 1192 << dynamic_",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:48553,Modifiability,variab,variables,48553,"input data for the training provided!"" << Endl;; 1150 }; 1151 ; 1152 if (fAnalysisType == Types::kRegression && mva->DataInfo().GetNTargets() < 1); 1153 Log() << kFATAL << ""You want to do regression training without specifying a target."" << Endl;; 1154 else if ((fAnalysisType == Types::kMulticlass || fAnalysisType == Types::kClassification) &&; 1155 mva->DataInfo().GetNClasses() < 2); 1156 Log() << kFATAL << ""You want to do classification training, but specified less than two classes."" << Endl;; 1157 ; 1158 // first print some information about the default dataset; 1159 if (!IsSilentFile()); 1160 WriteDataInformation(mva->fDataSetInfo);; 1161 ; 1162 if (mva->Data()->GetNTrainingEvents() < MinNoTrainingEvents) {; 1163 Log() << kWARNING << ""Method "" << mva->GetMethodName() << "" not trained (training tree has less entries [""; 1164 << mva->Data()->GetNTrainingEvents() << ""] than required ["" << MinNoTrainingEvents << ""]"" << Endl;; 1165 continue;; 1166 }; 1167 ; 1168 Log() << kHEADER << ""Train method: "" << mva->GetMethodName() << "" for ""; 1169 << (fAnalysisType == Types::kRegression; 1170 ? ""Regression""; 1171 : (fAnalysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 1172 << Endl << Endl;; 1173 mva->TrainMethod();; 1174 Log() << kHEADER << ""Training finished"" << Endl << Endl;; 1175 }; 1176 ; 1177 if (fAnalysisType != Types::kRegression) {; 1178 ; 1179 // variable ranking; 1180 // Log() << Endl;; 1181 Log() << kINFO << ""Ranking input variables (method specific)..."" << Endl;; 1182 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1183 MethodBase *mva = dynamic_cast<MethodBase *>(*itrMethod);; 1184 if (mva && mva->Data()->GetNTrainingEvents() >= MinNoTrainingEvents) {; 1185 ; 1186 // create and print ranking; 1187 const Ranking *ranking = (*itrMethod)->CreateRanking();; 1188 if (ranking != 0); 1189 ranking->Print();; 1190 else; 1191 Log() << kINFO << ""No variable ranking supplied by classifier: ""; 1192 << dynamic_",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:49010,Modifiability,variab,variable,49010,"nue;; 1166 }; 1167 ; 1168 Log() << kHEADER << ""Train method: "" << mva->GetMethodName() << "" for ""; 1169 << (fAnalysisType == Types::kRegression; 1170 ? ""Regression""; 1171 : (fAnalysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 1172 << Endl << Endl;; 1173 mva->TrainMethod();; 1174 Log() << kHEADER << ""Training finished"" << Endl << Endl;; 1175 }; 1176 ; 1177 if (fAnalysisType != Types::kRegression) {; 1178 ; 1179 // variable ranking; 1180 // Log() << Endl;; 1181 Log() << kINFO << ""Ranking input variables (method specific)..."" << Endl;; 1182 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1183 MethodBase *mva = dynamic_cast<MethodBase *>(*itrMethod);; 1184 if (mva && mva->Data()->GetNTrainingEvents() >= MinNoTrainingEvents) {; 1185 ; 1186 // create and print ranking; 1187 const Ranking *ranking = (*itrMethod)->CreateRanking();; 1188 if (ranking != 0); 1189 ranking->Print();; 1190 else; 1191 Log() << kINFO << ""No variable ranking supplied by classifier: ""; 1192 << dynamic_cast<MethodBase *>(*itrMethod)->GetMethodName() << Endl;; 1193 }; 1194 }; 1195 }; 1196 ; 1197 // save training history in case we are not in the silent mode; 1198 if (!IsSilentFile()) {; 1199 for (UInt_t i = 0; i < methods->size(); i++) {; 1200 MethodBase *m = dynamic_cast<MethodBase *>((*methods)[i]);; 1201 if (m == 0); 1202 continue;; 1203 m->BaseDir()->cd();; 1204 m->fTrainHistory.SaveHistory(m->GetMethodName());; 1205 }; 1206 }; 1207 ; 1208 // delete all methods and recreate them from weight file - this ensures that the application; 1209 // of the methods (in TMVAClassificationApplication) is consistent with the results obtained; 1210 // in the testing; 1211 // Log() << Endl;; 1212 if (fModelPersistence) {; 1213 ; 1214 Log() << kHEADER << ""=== Destroy and recreate all methods via weight files for testing ==="" << Endl << Endl;; 1215 ; 1216 if (!IsSilentFile()); 1217 RootBaseDir()->cd();; 1218 ; 1219 // iterate through all booked meth",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:55414,Modifiability,variab,variables,55414," }; 1327}; 1328 ; 1329////////////////////////////////////////////////////////////////////////////////; 1330/// Print predefined help message of classifier.; 1331/// Iterate over methods and test.; 1332 ; 1333void TMVA::Factory::PrintHelpMessage(const TString &datasetname, const TString &methodTitle) const; 1334{; 1335 if (methodTitle != """") {; 1336 IMethod *method = GetMethod(datasetname, methodTitle);; 1337 if (method); 1338 method->PrintHelpMessage();; 1339 else {; 1340 Log() << kWARNING << ""<PrintHelpMessage> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////////////////////////////////////////////////; 1358/// Iterates over all MVA input variables and evaluates them.; 1359 ; 1360void TMVA::Factory::EvaluateAllVariables(DataLoader *loader, TString options); 1361{; 1362 Log() << kINFO << ""Evaluating all variables..."" << Endl;; 1363 Event::SetIsTraining(kFALSE);; 1364 ; 1365 for (UInt_t i = 0; i < loader->GetDataSetInfo().GetNVariables(); i++) {; 1366 TString s = loader->GetDataSetInfo().GetVariableInfo(i).GetLabel();; 1367 if (options.Contains(""V"")); 1368 s += "":V"";; 1369 this->BookMethod(loader, ""Variable"", s);; 1370 }; 1371}; 1372 ; 1373////////////////////////////////////////////////////////////////////////////////; 1374/// Iterates over all MVAs that have been booked, and calls their evaluation methods.; 1375 ; 1376void TMVA::Factory::EvaluateAllMethods(void); 1377",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:55581,Modifiability,variab,variables,55581,"method = GetMethod(datasetname, methodTitle);; 1337 if (method); 1338 method->PrintHelpMessage();; 1339 else {; 1340 Log() << kWARNING << ""<PrintHelpMessage> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////////////////////////////////////////////////; 1358/// Iterates over all MVA input variables and evaluates them.; 1359 ; 1360void TMVA::Factory::EvaluateAllVariables(DataLoader *loader, TString options); 1361{; 1362 Log() << kINFO << ""Evaluating all variables..."" << Endl;; 1363 Event::SetIsTraining(kFALSE);; 1364 ; 1365 for (UInt_t i = 0; i < loader->GetDataSetInfo().GetNVariables(); i++) {; 1366 TString s = loader->GetDataSetInfo().GetVariableInfo(i).GetLabel();; 1367 if (options.Contains(""V"")); 1368 s += "":V"";; 1369 this->BookMethod(loader, ""Variable"", s);; 1370 }; 1371}; 1372 ; 1373////////////////////////////////////////////////////////////////////////////////; 1374/// Iterates over all MVAs that have been booked, and calls their evaluation methods.; 1375 ; 1376void TMVA::Factory::EvaluateAllMethods(void); 1377{; 1378 Log() << kHEADER << gTools().Color(""bold"") << ""Evaluate all methods"" << gTools().Color(""reset"") << Endl;; 1379 ; 1380 // don't do anything if no method booked; 1381 if (fMethodsMap.empty()) {; 1382 Log() << kINFO << ""...nothing found to evaluate"" << Endl;; 1383 return;; 1384 }; 1385 std::map<TString, MVector *>::iterator itrMap;; 1386 ; 1387 for (itr",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:56979,Modifiability,variab,variables,56979,"ate all methods"" << gTools().Color(""reset"") << Endl;; 1379 ; 1380 // don't do anything if no method booked; 1381 if (fMethodsMap.empty()) {; 1382 Log() << kINFO << ""...nothing found to evaluate"" << Endl;; 1383 return;; 1384 }; 1385 std::map<TString, MVector *>::iterator itrMap;; 1386 ; 1387 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1388 MVector *methods = itrMap->second;; 1389 ; 1390 // -----------------------------------------------------------------------; 1391 // First part of evaluation process; 1392 // --> compute efficiencies, and other separation estimators; 1393 // -----------------------------------------------------------------------; 1394 ; 1395 // although equal, we now want to separate the output for the variables; 1396 // and the real methods; 1397 Int_t isel; // will be 0 for a Method; 1 for a Variable; 1398 Int_t nmeth_used[2] = {0, 0}; // 0 Method; 1 Variable; 1399 ; 1400 std::vector<std::vector<TString>> mname(2);; 1401 std::vector<std::vector<Double_t>> sig(2), sep(2), roc(2);; 1402 std::vector<std::vector<Double_t>> eff01(2), eff10(2), eff30(2), effArea(2);; 1403 std::vector<std::vector<Double_t>> eff01err(2), eff10err(2), eff30err(2);; 1404 std::vector<std::vector<Double_t>> trainEff01(2), trainEff10(2), trainEff30(2);; 1405 ; 1406 std::vector<std::vector<Float_t>> multiclass_testEff;; 1407 std::vector<std::vector<Float_t>> multiclass_trainEff;; 1408 std::vector<std::vector<Float_t>> multiclass_testPur;; 1409 std::vector<std::vector<Float_t>> multiclass_trainPur;; 1410 ; 1411 std::vector<std::vector<Float_t>> train_history;; 1412 ; 1413 // Multiclass confusion matrices.; 1414 std::vector<TMatrixD> multiclass_trainConfusionEffB01;; 1415 std::vector<TMatrixD> multiclass_trainConfusionEffB10;; 1416 std::vector<TMatrixD> multiclass_trainConfusionEffB30;; 1417 std::vector<TMatrixD> multiclass_testConfusionEffB01;; 1418 std::vector<TMatrixD> multiclass_testConfusionEffB10;; 1419 std::vector<TMatrixD> multiclass_testCo",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:67378,Modifiability,variab,variables,67378,"h_back(rmstrainT[0]);; 1587 vtmp.push_back(minftestT[0]);; 1588 vtmp.push_back(minftrainT[0]);; 1589 gTools().UsefulSortAscending(vtmp, &vtemps);; 1590 mname[0] = vtemps;; 1591 devtest[0] = vtmp[0];; 1592 devtrain[0] = vtmp[1];; 1593 biastest[0] = vtmp[2];; 1594 biastrain[0] = vtmp[3];; 1595 rmstest[0] = vtmp[4];; 1596 rmstrain[0] = vtmp[5];; 1597 minftest[0] = vtmp[6];; 1598 minftrain[0] = vtmp[7];; 1599 rhotest[0] = vtmp[8];; 1600 rhotrain[0] = vtmp[9];; 1601 devtestT[0] = vtmp[10];; 1602 devtrainT[0] = vtmp[11];; 1603 biastestT[0] = vtmp[12];; 1604 biastrainT[0] = vtmp[13];; 1605 rmstestT[0] = vtmp[14];; 1606 rmstrainT[0] = vtmp[15];; 1607 minftestT[0] = vtmp[16];; 1608 minftrainT[0] = vtmp[17];; 1609 } else if (doMulticlass) {; 1610 // TODO: fill in something meaningful; 1611 // If there is some ranking of methods to be done it should be done here.; 1612 // However, this is not so easy to define for multiclass so it is left out for now.; 1613 ; 1614 } else {; 1615 // now sort the variables according to the best 'eff at Beff=0.10'; 1616 for (Int_t k = 0; k < 2; k++) {; 1617 std::vector<std::vector<Double_t>> vtemp;; 1618 vtemp.push_back(effArea[k]); // this is the vector that is ranked; 1619 vtemp.push_back(eff10[k]);; 1620 vtemp.push_back(eff01[k]);; 1621 vtemp.push_back(eff30[k]);; 1622 vtemp.push_back(eff10err[k]);; 1623 vtemp.push_back(eff01err[k]);; 1624 vtemp.push_back(eff30err[k]);; 1625 vtemp.push_back(trainEff10[k]);; 1626 vtemp.push_back(trainEff01[k]);; 1627 vtemp.push_back(trainEff30[k]);; 1628 vtemp.push_back(sig[k]);; 1629 vtemp.push_back(sep[k]);; 1630 vtemp.push_back(roc[k]);; 1631 std::vector<TString> vtemps = mname[k];; 1632 gTools().UsefulSortDescending(vtemp, &vtemps);; 1633 effArea[k] = vtemp[0];; 1634 eff10[k] = vtemp[1];; 1635 eff01[k] = vtemp[2];; 1636 eff30[k] = vtemp[3];; 1637 eff10err[k] = vtemp[4];; 1638 eff01err[k] = vtemp[5];; 1639 eff30err[k] = vtemp[6];; 1640 trainEff10[k] = vtemp[7];; 1641 trainEff01[k] = vtemp[8];; 1642 trainEff3",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:68739,Modifiability,variab,variables,68739,"temp;; 1618 vtemp.push_back(effArea[k]); // this is the vector that is ranked; 1619 vtemp.push_back(eff10[k]);; 1620 vtemp.push_back(eff01[k]);; 1621 vtemp.push_back(eff30[k]);; 1622 vtemp.push_back(eff10err[k]);; 1623 vtemp.push_back(eff01err[k]);; 1624 vtemp.push_back(eff30err[k]);; 1625 vtemp.push_back(trainEff10[k]);; 1626 vtemp.push_back(trainEff01[k]);; 1627 vtemp.push_back(trainEff30[k]);; 1628 vtemp.push_back(sig[k]);; 1629 vtemp.push_back(sep[k]);; 1630 vtemp.push_back(roc[k]);; 1631 std::vector<TString> vtemps = mname[k];; 1632 gTools().UsefulSortDescending(vtemp, &vtemps);; 1633 effArea[k] = vtemp[0];; 1634 eff10[k] = vtemp[1];; 1635 eff01[k] = vtemp[2];; 1636 eff30[k] = vtemp[3];; 1637 eff10err[k] = vtemp[4];; 1638 eff01err[k] = vtemp[5];; 1639 eff30err[k] = vtemp[6];; 1640 trainEff10[k] = vtemp[7];; 1641 trainEff01[k] = vtemp[8];; 1642 trainEff30[k] = vtemp[9];; 1643 sig[k] = vtemp[10];; 1644 sep[k] = vtemp[11];; 1645 roc[k] = vtemp[12];; 1646 mname[k] = vtemps;; 1647 }; 1648 }; 1649 ; 1650 // -----------------------------------------------------------------------; 1651 // Second part of evaluation process; 1652 // --> compute correlations among MVAs; 1653 // --> compute correlations between input variables and MVA (determines importance); 1654 // --> count overlaps; 1655 // -----------------------------------------------------------------------; 1656 if (fCorrelations) {; 1657 const Int_t nmeth = methodsNoCuts.size();; 1658 MethodBase *method = dynamic_cast<MethodBase *>(methods[0][0]);; 1659 const Int_t nvar = method->fDataSetInfo.GetNVariables();; 1660 if (!doRegression && !doMulticlass) {; 1661 ; 1662 if (nmeth > 0) {; 1663 ; 1664 // needed for correlations; 1665 Double_t *dvec = new Double_t[nmeth + nvar];; 1666 std::vector<Double_t> rvec;; 1667 ; 1668 // for correlations; 1669 TPrincipal *tpSig = new TPrincipal(nmeth + nvar, """");; 1670 TPrincipal *tpBkg = new TPrincipal(nmeth + nvar, """");; 1671 ; 1672 // set required tree branch references; 1673 In",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:72578,Modifiability,variab,variables,72578,"++) {; 1725 if ((dvec[im] - rvec[im]) * (dvec[jm] - rvec[jm]) > 0) {; 1726 (*theMat)(im, jm)++;; 1727 if (im != jm); 1728 (*theMat)(jm, im)++;; 1729 }; 1730 }; 1731 }; 1732 }; 1733 ; 1734 // renormalise overlap matrix; 1735 (*overlapS) *= (1.0 / defDs->GetNEvtSigTest()); // init...; 1736 (*overlapB) *= (1.0 / defDs->GetNEvtBkgdTest()); // init...; 1737 ; 1738 tpSig->MakePrincipals();; 1739 tpBkg->MakePrincipals();; 1740 ; 1741 const TMatrixD *covMatS = tpSig->GetCovarianceMatrix();; 1742 const TMatrixD *covMatB = tpBkg->GetCovarianceMatrix();; 1743 ; 1744 const TMatrixD *corrMatS = gTools().GetCorrelationMatrix(covMatS);; 1745 const TMatrixD *corrMatB = gTools().GetCorrelationMatrix(covMatB);; 1746 ; 1747 // print correlation matrices; 1748 if (corrMatS != 0 && corrMatB != 0) {; 1749 ; 1750 // extract MVA matrix; 1751 TMatrixD mvaMatS(nmeth, nmeth);; 1752 TMatrixD mvaMatB(nmeth, nmeth);; 1753 for (Int_t im = 0; im < nmeth; im++) {; 1754 for (Int_t jm = 0; jm < nmeth; jm++) {; 1755 mvaMatS(im, jm) = (*corrMatS)(im, jm);; 1756 mvaMatB(im, jm) = (*corrMatB)(im, jm);; 1757 }; 1758 }; 1759 ; 1760 // extract variables - to MVA matrix; 1761 std::vector<TString> theInputVars;; 1762 TMatrixD varmvaMatS(nvar, nmeth);; 1763 TMatrixD varmvaMatB(nvar, nmeth);; 1764 for (Int_t iv = 0; iv < nvar; iv++) {; 1765 theInputVars.push_back(method->fDataSetInfo.GetVariableInfo(iv).GetLabel());; 1766 for (Int_t jm = 0; jm < nmeth; jm++) {; 1767 varmvaMatS(iv, jm) = (*corrMatS)(nmeth + iv, jm);; 1768 varmvaMatB(iv, jm) = (*corrMatB)(nmeth + iv, jm);; 1769 }; 1770 }; 1771 ; 1772 if (nmeth > 1) {; 1773 Log() << kINFO << Endl;; 1774 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1775 << ""Inter-MVA correlation matrix (signal):"" << Endl;; 1776 gTools().FormattedOutput(mvaMatS, *theVars, Log());; 1777 Log() << kINFO << Endl;; 1778 ; 1779 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1780 << ""Inter-MVA correlation matrix (background):"" << Endl;",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:73679,Modifiability,variab,variables,73679,"1763 TMatrixD varmvaMatB(nvar, nmeth);; 1764 for (Int_t iv = 0; iv < nvar; iv++) {; 1765 theInputVars.push_back(method->fDataSetInfo.GetVariableInfo(iv).GetLabel());; 1766 for (Int_t jm = 0; jm < nmeth; jm++) {; 1767 varmvaMatS(iv, jm) = (*corrMatS)(nmeth + iv, jm);; 1768 varmvaMatB(iv, jm) = (*corrMatB)(nmeth + iv, jm);; 1769 }; 1770 }; 1771 ; 1772 if (nmeth > 1) {; 1773 Log() << kINFO << Endl;; 1774 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1775 << ""Inter-MVA correlation matrix (signal):"" << Endl;; 1776 gTools().FormattedOutput(mvaMatS, *theVars, Log());; 1777 Log() << kINFO << Endl;; 1778 ; 1779 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1780 << ""Inter-MVA correlation matrix (background):"" << Endl;; 1781 gTools().FormattedOutput(mvaMatB, *theVars, Log());; 1782 Log() << kINFO << Endl;; 1783 }; 1784 ; 1785 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1786 << ""Correlations between input variables and MVA response (signal):"" << Endl;; 1787 gTools().FormattedOutput(varmvaMatS, theInputVars, *theVars, Log());; 1788 Log() << kINFO << Endl;; 1789 ; 1790 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1791 << ""Correlations between input variables and MVA response (background):"" << Endl;; 1792 gTools().FormattedOutput(varmvaMatB, theInputVars, *theVars, Log());; 1793 Log() << kINFO << Endl;; 1794 } else; 1795 Log() << kWARNING << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1796 << ""<TestAllMethods> cannot compute correlation matrices"" << Endl;; 1797 ; 1798 // print overlap matrices; 1799 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1800 << ""The following \""overlap\"" matrices contain the fraction of events for which "" << Endl;; 1801 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1802 << ""the MVAs 'i' and 'j' have returned conform answers about \""signal-likeness\"""" << Endl;; 1803 Log() << kI",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:73954,Modifiability,variab,variables,73954,"vaMatB(iv, jm) = (*corrMatB)(nmeth + iv, jm);; 1769 }; 1770 }; 1771 ; 1772 if (nmeth > 1) {; 1773 Log() << kINFO << Endl;; 1774 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1775 << ""Inter-MVA correlation matrix (signal):"" << Endl;; 1776 gTools().FormattedOutput(mvaMatS, *theVars, Log());; 1777 Log() << kINFO << Endl;; 1778 ; 1779 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1780 << ""Inter-MVA correlation matrix (background):"" << Endl;; 1781 gTools().FormattedOutput(mvaMatB, *theVars, Log());; 1782 Log() << kINFO << Endl;; 1783 }; 1784 ; 1785 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1786 << ""Correlations between input variables and MVA response (signal):"" << Endl;; 1787 gTools().FormattedOutput(varmvaMatS, theInputVars, *theVars, Log());; 1788 Log() << kINFO << Endl;; 1789 ; 1790 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1791 << ""Correlations between input variables and MVA response (background):"" << Endl;; 1792 gTools().FormattedOutput(varmvaMatB, theInputVars, *theVars, Log());; 1793 Log() << kINFO << Endl;; 1794 } else; 1795 Log() << kWARNING << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1796 << ""<TestAllMethods> cannot compute correlation matrices"" << Endl;; 1797 ; 1798 // print overlap matrices; 1799 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1800 << ""The following \""overlap\"" matrices contain the fraction of events for which "" << Endl;; 1801 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1802 << ""the MVAs 'i' and 'j' have returned conform answers about \""signal-likeness\"""" << Endl;; 1803 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1804 << ""An event is signal-like, if its MVA output exceeds the following value:"" << Endl;; 1805 gTools().FormattedOutput(rvec, *theVars, ""Method"", ""Cut value"", Log());; 1806 Log() << kINFO << Form(""Dataset[%s] : """,MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:93676,Modifiability,variab,variables,93676,") {; 2196 MethodBase *theMethod = dynamic_cast<MethodBase *>((*methods)[i]);; 2197 if (theMethod == 0); 2198 continue;; 2199 // write test/training trees; 2200 RootBaseDir()->cd(theMethod->fDataSetInfo.GetName());; 2201 if (std::find(datasets.begin(), datasets.end(), theMethod->fDataSetInfo.GetName()) == datasets.end()) {; 2202 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTesting)->Write("""", TObject::kOverwrite);; 2203 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTraining)->Write("""", TObject::kOverwrite);; 2204 datasets.push_back(theMethod->fDataSetInfo.GetName());; 2205 }; 2206 }; 2207 }; 2208 }; 2209 } // end for MethodsMap; 2210 // references for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more th",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:93690,Modifiability,variab,variable,93690,") {; 2196 MethodBase *theMethod = dynamic_cast<MethodBase *>((*methods)[i]);; 2197 if (theMethod == 0); 2198 continue;; 2199 // write test/training trees; 2200 RootBaseDir()->cd(theMethod->fDataSetInfo.GetName());; 2201 if (std::find(datasets.begin(), datasets.end(), theMethod->fDataSetInfo.GetName()) == datasets.end()) {; 2202 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTesting)->Write("""", TObject::kOverwrite);; 2203 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTraining)->Write("""", TObject::kOverwrite);; 2204 datasets.push_back(theMethod->fDataSetInfo.GetName());; 2205 }; 2206 }; 2207 }; 2208 }; 2209 } // end for MethodsMap; 2210 // references for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more th",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:94472,Modifiability,variab,variables,94472,"ences for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo(",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:94611,Modifiability,variab,variables,94611,"rt); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:95044,Modifiability,variab,variables,95044,"rt); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:95058,Modifiability,variab,variable,95058,"rt); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:96026,Modifiability,variab,variable,96026,"on); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // adding variables from seed; 2286 for (int index = 0; index < nbits; index++) {; 2287 if (xbitset[index]); 2288 seedloader->AddVariable(varNames[index], 'F');; 2289 }; 2290 ; 2291 DataLoaderCopy(seedloader, loader);; 2292 seedloader->PrepareTrainingAndTestTree(loader->GetDataSetInfo().GetCut(""Signal""),; 2293 loader->GetDataSetInfo().GetCut(""Background""),; 2294 loader->GetDataSetInfo().GetSplitOptions());; 2295 ; 2296 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMV",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:96179,Modifiability,variab,variables,96179," of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // adding variables from seed; 2286 for (int index = 0; index < nbits; index++) {; 2287 if (xbitset[index]); 2288 seedloader->AddVariable(varNames[index], 'F');; 2289 }; 2290 ; 2291 DataLoaderCopy(seedloader, loader);; 2292 seedloader->PrepareTrainingAndTestTree(loader->GetDataSetInfo().GetCut(""Signal""),; 2293 loader->GetDataSetInfo().GetCut(""Background""),; 2294 loader->GetDataSetInfo().GetSplitOptions());; 2295 ; 2296 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2309 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2310 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2311 delete sresults;; 2312 delete seedloader;; 2313 this->Delet",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:97676,Modifiability,variab,variable,97676,"96 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2309 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2310 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2311 delete sresults;; 2312 delete seedloader;; 2313 this->DeleteAllMethods();; 2314 ; 2315 fMethodsMap.clear();; 2316 // removing global result because it is requiring a lot of RAM for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i < VIBITS; ++i) {; 2322 if (x & (uint64_t(1) << i)) {; 2323 y = x & ~(1 << i);; 2324 std::bitset<VIBITS> ybitset(y);; 2325 // need at least one variable; 2326 // NOTE: if sub-seed is zero then is the special case; 2327 // that count in xbitset is 1; 2328 uint32_t ny = static_cast<uint32_t>( log(x - y) / 0.693147 ) ;; 2329 if (y == 0) {; 2330 importances[ny] = SROC - 0.5;; 2331 continue;; 2332 }; 2333 ; 2334 // getting ROC; 2335 SSROC = ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356/////////////////////////////////////////////////////////////////",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:98841,Modifiability,variab,variables,98841," for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i < VIBITS; ++i) {; 2322 if (x & (uint64_t(1) << i)) {; 2323 y = x & ~(1 << i);; 2324 std::bitset<VIBITS> ybitset(y);; 2325 // need at least one variable; 2326 // NOTE: if sub-seed is zero then is the special case; 2327 // that count in xbitset is 1; 2328 uint32_t ny = static_cast<uint32_t>( log(x - y) / 0.693147 ) ;; 2329 if (y == 0) {; 2330 importances[ny] = SROC - 0.5;; 2331 continue;; 2332 }; 2333 ; 2334 // getting ROC; 2335 SSROC = ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356////////////////////////////////////////////////////////////////////////////////; 2357 ; 2358TH1F *TMVA::Factory::EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:98855,Modifiability,variab,variable,98855," for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i < VIBITS; ++i) {; 2322 if (x & (uint64_t(1) << i)) {; 2323 y = x & ~(1 << i);; 2324 std::bitset<VIBITS> ybitset(y);; 2325 // need at least one variable; 2326 // NOTE: if sub-seed is zero then is the special case; 2327 // that count in xbitset is 1; 2328 uint32_t ny = static_cast<uint32_t>( log(x - y) / 0.693147 ) ;; 2329 if (y == 0) {; 2330 importances[ny] = SROC - 0.5;; 2331 continue;; 2332 }; 2333 ; 2334 // getting ROC; 2335 SSROC = ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356////////////////////////////////////////////////////////////////////////////////; 2357 ; 2358TH1F *TMVA::Factory::EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:99599,Modifiability,variab,variable,99599,"> 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356////////////////////////////////////////////////////////////////////////////////; 2357 ; 2358TH1F *TMVA::Factory::EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 Double_t SROC, SSROC; // computed ROC value; 2380 ; 2381 x = range;; 2382 ; 2383 std::bitset<VIBITS> xbitset(x);; 2384 if (x == 0); 2385 Log() << kFATAL << ""Error: need at least one variable.""; // data loader need at least one variable; 2386 ; 2387 // creating loader for seed; 2388 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2389 ; 2390 // adding variables from seed; 2391 for (int index = 0; index < nbits; index++) {; 2392 if (xbitset[index]); 2393 seedloader->AddVariable(varNames[index], 'F');; 2394 }; 2395 ; 2396 // Loading Dataset; 2397 DataLoaderCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_stri",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:99644,Modifiability,variab,variable,99644,"ypes::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 Double_t SROC, SSROC; // computed ROC value; 2380 ; 2381 x = range;; 2382 ; 2383 std::bitset<VIBITS> xbitset(x);; 2384 if (x == 0); 2385 Log() << kFATAL << ""Error: need at least one variable.""; // data loader need at least one variable; 2386 ; 2387 // creating loader for seed; 2388 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2389 ; 2390 // adding variables from seed; 2391 for (int index = 0; index < nbits; index++) {; 2392 if (xbitset[index]); 2393 seedloader->AddVariable(varNames[index], 'F');; 2394 }; 2395 ; 2396 // Loading Dataset; 2397 DataLoaderCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2409 ; 2410 // cleaning information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);;",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:99797,Modifiability,variab,variables,99797,";; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 Double_t SROC, SSROC; // computed ROC value; 2380 ; 2381 x = range;; 2382 ; 2383 std::bitset<VIBITS> xbitset(x);; 2384 if (x == 0); 2385 Log() << kFATAL << ""Error: need at least one variable.""; // data loader need at least one variable; 2386 ; 2387 // creating loader for seed; 2388 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2389 ; 2390 // adding variables from seed; 2391 for (int index = 0; index < nbits; index++) {; 2392 if (xbitset[index]); 2393 seedloader->AddVariable(varNames[index], 'F');; 2394 }; 2395 ; 2396 // Loading Dataset; 2397 DataLoaderCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2409 ; 2410 // cleaning information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2414 delete sresults;; 2415 delete seedloader;; 2416 this->DeleteAllMethods();; 2417 fMethodsMap.clear();; 2418 ; 2419 // removing global result because it is requiring a lot of RAM for all seeds; 2420 ; 2421 for (uint32_t i = 0; i < VIBITS; ++i) {; 2422 if (x & (1 << i)) {; 2423 y = x & ~(uint64_t(1) << i);; 2424 std::bitset<VIBITS> ybitset(y);",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:101060,Modifiability,variab,variable,101060,"erCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2409 ; 2410 // cleaning information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2414 delete sresults;; 2415 delete seedloader;; 2416 this->DeleteAllMethods();; 2417 fMethodsMap.clear();; 2418 ; 2419 // removing global result because it is requiring a lot of RAM for all seeds; 2420 ; 2421 for (uint32_t i = 0; i < VIBITS; ++i) {; 2422 if (x & (1 << i)) {; 2423 y = x & ~(uint64_t(1) << i);; 2424 std::bitset<VIBITS> ybitset(y);; 2425 // need at least one variable; 2426 // NOTE: if sub-seed is zero then is the special case; 2427 // that count in xbitset is 1; 2428 uint32_t ny = static_cast<uint32_t>(log(x - y) / 0.693147);; 2429 if (y == 0) {; 2430 importances[ny] = SROC - 0.5;; 2431 continue;; 2432 }; 2433 ; 2434 // creating loader for sub-seed; 2435 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2436 // adding variables from sub-seed; 2437 for (int index = 0; index < nbits; index++) {; 2438 if (ybitset[index]); 2439 subseedloader->AddVariable(varNames[index], 'F');; 2440 }; 2441 ; 2442 // Loading Dataset; 2443 DataLoaderCopy(subseedloader, loader);; 2444 ; 2445 // Booking SubSeed; 2446 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2447 ; 2448 // Train/Test/Evaluation; 2449 TrainAllMethods();; 2450 TestAllMethods();; 2451 EvaluateAllMethods();; 2452 ; 2453 // getting ROC; 2454 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:101455,Modifiability,variab,variables,101455,"ults;; 2415 delete seedloader;; 2416 this->DeleteAllMethods();; 2417 fMethodsMap.clear();; 2418 ; 2419 // removing global result because it is requiring a lot of RAM for all seeds; 2420 ; 2421 for (uint32_t i = 0; i < VIBITS; ++i) {; 2422 if (x & (1 << i)) {; 2423 y = x & ~(uint64_t(1) << i);; 2424 std::bitset<VIBITS> ybitset(y);; 2425 // need at least one variable; 2426 // NOTE: if sub-seed is zero then is the special case; 2427 // that count in xbitset is 1; 2428 uint32_t ny = static_cast<uint32_t>(log(x - y) / 0.693147);; 2429 if (y == 0) {; 2430 importances[ny] = SROC - 0.5;; 2431 continue;; 2432 }; 2433 ; 2434 // creating loader for sub-seed; 2435 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2436 // adding variables from sub-seed; 2437 for (int index = 0; index < nbits; index++) {; 2438 if (ybitset[index]); 2439 subseedloader->AddVariable(varNames[index], 'F');; 2440 }; 2441 ; 2442 // Loading Dataset; 2443 DataLoaderCopy(subseedloader, loader);; 2444 ; 2445 // Booking SubSeed; 2446 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2447 ; 2448 // Train/Test/Evaluation; 2449 TrainAllMethods();; 2450 TestAllMethods();; 2451 EvaluateAllMethods();; 2452 ; 2453 // getting ROC; 2454 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle);; 2455 importances[ny] += SROC - SSROC;; 2456 ; 2457 // cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471//////////////////////////////////////////",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103056,Modifiability,variab,variables,103056,"/ cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(se",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103070,Modifiability,variab,variable,103070,"/ cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(se",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103691,Modifiability,variab,variable,103691,"d::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(seedloader, loader);; 2511 ; 2512 // Booking Seed; 2513 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2514 ; 2515 // Train/Test/Evaluation; 2516 TrainAllMethods();; 2517 TestAllMethods();; 2518 EvaluateAllMethods();; 2519 ; 2520 // getting ROC; 2521 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2522 // std::cout << ""Seed: n "" << n << "" x "" << x << "" xbitset:"" << xbitset << "" ROC "" << SROC << std::endl;; 2523 ; 2524 // cleaning info",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103844,Modifiability,variab,variables,103844," names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(seedloader, loader);; 2511 ; 2512 // Booking Seed; 2513 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2514 ; 2515 // Train/Test/Evaluation; 2516 TrainAllMethods();; 2517 TestAllMethods();; 2518 EvaluateAllMethods();; 2519 ; 2520 // getting ROC; 2521 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2522 // std::cout << ""Seed: n "" << n << "" x "" << x << "" xbitset:"" << xbitset << "" ROC "" << SROC << std::endl;; 2523 ; 2524 // cleaning information to process sub-seeds; 2525 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2526 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2527 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2528 delete sresults;; 2529 delete seedloader;; 2530 this->DeleteAllMethods();; 2531 fMethodsMap.clear();; 2532 ; 2533 // removing global result because it is requiring a lot of RAM for all seeds; 2534 ; 2535 for (uint32_t i = 0; i < 32",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:105210,Modifiability,variab,variable,105210," methodTitle, theOption);; 2514 ; 2515 // Train/Test/Evaluation; 2516 TrainAllMethods();; 2517 TestAllMethods();; 2518 EvaluateAllMethods();; 2519 ; 2520 // getting ROC; 2521 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2522 // std::cout << ""Seed: n "" << n << "" x "" << x << "" xbitset:"" << xbitset << "" ROC "" << SROC << std::endl;; 2523 ; 2524 // cleaning information to process sub-seeds; 2525 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2526 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2527 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2528 delete sresults;; 2529 delete seedloader;; 2530 this->DeleteAllMethods();; 2531 fMethodsMap.clear();; 2532 ; 2533 // removing global result because it is requiring a lot of RAM for all seeds; 2534 ; 2535 for (uint32_t i = 0; i < 32; ++i) {; 2536 if (x & (uint64_t(1) << i)) {; 2537 y = x & ~(1 << i);; 2538 std::bitset<32> ybitset(y);; 2539 // need at least one variable; 2540 // NOTE: if sub-seed is zero then is the special case; 2541 // that count in xbitset is 1; 2542 Double_t ny = log(x - y) / 0.693147;; 2543 if (y == 0) {; 2544 importances[ny] = SROC - 0.5;; 2545 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << ""ROC "" << 0.5 << std::endl;; 2546 continue;; 2547 }; 2548 ; 2549 // creating loader for sub-seed; 2550 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2551 // adding variables from sub-seed; 2552 for (int index = 0; index < nbits; index++) {; 2553 if (ybitset[index]); 2554 subseedloader->AddVariable(varNames[index], 'F');; 2555 }; 2556 ; 2557 // Loading Dataset; 2558 DataLoaderCopy(subseedloader, loader);; 2559 ; 2560 // Booking SubSeed; 2561 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2562 ; 2563 // Train/Test/Evaluation; 2564 TrainAllMethods();; 2565 TestAllMethods();; 2566 EvaluateAllMethods();; 2567 ; 2568 //",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:105672,Modifiability,variab,variables,105672,"();; 2531 fMethodsMap.clear();; 2532 ; 2533 // removing global result because it is requiring a lot of RAM for all seeds; 2534 ; 2535 for (uint32_t i = 0; i < 32; ++i) {; 2536 if (x & (uint64_t(1) << i)) {; 2537 y = x & ~(1 << i);; 2538 std::bitset<32> ybitset(y);; 2539 // need at least one variable; 2540 // NOTE: if sub-seed is zero then is the special case; 2541 // that count in xbitset is 1; 2542 Double_t ny = log(x - y) / 0.693147;; 2543 if (y == 0) {; 2544 importances[ny] = SROC - 0.5;; 2545 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << ""ROC "" << 0.5 << std::endl;; 2546 continue;; 2547 }; 2548 ; 2549 // creating loader for sub-seed; 2550 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2551 // adding variables from sub-seed; 2552 for (int index = 0; index < nbits; index++) {; 2553 if (ybitset[index]); 2554 subseedloader->AddVariable(varNames[index], 'F');; 2555 }; 2556 ; 2557 // Loading Dataset; 2558 DataLoaderCopy(subseedloader, loader);; 2559 ; 2560 // Booking SubSeed; 2561 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2562 ; 2563 // Train/Test/Evaluation; 2564 TrainAllMethods();; 2565 TestAllMethods();; 2566 EvaluateAllMethods();; 2567 ; 2568 // getting ROC; 2569 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle);; 2570 importances[ny] += SROC - SSROC;; 2571 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << "" x-y "" << x - y << "" "" << std::bitset<32>(x - y) <<; 2572 // "" ny "" << ny << "" SROC "" << SROC << "" SSROC "" << SSROC << "" Importance = "" << importances[ny] <<; 2573 // std::endl; cleaning information; 2574 TMVA::MethodBase *ssmethod =; 2575 dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2576 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2577 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2578 delete ssresults;; 2579 delete subseedloader;; 2580 this->DeleteAllMethods();; 2581 ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:116591,Modifiability,variab,variable,116591,"TMVA::Configurable::GetOptionsconst TString & GetOptions() constDefinition Configurable.h:84; TMVA::Configurable::LogMsgLogger & Log() constDefinition Configurable.h:122; TMVA::Configurable::fLoggerMsgLogger * fLogger! message loggerDefinition Configurable.h:128; TMVA::Configurable::CheckForUnusedOptionsvoid CheckForUnusedOptions() constchecks for unused options in option stringDefinition Configurable.cxx:270; TMVA::DataInputHandler::GetEntriesUInt_t GetEntries(const TString &name) constDefinition DataInputHandler.h:100; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::DataSetInfo::GetNVariablesUInt_t GetNVariables() constDefinition DataSetInfo.h:127; TMVA::DataSetInfo::GetNamevirtual const char * GetName() constReturns name of object.Definition DataSetInfo.h:71; TMVA::DataSetInfo::CorrelationMatrixconst TMatrixD * CorrelationMatrix(const TString &className) constDefinition DataSetInfo.cxx:197; TMVA::DataSetInfo::GetNClassesUInt_t GetNClasses() constDefinition DataSetInfo.h:155; TMVA::DataSetInfo::GetSplitOptionsconst TString & GetSplitOptions() constDefinition DataSetInfo.h:186; TMVA::DataSetInfo::GetNTargetsUInt_t GetNTargets() constDefinition DataSetInfo.h:128; TMVA::DataSetInfo::GetDataSetDataSet * GetDataSet() constreturns data setDefinition DataSetInfo.cxx:493; TMVA::DataSetInfo::CreateCorrelationMatrixHistTH2 * CreateC",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:117739,Modifiability,variab,variablesDefinition,117739,"formation.Definition DataSetInfo.h:62; TMVA::DataSetInfo::GetNVariablesUInt_t GetNVariables() constDefinition DataSetInfo.h:127; TMVA::DataSetInfo::GetNamevirtual const char * GetName() constReturns name of object.Definition DataSetInfo.h:71; TMVA::DataSetInfo::CorrelationMatrixconst TMatrixD * CorrelationMatrix(const TString &className) constDefinition DataSetInfo.cxx:197; TMVA::DataSetInfo::GetNClassesUInt_t GetNClasses() constDefinition DataSetInfo.h:155; TMVA::DataSetInfo::GetSplitOptionsconst TString & GetSplitOptions() constDefinition DataSetInfo.h:186; TMVA::DataSetInfo::GetNTargetsUInt_t GetNTargets() constDefinition DataSetInfo.h:128; TMVA::DataSetInfo::GetDataSetDataSet * GetDataSet() constreturns data setDefinition DataSetInfo.cxx:493; TMVA::DataSetInfo::CreateCorrelationMatrixHistTH2 * CreateCorrelationMatrixHist(const TMatrixD *m, const TString &hName, const TString &hTitle) constDefinition DataSetInfo.cxx:429; TMVA::DataSetInfo::GetListOfVariablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::DataSetInfo::GetClassInfoClassInfo * GetClassInfo(Int_t clNum) constDefinition DataSetInfo.cxx:146; TMVA::DataSetInfo::GetCutconst TCut & GetCut(Int_t i) constDefinition DataSetInfo.h:168; TMVA::DataSetInfo::GetVariableInfoVariableInfo & GetVariableInfo(Int_t i)Definition DataSetInfo.h:105; TMVA::DataSetInfo::IsSignalBool_t IsSignal(const Event *ev) constDefinition DataSetInfo.cxx:167; TMVA::DataSetInfo::GetDataSetManagerDataSetManager * GetDataSetManager()Definition DataSetInfo.h:194; TMVA::DataSetManager::DataInputDataInputHandler & DataInput()Definition DataSetManager.h:76; TMVA::DataSetClass that contains all the data information.Definition DataSet.h:58; TMVA::DataSet::GetNEvtSigTestLong64_t GetNEvtSigTest()return number of signal test events in datasetDefinition DataSet.cxx:427; TMVA::DataSet::GetTreeTTree * GetTree(Types::ETreeType type)create the test/trainings tree with all the variables, the ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:118685,Modifiability,variab,variables,118685,"o::GetListOfVariablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::DataSetInfo::GetClassInfoClassInfo * GetClassInfo(Int_t clNum) constDefinition DataSetInfo.cxx:146; TMVA::DataSetInfo::GetCutconst TCut & GetCut(Int_t i) constDefinition DataSetInfo.h:168; TMVA::DataSetInfo::GetVariableInfoVariableInfo & GetVariableInfo(Int_t i)Definition DataSetInfo.h:105; TMVA::DataSetInfo::IsSignalBool_t IsSignal(const Event *ev) constDefinition DataSetInfo.cxx:167; TMVA::DataSetInfo::GetDataSetManagerDataSetManager * GetDataSetManager()Definition DataSetInfo.h:194; TMVA::DataSetManager::DataInputDataInputHandler & DataInput()Definition DataSetManager.h:76; TMVA::DataSetClass that contains all the data information.Definition DataSet.h:58; TMVA::DataSet::GetNEvtSigTestLong64_t GetNEvtSigTest()return number of signal test events in datasetDefinition DataSet.cxx:427; TMVA::DataSet::GetTreeTTree * GetTree(Types::ETreeType type)create the test/trainings tree with all the variables, the weights, the classes, the targets,...Definition DataSet.cxx:609; TMVA::DataSet::GetEventconst Event * GetEvent() constreturns event without transformationsDefinition DataSet.cxx:202; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetResultsResults * GetResults(const TString &, Types::ETreeType type, Types::EAnalysisType analysistype)Definition DataSet.cxx:265; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::DataSet::SetCurrentTypevoid SetCurrentType(Types::ETreeType type) constDefinition DataSet.h:89; TMVA::DataSet::GetEventCollectionconst std::vector< Event * > & GetEventCollection(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:216; TMVA::DataSet::GetNEvtBkgdTestLong64_t GetNEvtBkgdTest()return number of background test events in datasetDefinition DataSet.cxx:435; TMVA::EventDefinit",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:119747,Modifiability,variab,variableDefinition,119747,"es, the targets,...Definition DataSet.cxx:609; TMVA::DataSet::GetEventconst Event * GetEvent() constreturns event without transformationsDefinition DataSet.cxx:202; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetResultsResults * GetResults(const TString &, Types::ETreeType type, Types::EAnalysisType analysistype)Definition DataSet.cxx:265; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::DataSet::SetCurrentTypevoid SetCurrentType(Types::ETreeType type) constDefinition DataSet.h:89; TMVA::DataSet::GetEventCollectionconst std::vector< Event * > & GetEventCollection(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:216; TMVA::DataSet::GetNEvtBkgdTestLong64_t GetNEvtBkgdTest()return number of background test events in datasetDefinition DataSet.cxx:435; TMVA::EventDefinition Event.h:51; TMVA::Event::GetValueFloat_t GetValue(UInt_t ivar) constreturn value of i'th variableDefinition Event.cxx:236; TMVA::Event::SetIsTrainingstatic void SetIsTraining(Bool_t)when this static function is called, it sets the flag whether events with negative event weight shoul...Definition Event.cxx:399; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::PrintHelpMessagevoid PrintHelpMessage(const TString &datasetname, const TString &methodTitle="""") constPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::VerboseBool_t Verbose(void) constDefinition Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:122106,Modifiability,variab,variables,122106,"tructor.Definition Factory.cxx:113; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::~Factoryvirtual ~Fac",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:132514,Modifiability,variab,variables,132514,"anager * fDataSetManagerDefinition MethodCategory.h:138; TMVA::MsgLoggerostringstream derivative to redirect and format outputDefinition MsgLogger.h:57; TMVA::MsgLogger::SetMinTypevoid SetMinType(EMsgType minType)Definition MsgLogger.h:70; TMVA::MsgLogger::SetSourcevoid SetSource(const std::string &source)Definition MsgLogger.h:68; TMVA::MsgLogger::InhibitOutputstatic void InhibitOutput()Definition MsgLogger.cxx:67; TMVA::ROCCurveDefinition ROCCurve.h:46; TMVA::ROCCurve::GetEffSForEffBDouble_t GetEffSForEffB(Double_t effB, const UInt_t num_points=41)Calculate the signal efficiency (sensitivity) for a given background efficiency (sensitivity).Definition ROCCurve.cxx:217; TMVA::ROCCurve::GetROCIntegralDouble_t GetROCIntegral(const UInt_t points=41)Calculates the ROC integral (AUC)Definition ROCCurve.cxx:248; TMVA::ROCCurve::GetROCCurveTGraph * GetROCCurve(const UInt_t points=100)Returns a new TGraph containing the ROC curve.Definition ROCCurve.cxx:274; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Ranking::Printvirtual void Print() constget maximum length of variable namesDefinition Ranking.cxx:111; TMVA::ResultsClassificationClass that is the base-class for a vector of result.Definition ResultsClassification.h:48; TMVA::ResultsMulticlassClass which takes the results of a multiclass classification.Definition ResultsMulticlass.h:55; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::Tools::FormattedOutputvoid FormattedOutput(const std::vector< Double_t > &, const std::vector< TString > &, const TString titleVars, const TString titleValues, MsgLogger &logger, TString format=""%+1.3f"")formatted output of simple tableDefinition Tools.cxx:887; TMVA::Tools::ROOTVersionMessagevoid ROOTVersionMessage(MsgLogger &logger)prints the ROOT release number and dateDefinition Tools.cxx:1325; TMVA::Tools::UsefulSortDescendingvoid UsefulSortDescending(std::vector< std::vector< Double_t > > &, std::v",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:132643,Modifiability,variab,variable,132643,"o redirect and format outputDefinition MsgLogger.h:57; TMVA::MsgLogger::SetMinTypevoid SetMinType(EMsgType minType)Definition MsgLogger.h:70; TMVA::MsgLogger::SetSourcevoid SetSource(const std::string &source)Definition MsgLogger.h:68; TMVA::MsgLogger::InhibitOutputstatic void InhibitOutput()Definition MsgLogger.cxx:67; TMVA::ROCCurveDefinition ROCCurve.h:46; TMVA::ROCCurve::GetEffSForEffBDouble_t GetEffSForEffB(Double_t effB, const UInt_t num_points=41)Calculate the signal efficiency (sensitivity) for a given background efficiency (sensitivity).Definition ROCCurve.cxx:217; TMVA::ROCCurve::GetROCIntegralDouble_t GetROCIntegral(const UInt_t points=41)Calculates the ROC integral (AUC)Definition ROCCurve.cxx:248; TMVA::ROCCurve::GetROCCurveTGraph * GetROCCurve(const UInt_t points=100)Returns a new TGraph containing the ROC curve.Definition ROCCurve.cxx:274; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Ranking::Printvirtual void Print() constget maximum length of variable namesDefinition Ranking.cxx:111; TMVA::ResultsClassificationClass that is the base-class for a vector of result.Definition ResultsClassification.h:48; TMVA::ResultsMulticlassClass which takes the results of a multiclass classification.Definition ResultsMulticlass.h:55; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::Tools::FormattedOutputvoid FormattedOutput(const std::vector< Double_t > &, const std::vector< TString > &, const TString titleVars, const TString titleValues, MsgLogger &logger, TString format=""%+1.3f"")formatted output of simple tableDefinition Tools.cxx:887; TMVA::Tools::ROOTVersionMessagevoid ROOTVersionMessage(MsgLogger &logger)prints the ROOT release number and dateDefinition Tools.cxx:1325; TMVA::Tools::UsefulSortDescendingvoid UsefulSortDescending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:135154,Modifiability,variab,variablesDefinition,135154,"urns covariance into correlation matrixDefinition Tools.cxx:324; TMVA::Tools::kHtmlLink@ kHtmlLinkDefinition Tools.h:212; TMVA::Tools::UsefulSortAscendingvoid UsefulSortAscending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:538; TMVA::Tools::TMVACitationvoid TMVACitation(MsgLogger &logger, ECitation citType=kPlainText)kinds of TMVA citationDefinition Tools.cxx:1440; TMVA::Tools::TMVAVersionMessagevoid TMVAVersionMessage(MsgLogger &logger)prints the TMVA release number and dateDefinition Tools.cxx:1316; TMVA::Tools::TMVAWelcomeMessagevoid TMVAWelcomeMessage()direct output, eg, when starting ROOT session -> no use of Logger hereDefinition Tools.cxx:1302; TMVA::TransformationHandlerClass that contains all the data information.Definition TransformationHandler.h:56; TMVA::TransformationHandler::PrintVariableRankingvoid PrintVariableRanking() constprints ranking of input variablesDefinition TransformationHandler.cxx:926; TMVA::TypesSingleton class for Global types used by TMVA.Definition Types.h:71; TMVA::Types::Instancestatic Types & Instance()The single instance of ""Types"" if existing already, or create it (Singleton)Definition Types.cxx:70; TMVA::Types::EMVAEMVADefinition Types.h:76; TMVA::Types::kCategory@ kCategoryDefinition Types.h:97; TMVA::Types::kCuts@ kCutsDefinition Types.h:78; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::kMulticlass@ kMulticlassDefinition Types.h:129; TMVA::Types::kNoAnalysisType@ kNoAnalysisTypeDefinition Types.h:130; TMVA::Types::kClassification@ kClassificationDefinition Types.h:127; TMVA::Types::kMaxAnalysisType@ kMaxAnalysisTypeDefinition Types.h:131; TMVA::Types::kRegression@ kRegressionDefinition Types.h:128; TMVA::Types::ETreeTypeETreeTypeDefinition Types.h:142; TMVA::Types::kTraining@ kTrainingDefinition Types.h:143; TMVA::Types::kTesting@ kTestingDe",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:13768,Performance,load,loader,13768,"; 326 std::map<TString, MVector *>::iterator itrMap;; 327 ; 328 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 329 MVector *methods = itrMap->second;; 330 // delete methods; 331 MVector::iterator itrMethod = methods->begin();; 332 for (; itrMethod != methods->end(); ++itrMethod) {; 333 Log() << kDEBUG << ""Delete method: "" << (*itrMethod)->GetName() << Endl;; 334 delete (*itrMethod);; 335 }; 336 methods->clear();; 337 delete methods;; 338 }; 339}; 340 ; 341////////////////////////////////////////////////////////////////////////////////; 342 ; 343void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:13901,Performance,load,loader,13901,"; 326 std::map<TString, MVector *>::iterator itrMap;; 327 ; 328 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 329 MVector *methods = itrMap->second;; 330 // delete methods; 331 MVector::iterator itrMethod = methods->begin();; 332 for (; itrMethod != methods->end(); ++itrMethod) {; 333 Log() << kDEBUG << ""Delete method: "" << (*itrMethod)->GetName() << Endl;; 334 delete (*itrMethod);; 335 }; 336 methods->clear();; 337 delete methods;; 338 }; 339}; 340 ; 341////////////////////////////////////////////////////////////////////////////////; 342 ; 343void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:13998,Performance,load,loader,13998,"; 326 std::map<TString, MVector *>::iterator itrMap;; 327 ; 328 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 329 MVector *methods = itrMap->second;; 330 // delete methods; 331 MVector::iterator itrMethod = methods->begin();; 332 for (; itrMethod != methods->end(); ++itrMethod) {; 333 Log() << kDEBUG << ""Delete method: "" << (*itrMethod)->GetName() << Endl;; 334 delete (*itrMethod);; 335 }; 336 methods->clear();; 337 delete methods;; 338 }; 339}; 340 ; 341////////////////////////////////////////////////////////////////////////////////; 342 ; 343void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:14084,Performance,load,loader,14084,"; 326 std::map<TString, MVector *>::iterator itrMap;; 327 ; 328 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 329 MVector *methods = itrMap->second;; 330 // delete methods; 331 MVector::iterator itrMethod = methods->begin();; 332 for (; itrMethod != methods->end(); ++itrMethod) {; 333 Log() << kDEBUG << ""Delete method: "" << (*itrMethod)->GetName() << Endl;; 334 delete (*itrMethod);; 335 }; 336 methods->clear();; 337 delete methods;; 338 }; 339}; 340 ; 341////////////////////////////////////////////////////////////////////////////////; 342 ; 343void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:14131,Performance,load,loader,14131,"od = methods->begin();; 332 for (; itrMethod != methods->end(); ++itrMethod) {; 333 Log() << kDEBUG << ""Delete method: "" << (*itrMethod)->GetName() << Endl;; 334 delete (*itrMethod);; 335 }; 336 methods->clear();; 337 delete methods;; 338 }; 339}; 340 ; 341////////////////////////////////////////////////////////////////////////////////; 342 ; 343void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()) {; 374 if (GetMethod(datasetname, methodTitle) != 0) {; 375 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 376 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;;",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:14195,Performance,load,loader,14195,">end(); ++itrMethod) {; 333 Log() << kDEBUG << ""Delete method: "" << (*itrMethod)->GetName() << Endl;; 334 delete (*itrMethod);; 335 }; 336 methods->clear();; 337 delete methods;; 338 }; 339}; 340 ; 341////////////////////////////////////////////////////////////////////////////////; 342 ; 343void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()) {; 374 if (GetMethod(datasetname, methodTitle) != 0) {; 375 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 376 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 377 }; 378 }; 379 ; 380 Log() << kHEADER << ""Booking m",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:14348,Performance,load,loader,14348,"lete (*itrMethod);; 335 }; 336 methods->clear();; 337 delete methods;; 338 }; 339}; 340 ; 341////////////////////////////////////////////////////////////////////////////////; 342 ; 343void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()) {; 374 if (GetMethod(datasetname, methodTitle) != 0) {; 375 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 376 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 377 }; 378 }; 379 ; 380 Log() << kHEADER << ""Booking method: "" << gTools().Color(""bold""); 381 << methodTitle; 382 // << gTools().Color(""reset"")<<"" DataSet Name: ""<",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:14558,Performance,load,loader,14558,"43void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()) {; 374 if (GetMethod(datasetname, methodTitle) != 0) {; 375 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 376 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 377 }; 378 }; 379 ; 380 Log() << kHEADER << ""Booking method: "" << gTools().Color(""bold""); 381 << methodTitle; 382 // << gTools().Color(""reset"")<<"" DataSet Name: ""<<gTools().Color(""bold"")<<loader->GetName(); 383 << gTools().Color(""reset"") << Endl << Endl;; 384 ; 385 // interpret option string with respect to a request for boosting (i.e., BostN",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:14624,Performance,load,loader,14624,"//////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()) {; 374 if (GetMethod(datasetname, methodTitle) != 0) {; 375 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 376 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 377 }; 378 }; 379 ; 380 Log() << kHEADER << ""Booking method: "" << gTools().Color(""bold""); 381 << methodTitle; 382 // << gTools().Color(""reset"")<<"" DataSet Name: ""<<gTools().Color(""bold"")<<loader->GetName(); 383 << gTools().Color(""reset"") << Endl << Endl;; 384 ; 385 // interpret option string with respect to a request for boosting (i.e., BostNum > 0); 386 Int_t boostNum = 0;; 387 TMVA::Configurable *conf = new TMVA::Configurable(theOption);; 388 conf->DeclareOptionRef(boostNum =",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:15098,Performance,load,loader,15098,"es::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()) {; 374 if (GetMethod(datasetname, methodTitle) != 0) {; 375 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 376 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 377 }; 378 }; 379 ; 380 Log() << kHEADER << ""Booking method: "" << gTools().Color(""bold""); 381 << methodTitle; 382 // << gTools().Color(""reset"")<<"" DataSet Name: ""<<gTools().Color(""bold"")<<loader->GetName(); 383 << gTools().Color(""reset"") << Endl << Endl;; 384 ; 385 // interpret option string with respect to a request for boosting (i.e., BostNum > 0); 386 Int_t boostNum = 0;; 387 TMVA::Configurable *conf = new TMVA::Configurable(theOption);; 388 conf->DeclareOptionRef(boostNum = 0, ""Boost_num"", ""Number of times the classifier will be boosted"");; 389 conf->ParseOptions();; 390 delete conf;; 391 // this is name of weight file directory; 392 TString fileDir;; 393 if (fModelPersistence) {; 394 // find prefix in fWeightFileDir;; 395 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 396 fileDir = prefix;; 397 if (!prefix.IsNull()); 398 if (fileDir[fileDir.Length() - 1] != '/'); 399 fileDir += ""/"";; ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:15322,Performance,load,loader,15322," 363 } else if (loader->GetDataSetInfo().GetNClasses() >= 2) {; 364 fAnalysisType = Types::kMulticlass; // if two classes, but not named ""Signal"" and ""Background""; 365 } else; 366 Log() << kFATAL << ""No analysis type for "" << loader->GetDataSetInfo().GetNClasses() << "" classes and ""; 367 << loader->GetDataSetInfo().GetNTargets() << "" regression targets."" << Endl;; 368 }; 369 ; 370 // booking via name; the names are translated into enums and the; 371 // corresponding overloaded BookMethod is called; 372 ; 373 if (fMethodsMap.find(datasetname) != fMethodsMap.end()) {; 374 if (GetMethod(datasetname, methodTitle) != 0) {; 375 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 376 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 377 }; 378 }; 379 ; 380 Log() << kHEADER << ""Booking method: "" << gTools().Color(""bold""); 381 << methodTitle; 382 // << gTools().Color(""reset"")<<"" DataSet Name: ""<<gTools().Color(""bold"")<<loader->GetName(); 383 << gTools().Color(""reset"") << Endl << Endl;; 384 ; 385 // interpret option string with respect to a request for boosting (i.e., BostNum > 0); 386 Int_t boostNum = 0;; 387 TMVA::Configurable *conf = new TMVA::Configurable(theOption);; 388 conf->DeclareOptionRef(boostNum = 0, ""Boost_num"", ""Number of times the classifier will be boosted"");; 389 conf->ParseOptions();; 390 delete conf;; 391 // this is name of weight file directory; 392 TString fileDir;; 393 if (fModelPersistence) {; 394 // find prefix in fWeightFileDir;; 395 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 396 fileDir = prefix;; 397 if (!prefix.IsNull()); 398 if (fileDir[fileDir.Length() - 1] != '/'); 399 fileDir += ""/"";; 400 fileDir += loader->GetName();; 401 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 402 }; 403 // initialize methods; 404 IMethod *im;; 405 if (!boostNum) {; 406 im = ClassifierFactory::Instance().Create(theMethodName.Data(), fJobName, methodTitle, loader->GetDataSetI",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:16067,Performance,load,loader,16067,"376 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 377 }; 378 }; 379 ; 380 Log() << kHEADER << ""Booking method: "" << gTools().Color(""bold""); 381 << methodTitle; 382 // << gTools().Color(""reset"")<<"" DataSet Name: ""<<gTools().Color(""bold"")<<loader->GetName(); 383 << gTools().Color(""reset"") << Endl << Endl;; 384 ; 385 // interpret option string with respect to a request for boosting (i.e., BostNum > 0); 386 Int_t boostNum = 0;; 387 TMVA::Configurable *conf = new TMVA::Configurable(theOption);; 388 conf->DeclareOptionRef(boostNum = 0, ""Boost_num"", ""Number of times the classifier will be boosted"");; 389 conf->ParseOptions();; 390 delete conf;; 391 // this is name of weight file directory; 392 TString fileDir;; 393 if (fModelPersistence) {; 394 // find prefix in fWeightFileDir;; 395 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 396 fileDir = prefix;; 397 if (!prefix.IsNull()); 398 if (fileDir[fileDir.Length() - 1] != '/'); 399 fileDir += ""/"";; 400 fileDir += loader->GetName();; 401 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 402 }; 403 // initialize methods; 404 IMethod *im;; 405 if (!boostNum) {; 406 im = ClassifierFactory::Instance().Create(theMethodName.Data(), fJobName, methodTitle, loader->GetDataSetInfo(),; 407 theOption);; 408 } else {; 409 // boosted classifier, requires a specific definition, making it transparent for the user; 410 Log() << kDEBUG << ""Boost Number is "" << boostNum << "" > 0: train boosted classifier"" << Endl;; 411 im = ClassifierFactory::Instance().Create(""Boost"", fJobName, methodTitle, loader->GetDataSetInfo(), theOption);; 412 MethodBoost *methBoost = dynamic_cast<MethodBoost *>(im); // DSMTEST divided into two lines; 413 if (!methBoost) { // DSMTEST; 414 Log() << kFATAL << ""Method with type kBoost cannot be casted to MethodCategory. /Factory"" << Endl; // DSMTEST; 415 return nullptr;; 416 }; 417 if (fModelPersistence); 418 methBoost->SetWeightFileDir(fileDir);; 419 methBoost->SetModelPersistence(",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:16313,Performance,load,loader,16313,"a request for boosting (i.e., BostNum > 0); 386 Int_t boostNum = 0;; 387 TMVA::Configurable *conf = new TMVA::Configurable(theOption);; 388 conf->DeclareOptionRef(boostNum = 0, ""Boost_num"", ""Number of times the classifier will be boosted"");; 389 conf->ParseOptions();; 390 delete conf;; 391 // this is name of weight file directory; 392 TString fileDir;; 393 if (fModelPersistence) {; 394 // find prefix in fWeightFileDir;; 395 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 396 fileDir = prefix;; 397 if (!prefix.IsNull()); 398 if (fileDir[fileDir.Length() - 1] != '/'); 399 fileDir += ""/"";; 400 fileDir += loader->GetName();; 401 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 402 }; 403 // initialize methods; 404 IMethod *im;; 405 if (!boostNum) {; 406 im = ClassifierFactory::Instance().Create(theMethodName.Data(), fJobName, methodTitle, loader->GetDataSetInfo(),; 407 theOption);; 408 } else {; 409 // boosted classifier, requires a specific definition, making it transparent for the user; 410 Log() << kDEBUG << ""Boost Number is "" << boostNum << "" > 0: train boosted classifier"" << Endl;; 411 im = ClassifierFactory::Instance().Create(""Boost"", fJobName, methodTitle, loader->GetDataSetInfo(), theOption);; 412 MethodBoost *methBoost = dynamic_cast<MethodBoost *>(im); // DSMTEST divided into two lines; 413 if (!methBoost) { // DSMTEST; 414 Log() << kFATAL << ""Method with type kBoost cannot be casted to MethodCategory. /Factory"" << Endl; // DSMTEST; 415 return nullptr;; 416 }; 417 if (fModelPersistence); 418 methBoost->SetWeightFileDir(fileDir);; 419 methBoost->SetModelPersistence(fModelPersistence);; 420 methBoost->SetBoostedMethodName(theMethodName); // DSMTEST divided into two lines; 421 methBoost->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 422 methBoost->SetFile(fgTargetFile);; 423 methBoost->SetSilentFile(IsSilentFile());; 424 }; 425 ; 426 MethodBase *method = dynamic_cast<MethodBase *>(im);; 427 if (method == 0); 428",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:16644,Performance,load,loader,16644,"f weight file directory; 392 TString fileDir;; 393 if (fModelPersistence) {; 394 // find prefix in fWeightFileDir;; 395 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 396 fileDir = prefix;; 397 if (!prefix.IsNull()); 398 if (fileDir[fileDir.Length() - 1] != '/'); 399 fileDir += ""/"";; 400 fileDir += loader->GetName();; 401 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 402 }; 403 // initialize methods; 404 IMethod *im;; 405 if (!boostNum) {; 406 im = ClassifierFactory::Instance().Create(theMethodName.Data(), fJobName, methodTitle, loader->GetDataSetInfo(),; 407 theOption);; 408 } else {; 409 // boosted classifier, requires a specific definition, making it transparent for the user; 410 Log() << kDEBUG << ""Boost Number is "" << boostNum << "" > 0: train boosted classifier"" << Endl;; 411 im = ClassifierFactory::Instance().Create(""Boost"", fJobName, methodTitle, loader->GetDataSetInfo(), theOption);; 412 MethodBoost *methBoost = dynamic_cast<MethodBoost *>(im); // DSMTEST divided into two lines; 413 if (!methBoost) { // DSMTEST; 414 Log() << kFATAL << ""Method with type kBoost cannot be casted to MethodCategory. /Factory"" << Endl; // DSMTEST; 415 return nullptr;; 416 }; 417 if (fModelPersistence); 418 methBoost->SetWeightFileDir(fileDir);; 419 methBoost->SetModelPersistence(fModelPersistence);; 420 methBoost->SetBoostedMethodName(theMethodName); // DSMTEST divided into two lines; 421 methBoost->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 422 methBoost->SetFile(fgTargetFile);; 423 methBoost->SetSilentFile(IsSilentFile());; 424 }; 425 ; 426 MethodBase *method = dynamic_cast<MethodBase *>(im);; 427 if (method == 0); 428 return 0; // could not create method; 429 ; 430 // set fDataSetManager if MethodCategory (to enable Category to create datasetinfo objects) // DSMTEST; 431 if (method->GetMethodType() == Types::kCategory) { // DSMTEST; 432 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(im)); // DSMTEST; 433 if ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:17204,Performance,load,loader,17204," += loader->GetName();; 401 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 402 }; 403 // initialize methods; 404 IMethod *im;; 405 if (!boostNum) {; 406 im = ClassifierFactory::Instance().Create(theMethodName.Data(), fJobName, methodTitle, loader->GetDataSetInfo(),; 407 theOption);; 408 } else {; 409 // boosted classifier, requires a specific definition, making it transparent for the user; 410 Log() << kDEBUG << ""Boost Number is "" << boostNum << "" > 0: train boosted classifier"" << Endl;; 411 im = ClassifierFactory::Instance().Create(""Boost"", fJobName, methodTitle, loader->GetDataSetInfo(), theOption);; 412 MethodBoost *methBoost = dynamic_cast<MethodBoost *>(im); // DSMTEST divided into two lines; 413 if (!methBoost) { // DSMTEST; 414 Log() << kFATAL << ""Method with type kBoost cannot be casted to MethodCategory. /Factory"" << Endl; // DSMTEST; 415 return nullptr;; 416 }; 417 if (fModelPersistence); 418 methBoost->SetWeightFileDir(fileDir);; 419 methBoost->SetModelPersistence(fModelPersistence);; 420 methBoost->SetBoostedMethodName(theMethodName); // DSMTEST divided into two lines; 421 methBoost->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 422 methBoost->SetFile(fgTargetFile);; 423 methBoost->SetSilentFile(IsSilentFile());; 424 }; 425 ; 426 MethodBase *method = dynamic_cast<MethodBase *>(im);; 427 if (method == 0); 428 return 0; // could not create method; 429 ; 430 // set fDataSetManager if MethodCategory (to enable Category to create datasetinfo objects) // DSMTEST; 431 if (method->GetMethodType() == Types::kCategory) { // DSMTEST; 432 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(im)); // DSMTEST; 433 if (!methCat) { // DSMTEST; 434 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory""; 435 << Endl; // DSMTEST; 436 return nullptr;; 437 }; 438 if (fModelPersistence); 439 methCat->SetWeightFileDir(fileDir);; 440 methCat->SetModelPersistence(fModelPersistence);; 441 methCat->",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:18081,Performance,load,loader,18081," 418 methBoost->SetWeightFileDir(fileDir);; 419 methBoost->SetModelPersistence(fModelPersistence);; 420 methBoost->SetBoostedMethodName(theMethodName); // DSMTEST divided into two lines; 421 methBoost->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 422 methBoost->SetFile(fgTargetFile);; 423 methBoost->SetSilentFile(IsSilentFile());; 424 }; 425 ; 426 MethodBase *method = dynamic_cast<MethodBase *>(im);; 427 if (method == 0); 428 return 0; // could not create method; 429 ; 430 // set fDataSetManager if MethodCategory (to enable Category to create datasetinfo objects) // DSMTEST; 431 if (method->GetMethodType() == Types::kCategory) { // DSMTEST; 432 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(im)); // DSMTEST; 433 if (!methCat) { // DSMTEST; 434 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory""; 435 << Endl; // DSMTEST; 436 return nullptr;; 437 }; 438 if (fModelPersistence); 439 methCat->SetWeightFileDir(fileDir);; 440 methCat->SetModelPersistence(fModelPersistence);; 441 methCat->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 442 methCat->SetFile(fgTargetFile);; 443 methCat->SetSilentFile(IsSilentFile());; 444 } // DSMTEST; 445 ; 446 if (!method->HasAnalysisType(fAnalysisType, loader->GetDataSetInfo().GetNClasses(),; 447 loader->GetDataSetInfo().GetNTargets())) {; 448 Log() << kWARNING << ""Method "" << method->GetMethodTypeName() << "" is not capable of handling "";; 449 if (fAnalysisType == Types::kRegression) {; 450 Log() << ""regression with "" << loader->GetDataSetInfo().GetNTargets() << "" targets."" << Endl;; 451 } else if (fAnalysisType == Types::kMulticlass) {; 452 Log() << ""multiclass classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 453 } else {; 454 Log() << ""classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 455 }; 456 return 0;; 457 }; 458 ; 459 if (fModelPersistence); 460 ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:18293,Performance,load,loader,18293,"GetDataSetInfo().GetDataSetManager(); // DSMTEST; 422 methBoost->SetFile(fgTargetFile);; 423 methBoost->SetSilentFile(IsSilentFile());; 424 }; 425 ; 426 MethodBase *method = dynamic_cast<MethodBase *>(im);; 427 if (method == 0); 428 return 0; // could not create method; 429 ; 430 // set fDataSetManager if MethodCategory (to enable Category to create datasetinfo objects) // DSMTEST; 431 if (method->GetMethodType() == Types::kCategory) { // DSMTEST; 432 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(im)); // DSMTEST; 433 if (!methCat) { // DSMTEST; 434 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory""; 435 << Endl; // DSMTEST; 436 return nullptr;; 437 }; 438 if (fModelPersistence); 439 methCat->SetWeightFileDir(fileDir);; 440 methCat->SetModelPersistence(fModelPersistence);; 441 methCat->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 442 methCat->SetFile(fgTargetFile);; 443 methCat->SetSilentFile(IsSilentFile());; 444 } // DSMTEST; 445 ; 446 if (!method->HasAnalysisType(fAnalysisType, loader->GetDataSetInfo().GetNClasses(),; 447 loader->GetDataSetInfo().GetNTargets())) {; 448 Log() << kWARNING << ""Method "" << method->GetMethodTypeName() << "" is not capable of handling "";; 449 if (fAnalysisType == Types::kRegression) {; 450 Log() << ""regression with "" << loader->GetDataSetInfo().GetNTargets() << "" targets."" << Endl;; 451 } else if (fAnalysisType == Types::kMulticlass) {; 452 Log() << ""multiclass classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 453 } else {; 454 Log() << ""classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 455 }; 456 return 0;; 457 }; 458 ; 459 if (fModelPersistence); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 meth",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:18338,Performance,load,loader,18338,"le());; 424 }; 425 ; 426 MethodBase *method = dynamic_cast<MethodBase *>(im);; 427 if (method == 0); 428 return 0; // could not create method; 429 ; 430 // set fDataSetManager if MethodCategory (to enable Category to create datasetinfo objects) // DSMTEST; 431 if (method->GetMethodType() == Types::kCategory) { // DSMTEST; 432 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(im)); // DSMTEST; 433 if (!methCat) { // DSMTEST; 434 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory""; 435 << Endl; // DSMTEST; 436 return nullptr;; 437 }; 438 if (fModelPersistence); 439 methCat->SetWeightFileDir(fileDir);; 440 methCat->SetModelPersistence(fModelPersistence);; 441 methCat->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 442 methCat->SetFile(fgTargetFile);; 443 methCat->SetSilentFile(IsSilentFile());; 444 } // DSMTEST; 445 ; 446 if (!method->HasAnalysisType(fAnalysisType, loader->GetDataSetInfo().GetNClasses(),; 447 loader->GetDataSetInfo().GetNTargets())) {; 448 Log() << kWARNING << ""Method "" << method->GetMethodTypeName() << "" is not capable of handling "";; 449 if (fAnalysisType == Types::kRegression) {; 450 Log() << ""regression with "" << loader->GetDataSetInfo().GetNTargets() << "" targets."" << Endl;; 451 } else if (fAnalysisType == Types::kMulticlass) {; 452 Log() << ""multiclass classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 453 } else {; 454 Log() << ""classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 455 }; 456 return 0;; 457 }; 458 ; 459 if (fModelPersistence); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may b",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:18567,Performance,load,loader,18567,"thod; 429 ; 430 // set fDataSetManager if MethodCategory (to enable Category to create datasetinfo objects) // DSMTEST; 431 if (method->GetMethodType() == Types::kCategory) { // DSMTEST; 432 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(im)); // DSMTEST; 433 if (!methCat) { // DSMTEST; 434 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory""; 435 << Endl; // DSMTEST; 436 return nullptr;; 437 }; 438 if (fModelPersistence); 439 methCat->SetWeightFileDir(fileDir);; 440 methCat->SetModelPersistence(fModelPersistence);; 441 methCat->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 442 methCat->SetFile(fgTargetFile);; 443 methCat->SetSilentFile(IsSilentFile());; 444 } // DSMTEST; 445 ; 446 if (!method->HasAnalysisType(fAnalysisType, loader->GetDataSetInfo().GetNClasses(),; 447 loader->GetDataSetInfo().GetNTargets())) {; 448 Log() << kWARNING << ""Method "" << method->GetMethodTypeName() << "" is not capable of handling "";; 449 if (fAnalysisType == Types::kRegression) {; 450 Log() << ""regression with "" << loader->GetDataSetInfo().GetNTargets() << "" targets."" << Endl;; 451 } else if (fAnalysisType == Types::kMulticlass) {; 452 Log() << ""multiclass classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 453 } else {; 454 Log() << ""classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 455 }; 456 return 0;; 457 }; 458 ; 459 if (fModelPersistence); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVec",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:18736,Performance,load,loader,18736,"t = (dynamic_cast<MethodCategory *>(im)); // DSMTEST; 433 if (!methCat) { // DSMTEST; 434 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory""; 435 << Endl; // DSMTEST; 436 return nullptr;; 437 }; 438 if (fModelPersistence); 439 methCat->SetWeightFileDir(fileDir);; 440 methCat->SetModelPersistence(fModelPersistence);; 441 methCat->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 442 methCat->SetFile(fgTargetFile);; 443 methCat->SetSilentFile(IsSilentFile());; 444 } // DSMTEST; 445 ; 446 if (!method->HasAnalysisType(fAnalysisType, loader->GetDataSetInfo().GetNClasses(),; 447 loader->GetDataSetInfo().GetNTargets())) {; 448 Log() << kWARNING << ""Method "" << method->GetMethodTypeName() << "" is not capable of handling "";; 449 if (fAnalysisType == Types::kRegression) {; 450 Log() << ""regression with "" << loader->GetDataSetInfo().GetNTargets() << "" targets."" << Endl;; 451 } else if (fAnalysisType == Types::kMulticlass) {; 452 Log() << ""multiclass classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 453 } else {; 454 Log() << ""classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 455 }; 456 return 0;; 457 }; 458 ; 459 if (fModelPersistence); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480//////////////////////////////////////////////////",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:18853,Performance,load,loader,18853," be casted to MethodCategory. /Factory""; 435 << Endl; // DSMTEST; 436 return nullptr;; 437 }; 438 if (fModelPersistence); 439 methCat->SetWeightFileDir(fileDir);; 440 methCat->SetModelPersistence(fModelPersistence);; 441 methCat->fDataSetManager = loader->GetDataSetInfo().GetDataSetManager(); // DSMTEST; 442 methCat->SetFile(fgTargetFile);; 443 methCat->SetSilentFile(IsSilentFile());; 444 } // DSMTEST; 445 ; 446 if (!method->HasAnalysisType(fAnalysisType, loader->GetDataSetInfo().GetNClasses(),; 447 loader->GetDataSetInfo().GetNTargets())) {; 448 Log() << kWARNING << ""Method "" << method->GetMethodTypeName() << "" is not capable of handling "";; 449 if (fAnalysisType == Types::kRegression) {; 450 Log() << ""regression with "" << loader->GetDataSetInfo().GetNTargets() << "" targets."" << Endl;; 451 } else if (fAnalysisType == Types::kMulticlass) {; 452 Log() << ""multiclass classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 453 } else {; 454 Log() << ""classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 455 }; 456 return 0;; 457 }; 458 ; 459 if (fModelPersistence); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theN",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:19325,Performance,perform,performed,19325,"etFile);; 443 methCat->SetSilentFile(IsSilentFile());; 444 } // DSMTEST; 445 ; 446 if (!method->HasAnalysisType(fAnalysisType, loader->GetDataSetInfo().GetNClasses(),; 447 loader->GetDataSetInfo().GetNTargets())) {; 448 Log() << kWARNING << ""Method "" << method->GetMethodTypeName() << "" is not capable of handling "";; 449 if (fAnalysisType == Types::kRegression) {; 450 Log() << ""regression with "" << loader->GetDataSetInfo().GetNTargets() << "" targets."" << Endl;; 451 } else if (fAnalysisType == Types::kMulticlass) {; 452 Log() << ""multiclass classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 453 } else {; 454 Log() << ""classification with "" << loader->GetDataSetInfo().GetNClasses() << "" classes."" << Endl;; 455 }; 456 return 0;; 457 }; 458 ; 459 if (fModelPersistence); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 retu",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:19965,Performance,perform,performance,19965,"e); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = Classifie",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:20080,Performance,load,loader,20080,"e); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = Classifie",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:20180,Performance,load,loader,20180,"e); 460 method->SetWeightFileDir(fileDir);; 461 method->SetModelPersistence(fModelPersistence);; 462 method->SetAnalysisType(fAnalysisType);; 463 method->SetupMethod();; 464 method->ParseOptions();; 465 method->ProcessSetup();; 466 method->SetFile(fgTargetFile);; 467 method->SetSilentFile(IsSilentFile());; 468 ; 469 // check-for-unused-options is performed; may be overridden by derived classes; 470 method->CheckSetup();; 471 ; 472 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 473 MVector *mvector = new MVector;; 474 fMethodsMap[datasetname] = mvector;; 475 }; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = Classifie",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:20553,Performance,load,loading,20553,"; 476 fMethodsMap[datasetname]->push_back(method);; 477 return method;; 478}; 479 ; 480////////////////////////////////////////////////////////////////////////////////; 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = ClassifierFactory::Instance().Create(methodTypeName, dsi, weightfile);; 508 MethodBase *method = (dynamic_cast<MethodBase *>(im));; 509 ; 510 if (method == nullptr); 511 return nullptr;; 512 ; 513 if (method->GetMethodType() == Types::kCategory) {; 514 Log() << kERROR << ""Cannot handle category methods for now."" << Endl;; 515 }; 516 ; 517 TString fileDir;; 518 if (fModelPersistence) {; 519 // find prefix in fWeightFileDir;; 520 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 521 fileDir = prefix;; 522 if (!prefix.IsNull()); 523 if (fileDir[fileDir.Length() - 1] != ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:20676,Performance,load,loader,20676," 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = ClassifierFactory::Instance().Create(methodTypeName, dsi, weightfile);; 508 MethodBase *method = (dynamic_cast<MethodBase *>(im));; 509 ; 510 if (method == nullptr); 511 return nullptr;; 512 ; 513 if (method->GetMethodType() == Types::kCategory) {; 514 Log() << kERROR << ""Cannot handle category methods for now."" << Endl;; 515 }; 516 ; 517 TString fileDir;; 518 if (fModelPersistence) {; 519 // find prefix in fWeightFileDir;; 520 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 521 fileDir = prefix;; 522 if (!prefix.IsNull()); 523 if (fileDir[fileDir.Length() - 1] != '/'); 524 fileDir += ""/"";; 525 fileDir = loader->GetName();; 526 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 527 }; 528 ; 529 if (fModelPersistence); 530 me",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:20774,Performance,load,loader,20774," 481/// Books MVA method. The option configuration string is custom for each MVA; 482/// the TString field ""theNameAppendix"" serves to define (and distinguish); 483/// several instances of a given MVA, eg, when one wants to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = ClassifierFactory::Instance().Create(methodTypeName, dsi, weightfile);; 508 MethodBase *method = (dynamic_cast<MethodBase *>(im));; 509 ; 510 if (method == nullptr); 511 return nullptr;; 512 ; 513 if (method->GetMethodType() == Types::kCategory) {; 514 Log() << kERROR << ""Cannot handle category methods for now."" << Endl;; 515 }; 516 ; 517 TString fileDir;; 518 if (fModelPersistence) {; 519 // find prefix in fWeightFileDir;; 520 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 521 fileDir = prefix;; 522 if (!prefix.IsNull()); 523 if (fileDir[fileDir.Length() - 1] != '/'); 524 fileDir += ""/"";; 525 fileDir = loader->GetName();; 526 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 527 }; 528 ; 529 if (fModelPersistence); 530 me",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:20916,Performance,load,loader,20916,"s to compare the; 484/// performance of various configurations; 485 ; 486TMVA::MethodBase *; 487TMVA::Factory::BookMethod(TMVA::DataLoader *loader, Types::EMVA theMethod, TString methodTitle, TString theOption); 488{; 489 return BookMethod(loader, Types::Instance().GetMethodName(theMethod), methodTitle, theOption);; 490}; 491 ; 492////////////////////////////////////////////////////////////////////////////////; 493/// Adds an already constructed method to be managed by this factory.; 494///; 495/// \note Private.; 496/// \note Know what you are doing when using this method. The method that you; 497/// are loading could be trained already.; 498///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = ClassifierFactory::Instance().Create(methodTypeName, dsi, weightfile);; 508 MethodBase *method = (dynamic_cast<MethodBase *>(im));; 509 ; 510 if (method == nullptr); 511 return nullptr;; 512 ; 513 if (method->GetMethodType() == Types::kCategory) {; 514 Log() << kERROR << ""Cannot handle category methods for now."" << Endl;; 515 }; 516 ; 517 TString fileDir;; 518 if (fModelPersistence) {; 519 // find prefix in fWeightFileDir;; 520 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 521 fileDir = prefix;; 522 if (!prefix.IsNull()); 523 if (fileDir[fileDir.Length() - 1] != '/'); 524 fileDir += ""/"";; 525 fileDir = loader->GetName();; 526 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 527 }; 528 ; 529 if (fModelPersistence); 530 method->SetWeightFileDir(fileDir);; 531 method->SetModelPersistence(fModelPersistence);; 532 method->SetAnalysisType(fAnalysisType);; 533 method->SetupMethod();; 534 method->SetFile(fgTargetFile);; 535 method->SetSilentF",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:21594,Performance,load,loader,21594,"8///; 499 ; 500TMVA::MethodBase *; 501TMVA::Factory::BookMethodWeightfile(DataLoader *loader, TMVA::Types::EMVA methodType, const TString &weightfile); 502{; 503 TString datasetname = loader->GetName();; 504 std::string methodTypeName = std::string(Types::Instance().GetMethodName(methodType).Data());; 505 DataSetInfo &dsi = loader->GetDataSetInfo();; 506 ; 507 IMethod *im = ClassifierFactory::Instance().Create(methodTypeName, dsi, weightfile);; 508 MethodBase *method = (dynamic_cast<MethodBase *>(im));; 509 ; 510 if (method == nullptr); 511 return nullptr;; 512 ; 513 if (method->GetMethodType() == Types::kCategory) {; 514 Log() << kERROR << ""Cannot handle category methods for now."" << Endl;; 515 }; 516 ; 517 TString fileDir;; 518 if (fModelPersistence) {; 519 // find prefix in fWeightFileDir;; 520 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 521 fileDir = prefix;; 522 if (!prefix.IsNull()); 523 if (fileDir[fileDir.Length() - 1] != '/'); 524 fileDir += ""/"";; 525 fileDir = loader->GetName();; 526 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 527 }; 528 ; 529 if (fModelPersistence); 530 method->SetWeightFileDir(fileDir);; 531 method->SetModelPersistence(fModelPersistence);; 532 method->SetAnalysisType(fAnalysisType);; 533 method->SetupMethod();; 534 method->SetFile(fgTargetFile);; 535 method->SetSilentFile(IsSilentFile());; 536 ; 537 method->DeclareCompatibilityOptions();; 538 ; 539 // read weight file; 540 method->ReadStateFromFile();; 541 ; 542 // method->CheckSetup();; 543 ; 544 TString methodTitle = method->GetName();; 545 if (HasMethod(datasetname, methodTitle) != 0) {; 546 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 547 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 548 }; 549 ; 550 Log() << kINFO << ""Booked classifier \"""" << method->GetMethodName() << ""\"" of type: \""""; 551 << method->GetMethodTypeName() << ""\"""" << Endl;; 552 ; 553 if (fMethodsMap.count(datase",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:22358,Performance,load,loader,22358,"d == nullptr); 511 return nullptr;; 512 ; 513 if (method->GetMethodType() == Types::kCategory) {; 514 Log() << kERROR << ""Cannot handle category methods for now."" << Endl;; 515 }; 516 ; 517 TString fileDir;; 518 if (fModelPersistence) {; 519 // find prefix in fWeightFileDir;; 520 TString prefix = gConfig().GetIONames().fWeightFileDirPrefix;; 521 fileDir = prefix;; 522 if (!prefix.IsNull()); 523 if (fileDir[fileDir.Length() - 1] != '/'); 524 fileDir += ""/"";; 525 fileDir = loader->GetName();; 526 fileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 527 }; 528 ; 529 if (fModelPersistence); 530 method->SetWeightFileDir(fileDir);; 531 method->SetModelPersistence(fModelPersistence);; 532 method->SetAnalysisType(fAnalysisType);; 533 method->SetupMethod();; 534 method->SetFile(fgTargetFile);; 535 method->SetSilentFile(IsSilentFile());; 536 ; 537 method->DeclareCompatibilityOptions();; 538 ; 539 // read weight file; 540 method->ReadStateFromFile();; 541 ; 542 // method->CheckSetup();; 543 ; 544 TString methodTitle = method->GetName();; 545 if (HasMethod(datasetname, methodTitle) != 0) {; 546 Log() << kFATAL << ""Booking failed since method with title <"" << methodTitle << ""> already exists ""; 547 << ""in with DataSet Name <"" << loader->GetName() << ""> "" << Endl;; 548 }; 549 ; 550 Log() << kINFO << ""Booked classifier \"""" << method->GetMethodName() << ""\"" of type: \""""; 551 << method->GetMethodTypeName() << ""\"""" << Endl;; 552 ; 553 if (fMethodsMap.count(datasetname) == 0) {; 554 MVector *mvector = new MVector;; 555 fMethodsMap[datasetname] = mvector;; 556 }; 557 ; 558 fMethodsMap[datasetname]->push_back(method);; 559 ; 560 return method;; 561}; 562 ; 563////////////////////////////////////////////////////////////////////////////////; 564/// Returns pointer to MVA that corresponds to given method title.; 565 ; 566TMVA::IMethod *TMVA::Factory::GetMethod(const TString &datasetname, const TString &methodTitle) const; 567{; 568 if (fMethodsMap.find(datasetname) == fMethodsMap.end())",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:24599,Performance,load,loader,24599," is defined for a given dataset.; 585 ; 586Bool_t TMVA::Factory::HasMethod(const TString &datasetname, const TString &methodTitle) const; 587{; 588 if (fMethodsMap.find(datasetname) == fMethodsMap.end()); 589 return 0;; 590 ; 591 std::string methodName = methodTitle.Data();; 592 auto isEqualToMethodName = [&methodName](TMVA::IMethod *m) { return (0 == methodName.compare(m->GetName())); };; 593 ; 594 TMVA::Factory::MVector *methods = this->fMethodsMap.at(datasetname);; 595 Bool_t isMethodNameExisting = std::any_of(methods->begin(), methods->end(), isEqualToMethodName);; 596 ; 597 return isMethodNameExisting;; 598}; 599 ; 600////////////////////////////////////////////////////////////////////////////////; 601 ; 602void TMVA::Factory::WriteDataInformation(DataSetInfo &fDataSetInfo); 603{; 604 RootBaseDir()->cd();; 605 ; 606 if (!RootBaseDir()->GetDirectory(fDataSetInfo.GetName())); 607 RootBaseDir()->mkdir(fDataSetInfo.GetName());; 608 else; 609 return; // loader is now in the output file, we dont need to save again; 610 ; 611 RootBaseDir()->cd(fDataSetInfo.GetName());; 612 fDataSetInfo.GetDataSet(); // builds dataset (including calculation of correlation matrix); 613 ; 614 // correlation matrix of the default DS; 615 const TMatrixD *m(0);; 616 const TH2 *h(0);; 617 ; 618 if (fAnalysisType == Types::kMulticlass) {; 619 for (UInt_t cls = 0; cls < fDataSetInfo.GetNClasses(); cls++) {; 620 m = fDataSetInfo.CorrelationMatrix(fDataSetInfo.GetClassInfo(cls)->GetName());; 621 h = fDataSetInfo.CreateCorrelationMatrixHist(; 622 m, TString(""CorrelationMatrix"") + fDataSetInfo.GetClassInfo(cls)->GetName(),; 623 TString(""Correlation Matrix ("") + fDataSetInfo.GetClassInfo(cls)->GetName() + TString("")""));; 624 if (h != 0) {; 625 h->Write();; 626 delete h;; 627 }; 628 }; 629 } else {; 630 m = fDataSetInfo.CorrelationMatrix(""Signal"");; 631 h = fDataSetInfo.CreateCorrelationMatrixHist(m, ""CorrelationMatrixS"", ""Correlation Matrix (signal)"");; 632 if (h != 0) {; 633 h->Write();; 634 delete",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:28698,Performance,optimiz,optimize,28698,"loop.; 700 ; 701std::map<TString, Double_t> TMVA::Factory::OptimizeAllMethods(TString fomType, TString fitType); 702{; 703 ; 704 std::map<TString, MVector *>::iterator itrMap;; 705 std::map<TString, Double_t> TunedParameters;; 706 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 707 MVector *methods = itrMap->second;; 708 ; 709 MVector::iterator itrMethod;; 710 ; 711 // iterate over methods and optimize; 712 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 713 Event::SetIsTraining(kTRUE);; 714 MethodBase *mva = dynamic_cast<MethodBase *>(*itrMethod);; 715 if (!mva) {; 716 Log() << kFATAL << ""Dynamic cast to MethodBase failed"" << Endl;; 717 return TunedParameters;; 718 }; 719 ; 720 if (mva->Data()->GetNTrainingEvents() < MinNoTrainingEvents) {; 721 Log() << kWARNING << ""Method "" << mva->GetMethodName() << "" not trained (training tree has less entries [""; 722 << mva->Data()->GetNTrainingEvents() << ""] than required ["" << MinNoTrainingEvents << ""]"" << Endl;; 723 continue;; 724 }; 725 ; 726 Log() << kINFO << ""Optimize method: "" << mva->GetMethodName() << "" for ""; 727 << (fAnalysisType == Types::kRegression; 728 ? ""Regression""; 729 : (fAnalysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 730 << Endl;; 731 ; 732 TunedParameters = mva->OptimizeTuningParameters(fomType, fitType);; 733 Log() << kINFO << ""Optimization of tuning parameters finished for Method:"" << mva->GetName() << Endl;; 734 }; 735 }; 736 ; 737 return TunedParameters;; 738}; 739 ; 740////////////////////////////////////////////////////////////////////////////////; 741/// Private method to generate a ROCCurve instance for a given method.; 742/// Handles the conversion from TMVA ResultSet to a format the ROCCurve class; 743/// understands.; 744///; 745/// \note You own the retured pointer.; 746///; 747 ; 748TMVA::ROCCurve *; 749TMVA::Factory::GetROC(TMVA::DataLoader *loader, TString theMethodName, UInt_t iClass, Types::E",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:30219,Performance,load,loader,30219,"< kINFO << ""Optimize method: "" << mva->GetMethodName() << "" for ""; 727 << (fAnalysisType == Types::kRegression; 728 ? ""Regression""; 729 : (fAnalysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 730 << Endl;; 731 ; 732 TunedParameters = mva->OptimizeTuningParameters(fomType, fitType);; 733 Log() << kINFO << ""Optimization of tuning parameters finished for Method:"" << mva->GetName() << Endl;; 734 }; 735 }; 736 ; 737 return TunedParameters;; 738}; 739 ; 740////////////////////////////////////////////////////////////////////////////////; 741/// Private method to generate a ROCCurve instance for a given method.; 742/// Handles the conversion from TMVA ResultSet to a format the ROCCurve class; 743/// understands.; 744///; 745/// \note You own the retured pointer.; 746///; 747 ; 748TMVA::ROCCurve *; 749TMVA::Factory::GetROC(TMVA::DataLoader *loader, TString theMethodName, UInt_t iClass, Types::ETreeType type); 750{; 751 return GetROC((TString)loader->GetName(), theMethodName, iClass, type);; 752}; 753 ; 754////////////////////////////////////////////////////////////////////////////////; 755/// Private method to generate a ROCCurve instance for a given method.; 756/// Handles the conversion from TMVA ResultSet to a format the ROCCurve class; 757/// understands.; 758///; 759/// \note You own the retured pointer.; 760///; 761 ; 762TMVA::ROCCurve *TMVA::Factory::GetROC(TString datasetname, TString theMethodName, UInt_t iClass, Types::ETreeType type); 763{; 764 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 765 Log() << kERROR << Form(""DataSet = %s not found in methods map."", datasetname.Data()) << Endl;; 766 return nullptr;; 767 }; 768 ; 769 if (!this->HasMethod(datasetname, theMethodName)) {; 770 Log() << kERROR << Form(""Method = %s not found with Dataset = %s "", theMethodName.Data(), datasetname.Data()); 771 << Endl;; 772 return nullptr;; 773 }; 774 ; 775 std::set<Types::EAnalysisType> allowedAnalysisTypes = {Types::kClassification,",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:30322,Performance,load,loader,30322,"< kINFO << ""Optimize method: "" << mva->GetMethodName() << "" for ""; 727 << (fAnalysisType == Types::kRegression; 728 ? ""Regression""; 729 : (fAnalysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 730 << Endl;; 731 ; 732 TunedParameters = mva->OptimizeTuningParameters(fomType, fitType);; 733 Log() << kINFO << ""Optimization of tuning parameters finished for Method:"" << mva->GetName() << Endl;; 734 }; 735 }; 736 ; 737 return TunedParameters;; 738}; 739 ; 740////////////////////////////////////////////////////////////////////////////////; 741/// Private method to generate a ROCCurve instance for a given method.; 742/// Handles the conversion from TMVA ResultSet to a format the ROCCurve class; 743/// understands.; 744///; 745/// \note You own the retured pointer.; 746///; 747 ; 748TMVA::ROCCurve *; 749TMVA::Factory::GetROC(TMVA::DataLoader *loader, TString theMethodName, UInt_t iClass, Types::ETreeType type); 750{; 751 return GetROC((TString)loader->GetName(), theMethodName, iClass, type);; 752}; 753 ; 754////////////////////////////////////////////////////////////////////////////////; 755/// Private method to generate a ROCCurve instance for a given method.; 756/// Handles the conversion from TMVA ResultSet to a format the ROCCurve class; 757/// understands.; 758///; 759/// \note You own the retured pointer.; 760///; 761 ; 762TMVA::ROCCurve *TMVA::Factory::GetROC(TString datasetname, TString theMethodName, UInt_t iClass, Types::ETreeType type); 763{; 764 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 765 Log() << kERROR << Form(""DataSet = %s not found in methods map."", datasetname.Data()) << Endl;; 766 return nullptr;; 767 }; 768 ; 769 if (!this->HasMethod(datasetname, theMethodName)) {; 770 Log() << kERROR << Form(""Method = %s not found with Dataset = %s "", theMethodName.Data(), datasetname.Data()); 771 << Endl;; 772 return nullptr;; 773 }; 774 ; 775 std::set<Types::EAnalysisType> allowedAnalysisTypes = {Types::kClassification,",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:34299,Performance,load,loader,34299,"ection = dataset->GetEventCollection(type);; 827 mvaResTypes.reserve(eventCollection.size());; 828 mvaResWeights.reserve(eventCollection.size());; 829 for (auto ev : eventCollection) {; 830 mvaResTypes.push_back(ev->GetClass() == iClass);; 831 mvaResWeights.push_back(ev->GetWeight());; 832 }; 833 ; 834 rocCurve = new TMVA::ROCCurve(mvaRes, mvaResTypes, mvaResWeights);; 835 }; 836 ; 837 return rocCurve;; 838}; 839 ; 840////////////////////////////////////////////////////////////////////////////////; 841/// Calculate the integral of the ROC curve, also known as the area under curve; 842/// (AUC), for a given method.; 843///; 844/// Argument iClass specifies the class to generate the ROC curve in a; 845/// multiclass setting. It is ignored for binary classification.; 846///; 847 ; 848Double_t; 849TMVA::Factory::GetROCIntegral(TMVA::DataLoader *loader, TString theMethodName, UInt_t iClass, Types::ETreeType type); 850{; 851 return GetROCIntegral((TString)loader->GetName(), theMethodName, iClass, type);; 852}; 853 ; 854////////////////////////////////////////////////////////////////////////////////; 855/// Calculate the integral of the ROC curve, also known as the area under curve; 856/// (AUC), for a given method.; 857///; 858/// Argument iClass specifies the class to generate the ROC curve in a; 859/// multiclass setting. It is ignored for binary classification.; 860///; 861 ; 862Double_t TMVA::Factory::GetROCIntegral(TString datasetname, TString theMethodName, UInt_t iClass, Types::ETreeType type); 863{; 864 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 865 Log() << kERROR << Form(""DataSet = %s not found in methods map."", datasetname.Data()) << Endl;; 866 return 0;; 867 }; 868 ; 869 if (!this->HasMethod(datasetname, theMethodName)) {; 870 Log() << kERROR << Form(""Method = %s not found with Dataset = %s "", theMethodName.Data(), datasetname.Data()); 871 << Endl;; 872 return 0;; 873 }; 874 ; 875 std::set<Types::EAnalysisType> allowedAnalysisTypes = {Types::kCl",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:34410,Performance,load,loader,34410,"ection = dataset->GetEventCollection(type);; 827 mvaResTypes.reserve(eventCollection.size());; 828 mvaResWeights.reserve(eventCollection.size());; 829 for (auto ev : eventCollection) {; 830 mvaResTypes.push_back(ev->GetClass() == iClass);; 831 mvaResWeights.push_back(ev->GetWeight());; 832 }; 833 ; 834 rocCurve = new TMVA::ROCCurve(mvaRes, mvaResTypes, mvaResWeights);; 835 }; 836 ; 837 return rocCurve;; 838}; 839 ; 840////////////////////////////////////////////////////////////////////////////////; 841/// Calculate the integral of the ROC curve, also known as the area under curve; 842/// (AUC), for a given method.; 843///; 844/// Argument iClass specifies the class to generate the ROC curve in a; 845/// multiclass setting. It is ignored for binary classification.; 846///; 847 ; 848Double_t; 849TMVA::Factory::GetROCIntegral(TMVA::DataLoader *loader, TString theMethodName, UInt_t iClass, Types::ETreeType type); 850{; 851 return GetROCIntegral((TString)loader->GetName(), theMethodName, iClass, type);; 852}; 853 ; 854////////////////////////////////////////////////////////////////////////////////; 855/// Calculate the integral of the ROC curve, also known as the area under curve; 856/// (AUC), for a given method.; 857///; 858/// Argument iClass specifies the class to generate the ROC curve in a; 859/// multiclass setting. It is ignored for binary classification.; 860///; 861 ; 862Double_t TMVA::Factory::GetROCIntegral(TString datasetname, TString theMethodName, UInt_t iClass, Types::ETreeType type); 863{; 864 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 865 Log() << kERROR << Form(""DataSet = %s not found in methods map."", datasetname.Data()) << Endl;; 866 return 0;; 867 }; 868 ; 869 if (!this->HasMethod(datasetname, theMethodName)) {; 870 Log() << kERROR << Form(""Method = %s not found with Dataset = %s "", theMethodName.Data(), datasetname.Data()); 871 << Endl;; 872 return 0;; 873 }; 874 ; 875 std::set<Types::EAnalysisType> allowedAnalysisTypes = {Types::kCl",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:37024,Performance,load,loader,37024," rocIntegral;; 896}; 897 ; 898////////////////////////////////////////////////////////////////////////////////; 899/// Argument iClass specifies the class to generate the ROC curve in a; 900/// multiclass setting. It is ignored for binary classification.; 901///; 902/// Returns a ROC graph for a given method, or nullptr on error.; 903///; 904/// Note: Evaluation of the given method must have been run prior to ROC; 905/// generation through Factory::EvaluateAllMetods.; 906///; 907/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 908/// and the others considered background. This is ok in binary classification; 909/// but in in multi class classification, the ROC surface is an N dimensional; 910/// shape, where N is number of classes - 1.; 911 ; 912TGraph *TMVA::Factory::GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles, UInt_t iClass,; 913 Types::ETreeType type); 914{; 915 return GetROCCurve((TString)loader->GetName(), theMethodName, setTitles, iClass, type);; 916}; 917 ; 918////////////////////////////////////////////////////////////////////////////////; 919/// Argument iClass specifies the class to generate the ROC curve in a; 920/// multiclass setting. It is ignored for binary classification.; 921///; 922/// Returns a ROC graph for a given method, or nullptr on error.; 923///; 924/// Note: Evaluation of the given method must have been run prior to ROC; 925/// generation through Factory::EvaluateAllMetods.; 926///; 927/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 928/// and the others considered background. This is ok in binary classification; 929/// but in in multi class classification, the ROC surface is an N dimensional; 930/// shape, where N is number of classes - 1.; 931 ; 932TGraph *TMVA::Factory::GetROCCurve(TString datasetname, TString theMethodName, Bool_t setTitles, UInt_t iClass,; 933 Types::ETreeType type); 934{; 935 if (fMethodsMap.find(datasetname) == fMethodsMap.en",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:37155,Performance,load,loader,37155," rocIntegral;; 896}; 897 ; 898////////////////////////////////////////////////////////////////////////////////; 899/// Argument iClass specifies the class to generate the ROC curve in a; 900/// multiclass setting. It is ignored for binary classification.; 901///; 902/// Returns a ROC graph for a given method, or nullptr on error.; 903///; 904/// Note: Evaluation of the given method must have been run prior to ROC; 905/// generation through Factory::EvaluateAllMetods.; 906///; 907/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 908/// and the others considered background. This is ok in binary classification; 909/// but in in multi class classification, the ROC surface is an N dimensional; 910/// shape, where N is number of classes - 1.; 911 ; 912TGraph *TMVA::Factory::GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles, UInt_t iClass,; 913 Types::ETreeType type); 914{; 915 return GetROCCurve((TString)loader->GetName(), theMethodName, setTitles, iClass, type);; 916}; 917 ; 918////////////////////////////////////////////////////////////////////////////////; 919/// Argument iClass specifies the class to generate the ROC curve in a; 920/// multiclass setting. It is ignored for binary classification.; 921///; 922/// Returns a ROC graph for a given method, or nullptr on error.; 923///; 924/// Note: Evaluation of the given method must have been run prior to ROC; 925/// generation through Factory::EvaluateAllMetods.; 926///; 927/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 928/// and the others considered background. This is ok in binary classification; 929/// but in in multi class classification, the ROC surface is an N dimensional; 930/// shape, where N is number of classes - 1.; 931 ; 932TGraph *TMVA::Factory::GetROCCurve(TString datasetname, TString theMethodName, Bool_t setTitles, UInt_t iClass,; 933 Types::ETreeType type); 934{; 935 if (fMethodsMap.find(datasetname) == fMethodsMap.en",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:39839,Performance,perform,performance,39839,"lass.""); 949 << Endl;; 950 return nullptr;; 951 }; 952 ; 953 TMVA::ROCCurve *rocCurve = GetROC(datasetname, theMethodName, iClass, type);; 954 TGraph *graph = nullptr;; 955 ; 956 if (!rocCurve) {; 957 Log() << kFATAL; 958 << Form(""ROCCurve object was not created in Method = %s not found with Dataset = %s "", theMethodName.Data(),; 959 datasetname.Data()); 960 << Endl;; 961 return nullptr;; 962 }; 963 ; 964 graph = (TGraph *)rocCurve->GetROCCurve()->Clone();; 965 delete rocCurve;; 966 ; 967 if (setTitles) {; 968 graph->GetYaxis()->SetTitle(""Background rejection (Specificity)"");; 969 graph->GetXaxis()->SetTitle(""Signal efficiency (Sensitivity)"");; 970 graph->SetTitle(TString::Format(""Signal efficiency vs. Background rejection (%s)"", theMethodName.Data()).Data());; 971 }; 972 ; 973 return graph;; 974}; 975 ; 976////////////////////////////////////////////////////////////////////////////////; 977/// Generate a collection of graphs, for all methods for a given class. Suitable; 978/// for comparing method performance.; 979///; 980/// Argument iClass specifies the class to generate the ROC curve in a; 981/// multiclass setting. It is ignored for binary classification.; 982///; 983/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 984/// and the others considered background. This is ok in binary classification; 985/// but in in multi class classification, the ROC surface is an N dimensional; 986/// shape, where N is number of classes - 1.; 987 ; 988TMultiGraph *TMVA::Factory::GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type); 989{; 990 return GetROCCurveAsMultiGraph((TString)loader->GetName(), iClass, type);; 991}; 992 ; 993////////////////////////////////////////////////////////////////////////////////; 994/// Generate a collection of graphs, for all methods for a given class. Suitable; 995/// for comparing method performance.; 996///; 997/// Argument iClass specifies the class to generate the ROC curve in a; 998/",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:40383,Performance,load,loader,40383,"(TString::Format(""Signal efficiency vs. Background rejection (%s)"", theMethodName.Data()).Data());; 971 }; 972 ; 973 return graph;; 974}; 975 ; 976////////////////////////////////////////////////////////////////////////////////; 977/// Generate a collection of graphs, for all methods for a given class. Suitable; 978/// for comparing method performance.; 979///; 980/// Argument iClass specifies the class to generate the ROC curve in a; 981/// multiclass setting. It is ignored for binary classification.; 982///; 983/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 984/// and the others considered background. This is ok in binary classification; 985/// but in in multi class classification, the ROC surface is an N dimensional; 986/// shape, where N is number of classes - 1.; 987 ; 988TMultiGraph *TMVA::Factory::GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type); 989{; 990 return GetROCCurveAsMultiGraph((TString)loader->GetName(), iClass, type);; 991}; 992 ; 993////////////////////////////////////////////////////////////////////////////////; 994/// Generate a collection of graphs, for all methods for a given class. Suitable; 995/// for comparing method performance.; 996///; 997/// Argument iClass specifies the class to generate the ROC curve in a; 998/// multiclass setting. It is ignored for binary classification.; 999///; 1000/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 1001/// and the others considered background. This is ok in binary classification; 1002/// but in in multi class classification, the ROC surface is an N dimensional; 1003/// shape, where N is number of classes - 1.; 1004 ; 1005TMultiGraph *TMVA::Factory::GetROCCurveAsMultiGraph(TString datasetname, UInt_t iClass, Types::ETreeType type); 1006{; 1007 UInt_t line_color = 1;; 1008 ; 1009 TMultiGraph *multigraph = new TMultiGraph();; 1010 ; 1011 MVector *methods = fMethodsMap[datasetname.Data()];; 1012 for (auto ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:40480,Performance,load,loader,40480,"(TString::Format(""Signal efficiency vs. Background rejection (%s)"", theMethodName.Data()).Data());; 971 }; 972 ; 973 return graph;; 974}; 975 ; 976////////////////////////////////////////////////////////////////////////////////; 977/// Generate a collection of graphs, for all methods for a given class. Suitable; 978/// for comparing method performance.; 979///; 980/// Argument iClass specifies the class to generate the ROC curve in a; 981/// multiclass setting. It is ignored for binary classification.; 982///; 983/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 984/// and the others considered background. This is ok in binary classification; 985/// but in in multi class classification, the ROC surface is an N dimensional; 986/// shape, where N is number of classes - 1.; 987 ; 988TMultiGraph *TMVA::Factory::GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type); 989{; 990 return GetROCCurveAsMultiGraph((TString)loader->GetName(), iClass, type);; 991}; 992 ; 993////////////////////////////////////////////////////////////////////////////////; 994/// Generate a collection of graphs, for all methods for a given class. Suitable; 995/// for comparing method performance.; 996///; 997/// Argument iClass specifies the class to generate the ROC curve in a; 998/// multiclass setting. It is ignored for binary classification.; 999///; 1000/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 1001/// and the others considered background. This is ok in binary classification; 1002/// but in in multi class classification, the ROC surface is an N dimensional; 1003/// shape, where N is number of classes - 1.; 1004 ; 1005TMultiGraph *TMVA::Factory::GetROCCurveAsMultiGraph(TString datasetname, UInt_t iClass, Types::ETreeType type); 1006{; 1007 UInt_t line_color = 1;; 1008 ; 1009 TMultiGraph *multigraph = new TMultiGraph();; 1010 ; 1011 MVector *methods = fMethodsMap[datasetname.Data()];; 1012 for (auto ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:40725,Performance,perform,performance,40725,"/////////////; 977/// Generate a collection of graphs, for all methods for a given class. Suitable; 978/// for comparing method performance.; 979///; 980/// Argument iClass specifies the class to generate the ROC curve in a; 981/// multiclass setting. It is ignored for binary classification.; 982///; 983/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 984/// and the others considered background. This is ok in binary classification; 985/// but in in multi class classification, the ROC surface is an N dimensional; 986/// shape, where N is number of classes - 1.; 987 ; 988TMultiGraph *TMVA::Factory::GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type); 989{; 990 return GetROCCurveAsMultiGraph((TString)loader->GetName(), iClass, type);; 991}; 992 ; 993////////////////////////////////////////////////////////////////////////////////; 994/// Generate a collection of graphs, for all methods for a given class. Suitable; 995/// for comparing method performance.; 996///; 997/// Argument iClass specifies the class to generate the ROC curve in a; 998/// multiclass setting. It is ignored for binary classification.; 999///; 1000/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 1001/// and the others considered background. This is ok in binary classification; 1002/// but in in multi class classification, the ROC surface is an N dimensional; 1003/// shape, where N is number of classes - 1.; 1004 ; 1005TMultiGraph *TMVA::Factory::GetROCCurveAsMultiGraph(TString datasetname, UInt_t iClass, Types::ETreeType type); 1006{; 1007 UInt_t line_color = 1;; 1008 ; 1009 TMultiGraph *multigraph = new TMultiGraph();; 1010 ; 1011 MVector *methods = fMethodsMap[datasetname.Data()];; 1012 for (auto *method_raw : *methods) {; 1013 TMVA::MethodBase *method = dynamic_cast<TMVA::MethodBase *>(method_raw);; 1014 if (method == nullptr) {; 1015 continue;; 1016 }; 1017 ; 1018 TString methodName = method->GetMethodName",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:43356,Performance,load,loader,43356,"2 Log() << kERROR << Form(""No metohds have class %i defined."", iClass) << Endl;; 1043 return nullptr;; 1044 }; 1045 ; 1046 return multigraph;; 1047}; 1048 ; 1049////////////////////////////////////////////////////////////////////////////////; 1050/// Draws ROC curves for all methods booked with the factory for a given class; 1051/// onto a canvas.; 1052///; 1053/// Argument iClass specifies the class to generate the ROC curve in a; 1054/// multiclass setting. It is ignored for binary classification.; 1055///; 1056/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 1057/// and the others considered background. This is ok in binary classification; 1058/// but in in multi class classification, the ROC surface is an N dimensional; 1059/// shape, where N is number of classes - 1.; 1060 ; 1061TCanvas *TMVA::Factory::GetROCCurve(TMVA::DataLoader *loader, UInt_t iClass, Types::ETreeType type); 1062{; 1063 return GetROCCurve((TString)loader->GetName(), iClass, type);; 1064}; 1065 ; 1066////////////////////////////////////////////////////////////////////////////////; 1067/// Draws ROC curves for all methods booked with the factory for a given class.; 1068///; 1069/// Argument iClass specifies the class to generate the ROC curve in a; 1070/// multiclass setting. It is ignored for binary classification.; 1071///; 1072/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 1073/// and the others considered background. This is ok in binary classification; 1074/// but in in multi class classification, the ROC surface is an N dimensional; 1075/// shape, where N is number of classes - 1.; 1076 ; 1077TCanvas *TMVA::Factory::GetROCCurve(TString datasetname, UInt_t iClass, Types::ETreeType type); 1078{; 1079 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 1080 Log() << kERROR << Form(""DataSet = %s not found in methods map."", datasetname.Data()) << Endl;; 1081 return 0;; 1082 }; 1083 ; 1084 TString name = TString::Format(""",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:43443,Performance,load,loader,43443,"2 Log() << kERROR << Form(""No metohds have class %i defined."", iClass) << Endl;; 1043 return nullptr;; 1044 }; 1045 ; 1046 return multigraph;; 1047}; 1048 ; 1049////////////////////////////////////////////////////////////////////////////////; 1050/// Draws ROC curves for all methods booked with the factory for a given class; 1051/// onto a canvas.; 1052///; 1053/// Argument iClass specifies the class to generate the ROC curve in a; 1054/// multiclass setting. It is ignored for binary classification.; 1055///; 1056/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 1057/// and the others considered background. This is ok in binary classification; 1058/// but in in multi class classification, the ROC surface is an N dimensional; 1059/// shape, where N is number of classes - 1.; 1060 ; 1061TCanvas *TMVA::Factory::GetROCCurve(TMVA::DataLoader *loader, UInt_t iClass, Types::ETreeType type); 1062{; 1063 return GetROCCurve((TString)loader->GetName(), iClass, type);; 1064}; 1065 ; 1066////////////////////////////////////////////////////////////////////////////////; 1067/// Draws ROC curves for all methods booked with the factory for a given class.; 1068///; 1069/// Argument iClass specifies the class to generate the ROC curve in a; 1070/// multiclass setting. It is ignored for binary classification.; 1071///; 1072/// NOTE: The ROC curve is 1 vs. all where the given class is considered signal; 1073/// and the others considered background. This is ok in binary classification; 1074/// but in in multi class classification, the ROC surface is an N dimensional; 1075/// shape, where N is number of classes - 1.; 1076 ; 1077TCanvas *TMVA::Factory::GetROCCurve(TString datasetname, UInt_t iClass, Types::ETreeType type); 1078{; 1079 if (fMethodsMap.find(datasetname) == fMethodsMap.end()) {; 1080 Log() << kERROR << Form(""DataSet = %s not found in methods map."", datasetname.Data()) << Endl;; 1081 return 0;; 1082 }; 1083 ; 1084 TString name = TString::Format(""",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:53063,Performance,perform,performance,53063,"lor(""reset"") << Endl;; 1274 ; 1275 // don't do anything if no method booked; 1276 if (fMethodsMap.empty()) {; 1277 Log() << kINFO << ""...nothing found to test"" << Endl;; 1278 return;; 1279 }; 1280 std::map<TString, MVector *>::iterator itrMap;; 1281 ; 1282 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1283 MVector *methods = itrMap->second;; 1284 MVector::iterator itrMethod;; 1285 ; 1286 // iterate over methods and test; 1287 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1288 Event::SetIsTraining(kFALSE);; 1289 MethodBase *mva = dynamic_cast<MethodBase *>(*itrMethod);; 1290 if (mva == 0); 1291 continue;; 1292 Types::EAnalysisType analysisType = mva->GetAnalysisType();; 1293 Log() << kHEADER << ""Test method: "" << mva->GetMethodName() << "" for ""; 1294 << (analysisType == Types::kRegression; 1295 ? ""Regression""; 1296 : (analysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 1297 << "" performance"" << Endl << Endl;; 1298 mva->AddOutput(Types::kTesting, analysisType);; 1299 }; 1300 }; 1301}; 1302 ; 1303////////////////////////////////////////////////////////////////////////////////; 1304 ; 1305void TMVA::Factory::MakeClass(const TString &datasetname, const TString &methodTitle) const; 1306{; 1307 if (methodTitle != """") {; 1308 IMethod *method = GetMethod(datasetname, methodTitle);; 1309 if (method); 1310 method->MakeClass();; 1311 else {; 1312 Log() << kWARNING << ""<MakeClass> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1313 }; 1314 } else {; 1315 ; 1316 // no classifier specified, print all help messages; 1317 MVector *methods = fMethodsMap.find(datasetname)->second;; 1318 MVector::const_iterator itrMethod;; 1319 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1320 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1321 if (method == 0); 1322 continue;; 1323 Log() << kINFO << ""Make response class for classi",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:55509,Performance,load,loader,55509,"method = GetMethod(datasetname, methodTitle);; 1337 if (method); 1338 method->PrintHelpMessage();; 1339 else {; 1340 Log() << kWARNING << ""<PrintHelpMessage> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////////////////////////////////////////////////; 1358/// Iterates over all MVA input variables and evaluates them.; 1359 ; 1360void TMVA::Factory::EvaluateAllVariables(DataLoader *loader, TString options); 1361{; 1362 Log() << kINFO << ""Evaluating all variables..."" << Endl;; 1363 Event::SetIsTraining(kFALSE);; 1364 ; 1365 for (UInt_t i = 0; i < loader->GetDataSetInfo().GetNVariables(); i++) {; 1366 TString s = loader->GetDataSetInfo().GetVariableInfo(i).GetLabel();; 1367 if (options.Contains(""V"")); 1368 s += "":V"";; 1369 this->BookMethod(loader, ""Variable"", s);; 1370 }; 1371}; 1372 ; 1373////////////////////////////////////////////////////////////////////////////////; 1374/// Iterates over all MVAs that have been booked, and calls their evaluation methods.; 1375 ; 1376void TMVA::Factory::EvaluateAllMethods(void); 1377{; 1378 Log() << kHEADER << gTools().Color(""bold"") << ""Evaluate all methods"" << gTools().Color(""reset"") << Endl;; 1379 ; 1380 // don't do anything if no method booked; 1381 if (fMethodsMap.empty()) {; 1382 Log() << kINFO << ""...nothing found to evaluate"" << Endl;; 1383 return;; 1384 }; 1385 std::map<TString, MVector *>::iterator itrMap;; 1386 ; 1387 for (itr",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:55676,Performance,load,loader,55676,"NING << ""<PrintHelpMessage> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////////////////////////////////////////////////; 1358/// Iterates over all MVA input variables and evaluates them.; 1359 ; 1360void TMVA::Factory::EvaluateAllVariables(DataLoader *loader, TString options); 1361{; 1362 Log() << kINFO << ""Evaluating all variables..."" << Endl;; 1363 Event::SetIsTraining(kFALSE);; 1364 ; 1365 for (UInt_t i = 0; i < loader->GetDataSetInfo().GetNVariables(); i++) {; 1366 TString s = loader->GetDataSetInfo().GetVariableInfo(i).GetLabel();; 1367 if (options.Contains(""V"")); 1368 s += "":V"";; 1369 this->BookMethod(loader, ""Variable"", s);; 1370 }; 1371}; 1372 ; 1373////////////////////////////////////////////////////////////////////////////////; 1374/// Iterates over all MVAs that have been booked, and calls their evaluation methods.; 1375 ; 1376void TMVA::Factory::EvaluateAllMethods(void); 1377{; 1378 Log() << kHEADER << gTools().Color(""bold"") << ""Evaluate all methods"" << gTools().Color(""reset"") << Endl;; 1379 ; 1380 // don't do anything if no method booked; 1381 if (fMethodsMap.empty()) {; 1382 Log() << kINFO << ""...nothing found to evaluate"" << Endl;; 1383 return;; 1384 }; 1385 std::map<TString, MVector *>::iterator itrMap;; 1386 ; 1387 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1388 MVector *methods = itrMap->second;; 1389 ; 1390 // -----",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:55743,Performance,load,loader,55743,""" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////////////////////////////////////////////////; 1358/// Iterates over all MVA input variables and evaluates them.; 1359 ; 1360void TMVA::Factory::EvaluateAllVariables(DataLoader *loader, TString options); 1361{; 1362 Log() << kINFO << ""Evaluating all variables..."" << Endl;; 1363 Event::SetIsTraining(kFALSE);; 1364 ; 1365 for (UInt_t i = 0; i < loader->GetDataSetInfo().GetNVariables(); i++) {; 1366 TString s = loader->GetDataSetInfo().GetVariableInfo(i).GetLabel();; 1367 if (options.Contains(""V"")); 1368 s += "":V"";; 1369 this->BookMethod(loader, ""Variable"", s);; 1370 }; 1371}; 1372 ; 1373////////////////////////////////////////////////////////////////////////////////; 1374/// Iterates over all MVAs that have been booked, and calls their evaluation methods.; 1375 ; 1376void TMVA::Factory::EvaluateAllMethods(void); 1377{; 1378 Log() << kHEADER << gTools().Color(""bold"") << ""Evaluate all methods"" << gTools().Color(""reset"") << Endl;; 1379 ; 1380 // don't do anything if no method booked; 1381 if (fMethodsMap.empty()) {; 1382 Log() << kINFO << ""...nothing found to evaluate"" << Endl;; 1383 return;; 1384 }; 1385 std::map<TString, MVector *>::iterator itrMap;; 1386 ; 1387 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1388 MVector *methods = itrMap->second;; 1389 ; 1390 // -----------------------------------------------------------------------; 1391 // First part o",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:55872,Performance,load,loader,55872,"hod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////////////////////////////////////////////////; 1358/// Iterates over all MVA input variables and evaluates them.; 1359 ; 1360void TMVA::Factory::EvaluateAllVariables(DataLoader *loader, TString options); 1361{; 1362 Log() << kINFO << ""Evaluating all variables..."" << Endl;; 1363 Event::SetIsTraining(kFALSE);; 1364 ; 1365 for (UInt_t i = 0; i < loader->GetDataSetInfo().GetNVariables(); i++) {; 1366 TString s = loader->GetDataSetInfo().GetVariableInfo(i).GetLabel();; 1367 if (options.Contains(""V"")); 1368 s += "":V"";; 1369 this->BookMethod(loader, ""Variable"", s);; 1370 }; 1371}; 1372 ; 1373////////////////////////////////////////////////////////////////////////////////; 1374/// Iterates over all MVAs that have been booked, and calls their evaluation methods.; 1375 ; 1376void TMVA::Factory::EvaluateAllMethods(void); 1377{; 1378 Log() << kHEADER << gTools().Color(""bold"") << ""Evaluate all methods"" << gTools().Color(""reset"") << Endl;; 1379 ; 1380 // don't do anything if no method booked; 1381 if (fMethodsMap.empty()) {; 1382 Log() << kINFO << ""...nothing found to evaluate"" << Endl;; 1383 return;; 1384 }; 1385 std::map<TString, MVector *>::iterator itrMap;; 1386 ; 1387 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1388 MVector *methods = itrMap->second;; 1389 ; 1390 // -----------------------------------------------------------------------; 1391 // First part of evaluation process; 1392 // --> compute efficiencies, and other separation estimators; 1393 // -----------------------------------------------------------------------; 1394 ; 1395 // although equal, we now want to separ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:64170,Performance,perform,perform,64170,"fB30.push_back(theMethod->GetMulticlassConfusionMatrix(0.30, Types::kTraining));; 1517 ; 1518 multiclass_testConfusionEffB01.push_back(theMethod->GetMulticlassConfusionMatrix(0.01, Types::kTesting));; 1519 multiclass_testConfusionEffB10.push_back(theMethod->GetMulticlassConfusionMatrix(0.10, Types::kTesting));; 1520 multiclass_testConfusionEffB30.push_back(theMethod->GetMulticlassConfusionMatrix(0.30, Types::kTesting));; 1521 ; 1522 if (!IsSilentFile()) {; 1523 Log() << kDEBUG << ""\tWrite evaluation histograms to file"" << Endl;; 1524 theMethod->WriteEvaluationHistosToFile(Types::kTesting);; 1525 theMethod->WriteEvaluationHistosToFile(Types::kTraining);; 1526 }; 1527 ; 1528 nmeth_used[0]++;; 1529 mname[0].push_back(theMethod->GetMethodName());; 1530 } else {; 1531 ; 1532 Log() << kHEADER << ""Evaluate classifier: "" << theMethod->GetMethodName() << Endl << Endl;; 1533 isel = (theMethod->GetMethodTypeName().Contains(""Variable"")) ? 1 : 0;; 1534 ; 1535 // perform the evaluation; 1536 theMethod->TestClassification();; 1537 ; 1538 // evaluate the classifier; 1539 mname[isel].push_back(theMethod->GetMethodName());; 1540 sig[isel].push_back(theMethod->GetSignificance());; 1541 sep[isel].push_back(theMethod->GetSeparation());; 1542 roc[isel].push_back(theMethod->GetROCIntegral());; 1543 ; 1544 Double_t err;; 1545 eff01[isel].push_back(theMethod->GetEfficiency(""Efficiency:0.01"", Types::kTesting, err));; 1546 eff01err[isel].push_back(err);; 1547 eff10[isel].push_back(theMethod->GetEfficiency(""Efficiency:0.10"", Types::kTesting, err));; 1548 eff10err[isel].push_back(err);; 1549 eff30[isel].push_back(theMethod->GetEfficiency(""Efficiency:0.30"", Types::kTesting, err));; 1550 eff30err[isel].push_back(err);; 1551 effArea[isel].push_back(theMethod->GetEfficiency("""", Types::kTesting, err)); // computes the area (average); 1552 ; 1553 trainEff01[isel].push_back(; 1554 theMethod->GetTrainingEfficiency(""Efficiency:0.01"")); // the first pass takes longer; 1555 trainEff10[isel].push_back(theMe",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:81215,Performance,perform,performance,81215,"s"", theMethod->fDataSetInfo.GetName(), mname[0][i].Data());; 1924 // for (UInt_t icls = 0; icls < theMethod->fDataSetInfo.GetNClasses(); ++icls) {; 1925 // res += TString::Format(""%#1.3f "", (multiclass_testEff[i][icls]) * (multiclass_testPur[i][icls]));; 1926 // }; 1927 // Log() << kINFO << res << Endl;; 1928 // }; 1929 ; 1930 // Log() << kINFO << hLine << Endl;; 1931 // Log() << kINFO << Endl;; 1932 // }; 1933 ; 1934 // --- 1 vs Rest ROC AUC, signal efficiency @ given background efficiency; 1935 // --------------------------------------------------------------------; 1936 TString header1 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Dataset"", ""MVA Method"", ""ROC AUC"", ""Sig eff@B=0.01"",; 1937 ""Sig eff@B=0.10"", ""Sig eff@B=0.30"");; 1938 TString header2 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Name:"", ""/ Class:"", ""test (train)"", ""test (train)"",; 1939 ""test (train)"", ""test (train)"");; 1940 Log() << kINFO << Endl;; 1941 Log() << kINFO << ""1-vs-rest performance metrics per class"" << Endl;; 1942 Log() << kINFO << hLine << Endl;; 1943 Log() << kINFO << Endl;; 1944 Log() << kINFO << ""Considers the listed class as signal and the other classes"" << Endl;; 1945 Log() << kINFO << ""as background, reporting the resulting binary performance."" << Endl;; 1946 Log() << kINFO << ""A score of 0.820 (0.850) means 0.820 was acheived on the"" << Endl;; 1947 Log() << kINFO << ""test set and 0.850 on the training set."" << Endl;; 1948 ; 1949 Log() << kINFO << Endl;; 1950 Log() << kINFO << header1 << Endl;; 1951 Log() << kINFO << header2 << Endl;; 1952 for (Int_t k = 0; k < 2; k++) {; 1953 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 1954 if (k == 1) {; 1955 mname[k][i].ReplaceAll(""Variable_"", """");; 1956 }; 1957 ; 1958 const TString datasetName = itrMap->first;; 1959 const TString mvaName = mname[k][i];; 1960 ; 1961 MethodBase *theMethod = dynamic_cast<MethodBase *>(GetMethod(datasetName, mvaName));; 1962 if (theMethod == 0) {; 1963 continue;; 1964 }; 1965 ; 1966 Log() << kINFO <<",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:81489,Performance,perform,performance,81489,"s"", theMethod->fDataSetInfo.GetName(), mname[0][i].Data());; 1924 // for (UInt_t icls = 0; icls < theMethod->fDataSetInfo.GetNClasses(); ++icls) {; 1925 // res += TString::Format(""%#1.3f "", (multiclass_testEff[i][icls]) * (multiclass_testPur[i][icls]));; 1926 // }; 1927 // Log() << kINFO << res << Endl;; 1928 // }; 1929 ; 1930 // Log() << kINFO << hLine << Endl;; 1931 // Log() << kINFO << Endl;; 1932 // }; 1933 ; 1934 // --- 1 vs Rest ROC AUC, signal efficiency @ given background efficiency; 1935 // --------------------------------------------------------------------; 1936 TString header1 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Dataset"", ""MVA Method"", ""ROC AUC"", ""Sig eff@B=0.01"",; 1937 ""Sig eff@B=0.10"", ""Sig eff@B=0.30"");; 1938 TString header2 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Name:"", ""/ Class:"", ""test (train)"", ""test (train)"",; 1939 ""test (train)"", ""test (train)"");; 1940 Log() << kINFO << Endl;; 1941 Log() << kINFO << ""1-vs-rest performance metrics per class"" << Endl;; 1942 Log() << kINFO << hLine << Endl;; 1943 Log() << kINFO << Endl;; 1944 Log() << kINFO << ""Considers the listed class as signal and the other classes"" << Endl;; 1945 Log() << kINFO << ""as background, reporting the resulting binary performance."" << Endl;; 1946 Log() << kINFO << ""A score of 0.820 (0.850) means 0.820 was acheived on the"" << Endl;; 1947 Log() << kINFO << ""test set and 0.850 on the training set."" << Endl;; 1948 ; 1949 Log() << kINFO << Endl;; 1950 Log() << kINFO << header1 << Endl;; 1951 Log() << kINFO << header2 << Endl;; 1952 for (Int_t k = 0; k < 2; k++) {; 1953 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 1954 if (k == 1) {; 1955 mname[k][i].ReplaceAll(""Variable_"", """");; 1956 }; 1957 ; 1958 const TString datasetName = itrMap->first;; 1959 const TString mvaName = mname[k][i];; 1960 ; 1961 MethodBase *theMethod = dynamic_cast<MethodBase *>(GetMethod(datasetName, mvaName));; 1962 if (theMethod == 0) {; 1963 continue;; 1964 }; 1965 ; 1966 Log() << kINFO <<",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:93406,Performance,load,loader,93406,") {; 2196 MethodBase *theMethod = dynamic_cast<MethodBase *>((*methods)[i]);; 2197 if (theMethod == 0); 2198 continue;; 2199 // write test/training trees; 2200 RootBaseDir()->cd(theMethod->fDataSetInfo.GetName());; 2201 if (std::find(datasets.begin(), datasets.end(), theMethod->fDataSetInfo.GetName()) == datasets.end()) {; 2202 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTesting)->Write("""", TObject::kOverwrite);; 2203 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTraining)->Write("""", TObject::kOverwrite);; 2204 datasets.push_back(theMethod->fDataSetInfo.GetName());; 2205 }; 2206 }; 2207 }; 2208 }; 2209 } // end for MethodsMap; 2210 // references for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more th",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:93710,Performance,load,loader,93710,") {; 2196 MethodBase *theMethod = dynamic_cast<MethodBase *>((*methods)[i]);; 2197 if (theMethod == 0); 2198 continue;; 2199 // write test/training trees; 2200 RootBaseDir()->cd(theMethod->fDataSetInfo.GetName());; 2201 if (std::find(datasets.begin(), datasets.end(), theMethod->fDataSetInfo.GetName()) == datasets.end()) {; 2202 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTesting)->Write("""", TObject::kOverwrite);; 2203 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTraining)->Write("""", TObject::kOverwrite);; 2204 datasets.push_back(theMethod->fDataSetInfo.GetName());; 2205 }; 2206 }; 2207 }; 2208 }; 2209 } // end for MethodsMap; 2210 // references for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more th",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:93741,Performance,load,loader,93741,") {; 2196 MethodBase *theMethod = dynamic_cast<MethodBase *>((*methods)[i]);; 2197 if (theMethod == 0); 2198 continue;; 2199 // write test/training trees; 2200 RootBaseDir()->cd(theMethod->fDataSetInfo.GetName());; 2201 if (std::find(datasets.begin(), datasets.end(), theMethod->fDataSetInfo.GetName()) == datasets.end()) {; 2202 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTesting)->Write("""", TObject::kOverwrite);; 2203 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTraining)->Write("""", TObject::kOverwrite);; 2204 datasets.push_back(theMethod->fDataSetInfo.GetName());; 2205 }; 2206 }; 2207 }; 2208 }; 2209 } // end for MethodsMap; 2210 // references for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more th",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:93856,Performance,load,loader,93856,"ences for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo(",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:93974,Performance,load,loader,93974,"ences for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo(",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:94270,Performance,load,loader,94270,"ences for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo(",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:94871,Performance,load,loader,94871,"rt); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:95078,Performance,load,loader,95078,"rt); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:95109,Performance,load,loader,95109,"rt); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:95189,Performance,load,loader,95189,"e to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // ad",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:96001,Performance,load,loader,96001,"on); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // adding variables from seed; 2286 for (int index = 0; index < nbits; index++) {; 2287 if (xbitset[index]); 2288 seedloader->AddVariable(varNames[index], 'F');; 2289 }; 2290 ; 2291 DataLoaderCopy(seedloader, loader);; 2292 seedloader->PrepareTrainingAndTestTree(loader->GetDataSetInfo().GetCut(""Signal""),; 2293 loader->GetDataSetInfo().GetCut(""Background""),; 2294 loader->GetDataSetInfo().GetSplitOptions());; 2295 ; 2296 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMV",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:96060,Performance,load,loader,96060,"on); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2255 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2256 ; 2257 if (nbits > 60) {; 2258 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // adding variables from seed; 2286 for (int index = 0; index < nbits; index++) {; 2287 if (xbitset[index]); 2288 seedloader->AddVariable(varNames[index], 'F');; 2289 }; 2290 ; 2291 DataLoaderCopy(seedloader, loader);; 2292 seedloader->PrepareTrainingAndTestTree(loader->GetDataSetInfo().GetCut(""Signal""),; 2293 loader->GetDataSetInfo().GetCut(""Background""),; 2294 loader->GetDataSetInfo().GetSplitOptions());; 2295 ; 2296 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMV",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:96378,Performance,load,loader,96378," of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // adding variables from seed; 2286 for (int index = 0; index < nbits; index++) {; 2287 if (xbitset[index]); 2288 seedloader->AddVariable(varNames[index], 'F');; 2289 }; 2290 ; 2291 DataLoaderCopy(seedloader, loader);; 2292 seedloader->PrepareTrainingAndTestTree(loader->GetDataSetInfo().GetCut(""Signal""),; 2293 loader->GetDataSetInfo().GetCut(""Background""),; 2294 loader->GetDataSetInfo().GetSplitOptions());; 2295 ; 2296 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2309 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2310 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2311 delete sresults;; 2312 delete seedloader;; 2313 this->Delet",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:96432,Performance,load,loader,96432," of combinations is too large , is 2^"" << nbits << Endl;; 2259 return nullptr;; 2260 }; 2261 if (nbits > 20) {; 2262 Log() << kWARNING << ""Number of combinations is very large , is 2^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // adding variables from seed; 2286 for (int index = 0; index < nbits; index++) {; 2287 if (xbitset[index]); 2288 seedloader->AddVariable(varNames[index], 'F');; 2289 }; 2290 ; 2291 DataLoaderCopy(seedloader, loader);; 2292 seedloader->PrepareTrainingAndTestTree(loader->GetDataSetInfo().GetCut(""Signal""),; 2293 loader->GetDataSetInfo().GetCut(""Background""),; 2294 loader->GetDataSetInfo().GetSplitOptions());; 2295 ; 2296 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2309 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2310 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2311 delete sresults;; 2312 delete seedloader;; 2313 this->Delet",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:96481,Performance,load,loader,96481,"^"" << nbits << Endl;; 2263 }; 2264 uint64_t range = static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // adding variables from seed; 2286 for (int index = 0; index < nbits; index++) {; 2287 if (xbitset[index]); 2288 seedloader->AddVariable(varNames[index], 'F');; 2289 }; 2290 ; 2291 DataLoaderCopy(seedloader, loader);; 2292 seedloader->PrepareTrainingAndTestTree(loader->GetDataSetInfo().GetCut(""Signal""),; 2293 loader->GetDataSetInfo().GetCut(""Background""),; 2294 loader->GetDataSetInfo().GetSplitOptions());; 2295 ; 2296 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2309 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2310 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2311 delete sresults;; 2312 delete seedloader;; 2313 this->DeleteAllMethods();; 2314 ; 2315 fMethodsMap.clear();; 2316 // removing global result because it is requiring a lot of RAM for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:96534,Performance,load,loader,96534," static_cast<uint64_t>(pow(2, nbits));; 2265 ; 2266 ; 2267 // vector to save importances; 2268 std::vector<Double_t> importances(nbits);; 2269 // vector to save ROC; 2270 std::vector<Double_t> ROC(range);; 2271 ROC[0] = 0.5;; 2272 for (int i = 0; i < nbits; i++); 2273 importances[i] = 0;; 2274 ; 2275 Double_t SROC, SSROC; // computed ROC value; 2276 for (x = 1; x < range; x++) {; 2277 ; 2278 std::bitset<VIBITS> xbitset(x);; 2279 if (x == 0); 2280 continue; // data loader need at least one variable; 2281 ; 2282 // creating loader for seed; 2283 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2284 ; 2285 // adding variables from seed; 2286 for (int index = 0; index < nbits; index++) {; 2287 if (xbitset[index]); 2288 seedloader->AddVariable(varNames[index], 'F');; 2289 }; 2290 ; 2291 DataLoaderCopy(seedloader, loader);; 2292 seedloader->PrepareTrainingAndTestTree(loader->GetDataSetInfo().GetCut(""Signal""),; 2293 loader->GetDataSetInfo().GetCut(""Background""),; 2294 loader->GetDataSetInfo().GetSplitOptions());; 2295 ; 2296 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2309 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2310 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2311 delete sresults;; 2312 delete seedloader;; 2313 this->DeleteAllMethods();; 2314 ; 2315 fMethodsMap.clear();; 2316 // removing global result because it is requiring a lot of RAM for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:98675,Performance,load,loader,98675," for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i < VIBITS; ++i) {; 2322 if (x & (uint64_t(1) << i)) {; 2323 y = x & ~(1 << i);; 2324 std::bitset<VIBITS> ybitset(y);; 2325 // need at least one variable; 2326 // NOTE: if sub-seed is zero then is the special case; 2327 // that count in xbitset is 1; 2328 uint32_t ny = static_cast<uint32_t>( log(x - y) / 0.693147 ) ;; 2329 if (y == 0) {; 2330 importances[ny] = SROC - 0.5;; 2331 continue;; 2332 }; 2333 ; 2334 // getting ROC; 2335 SSROC = ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356////////////////////////////////////////////////////////////////////////////////; 2357 ; 2358TH1F *TMVA::Factory::EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:98875,Performance,load,loader,98875," for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i < VIBITS; ++i) {; 2322 if (x & (uint64_t(1) << i)) {; 2323 y = x & ~(1 << i);; 2324 std::bitset<VIBITS> ybitset(y);; 2325 // need at least one variable; 2326 // NOTE: if sub-seed is zero then is the special case; 2327 // that count in xbitset is 1; 2328 uint32_t ny = static_cast<uint32_t>( log(x - y) / 0.693147 ) ;; 2329 if (y == 0) {; 2330 importances[ny] = SROC - 0.5;; 2331 continue;; 2332 }; 2333 ; 2334 // getting ROC; 2335 SSROC = ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356////////////////////////////////////////////////////////////////////////////////; 2357 ; 2358TH1F *TMVA::Factory::EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:98906,Performance,load,loader,98906," for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i < VIBITS; ++i) {; 2322 if (x & (uint64_t(1) << i)) {; 2323 y = x & ~(1 << i);; 2324 std::bitset<VIBITS> ybitset(y);; 2325 // need at least one variable; 2326 // NOTE: if sub-seed is zero then is the special case; 2327 // that count in xbitset is 1; 2328 uint32_t ny = static_cast<uint32_t>( log(x - y) / 0.693147 ) ;; 2329 if (y == 0) {; 2330 importances[ny] = SROC - 0.5;; 2331 continue;; 2332 }; 2333 ; 2334 // getting ROC; 2335 SSROC = ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356////////////////////////////////////////////////////////////////////////////////; 2357 ; 2358TH1F *TMVA::Factory::EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:98986,Performance,load,loader,98986," ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356////////////////////////////////////////////////////////////////////////////////; 2357 ; 2358TH1F *TMVA::Factory::EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 Double_t SROC, SSROC; // computed ROC value; 2380 ; 2381 x = range;; 2382 ; 2383 std::bitset<VIBITS> xbitset(x);; 2384 if (x == 0); 2385 Log() << kFATAL << ""Error: need at least one variable.""; // data loader need at least one variable; 2386 ; 2387 // creating loader for seed; 2388 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2389 ; 2390 // adding variables from seed; 2391 for (int index = 0; index < nbits; index++) {; 2392 if (xbitset[index]); 2393 seedloader->AddVariable(varNames[index], 'F');; 2394 }; 2395 ; 2396 //",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:99619,Performance,load,loader,99619,"ypes::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 Double_t SROC, SSROC; // computed ROC value; 2380 ; 2381 x = range;; 2382 ; 2383 std::bitset<VIBITS> xbitset(x);; 2384 if (x == 0); 2385 Log() << kFATAL << ""Error: need at least one variable.""; // data loader need at least one variable; 2386 ; 2387 // creating loader for seed; 2388 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2389 ; 2390 // adding variables from seed; 2391 for (int index = 0; index < nbits; index++) {; 2392 if (xbitset[index]); 2393 seedloader->AddVariable(varNames[index], 'F');; 2394 }; 2395 ; 2396 // Loading Dataset; 2397 DataLoaderCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2409 ; 2410 // cleaning information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);;",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:99678,Performance,load,loader,99678,"ypes::EMVA theMethod, TString methodTitle,; 2359 const char *theOption); 2360{; 2361 uint64_t x = 0;; 2362 uint64_t y = 0;; 2363 ; 2364 // getting number of variables and variable names from loader; 2365 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2366 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 Double_t SROC, SSROC; // computed ROC value; 2380 ; 2381 x = range;; 2382 ; 2383 std::bitset<VIBITS> xbitset(x);; 2384 if (x == 0); 2385 Log() << kFATAL << ""Error: need at least one variable.""; // data loader need at least one variable; 2386 ; 2387 // creating loader for seed; 2388 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2389 ; 2390 // adding variables from seed; 2391 for (int index = 0; index < nbits; index++) {; 2392 if (xbitset[index]); 2393 seedloader->AddVariable(varNames[index], 'F');; 2394 }; 2395 ; 2396 // Loading Dataset; 2397 DataLoaderCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2409 ; 2410 // cleaning information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);;",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:100021,Performance,load,loader,100021,";; 2367 ; 2368 if (nbits > 60) {; 2369 Log() << kERROR << ""Number of combinations is too large , is 2^"" << nbits << Endl;; 2370 return nullptr;; 2371 }; 2372 long int range = sum(nbits);; 2373 // std::cout<<range<<std::endl;; 2374 // vector to save importances; 2375 std::vector<Double_t> importances(nbits);; 2376 for (int i = 0; i < nbits; i++); 2377 importances[i] = 0;; 2378 ; 2379 Double_t SROC, SSROC; // computed ROC value; 2380 ; 2381 x = range;; 2382 ; 2383 std::bitset<VIBITS> xbitset(x);; 2384 if (x == 0); 2385 Log() << kFATAL << ""Error: need at least one variable.""; // data loader need at least one variable; 2386 ; 2387 // creating loader for seed; 2388 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2389 ; 2390 // adding variables from seed; 2391 for (int index = 0; index < nbits; index++) {; 2392 if (xbitset[index]); 2393 seedloader->AddVariable(varNames[index], 'F');; 2394 }; 2395 ; 2396 // Loading Dataset; 2397 DataLoaderCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2409 ; 2410 // cleaning information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2414 delete sresults;; 2415 delete seedloader;; 2416 this->DeleteAllMethods();; 2417 fMethodsMap.clear();; 2418 ; 2419 // removing global result because it is requiring a lot of RAM for all seeds; 2420 ; 2421 for (uint32_t i = 0; i < VIBITS; ++i) {; 2422 if (x & (1 << i)) {; 2423 y = x & ~(uint64_t(1) << i);; 2424 std::bitset<VIBITS> ybitset(y);",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:101336,Performance,load,loader,101336," information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2414 delete sresults;; 2415 delete seedloader;; 2416 this->DeleteAllMethods();; 2417 fMethodsMap.clear();; 2418 ; 2419 // removing global result because it is requiring a lot of RAM for all seeds; 2420 ; 2421 for (uint32_t i = 0; i < VIBITS; ++i) {; 2422 if (x & (1 << i)) {; 2423 y = x & ~(uint64_t(1) << i);; 2424 std::bitset<VIBITS> ybitset(y);; 2425 // need at least one variable; 2426 // NOTE: if sub-seed is zero then is the special case; 2427 // that count in xbitset is 1; 2428 uint32_t ny = static_cast<uint32_t>(log(x - y) / 0.693147);; 2429 if (y == 0) {; 2430 importances[ny] = SROC - 0.5;; 2431 continue;; 2432 }; 2433 ; 2434 // creating loader for sub-seed; 2435 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2436 // adding variables from sub-seed; 2437 for (int index = 0; index < nbits; index++) {; 2438 if (ybitset[index]); 2439 subseedloader->AddVariable(varNames[index], 'F');; 2440 }; 2441 ; 2442 // Loading Dataset; 2443 DataLoaderCopy(subseedloader, loader);; 2444 ; 2445 // Booking SubSeed; 2446 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2447 ; 2448 // Train/Test/Evaluation; 2449 TrainAllMethods();; 2450 TestAllMethods();; 2451 EvaluateAllMethods();; 2452 ; 2453 // getting ROC; 2454 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle);; 2455 importances[ny] += SROC - SSROC;; 2456 ; 2457 // cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting,",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:101689,Performance,load,loader,101689,"ults;; 2415 delete seedloader;; 2416 this->DeleteAllMethods();; 2417 fMethodsMap.clear();; 2418 ; 2419 // removing global result because it is requiring a lot of RAM for all seeds; 2420 ; 2421 for (uint32_t i = 0; i < VIBITS; ++i) {; 2422 if (x & (1 << i)) {; 2423 y = x & ~(uint64_t(1) << i);; 2424 std::bitset<VIBITS> ybitset(y);; 2425 // need at least one variable; 2426 // NOTE: if sub-seed is zero then is the special case; 2427 // that count in xbitset is 1; 2428 uint32_t ny = static_cast<uint32_t>(log(x - y) / 0.693147);; 2429 if (y == 0) {; 2430 importances[ny] = SROC - 0.5;; 2431 continue;; 2432 }; 2433 ; 2434 // creating loader for sub-seed; 2435 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2436 // adding variables from sub-seed; 2437 for (int index = 0; index < nbits; index++) {; 2438 if (ybitset[index]); 2439 subseedloader->AddVariable(varNames[index], 'F');; 2440 }; 2441 ; 2442 // Loading Dataset; 2443 DataLoaderCopy(subseedloader, loader);; 2444 ; 2445 // Booking SubSeed; 2446 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2447 ; 2448 // Train/Test/Evaluation; 2449 TrainAllMethods();; 2450 TestAllMethods();; 2451 EvaluateAllMethods();; 2452 ; 2453 // getting ROC; 2454 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle);; 2455 importances[ny] += SROC - SSROC;; 2456 ; 2457 // cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471//////////////////////////////////////////",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:102811,Performance,load,loader,102811,"eed; 2446 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2447 ; 2448 // Train/Test/Evaluation; 2449 TrainAllMethods();; 2450 TestAllMethods();; 2451 EvaluateAllMethods();; 2452 ; 2453 // getting ROC; 2454 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle);; 2455 importances[ny] += SROC - SSROC;; 2456 ; 2457 // cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating lo",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103090,Performance,load,loader,103090,"/ cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(se",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103121,Performance,load,loader,103121,"/ cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(se",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103201,Performance,load,loader,103201,"()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(seedloader, loader);; 2511 ; 2512 // Booking Seed; 2513 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2514 ; 2515 //",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103666,Performance,load,loader,103666,"d::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(seedloader, loader);; 2511 ; 2512 // Booking Seed; 2513 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2514 ; 2515 // Train/Test/Evaluation; 2516 TrainAllMethods();; 2517 TestAllMethods();; 2518 EvaluateAllMethods();; 2519 ; 2520 // getting ROC; 2521 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2522 // std::cout << ""Seed: n "" << n << "" x "" << x << "" xbitset:"" << xbitset << "" ROC "" << SROC << std::endl;; 2523 ; 2524 // cleaning info",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:103725,Performance,load,loader,103725,"d::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(seedloader, loader);; 2511 ; 2512 // Booking Seed; 2513 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2514 ; 2515 // Train/Test/Evaluation; 2516 TrainAllMethods();; 2517 TestAllMethods();; 2518 EvaluateAllMethods();; 2519 ; 2520 // getting ROC; 2521 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2522 // std::cout << ""Seed: n "" << n << "" x "" << x << "" xbitset:"" << xbitset << "" ROC "" << SROC << std::endl;; 2523 ; 2524 // cleaning info",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:104068,Performance,load,loader,104068," names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating loader for seed; 2501 TMVA::DataLoader *seedloader = new TMVA::DataLoader(xbitset.to_string());; 2502 ; 2503 // adding variables from seed; 2504 for (int index = 0; index < nbits; index++) {; 2505 if (xbitset[index]); 2506 seedloader->AddVariable(varNames[index], 'F');; 2507 }; 2508 ; 2509 // Loading Dataset; 2510 DataLoaderCopy(seedloader, loader);; 2511 ; 2512 // Booking Seed; 2513 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2514 ; 2515 // Train/Test/Evaluation; 2516 TrainAllMethods();; 2517 TestAllMethods();; 2518 EvaluateAllMethods();; 2519 ; 2520 // getting ROC; 2521 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2522 // std::cout << ""Seed: n "" << n << "" x "" << x << "" xbitset:"" << xbitset << "" ROC "" << SROC << std::endl;; 2523 ; 2524 // cleaning information to process sub-seeds; 2525 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2526 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2527 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2528 delete sresults;; 2529 delete seedloader;; 2530 this->DeleteAllMethods();; 2531 fMethodsMap.clear();; 2532 ; 2533 // removing global result because it is requiring a lot of RAM for all seeds; 2534 ; 2535 for (uint32_t i = 0; i < 32",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:105553,Performance,load,loader,105553,"ase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2526 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2527 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2528 delete sresults;; 2529 delete seedloader;; 2530 this->DeleteAllMethods();; 2531 fMethodsMap.clear();; 2532 ; 2533 // removing global result because it is requiring a lot of RAM for all seeds; 2534 ; 2535 for (uint32_t i = 0; i < 32; ++i) {; 2536 if (x & (uint64_t(1) << i)) {; 2537 y = x & ~(1 << i);; 2538 std::bitset<32> ybitset(y);; 2539 // need at least one variable; 2540 // NOTE: if sub-seed is zero then is the special case; 2541 // that count in xbitset is 1; 2542 Double_t ny = log(x - y) / 0.693147;; 2543 if (y == 0) {; 2544 importances[ny] = SROC - 0.5;; 2545 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << ""ROC "" << 0.5 << std::endl;; 2546 continue;; 2547 }; 2548 ; 2549 // creating loader for sub-seed; 2550 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2551 // adding variables from sub-seed; 2552 for (int index = 0; index < nbits; index++) {; 2553 if (ybitset[index]); 2554 subseedloader->AddVariable(varNames[index], 'F');; 2555 }; 2556 ; 2557 // Loading Dataset; 2558 DataLoaderCopy(subseedloader, loader);; 2559 ; 2560 // Booking SubSeed; 2561 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2562 ; 2563 // Train/Test/Evaluation; 2564 TrainAllMethods();; 2565 TestAllMethods();; 2566 EvaluateAllMethods();; 2567 ; 2568 // getting ROC; 2569 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle);; 2570 importances[ny] += SROC - SSROC;; 2571 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << "" x-y "" << x - y << "" "" << std::bitset<32>(x - y) <<; 2572 // "" ny "" << ny << "" SROC "" << SROC << "" SSROC "" << SSROC << "" Importance = "" << importances[ny] <<; 2573 // std::endl; cleaning information; 2574 TMVA::MethodBase *ssmethod =; 2575 d",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:105906,Performance,load,loader,105906,"();; 2531 fMethodsMap.clear();; 2532 ; 2533 // removing global result because it is requiring a lot of RAM for all seeds; 2534 ; 2535 for (uint32_t i = 0; i < 32; ++i) {; 2536 if (x & (uint64_t(1) << i)) {; 2537 y = x & ~(1 << i);; 2538 std::bitset<32> ybitset(y);; 2539 // need at least one variable; 2540 // NOTE: if sub-seed is zero then is the special case; 2541 // that count in xbitset is 1; 2542 Double_t ny = log(x - y) / 0.693147;; 2543 if (y == 0) {; 2544 importances[ny] = SROC - 0.5;; 2545 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << ""ROC "" << 0.5 << std::endl;; 2546 continue;; 2547 }; 2548 ; 2549 // creating loader for sub-seed; 2550 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2551 // adding variables from sub-seed; 2552 for (int index = 0; index < nbits; index++) {; 2553 if (ybitset[index]); 2554 subseedloader->AddVariable(varNames[index], 'F');; 2555 }; 2556 ; 2557 // Loading Dataset; 2558 DataLoaderCopy(subseedloader, loader);; 2559 ; 2560 // Booking SubSeed; 2561 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2562 ; 2563 // Train/Test/Evaluation; 2564 TrainAllMethods();; 2565 TestAllMethods();; 2566 EvaluateAllMethods();; 2567 ; 2568 // getting ROC; 2569 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle);; 2570 importances[ny] += SROC - SSROC;; 2571 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << "" x-y "" << x - y << "" "" << std::bitset<32>(x - y) <<; 2572 // "" ny "" << ny << "" SROC "" << SROC << "" SSROC "" << SSROC << "" Importance = "" << importances[ny] <<; 2573 // std::endl; cleaning information; 2574 TMVA::MethodBase *ssmethod =; 2575 dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2576 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2577 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2578 delete ssresults;; 2579 delete subseedloader;; 2580 this->DeleteAllMethods();; 2581 ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:120807,Performance,load,loader,120807,"ning(Bool_t)when this static function is called, it sets the flag whether events with negative event weight shoul...Definition Event.cxx:399; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::PrintHelpMessagevoid PrintHelpMessage(const TString &datasetname, const TString &methodTitle="""") constPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::VerboseBool_t Verbose(void) constDefinition Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)Definition Factory.cxx:602; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::FactoryFactory(TString theJobName, TFile *theTargetFile, TString theOption="""")Standard constructor.Definition Factory.cxx:113; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances,",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:121605,Performance,load,loader,121605,"n Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)Definition Factory.cxx:602; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::FactoryFactory(TString theJobName, TFile *theTargetFile, TString theOption="""")Standard constructor.Definition Factory.cxx:113; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VI",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:122051,Performance,load,loader,122051,"tructor.Definition Factory.cxx:113; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::~Factoryvirtual ~Fac",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:122377,Performance,load,loader,122377,"ateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::~Factoryvirtual ~Factory()Destructor.Definition Factory.cxx:306; TMVA::Factory::MakeClassvirtual void MakeClass(const TString &datasetname, const TString &methodTitle="""") constDefinition Factory.cxx:1305; TMVA::Factory::BookMethodWeightfileMethodBase * BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile)Adds an",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:122604,Performance,load,loader,122604,"ds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::~Factoryvirtual ~Factory()Destructor.Definition Factory.cxx:306; TMVA::Factory::MakeClassvirtual void MakeClass(const TString &datasetname, const TString &methodTitle="""") constDefinition Factory.cxx:1305; TMVA::Factory::BookMethodWeightfileMethodBase * BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile)Adds an already constructed method to be managed by this factory.Definition Factory.cxx:501; TMVA::Factory::fModelPersistenceBool_t fModelPersistence! option to save the trained model in xml file or using serializationDefinition Factory.h",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:122819,Performance,load,loader,122819,"ition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::~Factoryvirtual ~Factory()Destructor.Definition Factory.cxx:306; TMVA::Factory::MakeClassvirtual void MakeClass(const TString &datasetname, const TString &methodTitle="""") constDefinition Factory.cxx:1305; TMVA::Factory::BookMethodWeightfileMethodBase * BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile)Adds an already constructed method to be managed by this factory.Definition Factory.cxx:501; TMVA::Factory::fModelPersistenceBool_t fModelPersistence! option to save the trained model in xml file or using serializationDefinition Factory.h:222; TMVA::Factory::OptimizeAllMethodsstd::map< TString, Double_t > OptimizeAllMethods(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Iterates through all booked methods and sees if they use parameter tuning and if so does just that",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:123949,Performance,load,loader,123949,"ition Factory.cxx:849; TMVA::Factory::~Factoryvirtual ~Factory()Destructor.Definition Factory.cxx:306; TMVA::Factory::MakeClassvirtual void MakeClass(const TString &datasetname, const TString &methodTitle="""") constDefinition Factory.cxx:1305; TMVA::Factory::BookMethodWeightfileMethodBase * BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile)Adds an already constructed method to be managed by this factory.Definition Factory.cxx:501; TMVA::Factory::fModelPersistenceBool_t fModelPersistence! option to save the trained model in xml file or using serializationDefinition Factory.h:222; TMVA::Factory::OptimizeAllMethodsstd::map< TString, Double_t > OptimizeAllMethods(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Iterates through all booked methods and sees if they use parameter tuning and if so does just that,...Definition Factory.cxx:701; TMVA::Factory::GetROCROCCurve * GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Private method to generate a ROCCurve instance for a given method.Definition Factory.cxx:749; TMVA::Factory::EvaluateImportanceShortTH1F * EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2358; TMVA::Factory::fAnalysisTypeTypes::EAnalysisType fAnalysisType! the training typeDefinition Factory.h:221; TMVA::Factory::HasMethodBool_t HasMethod(const TString &datasetname, const TString &title) constChecks whether a given method name is defined for a given dataset.Definition Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA th",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:124210,Performance,load,loader,124210,"nition Factory.cxx:1305; TMVA::Factory::BookMethodWeightfileMethodBase * BookMethodWeightfile(DataLoader *dataloader, TMVA::Types::EMVA methodType, const TString &weightfile)Adds an already constructed method to be managed by this factory.Definition Factory.cxx:501; TMVA::Factory::fModelPersistenceBool_t fModelPersistence! option to save the trained model in xml file or using serializationDefinition Factory.h:222; TMVA::Factory::OptimizeAllMethodsstd::map< TString, Double_t > OptimizeAllMethods(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")Iterates through all booked methods and sees if they use parameter tuning and if so does just that,...Definition Factory.cxx:701; TMVA::Factory::GetROCROCCurve * GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Private method to generate a ROCCurve instance for a given method.Definition Factory.cxx:749; TMVA::Factory::EvaluateImportanceShortTH1F * EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2358; TMVA::Factory::fAnalysisTypeTypes::EAnalysisType fAnalysisType! the training typeDefinition Factory.h:221; TMVA::Factory::HasMethodBool_t HasMethod(const TString &datasetname, const TString &title) constChecks whether a given method name is defined for a given dataset.Definition Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! R",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:124672,Performance,load,loader,124672,"egral"", TString fitType=""FitGA"")Iterates through all booked methods and sees if they use parameter tuning and if so does just that,...Definition Factory.cxx:701; TMVA::Factory::GetROCROCCurve * GetROC(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Private method to generate a ROCCurve instance for a given method.Definition Factory.cxx:749; TMVA::Factory::EvaluateImportanceShortTH1F * EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2358; TMVA::Factory::fAnalysisTypeTypes::EAnalysisType fAnalysisType! the training typeDefinition Factory.h:221; TMVA::Factory::HasMethodBool_t HasMethod(const TString &datasetname, const TString &title) constChecks whether a given method name is defined for a given dataset.Definition Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetname, const TString &title) constReturns pointer to MVA that corresponds to given method title.Definition Factory.cxx:566; TMVA::Factory::DeleteAllMethodsvoid DeleteAllMethods(void)Delete methods.Definition Factory.cxx:324; TMVA::Factory::fTransformationsTString fTransformations! list of transformations to testDefinition Factory.h:212; TMVA::Factory::Greetingsvoid Greetings()Print welcome message.Definition Facto",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:124973,Performance,load,loader,124973,", UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Private method to generate a ROCCurve instance for a given method.Definition Factory.cxx:749; TMVA::Factory::EvaluateImportanceShortTH1F * EvaluateImportanceShort(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2358; TMVA::Factory::fAnalysisTypeTypes::EAnalysisType fAnalysisType! the training typeDefinition Factory.h:221; TMVA::Factory::HasMethodBool_t HasMethod(const TString &datasetname, const TString &title) constChecks whether a given method name is defined for a given dataset.Definition Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetname, const TString &title) constReturns pointer to MVA that corresponds to given method title.Definition Factory.cxx:566; TMVA::Factory::DeleteAllMethodsvoid DeleteAllMethods(void)Delete methods.Definition Factory.cxx:324; TMVA::Factory::fTransformationsTString fTransformations! list of transformations to testDefinition Factory.h:212; TMVA::Factory::Greetingsvoid Greetings()Print welcome message.Definition Factory.cxx:295; TMVA::IMethodInterface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::IMethod::PrintHelpMessagevirtual void PrintHelpMessage() const =0; TMVA::IMethod::HasAnalysisTypevirtual Bool_t HasAnalysisType(Type",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:130205,Performance,tune,tuned,130205,"dOptions"" is done in an independent call, since it may be overr...Definition MethodBase.cxx:423; TMVA::MethodBase::GetTrainingEfficiencyvirtual Double_t GetTrainingEfficiency(const TString &)Definition MethodBase.cxx:2528; TMVA::MethodBase::DataInfoDataSetInfo & DataInfo() constDefinition MethodBase.h:410; TMVA::MethodBase::MakeClassvirtual void MakeClass(const TString &classFileName=TString("""")) constcreate reader class for method (classification only at present)Definition MethodBase.cxx:3003; TMVA::MethodBase::TestClassificationvirtual void TestClassification()initializationDefinition MethodBase.cxx:1127; TMVA::MethodBase::AddOutputvoid AddOutput(Types::ETreeType type, Types::EAnalysisType analysisType)Definition MethodBase.cxx:1315; TMVA::MethodBase::ReadStateFromFilevoid ReadStateFromFile()Function to write options and weights to file.Definition MethodBase.cxx:1426; TMVA::MethodBase::OptimizeTuningParametersvirtual std::map< TString, Double_t > OptimizeTuningParameters(TString fomType=""ROCIntegral"", TString fitType=""FitGA"")call the Optimizer with the set of parameters and ranges that are meant to be tuned.Definition MethodBase.cxx:623; TMVA::MethodBase::fDataSetInfoDataSetInfo & fDataSetInfoDefinition MethodBase.h:607; TMVA::MethodBase::GetMethodTypeTypes::EMVA GetMethodType() constDefinition MethodBase.h:333; TMVA::MethodBase::SetFilevoid SetFile(TFile *file)Definition MethodBase.h:375; TMVA::MethodBase::DataDataSet * Data() constDefinition MethodBase.h:409; TMVA::MethodBase::SetModelPersistencevoid SetModelPersistence(Bool_t status)Definition MethodBase.h:382; TMVA::MethodBase::GetROCIntegralvirtual Double_t GetROCIntegral(TH1D *histS, TH1D *histB) constcalculate the area (integral) under the ROC curve as a overall quality measure of the classificationDefinition MethodBase.cxx:2822; TMVA::MethodBase::CheckSetupvirtual void CheckSetup()check may be overridden by derived class (sometimes, eg, fitters are used which can only be implement...Definition MethodBase.c",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:26421,Safety,avoid,avoid,26421,"rix ("") + fDataSetInfo.GetClassInfo(cls)->GetName() + TString("")""));; 624 if (h != 0) {; 625 h->Write();; 626 delete h;; 627 }; 628 }; 629 } else {; 630 m = fDataSetInfo.CorrelationMatrix(""Signal"");; 631 h = fDataSetInfo.CreateCorrelationMatrixHist(m, ""CorrelationMatrixS"", ""Correlation Matrix (signal)"");; 632 if (h != 0) {; 633 h->Write();; 634 delete h;; 635 }; 636 ; 637 m = fDataSetInfo.CorrelationMatrix(""Background"");; 638 h = fDataSetInfo.CreateCorrelationMatrixHist(m, ""CorrelationMatrixB"", ""Correlation Matrix (background)"");; 639 if (h != 0) {; 640 h->Write();; 641 delete h;; 642 }; 643 ; 644 m = fDataSetInfo.CorrelationMatrix(""Regression"");; 645 h = fDataSetInfo.CreateCorrelationMatrixHist(m, ""CorrelationMatrix"", ""Correlation Matrix"");; 646 if (h != 0) {; 647 h->Write();; 648 delete h;; 649 }; 650 }; 651 ; 652 // some default transformations to evaluate; 653 // NOTE: all transformations are destroyed after this test; 654 TString processTrfs = ""I""; //""I;N;D;P;U;G,D;""; 655 ; 656 // plus some user defined transformations; 657 processTrfs = fTransformations;; 658 ; 659 // remove any trace of identity transform - if given (avoid to apply it twice); 660 std::vector<TMVA::TransformationHandler *> trfs;; 661 TransformationHandler *identityTrHandler = 0;; 662 ; 663 std::vector<TString> trfsDef = gTools().SplitString(processTrfs, ';');; 664 std::vector<TString>::iterator trfsDefIt = trfsDef.begin();; 665 for (; trfsDefIt != trfsDef.end(); ++trfsDefIt) {; 666 trfs.push_back(new TMVA::TransformationHandler(fDataSetInfo, ""Factory""));; 667 TString trfS = (*trfsDefIt);; 668 ; 669 // Log() << kINFO << Endl;; 670 Log() << kDEBUG << ""current transformation string: '"" << trfS.Data() << ""'"" << Endl;; 671 TMVA::CreateVariableTransforms(trfS, fDataSetInfo, *(trfs.back()), Log());; 672 ; 673 if (trfS.BeginsWith('I')); 674 identityTrHandler = trfs.back();; 675 }; 676 ; 677 const std::vector<Event *> &inputEvents = fDataSetInfo.GetDataSet()->GetEventCollection();; 678 ; 679 // apply a",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:94141,Safety,avoid,avoid,94141,"ences for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle,; 2218 const char *theOption); 2219{; 2220 fModelPersistence = kFALSE;; 2221 fSilentFile = kTRUE; // we need silent file here because we need fast classification results; 2222 ; 2223 // getting number of variables and variable names from loader; 2224 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2225 if (vitype == VIType::kShort); 2226 return EvaluateImportanceShort(loader, theMethod, methodTitle, theOption);; 2227 else if (vitype == VIType::kAll); 2228 return EvaluateImportanceAll(loader, theMethod, methodTitle, theOption);; 2229 else if (vitype == VIType::kRandom) {; 2230 if ( nbits > 10 && nbits < 30) {; 2231 // limit nbits to less than 30 to avoid error converting from double to uint and also cannot deal with too many combinations; 2232 return EvaluateImportanceRandom(loader, static_cast<UInt_t>( pow(2, nbits) ), theMethod, methodTitle, theOption);; 2233 } else if (nbits < 10) {; 2234 Log() << kERROR << ""Error in Variable Importance: Random mode require more that 10 variables in the dataset.""; 2235 << Endl;; 2236 } else if (nbits > 30) {; 2237 Log() << kERROR << ""Error in Variable Importance: Number of variables is too large for Random mode""; 2238 << Endl;; 2239 }; 2240 }; 2241 return nullptr;; 2242}; 2243 ; 2244////////////////////////////////////////////////////////////////////////////////; 2245 ; 2246TH1F *TMVA::Factory::EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle,; 2247 const char *theOption); 2248{; 2249 ; 2250 uint64_t x = 0;; 2251 uint64_t y = 0;; 2252 ; 2253 // getting number of variables and variable names from loader; 2254 const int nbits = loader->GetDataSetInfo(",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:114165,Security,access,access,114165,"TRUE)Sets the flag controlling the automatic add of histograms in memory.Definition TH1.cxx:1294; TH1::GetXaxisTAxis * GetXaxis()Definition TH1.h:324; TH1::GetYaxisTAxis * GetYaxis()Definition TH1.h:325; TH1::SetBinContentvirtual void SetBinContent(Int_t bin, Double_t content)Set bin content see convention for numbering bins in TH1::GetBin In case the bin number is greater th...Definition TH1.cxx:9222; TH1::SetBarWidthvirtual void SetBarWidth(Float_t width=0.5)Set the width of bars as fraction of the bin width for drawing mode ""B"".Definition TH1.h:365; TH2Service class for 2-D histogram classes.Definition TH2.h:30; TMVA::ClassifierFactory::CreateIMethod * Create(const std::string &name, const TString &job, const TString &title, DataSetInfo &dsi, const TString &option)creates the method if needed based on the method name using the creator function the factory has stor...Definition ClassifierFactory.cxx:89; TMVA::ClassifierFactory::Instancestatic ClassifierFactory & Instance()access to the ClassifierFactory singleton creates the instance if neededDefinition ClassifierFactory.cxx:48; TMVA::Config::IONames::fWeightFileDirTString fWeightFileDirDefinition Config.h:124; TMVA::Config::IONames::fWeightFileDirPrefixTString fWeightFileDirPrefixDefinition Config.h:123; TMVA::Config::VariablePlotting::fNbinsXOfROCCurveInt_t fNbinsXOfROCCurveDefinition Config.h:111; TMVA::Config::SetDrawProgressBarvoid SetDrawProgressBar(Bool_t d)Definition Config.h:69; TMVA::Config::SetUseColorvoid SetUseColor(Bool_t uc)Definition Config.h:60; TMVA::Config::fVariablePlottingclass TMVA::Config::VariablePlotting fVariablePlotting; TMVA::Config::SetSilentvoid SetSilent(Bool_t s)Definition Config.h:63; TMVA::Config::GetIONamesIONames & GetIONames()Definition Config.h:98; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::Configurable::SetConfigDescriptionvoid SetConfigDescription(const char *d)Definition Configurable.h:64; TMVA::Configurable::DeclareOptionRefOptionBase * DeclareOptionRef(T &ref, c",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:2062,Testability,test,testing,2062,"@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 22 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 23 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 24 * Kim Albertsson <kim.albertsson@cern.ch> - LTU & CERN *; 25 * *; 26 * Copyright (c) 2005-2015: *; 27 * CERN, Switzerland *; 28 * U. of Victoria, Canada *; 29 * MPI-K Heidelberg, Germany *; 30 * U. of Bonn, Germany *; 31 * UdeA/ITM, Colombia *; 32 * U. of Florida, USA *; 33 * *; 34 * Redistribution and use in source and binary forms, with or without *; 35 * modification, are permitted according to the terms listed in LICENSE *; 36 * (see tmva/doc/LICENSE) *; 37 **********************************************************************************/; 38 ; 39/*! \class TMVA::Factory; 40\ingroup TMVA; 41 ; 42This is the main MVA steering class.; 43It creates all MVA methods, and guides them through the training, testing and; 44evaluation phases.; 45*/; 46 ; 47#include ""TMVA/Factory.h""; 48 ; 49#include ""TMVA/ClassifierFactory.h""; 50#include ""TMVA/Config.h""; 51#include ""TMVA/Configurable.h""; 52#include ""TMVA/Tools.h""; 53#include ""TMVA/Ranking.h""; 54#include ""TMVA/DataSet.h""; 55#include ""TMVA/IMethod.h""; 56#include ""TMVA/MethodBase.h""; 57#include ""TMVA/DataInputHandler.h""; 58#include ""TMVA/DataSetManager.h""; 59#include ""TMVA/DataSetInfo.h""; 60#include ""TMVA/DataLoader.h""; 61#include ""TMVA/MethodBoost.h""; 62#include ""TMVA/MethodCategory.h""; 63#include ""TMVA/ROCCalc.h""; 64#include ""TMVA/ROCCurve.h""; 65#include ""TMVA/MsgLogger.h""; 66 ; 67#include ""TMVA/VariableInfo.h""; 68#include ""TMVA/VariableTransform.h""; 69 ; 70#include ""TMVA/Results.h""; 71#include ""TMVA/ResultsClassification.h""; 72#include ""TMVA/ResultsRegression.h""; 73#include ""TMVA/ResultsMulticlass.h""; 74#include <list>; 75#include <bitset>; 76#include <set>; 77 ; 78#include ""TMVA/Types.h""; 79 ; 80#include ""TROOT.h""; 81#incl",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:3826,Testability,test,test,3826,"nclude ""TFile.h""; 82#include ""TLeaf.h""; 83#include ""TEventList.h""; 84#include ""TH2.h""; 85#include ""TGraph.h""; 86#include ""TStyle.h""; 87#include ""TMatrixF.h""; 88#include ""TMatrixDSym.h""; 89#include ""TMultiGraph.h""; 90#include ""TPrincipal.h""; 91#include ""TMath.h""; 92#include ""TSystem.h""; 93#include ""TCanvas.h""; 94 ; 95const Int_t MinNoTrainingEvents = 10;; 96// const Int_t MinNoTestEvents = 1;; 97 ; 98ClassImp(TMVA::Factory);; 99 ; 100#define READXML kTRUE; 101 ; 102// number of bits for bitset; 103#define VIBITS 32; 104 ; 105////////////////////////////////////////////////////////////////////////////////; 106/// Standard constructor.; 107///; 108/// - jobname : this name will appear in all weight file names produced by the MVAs; 109/// - theTargetFile : output ROOT file; the test tree and all evaluation plots; 110/// will be stored here; 111/// - theOption : option string; currently: ""V"" for verbose; 112 ; 113TMVA::Factory::Factory(TString jobName, TFile *theTargetFile, TString theOption); 114 : Configurable(theOption), fTransformations(""I""), fVerbose(kFALSE), fVerboseLevel(kINFO), fCorrelations(kFALSE),; 115 fROC(kTRUE), fSilentFile(theTargetFile == nullptr), fJobName(jobName), fAnalysisType(Types::kClassification),; 116 fModelPersistence(kTRUE); 117{; 118 fName = ""Factory"";; 119 fgTargetFile = theTargetFile;; 120 fLogger->SetSource(fName.Data());; 121 ; 122 // render silent; 123 if (gTools().CheckForSilentOption(GetOptions())); 124 Log().InhibitOutput(); // make sure is silent if wanted to; 125 ; 126 // init configurable; 127 SetConfigDescription(""Configuration options for Factory running"");; 128 SetConfigName(GetName());; 129 ; 130 // histograms are not automatically associated with the current; 131 // directory and hence don't go out of scope when closing the file; 132 // TH1::AddDirectory(kFALSE);; 133 Bool_t silent = kFALSE;; 134#ifdef WIN32; 135 // under Windows, switch progress bar and color off by default, as the typical windows shell doesn't handle these; 13",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:5733,Testability,test,test,5733,"fferent sequences..); 137 Bool_t color = kFALSE;; 138 Bool_t drawProgressBar = kFALSE;; 139#else; 140 Bool_t color = !gROOT->IsBatch();; 141 Bool_t drawProgressBar = kTRUE;; 142#endif; 143 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 144 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 145 AddPreDefVal(TString(""Debug""));; 146 AddPreDefVal(TString(""Verbose""));; 147 AddPreDefVal(TString(""Info""));; 148 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 149 DeclareOptionRef(; 150 fTransformations, ""Transformations"",; 151 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", for identity, ""; 152 ""decorrelation, PCA, Uniform and Gaussianisation followed by decorrelation transformations"");; 153 DeclareOptionRef(fCorrelations, ""Correlations"", ""boolean to show correlation in output"");; 154 DeclareOptionRef(fROC, ""ROC"", ""boolean to show ROC in output"");; 155 DeclareOptionRef(silent, ""Silent"",; 156 ""Batch mode: boolean silent flag inhibiting any output from TMVA after the creation of the factory ""; 157 ""class object (default: False)"");; 158 DeclareOptionRef(drawProgressBar, ""DrawProgressBar"",; 159 ""Draw progress bar to display training, testing and evaluation schedule (default: True)"");; 160 DeclareOptionRef(fModelPersistence, ""ModelPersistence"",; 161 ""Option to save the trained model in xml file or using serialization"");; 162 ; 163 TString analysisType(""Auto"");; 164 DeclareOptionRef(analysisType, ""AnalysisType"",; 165 ""Set the analysis type (Classification, Regression, Multiclass, Auto) (default: Auto)"");; 166 AddPreDefVal(TString(""Classification""));; 167 AddPreDefVal(TString(""Regression""));; 168 AddPreDefVal(TString(""Multiclass""));; 169 AddPreDefVal(TString(""Auto""));; 170 ; 171 ParseOptions();; 172 CheckForUnusedOptions();; 173 ; 174 if (Verbose()); 175 fLogger->SetMinType(kVERBOSE);; 176 if (fVerboseLevel.CompareTo(""Debug"") ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:6359,Testability,test,testing,6359,"fferent sequences..); 137 Bool_t color = kFALSE;; 138 Bool_t drawProgressBar = kFALSE;; 139#else; 140 Bool_t color = !gROOT->IsBatch();; 141 Bool_t drawProgressBar = kTRUE;; 142#endif; 143 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 144 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 145 AddPreDefVal(TString(""Debug""));; 146 AddPreDefVal(TString(""Verbose""));; 147 AddPreDefVal(TString(""Info""));; 148 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 149 DeclareOptionRef(; 150 fTransformations, ""Transformations"",; 151 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", for identity, ""; 152 ""decorrelation, PCA, Uniform and Gaussianisation followed by decorrelation transformations"");; 153 DeclareOptionRef(fCorrelations, ""Correlations"", ""boolean to show correlation in output"");; 154 DeclareOptionRef(fROC, ""ROC"", ""boolean to show ROC in output"");; 155 DeclareOptionRef(silent, ""Silent"",; 156 ""Batch mode: boolean silent flag inhibiting any output from TMVA after the creation of the factory ""; 157 ""class object (default: False)"");; 158 DeclareOptionRef(drawProgressBar, ""DrawProgressBar"",; 159 ""Draw progress bar to display training, testing and evaluation schedule (default: True)"");; 160 DeclareOptionRef(fModelPersistence, ""ModelPersistence"",; 161 ""Option to save the trained model in xml file or using serialization"");; 162 ; 163 TString analysisType(""Auto"");; 164 DeclareOptionRef(analysisType, ""AnalysisType"",; 165 ""Set the analysis type (Classification, Regression, Multiclass, Auto) (default: Auto)"");; 166 AddPreDefVal(TString(""Classification""));; 167 AddPreDefVal(TString(""Regression""));; 168 AddPreDefVal(TString(""Multiclass""));; 169 AddPreDefVal(TString(""Auto""));; 170 ; 171 ParseOptions();; 172 CheckForUnusedOptions();; 173 ; 174 if (Verbose()); 175 fLogger->SetMinType(kVERBOSE);; 176 if (fVerboseLevel.CompareTo(""Debug"") ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:9643,Testability,test,test,9643,"fferent sequences..); 227 Bool_t color = kFALSE;; 228 Bool_t drawProgressBar = kFALSE;; 229#else; 230 Bool_t color = !gROOT->IsBatch();; 231 Bool_t drawProgressBar = kTRUE;; 232#endif; 233 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 234 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 235 AddPreDefVal(TString(""Debug""));; 236 AddPreDefVal(TString(""Verbose""));; 237 AddPreDefVal(TString(""Info""));; 238 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 239 DeclareOptionRef(; 240 fTransformations, ""Transformations"",; 241 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", for identity, ""; 242 ""decorrelation, PCA, Uniform and Gaussianisation followed by decorrelation transformations"");; 243 DeclareOptionRef(fCorrelations, ""Correlations"", ""boolean to show correlation in output"");; 244 DeclareOptionRef(fROC, ""ROC"", ""boolean to show ROC in output"");; 245 DeclareOptionRef(silent, ""Silent"",; 246 ""Batch mode: boolean silent flag inhibiting any output from TMVA after the creation of the factory ""; 247 ""class object (default: False)"");; 248 DeclareOptionRef(drawProgressBar, ""DrawProgressBar"",; 249 ""Draw progress bar to display training, testing and evaluation schedule (default: True)"");; 250 DeclareOptionRef(fModelPersistence, ""ModelPersistence"",; 251 ""Option to save the trained model in xml file or using serialization"");; 252 ; 253 TString analysisType(""Auto"");; 254 DeclareOptionRef(analysisType, ""AnalysisType"",; 255 ""Set the analysis type (Classification, Regression, Multiclass, Auto) (default: Auto)"");; 256 AddPreDefVal(TString(""Classification""));; 257 AddPreDefVal(TString(""Regression""));; 258 AddPreDefVal(TString(""Multiclass""));; 259 AddPreDefVal(TString(""Auto""));; 260 ; 261 ParseOptions();; 262 CheckForUnusedOptions();; 263 ; 264 if (Verbose()); 265 fLogger->SetMinType(kVERBOSE);; 266 if (fVerboseLevel.CompareTo(""Debug"") ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:10269,Testability,test,testing,10269,"fferent sequences..); 227 Bool_t color = kFALSE;; 228 Bool_t drawProgressBar = kFALSE;; 229#else; 230 Bool_t color = !gROOT->IsBatch();; 231 Bool_t drawProgressBar = kTRUE;; 232#endif; 233 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 234 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 235 AddPreDefVal(TString(""Debug""));; 236 AddPreDefVal(TString(""Verbose""));; 237 AddPreDefVal(TString(""Info""));; 238 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 239 DeclareOptionRef(; 240 fTransformations, ""Transformations"",; 241 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", for identity, ""; 242 ""decorrelation, PCA, Uniform and Gaussianisation followed by decorrelation transformations"");; 243 DeclareOptionRef(fCorrelations, ""Correlations"", ""boolean to show correlation in output"");; 244 DeclareOptionRef(fROC, ""ROC"", ""boolean to show ROC in output"");; 245 DeclareOptionRef(silent, ""Silent"",; 246 ""Batch mode: boolean silent flag inhibiting any output from TMVA after the creation of the factory ""; 247 ""class object (default: False)"");; 248 DeclareOptionRef(drawProgressBar, ""DrawProgressBar"",; 249 ""Draw progress bar to display training, testing and evaluation schedule (default: True)"");; 250 DeclareOptionRef(fModelPersistence, ""ModelPersistence"",; 251 ""Option to save the trained model in xml file or using serialization"");; 252 ; 253 TString analysisType(""Auto"");; 254 DeclareOptionRef(analysisType, ""AnalysisType"",; 255 ""Set the analysis type (Classification, Regression, Multiclass, Auto) (default: Auto)"");; 256 AddPreDefVal(TString(""Classification""));; 257 AddPreDefVal(TString(""Regression""));; 258 AddPreDefVal(TString(""Multiclass""));; 259 AddPreDefVal(TString(""Auto""));; 260 ; 261 ParseOptions();; 262 CheckForUnusedOptions();; 263 ; 264 if (Verbose()); 265 fLogger->SetMinType(kVERBOSE);; 266 if (fVerboseLevel.CompareTo(""Debug"") ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:26210,Testability,test,test,26210,"rix ("") + fDataSetInfo.GetClassInfo(cls)->GetName() + TString("")""));; 624 if (h != 0) {; 625 h->Write();; 626 delete h;; 627 }; 628 }; 629 } else {; 630 m = fDataSetInfo.CorrelationMatrix(""Signal"");; 631 h = fDataSetInfo.CreateCorrelationMatrixHist(m, ""CorrelationMatrixS"", ""Correlation Matrix (signal)"");; 632 if (h != 0) {; 633 h->Write();; 634 delete h;; 635 }; 636 ; 637 m = fDataSetInfo.CorrelationMatrix(""Background"");; 638 h = fDataSetInfo.CreateCorrelationMatrixHist(m, ""CorrelationMatrixB"", ""Correlation Matrix (background)"");; 639 if (h != 0) {; 640 h->Write();; 641 delete h;; 642 }; 643 ; 644 m = fDataSetInfo.CorrelationMatrix(""Regression"");; 645 h = fDataSetInfo.CreateCorrelationMatrixHist(m, ""CorrelationMatrix"", ""Correlation Matrix"");; 646 if (h != 0) {; 647 h->Write();; 648 delete h;; 649 }; 650 }; 651 ; 652 // some default transformations to evaluate; 653 // NOTE: all transformations are destroyed after this test; 654 TString processTrfs = ""I""; //""I;N;D;P;U;G,D;""; 655 ; 656 // plus some user defined transformations; 657 processTrfs = fTransformations;; 658 ; 659 // remove any trace of identity transform - if given (avoid to apply it twice); 660 std::vector<TMVA::TransformationHandler *> trfs;; 661 TransformationHandler *identityTrHandler = 0;; 662 ; 663 std::vector<TString> trfsDef = gTools().SplitString(processTrfs, ';');; 664 std::vector<TString>::iterator trfsDefIt = trfsDef.begin();; 665 for (; trfsDefIt != trfsDef.end(); ++trfsDefIt) {; 666 trfs.push_back(new TMVA::TransformationHandler(fDataSetInfo, ""Factory""));; 667 TString trfS = (*trfsDefIt);; 668 ; 669 // Log() << kINFO << Endl;; 670 Log() << kDEBUG << ""current transformation string: '"" << trfS.Data() << ""'"" << Endl;; 671 TMVA::CreateVariableTransforms(trfS, fDataSetInfo, *(trfs.back()), Log());; 672 ; 673 if (trfS.BeginsWith('I')); 674 identityTrHandler = trfs.back();; 675 }; 676 ; 677 const std::vector<Event *> &inputEvents = fDataSetInfo.GetDataSet()->GetEventCollection();; 678 ; 679 // apply a",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:49727,Testability,test,testing,49727,"itrMethod)->CreateRanking();; 1188 if (ranking != 0); 1189 ranking->Print();; 1190 else; 1191 Log() << kINFO << ""No variable ranking supplied by classifier: ""; 1192 << dynamic_cast<MethodBase *>(*itrMethod)->GetMethodName() << Endl;; 1193 }; 1194 }; 1195 }; 1196 ; 1197 // save training history in case we are not in the silent mode; 1198 if (!IsSilentFile()) {; 1199 for (UInt_t i = 0; i < methods->size(); i++) {; 1200 MethodBase *m = dynamic_cast<MethodBase *>((*methods)[i]);; 1201 if (m == 0); 1202 continue;; 1203 m->BaseDir()->cd();; 1204 m->fTrainHistory.SaveHistory(m->GetMethodName());; 1205 }; 1206 }; 1207 ; 1208 // delete all methods and recreate them from weight file - this ensures that the application; 1209 // of the methods (in TMVAClassificationApplication) is consistent with the results obtained; 1210 // in the testing; 1211 // Log() << Endl;; 1212 if (fModelPersistence) {; 1213 ; 1214 Log() << kHEADER << ""=== Destroy and recreate all methods via weight files for testing ==="" << Endl << Endl;; 1215 ; 1216 if (!IsSilentFile()); 1217 RootBaseDir()->cd();; 1218 ; 1219 // iterate through all booked methods; 1220 for (UInt_t i = 0; i < methods->size(); i++) {; 1221 ; 1222 MethodBase *m = dynamic_cast<MethodBase *>((*methods)[i]);; 1223 if (m == nullptr); 1224 continue;; 1225 ; 1226 TMVA::Types::EMVA methodType = m->GetMethodType();; 1227 TString weightfile = m->GetWeightFileName();; 1228 ; 1229 // decide if .txt or .xml file should be read:; 1230 if (READXML); 1231 weightfile.ReplaceAll("".txt"", "".xml"");; 1232 ; 1233 DataSetInfo &dataSetInfo = m->DataInfo();; 1234 TString testvarName = m->GetTestvarName();; 1235 delete m; // itrMethod[i];; 1236 ; 1237 // recreate; 1238 m = dynamic_cast<MethodBase *>(ClassifierFactory::Instance().Create(; 1239 Types::Instance().GetMethodName(methodType).Data(), dataSetInfo, weightfile));; 1240 if (m->GetMethodType() == Types::kCategory) {; 1241 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(m));; 1242 if (!methCat); 12",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:49882,Testability,test,testing,49882,"itrMethod)->CreateRanking();; 1188 if (ranking != 0); 1189 ranking->Print();; 1190 else; 1191 Log() << kINFO << ""No variable ranking supplied by classifier: ""; 1192 << dynamic_cast<MethodBase *>(*itrMethod)->GetMethodName() << Endl;; 1193 }; 1194 }; 1195 }; 1196 ; 1197 // save training history in case we are not in the silent mode; 1198 if (!IsSilentFile()) {; 1199 for (UInt_t i = 0; i < methods->size(); i++) {; 1200 MethodBase *m = dynamic_cast<MethodBase *>((*methods)[i]);; 1201 if (m == 0); 1202 continue;; 1203 m->BaseDir()->cd();; 1204 m->fTrainHistory.SaveHistory(m->GetMethodName());; 1205 }; 1206 }; 1207 ; 1208 // delete all methods and recreate them from weight file - this ensures that the application; 1209 // of the methods (in TMVAClassificationApplication) is consistent with the results obtained; 1210 // in the testing; 1211 // Log() << Endl;; 1212 if (fModelPersistence) {; 1213 ; 1214 Log() << kHEADER << ""=== Destroy and recreate all methods via weight files for testing ==="" << Endl << Endl;; 1215 ; 1216 if (!IsSilentFile()); 1217 RootBaseDir()->cd();; 1218 ; 1219 // iterate through all booked methods; 1220 for (UInt_t i = 0; i < methods->size(); i++) {; 1221 ; 1222 MethodBase *m = dynamic_cast<MethodBase *>((*methods)[i]);; 1223 if (m == nullptr); 1224 continue;; 1225 ; 1226 TMVA::Types::EMVA methodType = m->GetMethodType();; 1227 TString weightfile = m->GetWeightFileName();; 1228 ; 1229 // decide if .txt or .xml file should be read:; 1230 if (READXML); 1231 weightfile.ReplaceAll("".txt"", "".xml"");; 1232 ; 1233 DataSetInfo &dataSetInfo = m->DataInfo();; 1234 TString testvarName = m->GetTestvarName();; 1235 delete m; // itrMethod[i];; 1236 ; 1237 // recreate; 1238 m = dynamic_cast<MethodBase *>(ClassifierFactory::Instance().Create(; 1239 Types::Instance().GetMethodName(methodType).Data(), dataSetInfo, weightfile));; 1240 if (m->GetMethodType() == Types::kCategory) {; 1241 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(m));; 1242 if (!methCat); 12",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:50497,Testability,test,testvarName,50497,"s and recreate them from weight file - this ensures that the application; 1209 // of the methods (in TMVAClassificationApplication) is consistent with the results obtained; 1210 // in the testing; 1211 // Log() << Endl;; 1212 if (fModelPersistence) {; 1213 ; 1214 Log() << kHEADER << ""=== Destroy and recreate all methods via weight files for testing ==="" << Endl << Endl;; 1215 ; 1216 if (!IsSilentFile()); 1217 RootBaseDir()->cd();; 1218 ; 1219 // iterate through all booked methods; 1220 for (UInt_t i = 0; i < methods->size(); i++) {; 1221 ; 1222 MethodBase *m = dynamic_cast<MethodBase *>((*methods)[i]);; 1223 if (m == nullptr); 1224 continue;; 1225 ; 1226 TMVA::Types::EMVA methodType = m->GetMethodType();; 1227 TString weightfile = m->GetWeightFileName();; 1228 ; 1229 // decide if .txt or .xml file should be read:; 1230 if (READXML); 1231 weightfile.ReplaceAll("".txt"", "".xml"");; 1232 ; 1233 DataSetInfo &dataSetInfo = m->DataInfo();; 1234 TString testvarName = m->GetTestvarName();; 1235 delete m; // itrMethod[i];; 1236 ; 1237 // recreate; 1238 m = dynamic_cast<MethodBase *>(ClassifierFactory::Instance().Create(; 1239 Types::Instance().GetMethodName(methodType).Data(), dataSetInfo, weightfile));; 1240 if (m->GetMethodType() == Types::kCategory) {; 1241 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(m));; 1242 if (!methCat); 1243 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory"" << Endl;; 1244 else; 1245 methCat->fDataSetManager = m->DataInfo().GetDataSetManager();; 1246 }; 1247 // ToDo, Do we need to fill the DataSetManager of MethodBoost here too?; 1248 ; 1249 TString wfileDir = m->DataInfo().GetName();; 1250 wfileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 1251 m->SetWeightFileDir(wfileDir);; 1252 m->SetModelPersistence(fModelPersistence);; 1253 m->SetSilentFile(IsSilentFile());; 1254 m->SetAnalysisType(fAnalysisType);; 1255 m->SetupMethod();; 1256 m->ReadStateFromFile();; 1257 m->SetTestvarName(testvarNam",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:51529,Testability,test,testvarName,51529,"8 m = dynamic_cast<MethodBase *>(ClassifierFactory::Instance().Create(; 1239 Types::Instance().GetMethodName(methodType).Data(), dataSetInfo, weightfile));; 1240 if (m->GetMethodType() == Types::kCategory) {; 1241 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(m));; 1242 if (!methCat); 1243 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory"" << Endl;; 1244 else; 1245 methCat->fDataSetManager = m->DataInfo().GetDataSetManager();; 1246 }; 1247 // ToDo, Do we need to fill the DataSetManager of MethodBoost here too?; 1248 ; 1249 TString wfileDir = m->DataInfo().GetName();; 1250 wfileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 1251 m->SetWeightFileDir(wfileDir);; 1252 m->SetModelPersistence(fModelPersistence);; 1253 m->SetSilentFile(IsSilentFile());; 1254 m->SetAnalysisType(fAnalysisType);; 1255 m->SetupMethod();; 1256 m->ReadStateFromFile();; 1257 m->SetTestvarName(testvarName);; 1258 ; 1259 // replace trained method by newly created one (from weight file) in methods vector; 1260 (*methods)[i] = m;; 1261 }; 1262 }; 1263 }; 1264}; 1265 ; 1266////////////////////////////////////////////////////////////////////////////////; 1267/// Evaluates all booked methods on the testing data and adds the output to the; 1268/// Results in the corresponiding DataSet.; 1269///; 1270 ; 1271void TMVA::Factory::TestAllMethods(); 1272{; 1273 Log() << kHEADER << gTools().Color(""bold"") << ""Test all methods"" << gTools().Color(""reset"") << Endl;; 1274 ; 1275 // don't do anything if no method booked; 1276 if (fMethodsMap.empty()) {; 1277 Log() << kINFO << ""...nothing found to test"" << Endl;; 1278 return;; 1279 }; 1280 std::map<TString, MVector *>::iterator itrMap;; 1281 ; 1282 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1283 MVector *methods = itrMap->second;; 1284 MVector::iterator itrMethod;; 1285 ; 1286 // iterate over methods and test; 1287 for (itrMethod = methods->begin(); itrMethod != methods->end()",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:51834,Testability,test,testing,51834,"8 m = dynamic_cast<MethodBase *>(ClassifierFactory::Instance().Create(; 1239 Types::Instance().GetMethodName(methodType).Data(), dataSetInfo, weightfile));; 1240 if (m->GetMethodType() == Types::kCategory) {; 1241 MethodCategory *methCat = (dynamic_cast<MethodCategory *>(m));; 1242 if (!methCat); 1243 Log() << kFATAL << ""Method with type kCategory cannot be casted to MethodCategory. /Factory"" << Endl;; 1244 else; 1245 methCat->fDataSetManager = m->DataInfo().GetDataSetManager();; 1246 }; 1247 // ToDo, Do we need to fill the DataSetManager of MethodBoost here too?; 1248 ; 1249 TString wfileDir = m->DataInfo().GetName();; 1250 wfileDir += ""/"" + gConfig().GetIONames().fWeightFileDir;; 1251 m->SetWeightFileDir(wfileDir);; 1252 m->SetModelPersistence(fModelPersistence);; 1253 m->SetSilentFile(IsSilentFile());; 1254 m->SetAnalysisType(fAnalysisType);; 1255 m->SetupMethod();; 1256 m->ReadStateFromFile();; 1257 m->SetTestvarName(testvarName);; 1258 ; 1259 // replace trained method by newly created one (from weight file) in methods vector; 1260 (*methods)[i] = m;; 1261 }; 1262 }; 1263 }; 1264}; 1265 ; 1266////////////////////////////////////////////////////////////////////////////////; 1267/// Evaluates all booked methods on the testing data and adds the output to the; 1268/// Results in the corresponiding DataSet.; 1269///; 1270 ; 1271void TMVA::Factory::TestAllMethods(); 1272{; 1273 Log() << kHEADER << gTools().Color(""bold"") << ""Test all methods"" << gTools().Color(""reset"") << Endl;; 1274 ; 1275 // don't do anything if no method booked; 1276 if (fMethodsMap.empty()) {; 1277 Log() << kINFO << ""...nothing found to test"" << Endl;; 1278 return;; 1279 }; 1280 std::map<TString, MVector *>::iterator itrMap;; 1281 ; 1282 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1283 MVector *methods = itrMap->second;; 1284 MVector::iterator itrMethod;; 1285 ; 1286 // iterate over methods and test; 1287 for (itrMethod = methods->begin(); itrMethod != methods->end()",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:52226,Testability,test,test,52226,";; 1251 m->SetWeightFileDir(wfileDir);; 1252 m->SetModelPersistence(fModelPersistence);; 1253 m->SetSilentFile(IsSilentFile());; 1254 m->SetAnalysisType(fAnalysisType);; 1255 m->SetupMethod();; 1256 m->ReadStateFromFile();; 1257 m->SetTestvarName(testvarName);; 1258 ; 1259 // replace trained method by newly created one (from weight file) in methods vector; 1260 (*methods)[i] = m;; 1261 }; 1262 }; 1263 }; 1264}; 1265 ; 1266////////////////////////////////////////////////////////////////////////////////; 1267/// Evaluates all booked methods on the testing data and adds the output to the; 1268/// Results in the corresponiding DataSet.; 1269///; 1270 ; 1271void TMVA::Factory::TestAllMethods(); 1272{; 1273 Log() << kHEADER << gTools().Color(""bold"") << ""Test all methods"" << gTools().Color(""reset"") << Endl;; 1274 ; 1275 // don't do anything if no method booked; 1276 if (fMethodsMap.empty()) {; 1277 Log() << kINFO << ""...nothing found to test"" << Endl;; 1278 return;; 1279 }; 1280 std::map<TString, MVector *>::iterator itrMap;; 1281 ; 1282 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1283 MVector *methods = itrMap->second;; 1284 MVector::iterator itrMethod;; 1285 ; 1286 // iterate over methods and test; 1287 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1288 Event::SetIsTraining(kFALSE);; 1289 MethodBase *mva = dynamic_cast<MethodBase *>(*itrMethod);; 1290 if (mva == 0); 1291 continue;; 1292 Types::EAnalysisType analysisType = mva->GetAnalysisType();; 1293 Log() << kHEADER << ""Test method: "" << mva->GetMethodName() << "" for ""; 1294 << (analysisType == Types::kRegression; 1295 ? ""Regression""; 1296 : (analysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 1297 << "" performance"" << Endl << Endl;; 1298 mva->AddOutput(Types::kTesting, analysisType);; 1299 }; 1300 }; 1301}; 1302 ; 1303////////////////////////////////////////////////////////////////////////////////; 1304 ; 1305void TMV",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:52522,Testability,test,test,52522,"lor(""reset"") << Endl;; 1274 ; 1275 // don't do anything if no method booked; 1276 if (fMethodsMap.empty()) {; 1277 Log() << kINFO << ""...nothing found to test"" << Endl;; 1278 return;; 1279 }; 1280 std::map<TString, MVector *>::iterator itrMap;; 1281 ; 1282 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 1283 MVector *methods = itrMap->second;; 1284 MVector::iterator itrMethod;; 1285 ; 1286 // iterate over methods and test; 1287 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1288 Event::SetIsTraining(kFALSE);; 1289 MethodBase *mva = dynamic_cast<MethodBase *>(*itrMethod);; 1290 if (mva == 0); 1291 continue;; 1292 Types::EAnalysisType analysisType = mva->GetAnalysisType();; 1293 Log() << kHEADER << ""Test method: "" << mva->GetMethodName() << "" for ""; 1294 << (analysisType == Types::kRegression; 1295 ? ""Regression""; 1296 : (analysisType == Types::kMulticlass ? ""Multiclass classification"" : ""Classification"")); 1297 << "" performance"" << Endl << Endl;; 1298 mva->AddOutput(Types::kTesting, analysisType);; 1299 }; 1300 }; 1301}; 1302 ; 1303////////////////////////////////////////////////////////////////////////////////; 1304 ; 1305void TMVA::Factory::MakeClass(const TString &datasetname, const TString &methodTitle) const; 1306{; 1307 if (methodTitle != """") {; 1308 IMethod *method = GetMethod(datasetname, methodTitle);; 1309 if (method); 1310 method->MakeClass();; 1311 else {; 1312 Log() << kWARNING << ""<MakeClass> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1313 }; 1314 } else {; 1315 ; 1316 // no classifier specified, print all help messages; 1317 MVector *methods = fMethodsMap.find(datasetname)->second;; 1318 MVector::const_iterator itrMethod;; 1319 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1320 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1321 if (method == 0); 1322 continue;; 1323 Log() << kINFO << ""Make response class for classi",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:54347,Testability,test,test,54347,"const TString &methodTitle) const; 1306{; 1307 if (methodTitle != """") {; 1308 IMethod *method = GetMethod(datasetname, methodTitle);; 1309 if (method); 1310 method->MakeClass();; 1311 else {; 1312 Log() << kWARNING << ""<MakeClass> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1313 }; 1314 } else {; 1315 ; 1316 // no classifier specified, print all help messages; 1317 MVector *methods = fMethodsMap.find(datasetname)->second;; 1318 MVector::const_iterator itrMethod;; 1319 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1320 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1321 if (method == 0); 1322 continue;; 1323 Log() << kINFO << ""Make response class for classifier: "" << method->GetMethodName() << Endl;; 1324 method->MakeClass();; 1325 }; 1326 }; 1327}; 1328 ; 1329////////////////////////////////////////////////////////////////////////////////; 1330/// Print predefined help message of classifier.; 1331/// Iterate over methods and test.; 1332 ; 1333void TMVA::Factory::PrintHelpMessage(const TString &datasetname, const TString &methodTitle) const; 1334{; 1335 if (methodTitle != """") {; 1336 IMethod *method = GetMethod(datasetname, methodTitle);; 1337 if (method); 1338 method->PrintHelpMessage();; 1339 else {; 1340 Log() << kWARNING << ""<PrintHelpMessage> Could not find classifier \"""" << methodTitle << ""\"" in list"" << Endl;; 1341 }; 1342 } else {; 1343 ; 1344 // no classifier specified, print all help messages; 1345 MVector *methods = fMethodsMap.find(datasetname)->second;; 1346 MVector::const_iterator itrMethod;; 1347 for (itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1348 MethodBase *method = dynamic_cast<MethodBase *>(*itrMethod);; 1349 if (method == 0); 1350 continue;; 1351 Log() << kINFO << ""Print help message for classifier: "" << method->GetMethodName() << Endl;; 1352 method->PrintHelpMessage();; 1353 }; 1354 }; 1355}; 1356 ; 1357////////////////////////////////////",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:58429,Testability,test,test,58429,,MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:58624,Testability,test,test,58624,,MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:58819,Testability,test,test,58819,,MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:59018,Testability,test,test,59018,"multiclass_trainConfusionEffB30;; 1417 std::vector<TMatrixD> multiclass_testConfusionEffB01;; 1418 std::vector<TMatrixD> multiclass_testConfusionEffB10;; 1419 std::vector<TMatrixD> multiclass_testConfusionEffB30;; 1420 ; 1421 std::vector<std::vector<Double_t>> biastrain(1); // ""bias"" of the regression on the training data; 1422 std::vector<std::vector<Double_t>> biastest(1); // ""bias"" of the regression on test data; 1423 std::vector<std::vector<Double_t>> devtrain(1); // ""dev"" of the regression on the training data; 1424 std::vector<std::vector<Double_t>> devtest(1); // ""dev"" of the regression on test data; 1425 std::vector<std::vector<Double_t>> rmstrain(1); // ""rms"" of the regression on the training data; 1426 std::vector<std::vector<Double_t>> rmstest(1); // ""rms"" of the regression on test data; 1427 std::vector<std::vector<Double_t>> minftrain(1); // ""minf"" of the regression on the training data; 1428 std::vector<std::vector<Double_t>> minftest(1); // ""minf"" of the regression on test data; 1429 std::vector<std::vector<Double_t>> rhotrain(1); // correlation of the regression on the training data; 1430 std::vector<std::vector<Double_t>> rhotest(1); // correlation of the regression on test data; 1431 ; 1432 // same as above but for 'truncated' quantities (computed for events within 2sigma of RMS); 1433 std::vector<std::vector<Double_t>> biastrainT(1);; 1434 std::vector<std::vector<Double_t>> biastestT(1);; 1435 std::vector<std::vector<Double_t>> devtrainT(1);; 1436 std::vector<std::vector<Double_t>> devtestT(1);; 1437 std::vector<std::vector<Double_t>> rmstrainT(1);; 1438 std::vector<std::vector<Double_t>> rmstestT(1);; 1439 std::vector<std::vector<Double_t>> minftrainT(1);; 1440 std::vector<std::vector<Double_t>> minftestT(1);; 1441 ; 1442 // following vector contains all methods - with the exception of Cuts, which are special; 1443 MVector methodsNoCuts;; 1444 ; 1445 Bool_t doRegression = kFALSE;; 1446 Bool_t doMulticlass = kFALSE;; 1447 ; 1448 // iterate over met",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:59225,Testability,test,test,59225,"fB30;; 1420 ; 1421 std::vector<std::vector<Double_t>> biastrain(1); // ""bias"" of the regression on the training data; 1422 std::vector<std::vector<Double_t>> biastest(1); // ""bias"" of the regression on test data; 1423 std::vector<std::vector<Double_t>> devtrain(1); // ""dev"" of the regression on the training data; 1424 std::vector<std::vector<Double_t>> devtest(1); // ""dev"" of the regression on test data; 1425 std::vector<std::vector<Double_t>> rmstrain(1); // ""rms"" of the regression on the training data; 1426 std::vector<std::vector<Double_t>> rmstest(1); // ""rms"" of the regression on test data; 1427 std::vector<std::vector<Double_t>> minftrain(1); // ""minf"" of the regression on the training data; 1428 std::vector<std::vector<Double_t>> minftest(1); // ""minf"" of the regression on test data; 1429 std::vector<std::vector<Double_t>> rhotrain(1); // correlation of the regression on the training data; 1430 std::vector<std::vector<Double_t>> rhotest(1); // correlation of the regression on test data; 1431 ; 1432 // same as above but for 'truncated' quantities (computed for events within 2sigma of RMS); 1433 std::vector<std::vector<Double_t>> biastrainT(1);; 1434 std::vector<std::vector<Double_t>> biastestT(1);; 1435 std::vector<std::vector<Double_t>> devtrainT(1);; 1436 std::vector<std::vector<Double_t>> devtestT(1);; 1437 std::vector<std::vector<Double_t>> rmstrainT(1);; 1438 std::vector<std::vector<Double_t>> rmstestT(1);; 1439 std::vector<std::vector<Double_t>> minftrainT(1);; 1440 std::vector<std::vector<Double_t>> minftestT(1);; 1441 ; 1442 // following vector contains all methods - with the exception of Cuts, which are special; 1443 MVector methodsNoCuts;; 1444 ; 1445 Bool_t doRegression = kFALSE;; 1446 Bool_t doMulticlass = kFALSE;; 1447 ; 1448 // iterate over methods and evaluate; 1449 for (MVector::iterator itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1450 Event::SetIsTraining(kFALSE);; 1451 MethodBase *theMethod = dynamic_cast<MethodB",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:60796,Testability,test,testing,60796,"<Double_t>> minftrainT(1);; 1440 std::vector<std::vector<Double_t>> minftestT(1);; 1441 ; 1442 // following vector contains all methods - with the exception of Cuts, which are special; 1443 MVector methodsNoCuts;; 1444 ; 1445 Bool_t doRegression = kFALSE;; 1446 Bool_t doMulticlass = kFALSE;; 1447 ; 1448 // iterate over methods and evaluate; 1449 for (MVector::iterator itrMethod = methods->begin(); itrMethod != methods->end(); ++itrMethod) {; 1450 Event::SetIsTraining(kFALSE);; 1451 MethodBase *theMethod = dynamic_cast<MethodBase *>(*itrMethod);; 1452 if (theMethod == 0); 1453 continue;; 1454 theMethod->SetFile(fgTargetFile);; 1455 theMethod->SetSilentFile(IsSilentFile());; 1456 if (theMethod->GetMethodType() != Types::kCuts); 1457 methodsNoCuts.push_back(*itrMethod);; 1458 ; 1459 if (theMethod->DoRegression()) {; 1460 doRegression = kTRUE;; 1461 ; 1462 Log() << kINFO << ""Evaluate regression method: "" << theMethod->GetMethodName() << Endl;; 1463 Double_t bias, dev, rms, mInf;; 1464 Double_t biasT, devT, rmsT, mInfT;; 1465 Double_t rho;; 1466 ; 1467 Log() << kINFO << ""TestRegression (testing)"" << Endl;; 1468 theMethod->TestRegression(bias, biasT, dev, devT, rms, rmsT, mInf, mInfT, rho, TMVA::Types::kTesting);; 1469 biastest[0].push_back(bias);; 1470 devtest[0].push_back(dev);; 1471 rmstest[0].push_back(rms);; 1472 minftest[0].push_back(mInf);; 1473 rhotest[0].push_back(rho);; 1474 biastestT[0].push_back(biasT);; 1475 devtestT[0].push_back(devT);; 1476 rmstestT[0].push_back(rmsT);; 1477 minftestT[0].push_back(mInfT);; 1478 ; 1479 Log() << kINFO << ""TestRegression (training)"" << Endl;; 1480 theMethod->TestRegression(bias, biasT, dev, devT, rms, rmsT, mInf, mInfT, rho, TMVA::Types::kTraining);; 1481 biastrain[0].push_back(bias);; 1482 devtrain[0].push_back(dev);; 1483 rmstrain[0].push_back(rms);; 1484 minftrain[0].push_back(mInf);; 1485 rhotrain[0].push_back(rho);; 1486 biastrainT[0].push_back(biasT);; 1487 devtrainT[0].push_back(devT);; 1488 rmstrainT[0].push_back(rmsT)",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:70408,Testability,test,test,70408,"Principal(nmeth + nvar, """");; 1671 ; 1672 // set required tree branch references; 1673 Int_t ivar = 0;; 1674 std::vector<TString> *theVars = new std::vector<TString>;; 1675 std::vector<ResultsClassification *> mvaRes;; 1676 for (MVector::iterator itrMethod = methodsNoCuts.begin(); itrMethod != methodsNoCuts.end();; 1677 ++itrMethod, ++ivar) {; 1678 MethodBase *m = dynamic_cast<MethodBase *>(*itrMethod);; 1679 if (m == 0); 1680 continue;; 1681 theVars->push_back(m->GetTestvarName());; 1682 rvec.push_back(m->GetSignalReferenceCut());; 1683 theVars->back().ReplaceAll(""MVA_"", """");; 1684 mvaRes.push_back(dynamic_cast<ResultsClassification *>(; 1685 m->Data()->GetResults(m->GetMethodName(), Types::kTesting, Types::kMaxAnalysisType)));; 1686 }; 1687 ; 1688 // for overlap study; 1689 TMatrixD *overlapS = new TMatrixD(nmeth, nmeth);; 1690 TMatrixD *overlapB = new TMatrixD(nmeth, nmeth);; 1691 (*overlapS) *= 0; // init...; 1692 (*overlapB) *= 0; // init...; 1693 ; 1694 // loop over test tree; 1695 DataSet *defDs = method->fDataSetInfo.GetDataSet();; 1696 defDs->SetCurrentType(Types::kTesting);; 1697 for (Int_t ievt = 0; ievt < defDs->GetNEvents(); ievt++) {; 1698 const Event *ev = defDs->GetEvent(ievt);; 1699 ; 1700 // for correlations; 1701 TMatrixD *theMat = 0;; 1702 for (Int_t im = 0; im < nmeth; im++) {; 1703 // check for NaN value; 1704 Double_t retval = (Double_t)(*mvaRes[im])[ievt][0];; 1705 if (TMath::IsNaN(retval)) {; 1706 Log() << kWARNING << ""Found NaN return value in event: "" << ievt << "" for method \""""; 1707 << methodsNoCuts[im]->GetName() << ""\"""" << Endl;; 1708 dvec[im] = 0;; 1709 } else; 1710 dvec[im] = retval;; 1711 }; 1712 for (Int_t iv = 0; iv < nvar; iv++); 1713 dvec[iv + nmeth] = (Double_t)ev->GetValue(iv);; 1714 if (method->fDataSetInfo.IsSignal(ev)) {; 1715 tpSig->AddRow(dvec);; 1716 theMat = overlapS;; 1717 } else {; 1718 tpBkg->AddRow(dvec);; 1719 theMat = overlapB;; 1720 }; 1721 ; 1722 // count overlaps; 1723 for (Int_t im = 0; im < nmeth; im++) {; 172",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:75162,Testability,test,test,75162,"() << kWARNING << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1796 << ""<TestAllMethods> cannot compute correlation matrices"" << Endl;; 1797 ; 1798 // print overlap matrices; 1799 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1800 << ""The following \""overlap\"" matrices contain the fraction of events for which "" << Endl;; 1801 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1802 << ""the MVAs 'i' and 'j' have returned conform answers about \""signal-likeness\"""" << Endl;; 1803 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1804 << ""An event is signal-like, if its MVA output exceeds the following value:"" << Endl;; 1805 gTools().FormattedOutput(rvec, *theVars, ""Method"", ""Cut value"", Log());; 1806 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1807 << ""which correspond to the working point: eff(signal) = 1 - eff(background)"" << Endl;; 1808 ; 1809 // give notice that cut method has been excluded from this test; 1810 if (nmeth != (Int_t)methods->size()); 1811 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1812 << ""Note: no correlations and overlap with cut method are provided at present"" << Endl;; 1813 ; 1814 if (nmeth > 1) {; 1815 Log() << kINFO << Endl;; 1816 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1817 << ""Inter-MVA overlap matrix (signal):"" << Endl;; 1818 gTools().FormattedOutput(*overlapS, *theVars, Log());; 1819 Log() << kINFO << Endl;; 1820 ; 1821 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1822 << ""Inter-MVA overlap matrix (background):"" << Endl;; 1823 gTools().FormattedOutput(*overlapB, *theVars, Log());; 1824 }; 1825 ; 1826 // cleanup; 1827 delete tpSig;; 1828 delete tpBkg;; 1829 delete corrMatS;; 1830 delete corrMatB;; 1831 delete theVars;; 1832 delete overlapS;; 1833 delete overlapB;; 1834 delete[] dvec;; 1835 }; 1836 }; 1837 }; 1838 // --------------------",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:76597,Testability,test,test,76597,"etInfo.GetName()); 1812 << ""Note: no correlations and overlap with cut method are provided at present"" << Endl;; 1813 ; 1814 if (nmeth > 1) {; 1815 Log() << kINFO << Endl;; 1816 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1817 << ""Inter-MVA overlap matrix (signal):"" << Endl;; 1818 gTools().FormattedOutput(*overlapS, *theVars, Log());; 1819 Log() << kINFO << Endl;; 1820 ; 1821 Log() << kINFO << Form(""Dataset[%s] : "", method->fDataSetInfo.GetName()); 1822 << ""Inter-MVA overlap matrix (background):"" << Endl;; 1823 gTools().FormattedOutput(*overlapB, *theVars, Log());; 1824 }; 1825 ; 1826 // cleanup; 1827 delete tpSig;; 1828 delete tpBkg;; 1829 delete corrMatS;; 1830 delete corrMatB;; 1831 delete theVars;; 1832 delete overlapS;; 1833 delete overlapB;; 1834 delete[] dvec;; 1835 }; 1836 }; 1837 }; 1838 // -----------------------------------------------------------------------; 1839 // Third part of evaluation process; 1840 // --> output; 1841 // -----------------------------------------------------------------------; 1842 ; 1843 if (doRegression) {; 1844 ; 1845 Log() << kINFO << Endl;; 1846 TString hLine =; 1847 ""--------------------------------------------------------------------------------------------------"";; 1848 Log() << kINFO << ""Evaluation results ranked by smallest RMS on test sample:"" << Endl;; 1849 Log() << kINFO << ""(\""Bias\"" quotes the mean deviation of the regression from true target."" << Endl;; 1850 Log() << kINFO << "" \""MutInf\"" is the \""Mutual Information\"" between regression and target."" << Endl;; 1851 Log() << kINFO << "" Indicated by \""_T\"" are the corresponding \""truncated\"" quantities ob-"" << Endl;; 1852 Log() << kINFO << "" tained when removing events deviating more than 2sigma from average.)"" << Endl;; 1853 Log() << kINFO << hLine << Endl;; 1854 // Log() << kINFO << ""DataSet Name: MVA Method: <Bias> <Bias_T> RMS RMS_T | MutInf; 1855 // MutInf_T"" << Endl;; 1856 Log() << kINFO << hLine << Endl;; 1857 ; 1858 for (Int_t i = 0",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:81080,Testability,test,test,81080,"s"", theMethod->fDataSetInfo.GetName(), mname[0][i].Data());; 1924 // for (UInt_t icls = 0; icls < theMethod->fDataSetInfo.GetNClasses(); ++icls) {; 1925 // res += TString::Format(""%#1.3f "", (multiclass_testEff[i][icls]) * (multiclass_testPur[i][icls]));; 1926 // }; 1927 // Log() << kINFO << res << Endl;; 1928 // }; 1929 ; 1930 // Log() << kINFO << hLine << Endl;; 1931 // Log() << kINFO << Endl;; 1932 // }; 1933 ; 1934 // --- 1 vs Rest ROC AUC, signal efficiency @ given background efficiency; 1935 // --------------------------------------------------------------------; 1936 TString header1 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Dataset"", ""MVA Method"", ""ROC AUC"", ""Sig eff@B=0.01"",; 1937 ""Sig eff@B=0.10"", ""Sig eff@B=0.30"");; 1938 TString header2 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Name:"", ""/ Class:"", ""test (train)"", ""test (train)"",; 1939 ""test (train)"", ""test (train)"");; 1940 Log() << kINFO << Endl;; 1941 Log() << kINFO << ""1-vs-rest performance metrics per class"" << Endl;; 1942 Log() << kINFO << hLine << Endl;; 1943 Log() << kINFO << Endl;; 1944 Log() << kINFO << ""Considers the listed class as signal and the other classes"" << Endl;; 1945 Log() << kINFO << ""as background, reporting the resulting binary performance."" << Endl;; 1946 Log() << kINFO << ""A score of 0.820 (0.850) means 0.820 was acheived on the"" << Endl;; 1947 Log() << kINFO << ""test set and 0.850 on the training set."" << Endl;; 1948 ; 1949 Log() << kINFO << Endl;; 1950 Log() << kINFO << header1 << Endl;; 1951 Log() << kINFO << header2 << Endl;; 1952 for (Int_t k = 0; k < 2; k++) {; 1953 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 1954 if (k == 1) {; 1955 mname[k][i].ReplaceAll(""Variable_"", """");; 1956 }; 1957 ; 1958 const TString datasetName = itrMap->first;; 1959 const TString mvaName = mname[k][i];; 1960 ; 1961 MethodBase *theMethod = dynamic_cast<MethodBase *>(GetMethod(datasetName, mvaName));; 1962 if (theMethod == 0) {; 1963 continue;; 1964 }; 1965 ; 1966 Log() << kINFO <<",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:81096,Testability,test,test,81096,"s"", theMethod->fDataSetInfo.GetName(), mname[0][i].Data());; 1924 // for (UInt_t icls = 0; icls < theMethod->fDataSetInfo.GetNClasses(); ++icls) {; 1925 // res += TString::Format(""%#1.3f "", (multiclass_testEff[i][icls]) * (multiclass_testPur[i][icls]));; 1926 // }; 1927 // Log() << kINFO << res << Endl;; 1928 // }; 1929 ; 1930 // Log() << kINFO << hLine << Endl;; 1931 // Log() << kINFO << Endl;; 1932 // }; 1933 ; 1934 // --- 1 vs Rest ROC AUC, signal efficiency @ given background efficiency; 1935 // --------------------------------------------------------------------; 1936 TString header1 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Dataset"", ""MVA Method"", ""ROC AUC"", ""Sig eff@B=0.01"",; 1937 ""Sig eff@B=0.10"", ""Sig eff@B=0.30"");; 1938 TString header2 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Name:"", ""/ Class:"", ""test (train)"", ""test (train)"",; 1939 ""test (train)"", ""test (train)"");; 1940 Log() << kINFO << Endl;; 1941 Log() << kINFO << ""1-vs-rest performance metrics per class"" << Endl;; 1942 Log() << kINFO << hLine << Endl;; 1943 Log() << kINFO << Endl;; 1944 Log() << kINFO << ""Considers the listed class as signal and the other classes"" << Endl;; 1945 Log() << kINFO << ""as background, reporting the resulting binary performance."" << Endl;; 1946 Log() << kINFO << ""A score of 0.820 (0.850) means 0.820 was acheived on the"" << Endl;; 1947 Log() << kINFO << ""test set and 0.850 on the training set."" << Endl;; 1948 ; 1949 Log() << kINFO << Endl;; 1950 Log() << kINFO << header1 << Endl;; 1951 Log() << kINFO << header2 << Endl;; 1952 for (Int_t k = 0; k < 2; k++) {; 1953 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 1954 if (k == 1) {; 1955 mname[k][i].ReplaceAll(""Variable_"", """");; 1956 }; 1957 ; 1958 const TString datasetName = itrMap->first;; 1959 const TString mvaName = mname[k][i];; 1960 ; 1961 MethodBase *theMethod = dynamic_cast<MethodBase *>(GetMethod(datasetName, mvaName));; 1962 if (theMethod == 0) {; 1963 continue;; 1964 }; 1965 ; 1966 Log() << kINFO <<",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:81118,Testability,test,test,81118,"s"", theMethod->fDataSetInfo.GetName(), mname[0][i].Data());; 1924 // for (UInt_t icls = 0; icls < theMethod->fDataSetInfo.GetNClasses(); ++icls) {; 1925 // res += TString::Format(""%#1.3f "", (multiclass_testEff[i][icls]) * (multiclass_testPur[i][icls]));; 1926 // }; 1927 // Log() << kINFO << res << Endl;; 1928 // }; 1929 ; 1930 // Log() << kINFO << hLine << Endl;; 1931 // Log() << kINFO << Endl;; 1932 // }; 1933 ; 1934 // --- 1 vs Rest ROC AUC, signal efficiency @ given background efficiency; 1935 // --------------------------------------------------------------------; 1936 TString header1 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Dataset"", ""MVA Method"", ""ROC AUC"", ""Sig eff@B=0.01"",; 1937 ""Sig eff@B=0.10"", ""Sig eff@B=0.30"");; 1938 TString header2 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Name:"", ""/ Class:"", ""test (train)"", ""test (train)"",; 1939 ""test (train)"", ""test (train)"");; 1940 Log() << kINFO << Endl;; 1941 Log() << kINFO << ""1-vs-rest performance metrics per class"" << Endl;; 1942 Log() << kINFO << hLine << Endl;; 1943 Log() << kINFO << Endl;; 1944 Log() << kINFO << ""Considers the listed class as signal and the other classes"" << Endl;; 1945 Log() << kINFO << ""as background, reporting the resulting binary performance."" << Endl;; 1946 Log() << kINFO << ""A score of 0.820 (0.850) means 0.820 was acheived on the"" << Endl;; 1947 Log() << kINFO << ""test set and 0.850 on the training set."" << Endl;; 1948 ; 1949 Log() << kINFO << Endl;; 1950 Log() << kINFO << header1 << Endl;; 1951 Log() << kINFO << header2 << Endl;; 1952 for (Int_t k = 0; k < 2; k++) {; 1953 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 1954 if (k == 1) {; 1955 mname[k][i].ReplaceAll(""Variable_"", """");; 1956 }; 1957 ; 1958 const TString datasetName = itrMap->first;; 1959 const TString mvaName = mname[k][i];; 1960 ; 1961 MethodBase *theMethod = dynamic_cast<MethodBase *>(GetMethod(datasetName, mvaName));; 1962 if (theMethod == 0) {; 1963 continue;; 1964 }; 1965 ; 1966 Log() << kINFO <<",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:81134,Testability,test,test,81134,"s"", theMethod->fDataSetInfo.GetName(), mname[0][i].Data());; 1924 // for (UInt_t icls = 0; icls < theMethod->fDataSetInfo.GetNClasses(); ++icls) {; 1925 // res += TString::Format(""%#1.3f "", (multiclass_testEff[i][icls]) * (multiclass_testPur[i][icls]));; 1926 // }; 1927 // Log() << kINFO << res << Endl;; 1928 // }; 1929 ; 1930 // Log() << kINFO << hLine << Endl;; 1931 // Log() << kINFO << Endl;; 1932 // }; 1933 ; 1934 // --- 1 vs Rest ROC AUC, signal efficiency @ given background efficiency; 1935 // --------------------------------------------------------------------; 1936 TString header1 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Dataset"", ""MVA Method"", ""ROC AUC"", ""Sig eff@B=0.01"",; 1937 ""Sig eff@B=0.10"", ""Sig eff@B=0.30"");; 1938 TString header2 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Name:"", ""/ Class:"", ""test (train)"", ""test (train)"",; 1939 ""test (train)"", ""test (train)"");; 1940 Log() << kINFO << Endl;; 1941 Log() << kINFO << ""1-vs-rest performance metrics per class"" << Endl;; 1942 Log() << kINFO << hLine << Endl;; 1943 Log() << kINFO << Endl;; 1944 Log() << kINFO << ""Considers the listed class as signal and the other classes"" << Endl;; 1945 Log() << kINFO << ""as background, reporting the resulting binary performance."" << Endl;; 1946 Log() << kINFO << ""A score of 0.820 (0.850) means 0.820 was acheived on the"" << Endl;; 1947 Log() << kINFO << ""test set and 0.850 on the training set."" << Endl;; 1948 ; 1949 Log() << kINFO << Endl;; 1950 Log() << kINFO << header1 << Endl;; 1951 Log() << kINFO << header2 << Endl;; 1952 for (Int_t k = 0; k < 2; k++) {; 1953 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 1954 if (k == 1) {; 1955 mname[k][i].ReplaceAll(""Variable_"", """");; 1956 }; 1957 ; 1958 const TString datasetName = itrMap->first;; 1959 const TString mvaName = mname[k][i];; 1960 ; 1961 MethodBase *theMethod = dynamic_cast<MethodBase *>(GetMethod(datasetName, mvaName));; 1962 if (theMethod == 0) {; 1963 continue;; 1964 }; 1965 ; 1966 Log() << kINFO <<",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:81629,Testability,test,test,81629,"31 // Log() << kINFO << Endl;; 1932 // }; 1933 ; 1934 // --- 1 vs Rest ROC AUC, signal efficiency @ given background efficiency; 1935 // --------------------------------------------------------------------; 1936 TString header1 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Dataset"", ""MVA Method"", ""ROC AUC"", ""Sig eff@B=0.01"",; 1937 ""Sig eff@B=0.10"", ""Sig eff@B=0.30"");; 1938 TString header2 = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", ""Name:"", ""/ Class:"", ""test (train)"", ""test (train)"",; 1939 ""test (train)"", ""test (train)"");; 1940 Log() << kINFO << Endl;; 1941 Log() << kINFO << ""1-vs-rest performance metrics per class"" << Endl;; 1942 Log() << kINFO << hLine << Endl;; 1943 Log() << kINFO << Endl;; 1944 Log() << kINFO << ""Considers the listed class as signal and the other classes"" << Endl;; 1945 Log() << kINFO << ""as background, reporting the resulting binary performance."" << Endl;; 1946 Log() << kINFO << ""A score of 0.820 (0.850) means 0.820 was acheived on the"" << Endl;; 1947 Log() << kINFO << ""test set and 0.850 on the training set."" << Endl;; 1948 ; 1949 Log() << kINFO << Endl;; 1950 Log() << kINFO << header1 << Endl;; 1951 Log() << kINFO << header2 << Endl;; 1952 for (Int_t k = 0; k < 2; k++) {; 1953 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 1954 if (k == 1) {; 1955 mname[k][i].ReplaceAll(""Variable_"", """");; 1956 }; 1957 ; 1958 const TString datasetName = itrMap->first;; 1959 const TString mvaName = mname[k][i];; 1960 ; 1961 MethodBase *theMethod = dynamic_cast<MethodBase *>(GetMethod(datasetName, mvaName));; 1962 if (theMethod == 0) {; 1963 continue;; 1964 }; 1965 ; 1966 Log() << kINFO << Endl;; 1967 TString row = TString::Format(""%-15s%-15s"", datasetName.Data(), mvaName.Data());; 1968 Log() << kINFO << row << Endl;; 1969 Log() << kINFO << ""------------------------------"" << Endl;; 1970 ; 1971 UInt_t numClasses = theMethod->fDataSetInfo.GetNClasses();; 1972 for (UInt_t iClass = 0; iClass < numClasses; ++iClass) {; 1973 ; 1974 ROCCurve *rocCurveTrai",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:84446,Testability,assert,assert,84446," effB01Test = rocCurveTest->GetEffSForEffB(0.01);; 1984 const Double_t effB10Test = rocCurveTest->GetEffSForEffB(0.10);; 1985 const Double_t effB30Test = rocCurveTest->GetEffSForEffB(0.30);; 1986 const TString rocaucCmp = TString::Format(""%5.3f (%5.3f)"", rocaucTest, rocaucTrain);; 1987 const TString effB01Cmp = TString::Format(""%5.3f (%5.3f)"", effB01Test, effB01Train);; 1988 const TString effB10Cmp = TString::Format(""%5.3f (%5.3f)"", effB10Test, effB10Train);; 1989 const TString effB30Cmp = TString::Format(""%5.3f (%5.3f)"", effB30Test, effB30Train);; 1990 row = TString::Format(""%-15s%-15s%-15s%-15s%-15s%-15s"", """", className.Data(), rocaucCmp.Data(), effB01Cmp.Data(),; 1991 effB10Cmp.Data(), effB30Cmp.Data());; 1992 Log() << kINFO << row << Endl;; 1993 ; 1994 delete rocCurveTrain;; 1995 delete rocCurveTest;; 1996 }; 1997 }; 1998 }; 1999 Log() << kINFO << Endl;; 2000 Log() << kINFO << hLine << Endl;; 2001 Log() << kINFO << Endl;; 2002 ; 2003 // --- Confusion matrices; 2004 // --------------------------------------------------------------------; 2005 auto printMatrix = [](TMatrixD const &matTraining, TMatrixD const &matTesting, std::vector<TString> classnames,; 2006 UInt_t numClasses, MsgLogger &stream) {; 2007 // assert (classLabledWidth >= valueLabelWidth + 2); 2008 // if (...) {Log() << kWARN << ""..."" << Endl; }; 2009 ; 2010 // TODO: Ensure matrices are same size.; 2011 ; 2012 TString header = TString::Format("" %-14s"", "" "");; 2013 TString headerInfo = TString::Format("" %-14s"", "" "");; 2014 ; 2015 for (UInt_t iCol = 0; iCol < numClasses; ++iCol) {; 2016 header += TString::Format("" %-14s"", classnames[iCol].Data());; 2017 headerInfo += TString::Format("" %-14s"", "" test (train)"");; 2018 }; 2019 stream << kINFO << header << Endl;; 2020 stream << kINFO << headerInfo << Endl;; 2021 ; 2022 for (UInt_t iRow = 0; iRow < numClasses; ++iRow) {; 2023 stream << kINFO << TString::Format("" %-14s"", classnames[iRow].Data());; 2024 ; 2025 for (UInt_t iCol = 0; iCol < numClasses; ++iCol) {",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:84903,Testability,test,test,84903,"urveTrain;; 1995 delete rocCurveTest;; 1996 }; 1997 }; 1998 }; 1999 Log() << kINFO << Endl;; 2000 Log() << kINFO << hLine << Endl;; 2001 Log() << kINFO << Endl;; 2002 ; 2003 // --- Confusion matrices; 2004 // --------------------------------------------------------------------; 2005 auto printMatrix = [](TMatrixD const &matTraining, TMatrixD const &matTesting, std::vector<TString> classnames,; 2006 UInt_t numClasses, MsgLogger &stream) {; 2007 // assert (classLabledWidth >= valueLabelWidth + 2); 2008 // if (...) {Log() << kWARN << ""..."" << Endl; }; 2009 ; 2010 // TODO: Ensure matrices are same size.; 2011 ; 2012 TString header = TString::Format("" %-14s"", "" "");; 2013 TString headerInfo = TString::Format("" %-14s"", "" "");; 2014 ; 2015 for (UInt_t iCol = 0; iCol < numClasses; ++iCol) {; 2016 header += TString::Format("" %-14s"", classnames[iCol].Data());; 2017 headerInfo += TString::Format("" %-14s"", "" test (train)"");; 2018 }; 2019 stream << kINFO << header << Endl;; 2020 stream << kINFO << headerInfo << Endl;; 2021 ; 2022 for (UInt_t iRow = 0; iRow < numClasses; ++iRow) {; 2023 stream << kINFO << TString::Format("" %-14s"", classnames[iRow].Data());; 2024 ; 2025 for (UInt_t iCol = 0; iCol < numClasses; ++iCol) {; 2026 if (iCol == iRow) {; 2027 stream << kINFO << TString::Format("" %-14s"", ""-"");; 2028 } else {; 2029 Double_t trainValue = matTraining[iRow][iCol];; 2030 Double_t testValue = matTesting[iRow][iCol];; 2031 TString entry = TString::Format(""%-5.3f (%-5.3f)"", testValue, trainValue);; 2032 stream << kINFO << TString::Format("" %-14s"", entry.Data());; 2033 }; 2034 }; 2035 stream << kINFO << Endl;; 2036 }; 2037 };; 2038 ; 2039 Log() << kINFO << Endl;; 2040 Log() << kINFO << ""Confusion matrices for all methods"" << Endl;; 2041 Log() << kINFO << hLine << Endl;; 2042 Log() << kINFO << Endl;; 2043 Log() << kINFO << ""Does a binary comparison between the two classes given by a "" << Endl;; 2044 Log() << kINFO << ""particular row-column combination. In each case, the class "" << Endl",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:85384,Testability,test,testValue,85384,"trixD const &matTraining, TMatrixD const &matTesting, std::vector<TString> classnames,; 2006 UInt_t numClasses, MsgLogger &stream) {; 2007 // assert (classLabledWidth >= valueLabelWidth + 2); 2008 // if (...) {Log() << kWARN << ""..."" << Endl; }; 2009 ; 2010 // TODO: Ensure matrices are same size.; 2011 ; 2012 TString header = TString::Format("" %-14s"", "" "");; 2013 TString headerInfo = TString::Format("" %-14s"", "" "");; 2014 ; 2015 for (UInt_t iCol = 0; iCol < numClasses; ++iCol) {; 2016 header += TString::Format("" %-14s"", classnames[iCol].Data());; 2017 headerInfo += TString::Format("" %-14s"", "" test (train)"");; 2018 }; 2019 stream << kINFO << header << Endl;; 2020 stream << kINFO << headerInfo << Endl;; 2021 ; 2022 for (UInt_t iRow = 0; iRow < numClasses; ++iRow) {; 2023 stream << kINFO << TString::Format("" %-14s"", classnames[iRow].Data());; 2024 ; 2025 for (UInt_t iCol = 0; iCol < numClasses; ++iCol) {; 2026 if (iCol == iRow) {; 2027 stream << kINFO << TString::Format("" %-14s"", ""-"");; 2028 } else {; 2029 Double_t trainValue = matTraining[iRow][iCol];; 2030 Double_t testValue = matTesting[iRow][iCol];; 2031 TString entry = TString::Format(""%-5.3f (%-5.3f)"", testValue, trainValue);; 2032 stream << kINFO << TString::Format("" %-14s"", entry.Data());; 2033 }; 2034 }; 2035 stream << kINFO << Endl;; 2036 }; 2037 };; 2038 ; 2039 Log() << kINFO << Endl;; 2040 Log() << kINFO << ""Confusion matrices for all methods"" << Endl;; 2041 Log() << kINFO << hLine << Endl;; 2042 Log() << kINFO << Endl;; 2043 Log() << kINFO << ""Does a binary comparison between the two classes given by a "" << Endl;; 2044 Log() << kINFO << ""particular row-column combination. In each case, the class "" << Endl;; 2045 Log() << kINFO << ""given by the row is considered signal while the class given "" << Endl;; 2046 Log() << kINFO << ""by the column index is considered background."" << Endl;; 2047 Log() << kINFO << Endl;; 2048 for (UInt_t iMethod = 0; iMethod < methods->size(); ++iMethod) {; 2049 MethodBase *theMethod ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:85477,Testability,test,testValue,85477,"Log() << kWARN << ""..."" << Endl; }; 2009 ; 2010 // TODO: Ensure matrices are same size.; 2011 ; 2012 TString header = TString::Format("" %-14s"", "" "");; 2013 TString headerInfo = TString::Format("" %-14s"", "" "");; 2014 ; 2015 for (UInt_t iCol = 0; iCol < numClasses; ++iCol) {; 2016 header += TString::Format("" %-14s"", classnames[iCol].Data());; 2017 headerInfo += TString::Format("" %-14s"", "" test (train)"");; 2018 }; 2019 stream << kINFO << header << Endl;; 2020 stream << kINFO << headerInfo << Endl;; 2021 ; 2022 for (UInt_t iRow = 0; iRow < numClasses; ++iRow) {; 2023 stream << kINFO << TString::Format("" %-14s"", classnames[iRow].Data());; 2024 ; 2025 for (UInt_t iCol = 0; iCol < numClasses; ++iCol) {; 2026 if (iCol == iRow) {; 2027 stream << kINFO << TString::Format("" %-14s"", ""-"");; 2028 } else {; 2029 Double_t trainValue = matTraining[iRow][iCol];; 2030 Double_t testValue = matTesting[iRow][iCol];; 2031 TString entry = TString::Format(""%-5.3f (%-5.3f)"", testValue, trainValue);; 2032 stream << kINFO << TString::Format("" %-14s"", entry.Data());; 2033 }; 2034 }; 2035 stream << kINFO << Endl;; 2036 }; 2037 };; 2038 ; 2039 Log() << kINFO << Endl;; 2040 Log() << kINFO << ""Confusion matrices for all methods"" << Endl;; 2041 Log() << kINFO << hLine << Endl;; 2042 Log() << kINFO << Endl;; 2043 Log() << kINFO << ""Does a binary comparison between the two classes given by a "" << Endl;; 2044 Log() << kINFO << ""particular row-column combination. In each case, the class "" << Endl;; 2045 Log() << kINFO << ""given by the row is considered signal while the class given "" << Endl;; 2046 Log() << kINFO << ""by the column index is considered background."" << Endl;; 2047 Log() << kINFO << Endl;; 2048 for (UInt_t iMethod = 0; iMethod < methods->size(); ++iMethod) {; 2049 MethodBase *theMethod = dynamic_cast<MethodBase *>(methods->at(iMethod));; 2050 if (theMethod == nullptr) {; 2051 continue;; 2052 }; 2053 UInt_t numClasses = theMethod->fDataSetInfo.GetNClasses();; 2054 ; 2055 std::vector<TString> cl",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:91228,Testability,test,test,91228,"; 2133 // Log() << kDEBUG << Form(""%-20s %-15s: %#1.3f(%02i) %#1.3f(%02i) %#1.3f(%02i); 2134 // %#1.3f %#1.3f | -- --"",; 2135 // datasetName.Data(),; 2136 // methodName.Data(),; 2137 // eff01[k][i], Int_t(1000*eff01err[k][i]),; 2138 // eff10[k][i], Int_t(1000*eff10err[k][i]),; 2139 // eff30[k][i], Int_t(1000*eff30err[k][i]),; 2140 // effArea[k][i],rocIntegral) << Endl;; 2141 } else {; 2142 Log() << kINFO << Form(""%-13s %-15s: %#1.3f"", datasetName.Data(), methodName.Data(), rocIntegral); 2143 << Endl;; 2144 // Log() << kDEBUG << Form(""%-20s %-15s: %#1.3f(%02i) %#1.3f(%02i) %#1.3f(%02i); 2145 // %#1.3f %#1.3f | %#1.3f %#1.3f"",; 2146 // datasetName.Data(),; 2147 // methodName.Data(),; 2148 // eff01[k][i], Int_t(1000*eff01err[k][i]),; 2149 // eff10[k][i], Int_t(1000*eff10err[k][i]),; 2150 // eff30[k][i], Int_t(1000*eff30err[k][i]),; 2151 // effArea[k][i],rocIntegral,; 2152 // sep[k][i], sig[k][i]) << Endl;; 2153 }; 2154 }; 2155 }; 2156 Log() << kINFO << hLine << Endl;; 2157 Log() << kINFO << Endl;; 2158 Log() << kINFO << ""Testing efficiency compared to training efficiency (overtraining check)"" << Endl;; 2159 Log() << kINFO << hLine << Endl;; 2160 Log() << kINFO; 2161 << ""DataSet MVA Signal efficiency: from test sample (from training sample) ""; 2162 << Endl;; 2163 Log() << kINFO << ""Name: Method: @B=0.01 @B=0.10 @B=0.30 ""; 2164 << Endl;; 2165 Log() << kINFO << hLine << Endl;; 2166 for (Int_t k = 0; k < 2; k++) {; 2167 if (k == 1 && nmeth_used[k] > 0) {; 2168 Log() << kINFO << hLine << Endl;; 2169 Log() << kINFO << ""Input Variables: "" << Endl << hLine << Endl;; 2170 }; 2171 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 2172 if (k == 1); 2173 mname[k][i].ReplaceAll(""Variable_"", """");; 2174 MethodBase *theMethod = dynamic_cast<MethodBase *>((*methods)[i]);; 2175 if (theMethod == 0); 2176 continue;; 2177 ; 2178 Log() << kINFO; 2179 << Form(""%-20s %-15s: %#1.3f (%#1.3f) %#1.3f (%#1.3f) %#1.3f (%#1.3f)"",; 2180 theMethod->fDataSetInfo.GetName(), (const char *)mname[k][i], eff01[k",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:92599,Testability,test,test,92599,"] > 0) {; 2168 Log() << kINFO << hLine << Endl;; 2169 Log() << kINFO << ""Input Variables: "" << Endl << hLine << Endl;; 2170 }; 2171 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 2172 if (k == 1); 2173 mname[k][i].ReplaceAll(""Variable_"", """");; 2174 MethodBase *theMethod = dynamic_cast<MethodBase *>((*methods)[i]);; 2175 if (theMethod == 0); 2176 continue;; 2177 ; 2178 Log() << kINFO; 2179 << Form(""%-20s %-15s: %#1.3f (%#1.3f) %#1.3f (%#1.3f) %#1.3f (%#1.3f)"",; 2180 theMethod->fDataSetInfo.GetName(), (const char *)mname[k][i], eff01[k][i],; 2181 trainEff01[k][i], eff10[k][i], trainEff10[k][i], eff30[k][i], trainEff30[k][i]); 2182 << Endl;; 2183 }; 2184 }; 2185 Log() << kINFO << hLine << Endl;; 2186 Log() << kINFO << Endl;; 2187 ; 2188 if (gTools().CheckForSilentOption(GetOptions())); 2189 Log().InhibitOutput();; 2190 } // end fROC; 2191 }; 2192 if (!IsSilentFile()) {; 2193 std::list<TString> datasets;; 2194 for (Int_t k = 0; k < 2; k++) {; 2195 for (Int_t i = 0; i < nmeth_used[k]; i++) {; 2196 MethodBase *theMethod = dynamic_cast<MethodBase *>((*methods)[i]);; 2197 if (theMethod == 0); 2198 continue;; 2199 // write test/training trees; 2200 RootBaseDir()->cd(theMethod->fDataSetInfo.GetName());; 2201 if (std::find(datasets.begin(), datasets.end(), theMethod->fDataSetInfo.GetName()) == datasets.end()) {; 2202 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTesting)->Write("""", TObject::kOverwrite);; 2203 theMethod->fDataSetInfo.GetDataSet()->GetTree(Types::kTraining)->Write("""", TObject::kOverwrite);; 2204 datasets.push_back(theMethod->fDataSetInfo.GetName());; 2205 }; 2206 }; 2207 }; 2208 }; 2209 } // end for MethodsMap; 2210 // references for citation; 2211 gTools().TMVACitation(Log(), Tools::kHtmlLink);; 2212}; 2213 ; 2214////////////////////////////////////////////////////////////////////////////////; 2215/// Evaluate Variable Importance; 2216 ; 2217TH1F *TMVA::Factory::EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitl",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:97824,Testability,log,log,97824,"96 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2309 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2310 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2311 delete sresults;; 2312 delete seedloader;; 2313 this->DeleteAllMethods();; 2314 ; 2315 fMethodsMap.clear();; 2316 // removing global result because it is requiring a lot of RAM for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i < VIBITS; ++i) {; 2322 if (x & (uint64_t(1) << i)) {; 2323 y = x & ~(1 << i);; 2324 std::bitset<VIBITS> ybitset(y);; 2325 // need at least one variable; 2326 // NOTE: if sub-seed is zero then is the special case; 2327 // that count in xbitset is 1; 2328 uint32_t ny = static_cast<uint32_t>( log(x - y) / 0.693147 ) ;; 2329 if (y == 0) {; 2330 importances[ny] = SROC - 0.5;; 2331 continue;; 2332 }; 2333 ; 2334 // getting ROC; 2335 SSROC = ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356/////////////////////////////////////////////////////////////////",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:101207,Testability,log,log,101207,"erCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2409 ; 2410 // cleaning information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2414 delete sresults;; 2415 delete seedloader;; 2416 this->DeleteAllMethods();; 2417 fMethodsMap.clear();; 2418 ; 2419 // removing global result because it is requiring a lot of RAM for all seeds; 2420 ; 2421 for (uint32_t i = 0; i < VIBITS; ++i) {; 2422 if (x & (1 << i)) {; 2423 y = x & ~(uint64_t(1) << i);; 2424 std::bitset<VIBITS> ybitset(y);; 2425 // need at least one variable; 2426 // NOTE: if sub-seed is zero then is the special case; 2427 // that count in xbitset is 1; 2428 uint32_t ny = static_cast<uint32_t>(log(x - y) / 0.693147);; 2429 if (y == 0) {; 2430 importances[ny] = SROC - 0.5;; 2431 continue;; 2432 }; 2433 ; 2434 // creating loader for sub-seed; 2435 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2436 // adding variables from sub-seed; 2437 for (int index = 0; index < nbits; index++) {; 2438 if (ybitset[index]); 2439 subseedloader->AddVariable(varNames[index], 'F');; 2440 }; 2441 ; 2442 // Loading Dataset; 2443 DataLoaderCopy(subseedloader, loader);; 2444 ; 2445 // Booking SubSeed; 2446 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2447 ; 2448 // Train/Test/Evaluation; 2449 TrainAllMethods();; 2450 TestAllMethods();; 2451 EvaluateAllMethods();; 2452 ; 2453 // getting ROC; 2454 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:105335,Testability,log,log,105335," methodTitle, theOption);; 2514 ; 2515 // Train/Test/Evaluation; 2516 TrainAllMethods();; 2517 TestAllMethods();; 2518 EvaluateAllMethods();; 2519 ; 2520 // getting ROC; 2521 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2522 // std::cout << ""Seed: n "" << n << "" x "" << x << "" xbitset:"" << xbitset << "" ROC "" << SROC << std::endl;; 2523 ; 2524 // cleaning information to process sub-seeds; 2525 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2526 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2527 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2528 delete sresults;; 2529 delete seedloader;; 2530 this->DeleteAllMethods();; 2531 fMethodsMap.clear();; 2532 ; 2533 // removing global result because it is requiring a lot of RAM for all seeds; 2534 ; 2535 for (uint32_t i = 0; i < 32; ++i) {; 2536 if (x & (uint64_t(1) << i)) {; 2537 y = x & ~(1 << i);; 2538 std::bitset<32> ybitset(y);; 2539 // need at least one variable; 2540 // NOTE: if sub-seed is zero then is the special case; 2541 // that count in xbitset is 1; 2542 Double_t ny = log(x - y) / 0.693147;; 2543 if (y == 0) {; 2544 importances[ny] = SROC - 0.5;; 2545 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << ""ROC "" << 0.5 << std::endl;; 2546 continue;; 2547 }; 2548 ; 2549 // creating loader for sub-seed; 2550 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2551 // adding variables from sub-seed; 2552 for (int index = 0; index < nbits; index++) {; 2553 if (ybitset[index]); 2554 subseedloader->AddVariable(varNames[index], 'F');; 2555 }; 2556 ; 2557 // Loading Dataset; 2558 DataLoaderCopy(subseedloader, loader);; 2559 ; 2560 // Booking SubSeed; 2561 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2562 ; 2563 // Train/Test/Evaluation; 2564 TrainAllMethods();; 2565 TestAllMethods();; 2566 EvaluateAllMethods();; 2567 ; 2568 //",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:115742,Testability,log,loggerDefinition,115742,"fig::fVariablePlottingclass TMVA::Config::VariablePlotting fVariablePlotting; TMVA::Config::SetSilentvoid SetSilent(Bool_t s)Definition Config.h:63; TMVA::Config::GetIONamesIONames & GetIONames()Definition Config.h:98; TMVA::ConfigurableDefinition Configurable.h:45; TMVA::Configurable::SetConfigDescriptionvoid SetConfigDescription(const char *d)Definition Configurable.h:64; TMVA::Configurable::DeclareOptionRefOptionBase * DeclareOptionRef(T &ref, const TString &name, const TString &desc=""""); TMVA::Configurable::AddPreDefValvoid AddPreDefVal(const T &)Definition Configurable.h:168; TMVA::Configurable::SetConfigNamevoid SetConfigName(const char *n)Definition Configurable.h:63; TMVA::Configurable::ParseOptionsvirtual void ParseOptions()options parserDefinition Configurable.cxx:124; TMVA::Configurable::GetOptionsconst TString & GetOptions() constDefinition Configurable.h:84; TMVA::Configurable::LogMsgLogger & Log() constDefinition Configurable.h:122; TMVA::Configurable::fLoggerMsgLogger * fLogger! message loggerDefinition Configurable.h:128; TMVA::Configurable::CheckForUnusedOptionsvoid CheckForUnusedOptions() constchecks for unused options in option stringDefinition Configurable.cxx:270; TMVA::DataInputHandler::GetEntriesUInt_t GetEntries(const TString &name) constDefinition DataInputHandler.h:100; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::DataSetInfoClass that contains all the data information.Definition DataSe",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:116228,Testability,test,test,116228," &name, const TString &desc=""""); TMVA::Configurable::AddPreDefValvoid AddPreDefVal(const T &)Definition Configurable.h:168; TMVA::Configurable::SetConfigNamevoid SetConfigName(const char *n)Definition Configurable.h:63; TMVA::Configurable::ParseOptionsvirtual void ParseOptions()options parserDefinition Configurable.cxx:124; TMVA::Configurable::GetOptionsconst TString & GetOptions() constDefinition Configurable.h:84; TMVA::Configurable::LogMsgLogger & Log() constDefinition Configurable.h:122; TMVA::Configurable::fLoggerMsgLogger * fLogger! message loggerDefinition Configurable.h:128; TMVA::Configurable::CheckForUnusedOptionsvoid CheckForUnusedOptions() constchecks for unused options in option stringDefinition Configurable.cxx:270; TMVA::DataInputHandler::GetEntriesUInt_t GetEntries(const TString &name) constDefinition DataInputHandler.h:100; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::DataSetInfoClass that contains all the data information.Definition DataSetInfo.h:62; TMVA::DataSetInfo::GetNVariablesUInt_t GetNVariables() constDefinition DataSetInfo.h:127; TMVA::DataSetInfo::GetNamevirtual const char * GetName() constReturns name of object.Definition DataSetInfo.h:71; TMVA::DataSetInfo::CorrelationMatrixconst TMatrixD * CorrelationMatrix(const TString &className) constDefinition DataSetInfo.cxx:197; TMVA::DataSetInfo::GetNClassesUInt_t GetNClasses() constDefinition DataSetInfo.h:155; TMVA::DataSetInfo::GetSplitO",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:118531,Testability,test,test,118531,"reateCorrelationMatrixHist(const TMatrixD *m, const TString &hName, const TString &hTitle) constDefinition DataSetInfo.cxx:429; TMVA::DataSetInfo::GetListOfVariablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::DataSetInfo::GetClassInfoClassInfo * GetClassInfo(Int_t clNum) constDefinition DataSetInfo.cxx:146; TMVA::DataSetInfo::GetCutconst TCut & GetCut(Int_t i) constDefinition DataSetInfo.h:168; TMVA::DataSetInfo::GetVariableInfoVariableInfo & GetVariableInfo(Int_t i)Definition DataSetInfo.h:105; TMVA::DataSetInfo::IsSignalBool_t IsSignal(const Event *ev) constDefinition DataSetInfo.cxx:167; TMVA::DataSetInfo::GetDataSetManagerDataSetManager * GetDataSetManager()Definition DataSetInfo.h:194; TMVA::DataSetManager::DataInputDataInputHandler & DataInput()Definition DataSetManager.h:76; TMVA::DataSetClass that contains all the data information.Definition DataSet.h:58; TMVA::DataSet::GetNEvtSigTestLong64_t GetNEvtSigTest()return number of signal test events in datasetDefinition DataSet.cxx:427; TMVA::DataSet::GetTreeTTree * GetTree(Types::ETreeType type)create the test/trainings tree with all the variables, the weights, the classes, the targets,...Definition DataSet.cxx:609; TMVA::DataSet::GetEventconst Event * GetEvent() constreturns event without transformationsDefinition DataSet.cxx:202; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetResultsResults * GetResults(const TString &, Types::ETreeType type, Types::EAnalysisType analysistype)Definition DataSet.cxx:265; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::DataSet::SetCurrentTypevoid SetCurrentType(Types::ETreeType type) constDefinition DataSet.h:89; TMVA::DataSet::GetEventCollectionconst std::vector< Event * > & GetEventCollection(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:216; TMVA::Da",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:118652,Testability,test,test,118652,"o::GetListOfVariablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::DataSetInfo::GetClassInfoClassInfo * GetClassInfo(Int_t clNum) constDefinition DataSetInfo.cxx:146; TMVA::DataSetInfo::GetCutconst TCut & GetCut(Int_t i) constDefinition DataSetInfo.h:168; TMVA::DataSetInfo::GetVariableInfoVariableInfo & GetVariableInfo(Int_t i)Definition DataSetInfo.h:105; TMVA::DataSetInfo::IsSignalBool_t IsSignal(const Event *ev) constDefinition DataSetInfo.cxx:167; TMVA::DataSetInfo::GetDataSetManagerDataSetManager * GetDataSetManager()Definition DataSetInfo.h:194; TMVA::DataSetManager::DataInputDataInputHandler & DataInput()Definition DataSetManager.h:76; TMVA::DataSetClass that contains all the data information.Definition DataSet.h:58; TMVA::DataSet::GetNEvtSigTestLong64_t GetNEvtSigTest()return number of signal test events in datasetDefinition DataSet.cxx:427; TMVA::DataSet::GetTreeTTree * GetTree(Types::ETreeType type)create the test/trainings tree with all the variables, the weights, the classes, the targets,...Definition DataSet.cxx:609; TMVA::DataSet::GetEventconst Event * GetEvent() constreturns event without transformationsDefinition DataSet.cxx:202; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetResultsResults * GetResults(const TString &, Types::ETreeType type, Types::EAnalysisType analysistype)Definition DataSet.cxx:265; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::DataSet::SetCurrentTypevoid SetCurrentType(Types::ETreeType type) constDefinition DataSet.h:89; TMVA::DataSet::GetEventCollectionconst std::vector< Event * > & GetEventCollection(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:216; TMVA::DataSet::GetNEvtBkgdTestLong64_t GetNEvtBkgdTest()return number of background test events in datasetDefinition DataSet.cxx:435; TMVA::EventDefinit",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:119586,Testability,test,test,119586,"on DataSet.cxx:427; TMVA::DataSet::GetTreeTTree * GetTree(Types::ETreeType type)create the test/trainings tree with all the variables, the weights, the classes, the targets,...Definition DataSet.cxx:609; TMVA::DataSet::GetEventconst Event * GetEvent() constreturns event without transformationsDefinition DataSet.cxx:202; TMVA::DataSet::GetNEventsLong64_t GetNEvents(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:206; TMVA::DataSet::GetResultsResults * GetResults(const TString &, Types::ETreeType type, Types::EAnalysisType analysistype)Definition DataSet.cxx:265; TMVA::DataSet::GetNTrainingEventsLong64_t GetNTrainingEvents() constDefinition DataSet.h:68; TMVA::DataSet::SetCurrentTypevoid SetCurrentType(Types::ETreeType type) constDefinition DataSet.h:89; TMVA::DataSet::GetEventCollectionconst std::vector< Event * > & GetEventCollection(Types::ETreeType type=Types::kMaxTreeType) constDefinition DataSet.h:216; TMVA::DataSet::GetNEvtBkgdTestLong64_t GetNEvtBkgdTest()return number of background test events in datasetDefinition DataSet.cxx:435; TMVA::EventDefinition Event.h:51; TMVA::Event::GetValueFloat_t GetValue(UInt_t ivar) constreturn value of i'th variableDefinition Event.cxx:236; TMVA::Event::SetIsTrainingstatic void SetIsTraining(Bool_t)when this static function is called, it sets the flag whether events with negative event weight shoul...Definition Event.cxx:399; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::PrintHelpMessagevoid PrintHelpMessage(const TString &datasetname, const TString &methodTitle="""") constPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:121175,Testability,test,testing,121175,"tPrint predefined help message of classifier.Definition Factory.cxx:1333; TMVA::Factory::fCorrelationsBool_t fCorrelations! enable to calculate correlationsDefinition Factory.h:215; TMVA::Factory::MVectorstd::vector< IMethod * > MVectorDefinition Factory.h:84; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::VerboseBool_t Verbose(void) constDefinition Factory.h:134; TMVA::Factory::WriteDataInformationvoid WriteDataInformation(DataSetInfo &fDataSetInfo)Definition Factory.cxx:602; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::FactoryFactory(TString theJobName, TFile *theTargetFile, TString theOption="""")Standard constructor.Definition Factory.cxx:113; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:136",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:122256,Testability,log,loggingDefinition,122256,"ults in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::fVerboseBool_t fVerbose! verbose modeDefinition Factory.h:213; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::EvaluateImportanceRandomTH1F * EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2473; TMVA::Factory::GetImportanceTH1F * GetImportance(const int nbits, std::vector< Double_t > importances, std::vector< TString > varNames)Definition Factory.cxx:2591; TMVA::Factory::fROCBool_t fROC! enable to calculate ROC valuesDefinition Factory.h:216; TMVA::Factory::EvaluateAllVariablesvoid EvaluateAllVariables(DataLoader *loader, TString options="""")Iterates over all MVA input variables and evaluates them.Definition Factory.cxx:1360; TMVA::Factory::fVerboseLevelTString fVerboseLevel! verbosity level, controls granularity of loggingDefinition Factory.h:214; TMVA::Factory::GetROCCurveAsMultiGraphTMultiGraph * GetROCCurveAsMultiGraph(DataLoader *loader, UInt_t iClass, Types::ETreeType type=Types::kTesting)Generate a collection of graphs, for all methods for a given class.Definition Factory.cxx:988; TMVA::Factory::EvaluateImportanceTH1F * EvaluateImportance(DataLoader *loader, VIType vitype, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Evaluate Variable Importance.Definition Factory.cxx:2217; TMVA::Factory::GetROCIntegralDouble_t GetROCIntegral(DataLoader *loader, TString theMethodName, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Calculate the integral of the ROC curve, also known as the area under curve (AUC),...Definition Factory.cxx:849; TMVA::Factory::~Factoryvirtual ~Factory()Destructor.Definition Factory.cxx:306; TMVA::Factory::MakeClassvirtual void MakeClass(const TString &datasetname, const TString &methodTitle="""") constDefinitio",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:125628,Testability,test,testDefinition,125628,"on Factory.cxx:586; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::Factory::EvaluateImportanceAllTH1F * EvaluateImportanceAll(DataLoader *loader, Types::EMVA theMethod, TString methodTitle, const char *theOption="""")Definition Factory.cxx:2246; TMVA::Factory::SetVerbosevoid SetVerbose(Bool_t v=kTRUE)Definition Factory.cxx:343; TMVA::Factory::fgTargetFileTFile * fgTargetFile! ROOT output fileDefinition Factory.h:205; TMVA::Factory::GetMethodIMethod * GetMethod(const TString &datasetname, const TString &title) constReturns pointer to MVA that corresponds to given method title.Definition Factory.cxx:566; TMVA::Factory::DeleteAllMethodsvoid DeleteAllMethods(void)Delete methods.Definition Factory.cxx:324; TMVA::Factory::fTransformationsTString fTransformations! list of transformations to testDefinition Factory.h:212; TMVA::Factory::Greetingsvoid Greetings()Print welcome message.Definition Factory.cxx:295; TMVA::IMethodInterface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::IMethod::PrintHelpMessagevirtual void PrintHelpMessage() const =0; TMVA::IMethod::HasAnalysisTypevirtual Bool_t HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets)=0; TMVA::IMethod::MakeClassvirtual void MakeClass(const TString &classFileName=TString("""")) const =0; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::MethodBase::GetSeparationvirtual Double_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodBase.cxx:2789; TMVA::MethodBase::SetSilentFilevoid SetSilentFile(Bool_t status)Definition MethodBase.h:378; TMVA::MethodBase::SetWeightFileDirvoid SetWeightFileDir(TString fileDir)set directory of weight fileDefinition MethodBase.cxx:",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:126908,Testability,test,test,126908,"rface for all concrete MVA method implementations.Definition IMethod.h:53; TMVA::IMethod::PrintHelpMessagevirtual void PrintHelpMessage() const =0; TMVA::IMethod::HasAnalysisTypevirtual Bool_t HasAnalysisType(Types::EAnalysisType type, UInt_t numberClasses, UInt_t numberTargets)=0; TMVA::IMethod::MakeClassvirtual void MakeClass(const TString &classFileName=TString("""")) const =0; TMVA::MethodBaseVirtual base Class for all MVA method.Definition MethodBase.h:111; TMVA::MethodBase::GetSeparationvirtual Double_t GetSeparation(TH1 *, TH1 *) constcompute ""separation"" defined asDefinition MethodBase.cxx:2789; TMVA::MethodBase::SetSilentFilevoid SetSilentFile(Bool_t status)Definition MethodBase.h:378; TMVA::MethodBase::SetWeightFileDirvoid SetWeightFileDir(TString fileDir)set directory of weight fileDefinition MethodBase.cxx:2059; TMVA::MethodBase::TestRegressionvirtual void TestRegression(Double_t &bias, Double_t &biasT, Double_t &dev, Double_t &devT, Double_t &rms, Double_t &rmsT, Double_t &mInf, Double_t &mInfT, Double_t &corr, Types::ETreeType type)calculate <sum-of-deviation-squared> of regression output versus ""true"" value from test sampleDefinition MethodBase.cxx:992; TMVA::MethodBase::DeclareCompatibilityOptionsvirtual void DeclareCompatibilityOptions()options that are used ONLY for the READER to ensure backward compatibility they are hence without any...Definition MethodBase.cxx:596; TMVA::MethodBase::GetMethodTypeNameTString GetMethodTypeName() constDefinition MethodBase.h:332; TMVA::MethodBase::DoMulticlassBool_t DoMulticlass() constDefinition MethodBase.h:439; TMVA::MethodBase::GetSignificancevirtual Double_t GetSignificance() constcompute significance of mean differenceDefinition MethodBase.cxx:2776; TMVA::MethodBase::GetNameconst char * GetName() constDefinition MethodBase.h:334; TMVA::MethodBase::GetAnalysisTypeTypes::EAnalysisType GetAnalysisType() constDefinition MethodBase.h:437; TMVA::MethodBase::GetMulticlassConfusionMatrixvirtual TMatrixD GetMulticlassCo",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:128365,Testability,test,test,128365,"TMVA::MethodBase::GetSignificancevirtual Double_t GetSignificance() constcompute significance of mean differenceDefinition MethodBase.cxx:2776; TMVA::MethodBase::GetNameconst char * GetName() constDefinition MethodBase.h:334; TMVA::MethodBase::GetAnalysisTypeTypes::EAnalysisType GetAnalysisType() constDefinition MethodBase.h:437; TMVA::MethodBase::GetMulticlassConfusionMatrixvirtual TMatrixD GetMulticlassConfusionMatrix(Double_t effB, Types::ETreeType type)Construct a confusion matrix for a multiclass classifier.Definition MethodBase.cxx:2750; TMVA::MethodBase::PrintHelpMessagevoid PrintHelpMessage() constprints out method-specific help methodDefinition MethodBase.cxx:3264; TMVA::MethodBase::TrainMethodvoid TrainMethod()Definition MethodBase.cxx:650; TMVA::MethodBase::WriteEvaluationHistosToFilevirtual void WriteEvaluationHistosToFile(Types::ETreeType treetype)writes all MVA evaluation histograms to fileDefinition MethodBase.cxx:2094; TMVA::MethodBase::TestMulticlassvirtual void TestMulticlass()test multiclass classificationDefinition MethodBase.cxx:1100; TMVA::MethodBase::SetupMethodvoid SetupMethod()setup of methodsDefinition MethodBase.cxx:406; TMVA::MethodBase::GetEfficiencyvirtual Double_t GetEfficiency(const TString &, Types::ETreeType, Double_t &err)fill background efficiency (resp.Definition MethodBase.cxx:2302; TMVA::MethodBase::SetAnalysisTypevirtual void SetAnalysisType(Types::EAnalysisType type)Definition MethodBase.h:436; TMVA::MethodBase::GetMethodNameconst TString & GetMethodName() constDefinition MethodBase.h:331; TMVA::MethodBase::DoRegressionBool_t DoRegression() constDefinition MethodBase.h:438; TMVA::MethodBase::ProcessSetupvoid ProcessSetup()process all options the ""CheckForUnusedOptions"" is done in an independent call, since it may be overr...Definition MethodBase.cxx:423; TMVA::MethodBase::GetTrainingEfficiencyvirtual Double_t GetTrainingEfficiency(const TString &)Definition MethodBase.cxx:2528; TMVA::MethodBase::DataInfoDataSetInfo & DataInfo(",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:133189,Testability,log,logger,133189," (sensitivity) for a given background efficiency (sensitivity).Definition ROCCurve.cxx:217; TMVA::ROCCurve::GetROCIntegralDouble_t GetROCIntegral(const UInt_t points=41)Calculates the ROC integral (AUC)Definition ROCCurve.cxx:248; TMVA::ROCCurve::GetROCCurveTGraph * GetROCCurve(const UInt_t points=100)Returns a new TGraph containing the ROC curve.Definition ROCCurve.cxx:274; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Ranking::Printvirtual void Print() constget maximum length of variable namesDefinition Ranking.cxx:111; TMVA::ResultsClassificationClass that is the base-class for a vector of result.Definition ResultsClassification.h:48; TMVA::ResultsMulticlassClass which takes the results of a multiclass classification.Definition ResultsMulticlass.h:55; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::Tools::FormattedOutputvoid FormattedOutput(const std::vector< Double_t > &, const std::vector< TString > &, const TString titleVars, const TString titleValues, MsgLogger &logger, TString format=""%+1.3f"")formatted output of simple tableDefinition Tools.cxx:887; TMVA::Tools::ROOTVersionMessagevoid ROOTVersionMessage(MsgLogger &logger)prints the ROOT release number and dateDefinition Tools.cxx:1325; TMVA::Tools::UsefulSortDescendingvoid UsefulSortDescending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:564; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::GetCorrelationMatrixconst TMatrixD * GetCorrelationMatrix(const TMatrixD *c",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:133345,Testability,log,logger,133345,"8; TMVA::ROCCurve::GetROCCurveTGraph * GetROCCurve(const UInt_t points=100)Returns a new TGraph containing the ROC curve.Definition ROCCurve.cxx:274; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Ranking::Printvirtual void Print() constget maximum length of variable namesDefinition Ranking.cxx:111; TMVA::ResultsClassificationClass that is the base-class for a vector of result.Definition ResultsClassification.h:48; TMVA::ResultsMulticlassClass which takes the results of a multiclass classification.Definition ResultsMulticlass.h:55; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::Tools::FormattedOutputvoid FormattedOutput(const std::vector< Double_t > &, const std::vector< TString > &, const TString titleVars, const TString titleValues, MsgLogger &logger, TString format=""%+1.3f"")formatted output of simple tableDefinition Tools.cxx:887; TMVA::Tools::ROOTVersionMessagevoid ROOTVersionMessage(MsgLogger &logger)prints the ROOT release number and dateDefinition Tools.cxx:1325; TMVA::Tools::UsefulSortDescendingvoid UsefulSortDescending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:564; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::GetCorrelationMatrixconst TMatrixD * GetCorrelationMatrix(const TMatrixD *covMat)turns covariance into correlation matrixDefinition Tools.cxx:324; TMVA::Tools::kHtmlLink@ kHtmlLinkDefinition Tools.h:212; TMVA::Tools::UsefulSortAscendingvoid UsefulSortAscending(std::vector< std::vector< Double_t > > &,",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:134559,Testability,log,logger,134559,"tor (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:564; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::GetCorrelationMatrixconst TMatrixD * GetCorrelationMatrix(const TMatrixD *covMat)turns covariance into correlation matrixDefinition Tools.cxx:324; TMVA::Tools::kHtmlLink@ kHtmlLinkDefinition Tools.h:212; TMVA::Tools::UsefulSortAscendingvoid UsefulSortAscending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:538; TMVA::Tools::TMVACitationvoid TMVACitation(MsgLogger &logger, ECitation citType=kPlainText)kinds of TMVA citationDefinition Tools.cxx:1440; TMVA::Tools::TMVAVersionMessagevoid TMVAVersionMessage(MsgLogger &logger)prints the TMVA release number and dateDefinition Tools.cxx:1316; TMVA::Tools::TMVAWelcomeMessagevoid TMVAWelcomeMessage()direct output, eg, when starting ROOT session -> no use of Logger hereDefinition Tools.cxx:1302; TMVA::TransformationHandlerClass that contains all the data information.Definition TransformationHandler.h:56; TMVA::TransformationHandler::PrintVariableRankingvoid PrintVariableRanking() constprints ranking of input variablesDefinition TransformationHandler.cxx:926; TMVA::TypesSingleton class for Global types used by TMVA.Definition Types.h:71; TMVA::Types::Instancestatic Types & Instance()The single instance of ""Types"" if existing already, or create it (Singleton)Definition Types.cxx:70; TMVA::Types::EMVAEMVADefinition Types.h:76; TMVA::Types::kCategory@ kCategoryDefinition Types.h:97; TMVA::Types::kCuts@ kCutsDefinitio",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:134711,Testability,log,logger,134711,"ringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::GetCorrelationMatrixconst TMatrixD * GetCorrelationMatrix(const TMatrixD *covMat)turns covariance into correlation matrixDefinition Tools.cxx:324; TMVA::Tools::kHtmlLink@ kHtmlLinkDefinition Tools.h:212; TMVA::Tools::UsefulSortAscendingvoid UsefulSortAscending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:538; TMVA::Tools::TMVACitationvoid TMVACitation(MsgLogger &logger, ECitation citType=kPlainText)kinds of TMVA citationDefinition Tools.cxx:1440; TMVA::Tools::TMVAVersionMessagevoid TMVAVersionMessage(MsgLogger &logger)prints the TMVA release number and dateDefinition Tools.cxx:1316; TMVA::Tools::TMVAWelcomeMessagevoid TMVAWelcomeMessage()direct output, eg, when starting ROOT session -> no use of Logger hereDefinition Tools.cxx:1302; TMVA::TransformationHandlerClass that contains all the data information.Definition TransformationHandler.h:56; TMVA::TransformationHandler::PrintVariableRankingvoid PrintVariableRanking() constprints ranking of input variablesDefinition TransformationHandler.cxx:926; TMVA::TypesSingleton class for Global types used by TMVA.Definition Types.h:71; TMVA::Types::Instancestatic Types & Instance()The single instance of ""Types"" if existing already, or create it (Singleton)Definition Types.cxx:70; TMVA::Types::EMVAEMVADefinition Types.h:76; TMVA::Types::kCategory@ kCategoryDefinition Types.h:97; TMVA::Types::kCuts@ kCutsDefinition Types.h:78; TMVA::Types::EAnalysisTypeEAnalysisTypeDefinition Types.h:126; TMVA::Types::kMulticlass@ kMulticlassDefinition Types.h:129; T",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:140832,Testability,log,log,140832,"TString.h:414; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TString::ContainsBool_t Contains(const char *pat, ECaseCompare cmp=kExact) constDefinition TString.h:632; TStyle::SetOptStatvoid SetOptStat(Int_t stat=1)The type of information printed in the histogram statistics box can be selected via the parameter mod...Definition TStyle.cxx:1640; TStyle::SetTitleXOffsetvoid SetTitleXOffset(Float_t offset=1)Definition TStyle.h:409; TSystem::MakeDirectoryvirtual int MakeDirectory(const char *name)Make a directory.Definition TSystem.cxx:827; TTree::WriteInt_t Write(const char *name=nullptr, Int_t option=0, Int_t bufsize=0) overrideWrite this object to the current directory.Definition TTree.cxx:9753; bool; double; int; unsigned int; yDouble_t y[n]Definition legend1.C:17; xDouble_t x[n]Definition legend1.C:17; nconst Int_t nDefinition legend1.C:16; TMVA::DataLoaderCopyvoid DataLoaderCopy(TMVA::DataLoader *des, TMVA::DataLoader *src); TMVA::gConfigConfig & gConfig(); TMVA::gToolsTools & gTools(); TMVA::CreateVariableTransformsvoid CreateVariableTransforms(const TString &trafoDefinition, TMVA::DataSetInfo &dataInfo, TMVA::TransformationHandler &transformationHandler, TMVA::MsgLogger &log)Definition VariableTransform.cxx:59; TMVA::EndlMsgLogger & Endl(MsgLogger &ml)Definition MsgLogger.h:148; TMath::IsNaNBool_t IsNaN(Double_t x)Definition TMath.h:892; graphDefinition graph.py:1; v@ vDefinition rootcling_impl.cxx:3699; mTMarker mDefinition textangle.C:8; Config.h; Factory.h; Types.h; VIBITS#define VIBITSDefinition Factory.cxx:103; sumstatic uint64_t sum(uint64_t i)Definition Factory.cxx:2345; MinNoTrainingEventsconst Int_t MinNoTrainingEventsDefinition Factory.cxx:95; READXML#define READXMLDefinition Factory.cxx:100. tmvatmvasrcFactory.cxx. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:40:40 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:2028,Usability,guid,guides,2028,"@cern.ch> - MPI-K Heidelberg, Germany *; 20 * Kai Voss <Kai.Voss@cern.ch> - U. of Victoria, Canada *; 21 * Omar Zapata <Omar.Zapata@cern.ch> - UdeA/ITM Colombia *; 22 * Lorenzo Moneta <Lorenzo.Moneta@cern.ch> - CERN, Switzerland *; 23 * Sergei Gleyzer <Sergei.Gleyzer@cern.ch> - U of Florida & CERN *; 24 * Kim Albertsson <kim.albertsson@cern.ch> - LTU & CERN *; 25 * *; 26 * Copyright (c) 2005-2015: *; 27 * CERN, Switzerland *; 28 * U. of Victoria, Canada *; 29 * MPI-K Heidelberg, Germany *; 30 * U. of Bonn, Germany *; 31 * UdeA/ITM, Colombia *; 32 * U. of Florida, USA *; 33 * *; 34 * Redistribution and use in source and binary forms, with or without *; 35 * modification, are permitted according to the terms listed in LICENSE *; 36 * (see tmva/doc/LICENSE) *; 37 **********************************************************************************/; 38 ; 39/*! \class TMVA::Factory; 40\ingroup TMVA; 41 ; 42This is the main MVA steering class.; 43It creates all MVA methods, and guides them through the training, testing and; 44evaluation phases.; 45*/; 46 ; 47#include ""TMVA/Factory.h""; 48 ; 49#include ""TMVA/ClassifierFactory.h""; 50#include ""TMVA/Config.h""; 51#include ""TMVA/Configurable.h""; 52#include ""TMVA/Tools.h""; 53#include ""TMVA/Ranking.h""; 54#include ""TMVA/DataSet.h""; 55#include ""TMVA/IMethod.h""; 56#include ""TMVA/MethodBase.h""; 57#include ""TMVA/DataInputHandler.h""; 58#include ""TMVA/DataSetManager.h""; 59#include ""TMVA/DataSetInfo.h""; 60#include ""TMVA/DataLoader.h""; 61#include ""TMVA/MethodBoost.h""; 62#include ""TMVA/MethodCategory.h""; 63#include ""TMVA/ROCCalc.h""; 64#include ""TMVA/ROCCurve.h""; 65#include ""TMVA/MsgLogger.h""; 66 ; 67#include ""TMVA/VariableInfo.h""; 68#include ""TMVA/VariableTransform.h""; 69 ; 70#include ""TMVA/Results.h""; 71#include ""TMVA/ResultsClassification.h""; 72#include ""TMVA/ResultsRegression.h""; 73#include ""TMVA/ResultsMulticlass.h""; 74#include <list>; 75#include <bitset>; 76#include <set>; 77 ; 78#include ""TMVA/Types.h""; 79 ; 80#include ""TROOT.h""; 81#incl",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:4950,Usability,progress bar,progress bar,4950,"TargetFile : output ROOT file; the test tree and all evaluation plots; 110/// will be stored here; 111/// - theOption : option string; currently: ""V"" for verbose; 112 ; 113TMVA::Factory::Factory(TString jobName, TFile *theTargetFile, TString theOption); 114 : Configurable(theOption), fTransformations(""I""), fVerbose(kFALSE), fVerboseLevel(kINFO), fCorrelations(kFALSE),; 115 fROC(kTRUE), fSilentFile(theTargetFile == nullptr), fJobName(jobName), fAnalysisType(Types::kClassification),; 116 fModelPersistence(kTRUE); 117{; 118 fName = ""Factory"";; 119 fgTargetFile = theTargetFile;; 120 fLogger->SetSource(fName.Data());; 121 ; 122 // render silent; 123 if (gTools().CheckForSilentOption(GetOptions())); 124 Log().InhibitOutput(); // make sure is silent if wanted to; 125 ; 126 // init configurable; 127 SetConfigDescription(""Configuration options for Factory running"");; 128 SetConfigName(GetName());; 129 ; 130 // histograms are not automatically associated with the current; 131 // directory and hence don't go out of scope when closing the file; 132 // TH1::AddDirectory(kFALSE);; 133 Bool_t silent = kFALSE;; 134#ifdef WIN32; 135 // under Windows, switch progress bar and color off by default, as the typical windows shell doesn't handle these; 136 // (would need different sequences..); 137 Bool_t color = kFALSE;; 138 Bool_t drawProgressBar = kFALSE;; 139#else; 140 Bool_t color = !gROOT->IsBatch();; 141 Bool_t drawProgressBar = kTRUE;; 142#endif; 143 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 144 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 145 AddPreDefVal(TString(""Debug""));; 146 AddPreDefVal(TString(""Verbose""));; 147 AddPreDefVal(TString(""Info""));; 148 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 149 DeclareOptionRef(; 150 fTransformations, ""Transformations"",; 151 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:6325,Usability,progress bar,progress bar,6325,"fferent sequences..); 137 Bool_t color = kFALSE;; 138 Bool_t drawProgressBar = kFALSE;; 139#else; 140 Bool_t color = !gROOT->IsBatch();; 141 Bool_t drawProgressBar = kTRUE;; 142#endif; 143 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 144 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 145 AddPreDefVal(TString(""Debug""));; 146 AddPreDefVal(TString(""Verbose""));; 147 AddPreDefVal(TString(""Info""));; 148 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 149 DeclareOptionRef(; 150 fTransformations, ""Transformations"",; 151 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", for identity, ""; 152 ""decorrelation, PCA, Uniform and Gaussianisation followed by decorrelation transformations"");; 153 DeclareOptionRef(fCorrelations, ""Correlations"", ""boolean to show correlation in output"");; 154 DeclareOptionRef(fROC, ""ROC"", ""boolean to show ROC in output"");; 155 DeclareOptionRef(silent, ""Silent"",; 156 ""Batch mode: boolean silent flag inhibiting any output from TMVA after the creation of the factory ""; 157 ""class object (default: False)"");; 158 DeclareOptionRef(drawProgressBar, ""DrawProgressBar"",; 159 ""Draw progress bar to display training, testing and evaluation schedule (default: True)"");; 160 DeclareOptionRef(fModelPersistence, ""ModelPersistence"",; 161 ""Option to save the trained model in xml file or using serialization"");; 162 ; 163 TString analysisType(""Auto"");; 164 DeclareOptionRef(analysisType, ""AnalysisType"",; 165 ""Set the analysis type (Classification, Regression, Multiclass, Auto) (default: Auto)"");; 166 AddPreDefVal(TString(""Classification""));; 167 AddPreDefVal(TString(""Regression""));; 168 AddPreDefVal(TString(""Multiclass""));; 169 AddPreDefVal(TString(""Auto""));; 170 ; 171 ParseOptions();; 172 CheckForUnusedOptions();; 173 ; 174 if (Verbose()); 175 fLogger->SetMinType(kVERBOSE);; 176 if (fVerboseLevel.CompareTo(""Debug"") ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:8860,Usability,progress bar,progress bar,8860,"ticlass;; 195 else if (analysisType == ""auto""); 196 fAnalysisType = Types::kNoAnalysisType;; 197 ; 198 // Greetings();; 199}; 200 ; 201////////////////////////////////////////////////////////////////////////////////; 202/// Constructor.; 203 ; 204TMVA::Factory::Factory(TString jobName, TString theOption); 205 : Configurable(theOption), fTransformations(""I""), fVerbose(kFALSE), fCorrelations(kFALSE), fROC(kTRUE),; 206 fSilentFile(kTRUE), fJobName(jobName), fAnalysisType(Types::kClassification), fModelPersistence(kTRUE); 207{; 208 fName = ""Factory"";; 209 fgTargetFile = nullptr;; 210 fLogger->SetSource(fName.Data());; 211 ; 212 // render silent; 213 if (gTools().CheckForSilentOption(GetOptions())); 214 Log().InhibitOutput(); // make sure is silent if wanted to; 215 ; 216 // init configurable; 217 SetConfigDescription(""Configuration options for Factory running"");; 218 SetConfigName(GetName());; 219 ; 220 // histograms are not automatically associated with the current; 221 // directory and hence don't go out of scope when closing the file; 222 TH1::AddDirectory(kFALSE);; 223 Bool_t silent = kFALSE;; 224#ifdef WIN32; 225 // under Windows, switch progress bar and color off by default, as the typical windows shell doesn't handle these; 226 // (would need different sequences..); 227 Bool_t color = kFALSE;; 228 Bool_t drawProgressBar = kFALSE;; 229#else; 230 Bool_t color = !gROOT->IsBatch();; 231 Bool_t drawProgressBar = kTRUE;; 232#endif; 233 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 234 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 235 AddPreDefVal(TString(""Debug""));; 236 AddPreDefVal(TString(""Verbose""));; 237 AddPreDefVal(TString(""Info""));; 238 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 239 DeclareOptionRef(; 240 fTransformations, ""Transformations"",; 241 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", f",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:10235,Usability,progress bar,progress bar,10235,"fferent sequences..); 227 Bool_t color = kFALSE;; 228 Bool_t drawProgressBar = kFALSE;; 229#else; 230 Bool_t color = !gROOT->IsBatch();; 231 Bool_t drawProgressBar = kTRUE;; 232#endif; 233 DeclareOptionRef(fVerbose, ""V"", ""Verbose flag"");; 234 DeclareOptionRef(fVerboseLevel = TString(""Info""), ""VerboseLevel"", ""VerboseLevel (Debug/Verbose/Info)"");; 235 AddPreDefVal(TString(""Debug""));; 236 AddPreDefVal(TString(""Verbose""));; 237 AddPreDefVal(TString(""Info""));; 238 DeclareOptionRef(color, ""Color"", ""Flag for coloured screen output (default: True, if in batch mode: False)"");; 239 DeclareOptionRef(; 240 fTransformations, ""Transformations"",; 241 ""List of transformations to test; formatting example: \""Transformations=I;D;P;U;G,D\"", for identity, ""; 242 ""decorrelation, PCA, Uniform and Gaussianisation followed by decorrelation transformations"");; 243 DeclareOptionRef(fCorrelations, ""Correlations"", ""boolean to show correlation in output"");; 244 DeclareOptionRef(fROC, ""ROC"", ""boolean to show ROC in output"");; 245 DeclareOptionRef(silent, ""Silent"",; 246 ""Batch mode: boolean silent flag inhibiting any output from TMVA after the creation of the factory ""; 247 ""class object (default: False)"");; 248 DeclareOptionRef(drawProgressBar, ""DrawProgressBar"",; 249 ""Draw progress bar to display training, testing and evaluation schedule (default: True)"");; 250 DeclareOptionRef(fModelPersistence, ""ModelPersistence"",; 251 ""Option to save the trained model in xml file or using serialization"");; 252 ; 253 TString analysisType(""Auto"");; 254 DeclareOptionRef(analysisType, ""AnalysisType"",; 255 ""Set the analysis type (Classification, Regression, Multiclass, Auto) (default: Auto)"");; 256 AddPreDefVal(TString(""Classification""));; 257 AddPreDefVal(TString(""Regression""));; 258 AddPreDefVal(TString(""Multiclass""));; 259 AddPreDefVal(TString(""Auto""));; 260 ; 261 ParseOptions();; 262 CheckForUnusedOptions();; 263 ; 264 if (Verbose()); 265 fLogger->SetMinType(kVERBOSE);; 266 if (fVerboseLevel.CompareTo(""Debug"") ",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:13336,Usability,clear,clear,13336,"se *>::iterator trfIt = fDefaultTrfs.begin();; 309 for (; trfIt != fDefaultTrfs.end(); ++trfIt); 310 delete (*trfIt);; 311 ; 312 this->DeleteAllMethods();; 313 ; 314 // problem with call of REGISTER_METHOD macro ...; 315 // ClassifierFactory::DestroyInstance();; 316 // Types::DestroyInstance();; 317 // Tools::DestroyInstance();; 318 // Config::DestroyInstance();; 319}; 320 ; 321////////////////////////////////////////////////////////////////////////////////; 322/// Delete methods.; 323 ; 324void TMVA::Factory::DeleteAllMethods(void); 325{; 326 std::map<TString, MVector *>::iterator itrMap;; 327 ; 328 for (itrMap = fMethodsMap.begin(); itrMap != fMethodsMap.end(); ++itrMap) {; 329 MVector *methods = itrMap->second;; 330 // delete methods; 331 MVector::iterator itrMethod = methods->begin();; 332 for (; itrMethod != methods->end(); ++itrMethod) {; 333 Log() << kDEBUG << ""Delete method: "" << (*itrMethod)->GetName() << Endl;; 334 delete (*itrMethod);; 335 }; 336 methods->clear();; 337 delete methods;; 338 }; 339}; 340 ; 341////////////////////////////////////////////////////////////////////////////////; 342 ; 343void TMVA::Factory::SetVerbose(Bool_t v); 344{; 345 fVerbose = v;; 346}; 347 ; 348////////////////////////////////////////////////////////////////////////////////; 349/// Book a classifier or regression method.; 350 ; 351TMVA::MethodBase *; 352TMVA::Factory::BookMethod(TMVA::DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption); 353{; 354 if (fModelPersistence); 355 gSystem->MakeDirectory(loader->GetName()); // creating directory for DataLoader output; 356 ; 357 TString datasetname = loader->GetName();; 358 ; 359 if (fAnalysisType == Types::kNoAnalysisType) {; 360 if (loader->GetDataSetInfo().GetNClasses() == 2 && loader->GetDataSetInfo().GetClassInfo(""Signal"") != NULL &&; 361 loader->GetDataSetInfo().GetClassInfo(""Background"") != NULL) {; 362 fAnalysisType = Types::kClassification; // default is classification; 363 } else if (loader-",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:97340,Usability,clear,clear,97340,"96 // Booking Seed; 2297 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2298 ; 2299 // Train/Test/Evaluation; 2300 TrainAllMethods();; 2301 TestAllMethods();; 2302 EvaluateAllMethods();; 2303 ; 2304 // getting ROC; 2305 ROC[x] = GetROCIntegral(xbitset.to_string(), methodTitle);; 2306 ; 2307 // cleaning information to process sub-seeds; 2308 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2309 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2310 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2311 delete sresults;; 2312 delete seedloader;; 2313 this->DeleteAllMethods();; 2314 ; 2315 fMethodsMap.clear();; 2316 // removing global result because it is requiring a lot of RAM for all seeds; 2317 }; 2318 ; 2319 for (x = 0; x < range; x++) {; 2320 SROC = ROC[x];; 2321 for (uint32_t i = 0; i < VIBITS; ++i) {; 2322 if (x & (uint64_t(1) << i)) {; 2323 y = x & ~(1 << i);; 2324 std::bitset<VIBITS> ybitset(y);; 2325 // need at least one variable; 2326 // NOTE: if sub-seed is zero then is the special case; 2327 // that count in xbitset is 1; 2328 uint32_t ny = static_cast<uint32_t>( log(x - y) / 0.693147 ) ;; 2329 if (y == 0) {; 2330 importances[ny] = SROC - 0.5;; 2331 continue;; 2332 }; 2333 ; 2334 // getting ROC; 2335 SSROC = ROC[y];; 2336 importances[ny] += SROC - SSROC;; 2337 // cleaning information; 2338 }; 2339 }; 2340 }; 2341 std::cout << ""--- Variable Importance Results (All)"" << std::endl;; 2342 return GetImportance(nbits, importances, varNames);; 2343}; 2344 ; 2345static uint64_t sum(uint64_t i); 2346{; 2347 // add a limit for overflows; 2348 if (i > 62) return 0;; 2349 return static_cast<uint64_t>( std::pow(2, i + 1)) - 1;; 2350 // uint64_t _sum = 0;; 2351 // for (uint64_t n = 0; n < i; n++); 2352 // _sum += pow(2, n);; 2353 // return _sum;; 2354}; 2355 ; 2356/////////////////////////////////////////////////////////////////",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:100782,Usability,clear,clear,100782,"erCopy(seedloader, loader);; 2398 ; 2399 // Booking Seed; 2400 BookMethod(seedloader, theMethod, methodTitle, theOption);; 2401 ; 2402 // Train/Test/Evaluation; 2403 TrainAllMethods();; 2404 TestAllMethods();; 2405 EvaluateAllMethods();; 2406 ; 2407 // getting ROC; 2408 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2409 ; 2410 // cleaning information to process sub-seeds; 2411 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2412 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2413 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2414 delete sresults;; 2415 delete seedloader;; 2416 this->DeleteAllMethods();; 2417 fMethodsMap.clear();; 2418 ; 2419 // removing global result because it is requiring a lot of RAM for all seeds; 2420 ; 2421 for (uint32_t i = 0; i < VIBITS; ++i) {; 2422 if (x & (1 << i)) {; 2423 y = x & ~(uint64_t(1) << i);; 2424 std::bitset<VIBITS> ybitset(y);; 2425 // need at least one variable; 2426 // NOTE: if sub-seed is zero then is the special case; 2427 // that count in xbitset is 1; 2428 uint32_t ny = static_cast<uint32_t>(log(x - y) / 0.693147);; 2429 if (y == 0) {; 2430 importances[ny] = SROC - 0.5;; 2431 continue;; 2432 }; 2433 ; 2434 // creating loader for sub-seed; 2435 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2436 // adding variables from sub-seed; 2437 for (int index = 0; index < nbits; index++) {; 2438 if (ybitset[index]); 2439 subseedloader->AddVariable(varNames[index], 'F');; 2440 }; 2441 ; 2442 // Loading Dataset; 2443 DataLoaderCopy(subseedloader, loader);; 2444 ; 2445 // Booking SubSeed; 2446 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2447 ; 2448 // Train/Test/Evaluation; 2449 TrainAllMethods();; 2450 TestAllMethods();; 2451 EvaluateAllMethods();; 2452 ; 2453 // getting ROC; 2454 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:102483,Usability,clear,clear,102483,"eed; 2446 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2447 ; 2448 // Train/Test/Evaluation; 2449 TrainAllMethods();; 2450 TestAllMethods();; 2451 EvaluateAllMethods();; 2452 ; 2453 // getting ROC; 2454 SSROC = GetROCIntegral(ybitset.to_string(), methodTitle);; 2455 importances[ny] += SROC - SSROC;; 2456 ; 2457 // cleaning information; 2458 TMVA::MethodBase *ssmethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2459 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2460 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2461 delete ssresults;; 2462 delete subseedloader;; 2463 this->DeleteAllMethods();; 2464 fMethodsMap.clear();; 2465 }; 2466 }; 2467 std::cout << ""--- Variable Importance Results (Short)"" << std::endl;; 2468 return GetImportance(nbits, importances, varNames);; 2469}; 2470 ; 2471////////////////////////////////////////////////////////////////////////////////; 2472 ; 2473TH1F *TMVA::Factory::EvaluateImportanceRandom(DataLoader *loader, UInt_t nseeds, Types::EMVA theMethod,; 2474 TString methodTitle, const char *theOption); 2475{; 2476 TRandom3 *rangen = new TRandom3(0); // Random Gen.; 2477 ; 2478 uint64_t x = 0;; 2479 uint64_t y = 0;; 2480 ; 2481 // getting number of variables and variable names from loader; 2482 const int nbits = loader->GetDataSetInfo().GetNVariables();; 2483 std::vector<TString> varNames = loader->GetDataSetInfo().GetListOfVariables();; 2484 ; 2485 long int range = pow(2, nbits);; 2486 ; 2487 // vector to save importances; 2488 std::vector<Double_t> importances(nbits);; 2489 for (int i = 0; i < nbits; i++); 2490 importances[i] = 0;; 2491 ; 2492 Double_t SROC, SSROC; // computed ROC value; 2493 for (UInt_t n = 0; n < nseeds; n++) {; 2494 x = rangen->Integer(range);; 2495 ; 2496 std::bitset<32> xbitset(x);; 2497 if (x == 0); 2498 continue; // data loader need at least one variable; 2499 ; 2500 // creating lo",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:104940,Usability,clear,clear,104940," methodTitle, theOption);; 2514 ; 2515 // Train/Test/Evaluation; 2516 TrainAllMethods();; 2517 TestAllMethods();; 2518 EvaluateAllMethods();; 2519 ; 2520 // getting ROC; 2521 SROC = GetROCIntegral(xbitset.to_string(), methodTitle);; 2522 // std::cout << ""Seed: n "" << n << "" x "" << x << "" xbitset:"" << xbitset << "" ROC "" << SROC << std::endl;; 2523 ; 2524 // cleaning information to process sub-seeds; 2525 TMVA::MethodBase *smethod = dynamic_cast<TMVA::MethodBase *>(fMethodsMap[xbitset.to_string().c_str()][0][0]);; 2526 TMVA::ResultsClassification *sresults = (TMVA::ResultsClassification *)smethod->Data()->GetResults(; 2527 smethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2528 delete sresults;; 2529 delete seedloader;; 2530 this->DeleteAllMethods();; 2531 fMethodsMap.clear();; 2532 ; 2533 // removing global result because it is requiring a lot of RAM for all seeds; 2534 ; 2535 for (uint32_t i = 0; i < 32; ++i) {; 2536 if (x & (uint64_t(1) << i)) {; 2537 y = x & ~(1 << i);; 2538 std::bitset<32> ybitset(y);; 2539 // need at least one variable; 2540 // NOTE: if sub-seed is zero then is the special case; 2541 // that count in xbitset is 1; 2542 Double_t ny = log(x - y) / 0.693147;; 2543 if (y == 0) {; 2544 importances[ny] = SROC - 0.5;; 2545 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << ""ROC "" << 0.5 << std::endl;; 2546 continue;; 2547 }; 2548 ; 2549 // creating loader for sub-seed; 2550 TMVA::DataLoader *subseedloader = new TMVA::DataLoader(ybitset.to_string());; 2551 // adding variables from sub-seed; 2552 for (int index = 0; index < nbits; index++) {; 2553 if (ybitset[index]); 2554 subseedloader->AddVariable(varNames[index], 'F');; 2555 }; 2556 ; 2557 // Loading Dataset; 2558 DataLoaderCopy(subseedloader, loader);; 2559 ; 2560 // Booking SubSeed; 2561 BookMethod(subseedloader, theMethod, methodTitle, theOption);; 2562 ; 2563 // Train/Test/Evaluation; 2564 TrainAllMethods();; 2565 TestAllMethods();; 2566 EvaluateAllMethods();; 2567 ; 2568 //",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:106931,Usability,clear,clear,106931,"ng(), methodTitle);; 2570 importances[ny] += SROC - SSROC;; 2571 // std::cout << ""SubSeed: "" << y << "" y:"" << ybitset << "" x-y "" << x - y << "" "" << std::bitset<32>(x - y) <<; 2572 // "" ny "" << ny << "" SROC "" << SROC << "" SSROC "" << SSROC << "" Importance = "" << importances[ny] <<; 2573 // std::endl; cleaning information; 2574 TMVA::MethodBase *ssmethod =; 2575 dynamic_cast<TMVA::MethodBase *>(fMethodsMap[ybitset.to_string().c_str()][0][0]);; 2576 TMVA::ResultsClassification *ssresults = (TMVA::ResultsClassification *)ssmethod->Data()->GetResults(; 2577 ssmethod->GetMethodName(), Types::kTesting, Types::kClassification);; 2578 delete ssresults;; 2579 delete subseedloader;; 2580 this->DeleteAllMethods();; 2581 fMethodsMap.clear();; 2582 }; 2583 }; 2584 }; 2585 std::cout << ""--- Variable Importance Results (Random)"" << std::endl;; 2586 return GetImportance(nbits, importances, varNames);; 2587}; 2588 ; 2589////////////////////////////////////////////////////////////////////////////////; 2590 ; 2591TH1F *TMVA::Factory::GetImportance(const int nbits, std::vector<Double_t> importances, std::vector<TString> varNames); 2592{; 2593 TH1F *vih1 = new TH1F(""vih1"", """", nbits, 0, nbits);; 2594 ; 2595 gStyle->SetOptStat(000000);; 2596 ; 2597 Float_t normalization = 0.0;; 2598 for (int i = 0; i < nbits; i++) {; 2599 normalization = normalization + importances[i];; 2600 }; 2601 ; 2602 Float_t roc = 0.0;; 2603 ; 2604 gStyle->SetTitleXOffset(0.4);; 2605 gStyle->SetTitleXOffset(1.2);; 2606 ; 2607 std::vector<Double_t> x_ie(nbits), y_ie(nbits);; 2608 for (Int_t i = 1; i < nbits + 1; i++) {; 2609 x_ie[i - 1] = (i - 1) * 1.;; 2610 roc = 100.0 * importances[i - 1] / normalization;; 2611 y_ie[i - 1] = roc;; 2612 std::cout << ""--- "" << varNames[i - 1] << "" = "" << roc << "" %"" << std::endl;; 2613 vih1->GetXaxis()->SetBinLabel(i, varNames[i - 1].Data());; 2614 vih1->SetBinContent(i, roc);; 2615 }; 2616 TGraph *g_ie = new TGraph(nbits + 2, &x_ie[0], &y_ie[0]);; 2617 g_ie->SetTitle("""");; 2618 ; 261",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html:133241,Usability,simpl,simple,133241,"etROCIntegral(const UInt_t points=41)Calculates the ROC integral (AUC)Definition ROCCurve.cxx:248; TMVA::ROCCurve::GetROCCurveTGraph * GetROCCurve(const UInt_t points=100)Returns a new TGraph containing the ROC curve.Definition ROCCurve.cxx:274; TMVA::RankingRanking for variables in method (implementation)Definition Ranking.h:48; TMVA::Ranking::Printvirtual void Print() constget maximum length of variable namesDefinition Ranking.cxx:111; TMVA::ResultsClassificationClass that is the base-class for a vector of result.Definition ResultsClassification.h:48; TMVA::ResultsMulticlassClass which takes the results of a multiclass classification.Definition ResultsMulticlass.h:55; TMVA::ResultsClass that is the base-class for a vector of result.Definition Results.h:57; TMVA::Tools::FormattedOutputvoid FormattedOutput(const std::vector< Double_t > &, const std::vector< TString > &, const TString titleVars, const TString titleValues, MsgLogger &logger, TString format=""%+1.3f"")formatted output of simple tableDefinition Tools.cxx:887; TMVA::Tools::ROOTVersionMessagevoid ROOTVersionMessage(MsgLogger &logger)prints the ROOT release number and dateDefinition Tools.cxx:1325; TMVA::Tools::UsefulSortDescendingvoid UsefulSortDescending(std::vector< std::vector< Double_t > > &, std::vector< TString > *vs=nullptr)sort 2D vector (AND in parallel a TString vector) in such a way that the ""first vector is sorted"" and...Definition Tools.cxx:564; TMVA::Tools::SplitStringstd::vector< TString > SplitString(const TString &theOpt, const char separator) constsplits the option string at 'separator' and fills the list 'splitV' with the primitive stringsDefinition Tools.cxx:1199; TMVA::Tools::Colorconst TString & Color(const TString &)human readable color stringsDefinition Tools.cxx:828; TMVA::Tools::GetCorrelationMatrixconst TMatrixD * GetCorrelationMatrix(const TMatrixD *covMat)turns covariance into correlation matrixDefinition Tools.cxx:324; TMVA::Tools::kHtmlLink@ kHtmlLinkDefinition Tools.h:212; TMV",MatchSource.WIKI,doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/tmva_2tmva_2src_2Factory_8cxx_source.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:3328,Availability,error,error,3328,",DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : Layout: ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : InputLayout: ""0|0|0"" [The Layout of the input]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for v",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:6481,Availability,error,error,6481,"ngRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|16|16"" [The Layout of the input]; : Layout: ""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth o",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11305,Availability,error,error,11305," 0 DENSE Layer: ( Input = 256 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 1 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 2 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 3 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 4 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11575,Availability,error,error,11575," Layer 3 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 4 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11685,Availability,error,error,11685," ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/wei",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11795,Availability,error,error,11795,": Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11904,Availability,error,error,11904,"00 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: T",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:12013,Availability,error,error,12013,"Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_CNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14131,Availability,error,error,14131," size = 100 Loss function = C; Layer 0 CONV LAYER: ( W = 16 , H = 16 , D = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 1 BATCH NORM Layer: Input/Output = ( 10 , 256 , 100 ) Norm dim = 10 axis = 1; ; Layer 2 CONV LAYER: ( W = 16 , H = 16 , D = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 3 POOL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14401,Availability,error,error,14401," = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 3 POOL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : E",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14508,Availability,error,error,14508,"OL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluati",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14616,Availability,error,error,14616,"ESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14776,Availability,error,error,14776,"nction = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14884,Availability,error,error,14884," = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: P",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14991,Availability,error,error,14991," data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:15150,Availability,error,error,15150,---------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the t,MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:15259,Availability,error,error,15259,"nts/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:47551,Availability,avail,available,47551,"et();; // generate random means in range [3,7] to be not too much on the border; f1.SetParameter(1, gRandom->Uniform(3, 7));; f1.SetParameter(3, gRandom->Uniform(3, 7));; f2.SetParameter(1, gRandom->Uniform(3, 7));; f2.SetParameter(3, gRandom->Uniform(3, 7));; ; h1.FillRandom(""f1"", nRndmEvts);; h2.FillRandom(""f2"", nRndmEvts);; ; for (int k = 0; k < nh; ++k) {; for (int l = 0; l < nw; ++l) {; int m = k * nw + l;; // add some noise in each bin; x1[m] = h1.GetBinContent(k + 1, l + 1) + gRandom->Gaus(0, pixelNoise);; x2[m] = h2.GetBinContent(k + 1, l + 1) + gRandom->Gaus(0, pixelNoise);; }; }; sgn.Fill();; bkg.Fill();; }; sgn.Write();; bkg.Write();; ; Info(""MakeImagesTree"", ""Signal and background tree with images data written to the file %s"", f.GetName());; sgn.Print();; bkg.Print();; f.Close();; }; ; /// @brief Run the TMVA CNN Classification example; /// @param nevts : number of signal/background events. Use by default a low value (1000); /// but increase to at least 5000 to get a good result; /// @param opt : vector of bool with method used (default all on if available). The order is:; /// - TMVA CNN; /// - Keras CNN; /// - TMVA DNN; /// - TMVA BDT; /// - PyTorch CNN; void TMVA_CNN_Classification(int nevts = 1000, std::vector<bool> opt = {1, 1, 1, 1, 1}); {; ; int imgSize = 16 * 16;; TString inputFileName = ""images_data_16x16.root"";; ; bool fileExist = !gSystem->AccessPathName(inputFileName);; ; // if file does not exists create it; if (!fileExist) {; MakeImagesTree(nevts, 16, 16);; }; ; bool useTMVACNN = (opt.size() > 0) ? opt[0] : false;; bool useKerasCNN = (opt.size() > 1) ? opt[1] : false;; bool useTMVADNN = (opt.size() > 2) ? opt[2] : false;; bool useTMVABDT = (opt.size() > 3) ? opt[3] : false;; bool usePyTorchCNN = (opt.size() > 4) ? opt[4] : false;; #ifndef R__HAS_TMVACPU; #ifndef R__HAS_TMVAGPU; Warning(""TMVA_CNN_Classification"",; ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for CNN"");; useTMVACNN = false;; #endif; #en",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:55203,Availability,avail,available,55203,",DENSE|100|RELU,DENSE|1|LINEAR"");; ; // Training strategies; // one can catenate several training strings with different parameters (e.g. learning rates or regularizations; // parameters) The training string must be concatenates with the `|` delimiter; TString trainingString1(""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""MaxEpochs=10,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."");; ; TString trainingStrategyString(""TrainingStrategy="");; trainingStrategyString += trainingString1; // + ""|"" + trainingString2 + ....; ; // Build now the full DNN Option string; ; TString dnnOptions(""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append("":"");; dnnOptions.Append(layoutString);; dnnOptions.Append("":"");; dnnOptions.Append(trainingStrategyString);; ; TString dnnMethodName = ""TMVA_DNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""TMVA_DNN_GPU"";; #elif defined(R__HAS_TMVACPU); dnnOptions += "":Architecture=CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; /***; ### Book Convolutional Neural Network in TMVA; ; For building a CNN one needs to define; ; - Input Layout : number of channels (in this case = 1) | image height | image width; - Batch Layout : batch size | number of channels | image size = (height*width); ; Then one add Convolutional layers and MaxPool layers.; ; - For Convolutional layer the option string has to be:; - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; width | activation function; ; - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; conv layer equal to the input; ; - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; conver",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:57609,Availability,avail,available,57609,"LU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,""; ""RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"");; ; // Training strategies.; TString trainingString1(""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""MaxEpochs=10,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0"");; ; TString trainingStrategyString(""TrainingStrategy="");; trainingStrategyString +=; trainingString1; // + ""|"" + trainingString2 + ""|"" + trainingString3; for concatenating more training strings; ; // Build full CNN Options.; TString cnnOptions(""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:""; ""WeightInitialization=XAVIER"");; ; cnnOptions.Append("":"");; cnnOptions.Append(inputLayoutString);; cnnOptions.Append("":"");; cnnOptions.Append(layoutString);; cnnOptions.Append("":"");; cnnOptions.Append(trainingStrategyString);; ; //// New DL (CNN); TString cnnMethodName = ""TMVA_CNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; cnnOptions += "":Architecture=GPU"";; cnnMethodName = ""TMVA_CNN_GPU"";; #else; cnnOptions += "":Architecture=CPU"";; cnnMethodName = ""TMVA_CNN_CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, cnnMethodName, cnnOptions);; }; ; /**; ### Book Convolutional Neural Network in Keras using a generated model; ; **/; ; #ifdef R__HAS_PYMVA; // The next section uses Python packages, execute it only if PyMVA is available; TString tmva_python_exe{TMVA::Python_Executable()};; TString python_exe = tmva_python_exe.IsNull() ? ""python"" : tmva_python_exe;; ; if (useKerasCNN) {; ; Info(""TMVA_CNN_Classification"", ""Building convolutional keras model"");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(; ""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, R",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:58055,Availability,avail,available,58055,"opConfig=0.0+0.0+0.0+0.0"");; ; TString trainingStrategyString(""TrainingStrategy="");; trainingStrategyString +=; trainingString1; // + ""|"" + trainingString2 + ""|"" + trainingString3; for concatenating more training strings; ; // Build full CNN Options.; TString cnnOptions(""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:""; ""WeightInitialization=XAVIER"");; ; cnnOptions.Append("":"");; cnnOptions.Append(inputLayoutString);; cnnOptions.Append("":"");; cnnOptions.Append(layoutString);; cnnOptions.Append("":"");; cnnOptions.Append(trainingStrategyString);; ; //// New DL (CNN); TString cnnMethodName = ""TMVA_CNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; cnnOptions += "":Architecture=GPU"";; cnnMethodName = ""TMVA_CNN_GPU"";; #else; cnnOptions += "":Architecture=CPU"";; cnnMethodName = ""TMVA_CNN_CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, cnnMethodName, cnnOptions);; }; ; /**; ### Book Convolutional Neural Network in Keras using a generated model; ; **/; ; #ifdef R__HAS_PYMVA; // The next section uses Python packages, execute it only if PyMVA is available; TString tmva_python_exe{TMVA::Python_Executable()};; TString python_exe = tmva_python_exe.IsNull() ? ""python"" : tmva_python_exe;; ; if (useKerasCNN) {; ; Info(""TMVA_CNN_Classification"", ""Building convolutional keras model"");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(; ""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((16, 16, 1), input_shape = (256, )))"");; m.AddLine(""model.add(Conv2D(10, kernel_size = (3, 3), kernel_initializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; m.AddLine(""model.add(BatchNormaliza",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:61908,Availability,error,error,61908,"rch model could be created; Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model"");; TString methodOpt = ""H:!V:VarTransform=None:FilenameModel=PyTorchModelCNN.pt:""; ""FilenameTrainedModel=PyTorchTrainedModelCNN.pt:NumEpochs=10:BatchSize=100"";; methodOpt += TString("":UserCode="") + pyTorchFileName;; factory.BookMethod(&loader, TMVA::Types::kPyTorch, ""PyTorch"", methodOpt);; }; }; #endif; ; //// ## Train Methods; ; factory.TrainAllMethods();; ; /// ## Test and Evaluate Methods; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// ## Plot ROC Curve; ; auto c1 = factory.GetROCCurve(&loader);; c1->Draw();; ; // close outputfile to save output file; outputFile->Close();; }; f#define f(i)Definition RSha256.hxx:104; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; gROOT#define gROOTDefinition TROOT.h:406; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TF1::SetParametersvirtual void SetParameters(const Double_t *params)Definition TF1.h:677; TF1::SetParametervirtual void SetParameter(Int_t param, Double_t value)Definition TF1.h:667; TF2A 2-Dim function with parameters.Definition TF2.h:29; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(c",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:8781,Deployability,configurat,configuration,8781,"hape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); Non-trainable params: 20 (80.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: model_cnn.h5; Factory : Booking method: ␛[1mPyTorch␛[0m; : ; : Using PyTorch - setting special configuration options ; : Using PyTorch version 2; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchModelCNN.pt; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 800 bkg: 800; : #events: (unweighted) sig: 800 bkg: 800; : Training 200 Decision Trees ... patience please; : Elapsed time for training with 1600 events: 0.877 sec ; BDT : [dataset] : Evaluation of BDT on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0172 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11598,Deployability,configurat,configuration,11598," Layer 3 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 4 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11708,Deployability,configurat,configuration,11708," ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/wei",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11818,Deployability,configurat,configuration,11818,": Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11927,Deployability,configurat,configuration,11927,"00 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: T",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:12036,Deployability,configurat,configuration,12036,"Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_CNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14424,Deployability,configurat,configuration,14424," = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 3 POOL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : E",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14531,Deployability,configurat,configuration,14531,"OL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluati",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14639,Deployability,configurat,configuration,14639,"ESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14799,Deployability,configurat,configuration,14799,"nction = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14907,Deployability,configurat,configuration,14907," = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: P",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:15014,Deployability,configurat,configuration,15014," data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:15173,Deployability,configurat,configuration,15173,---------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the t,MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:15282,Deployability,configurat,configuration,15282,"nts/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:49791,Deployability,configurat,configuration,49791," to avoid conflict with tbb; gSystem->Setenv(""OMP_NUM_THREADS"", ""1"");; ; // do enable MT running; if (num_threads >= 0) {; ROOT::EnableImplicitMT(num_threads);; }; #endif; ; TMVA::Tools::Instance();; ; ; std::cout << ""Running with nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKerasCNN = false;; usePyTorchCNN = false;; #endif; ; TFile *outputFile = nullptr;; if (writeOutputFile); outputFile = TFile::Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:60781,Deployability,install,installed,60781,"VA_CNN_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_CNN_Classification"", ""Booking tf.Keras CNN model"");; factory.BookMethod(; &loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=model_cnn.h5:tf.keras:""; ""FilenameTrainedModel=trained_model_cnn.h5:NumEpochs=10:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; if (usePyTorchCNN) {; ; Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model"");; TString pyTorchFileName = gROOT->GetTutorialDir() + TString(""/tmva/PyTorch_Generate_CNN_Model.py"");; // check that pytorch can be imported and file defining the model and used later when booking the method is; // existing; if (gSystem->Exec(python_exe + "" -c 'import torch'"") || gSystem->AccessPathName(pyTorchFileName)) {; Warning(""TMVA_CNN_Classification"", ""PyTorch is not installed or model building file is not existing - skip using PyTorch"");; } else {; // book PyTorch method only if PyTorch model could be created; Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model"");; TString methodOpt = ""H:!V:VarTransform=None:FilenameModel=PyTorchModelCNN.pt:""; ""FilenameTrainedModel=PyTorchTrainedModelCNN.pt:NumEpochs=10:BatchSize=100"";; methodOpt += TString("":UserCode="") + pyTorchFileName;; factory.BookMethod(&loader, TMVA::Types::kPyTorch, ""PyTorch"", methodOpt);; }; }; #endif; ; //// ## Train Methods; ; factory.TrainAllMethods();; ; /// ## Test and Evaluate Methods; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// ## Plot ROC Curve; ; auto c1 = factory.GetROCCurve(&loader);; c1->Draw();; ; // close outputfile to save output file; outputFile->Close();; }; f#define f(i)Definition RSha256.hxx:104; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational m",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:66267,Deployability,install,installed,66267,"ble_t Uniform(Double_t x1=1)Returns a uniform deviate on the interval (0, x1).Definition TRandom.cxx:682; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::IsNullBool_t IsNull() constDefinition TString.h:414; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TTree::GetEntriesvirtual Long64_t GetEntries() constDefinition TTree.h:463; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_CNN_ClassificationDefinition TMVA_CNN_Classification.py:1; TMVA::Python_ExecutableTString Python_Executable()Function to find current Python executable used by ROOT If ""Python3"" is installed,...Definition PyMethodBase.cxx:43; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; AuthorLorenzo Moneta ; Definition in file TMVA_CNN_Classification.C. tutorialstmvaTMVA_CNN_Classification.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:60277,Energy Efficiency,allocate,allocates,60277,"ation = 'relu')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('model_cnn.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_cnn_model.py"");; // execute; gSystem->Exec(python_exe + "" make_cnn_model.py"");; ; if (gSystem->AccessPathName(""model_cnn.h5"")) {; Warning(""TMVA_CNN_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_CNN_Classification"", ""Booking tf.Keras CNN model"");; factory.BookMethod(; &loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=model_cnn.h5:tf.keras:""; ""FilenameTrainedModel=trained_model_cnn.h5:NumEpochs=10:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; if (usePyTorchCNN) {; ; Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model"");; TString pyTorchFileName = gROOT->GetTutorialDir() + TString(""/tmva/PyTorch_Generate_CNN_Model.py"");; // check that pytorch can be imported and file defining the model and used later when booking the method is; // existing; if (gSystem->Exec(python_exe + "" -c 'import torch'"") || gSystem->AccessPathName(pyTorchFileName)) {; Warning(""TMVA_CNN_Classification"", ""PyTorch is not installed or model building file is not existing - skip using PyTorch"");; } else {; // book PyTorch method only if PyTorch model could be created; Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model"");; TString methodOpt = ""H:!V:VarTransform=None:FilenameModel=PyTorchModelCNN.pt:""; ""FilenameTrainedModel=PyTorchTrainedModelCNN.pt:NumEpochs=10:BatchSize=100"";; methodOpt += TString("":UserCode="") + pyTorchFileName;; factory.BookMethod(&loader, TMVA::Types::kPyTorch, ""PyTorch"", methodOpt);; }; }; #endif; ; //// ## Train Methods; ; factory.Train",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:3126,Integrability,message,message,3126,"ns are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : Layout: ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:6205,Integrability,message,message,6205,"imes the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|16|16"" [The Layout of the input]; : Layout: ""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the t",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16151,Integrability,wrap,wraps,16151,"6 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16343,Integrability,interface,interface,16343,"psed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16482,Integrability,message,message,16482,": 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); N",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:28390,Integrability,wrap,wraps,28390," : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.261 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyTorch for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyTorch ] :␛[0m; : ; : PyTorch is a scientific computing package supporting; : automatic differentiation. This method wraps the training; : and predictions steps of the PyTorch Python package for; : TMVA, so that dataloading, preprocessing and evaluation; : can be done within the TMVA system. To use this PyTorch; : interface, you need to generatea model with PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:28589,Integrability,interface,interface,28589,"Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.261 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyTorch for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyTorch ] :␛[0m; : ; : PyTorch is a scientific computing package supporting; : automatic differentiation. This method wraps the training; : and predictions steps of the PyTorch Python package for; : TMVA, so that dataloading, preprocessing and evaluation; : can be done within the TMVA system. To use this PyTorch; : interface, you need to generatea model with PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; BDT : Ranking result (top variable is best ranked); : ------------------",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:28729,Integrability,message,message,28729,"t/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyTorch for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyTorch ] :␛[0m; : ; : PyTorch is a scientific computing package supporting; : automatic differentiation. This method wraps the training; : and predictions steps of the PyTorch Python package for; : TMVA, so that dataloading, preprocessing and evaluation; : can be done within the TMVA system. To use this PyTorch; : interface, you need to generatea model with PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; BDT : Ranking result (top variable is best ranked); : --------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------; : 1 : vars : 1.153e-02; : 2 : vars : 1.059e-02; : 3 : vars : 1.011e-02; : 4 : vars : 9.992e-03; : 5 : vars : 9.753e-03; : 6 : vars : 9.676e-03; : 7 : vars : 9.360e-03; : 8 : vars : 9.253e-03; : 9 ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:61782,Integrability,message,messages,61782,"Torch is not installed or model building file is not existing - skip using PyTorch"");; } else {; // book PyTorch method only if PyTorch model could be created; Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model"");; TString methodOpt = ""H:!V:VarTransform=None:FilenameModel=PyTorchModelCNN.pt:""; ""FilenameTrainedModel=PyTorchTrainedModelCNN.pt:NumEpochs=10:BatchSize=100"";; methodOpt += TString("":UserCode="") + pyTorchFileName;; factory.BookMethod(&loader, TMVA::Types::kPyTorch, ""PyTorch"", methodOpt);; }; }; #endif; ; //// ## Train Methods; ; factory.TrainAllMethods();; ; /// ## Test and Evaluate Methods; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// ## Plot ROC Curve; ; auto c1 = factory.GetROCCurve(&loader);; c1->Draw();; ; // close outputfile to save output file; outputFile->Close();; }; f#define f(i)Definition RSha256.hxx:104; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; gROOT#define gROOTDefinition TROOT.h:406; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TF1::SetParametersvirtual void SetParameters(const Double_t *params)Definition TF1.h:677; TF1::SetParametervirtual void SetParameter(Int_t param, Double_t value)Definition TF1.h:667; TF2A 2-Dim function with parameters.Definition TF2.h:29; TFileA ROOT file is an on-disk ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:870,Modifiability,variab,variable,870,"Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_CNN_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Convolutional Neural Network ; This is an example of using a CNN in TMVA. We do classification using a toy image data set that is generated when running the example macro. ; Running with nthreads = 4; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 1000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 1000 events; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Using variable vars[0] from array expression vars of size 256; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; : Using variable vars[0] from array expression vars of size 256; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 800; : Signal -- testing events : 200; : Signal -- training and testing events: 1000; : Background -- training events : 800; : Background -- testing events : 200; : Background -- training and testing events: 1000; : ; Factory : Booking method: ␛[1mTMVA_DNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:1045,Modifiability,variab,variable,1045,"Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_CNN_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Convolutional Neural Network ; This is an example of using a CNN in TMVA. We do classification using a toy image data set that is generated when running the example macro. ; Running with nthreads = 4; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 1000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 1000 events; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Using variable vars[0] from array expression vars of size 256; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; : Using variable vars[0] from array expression vars of size 256; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 800; : Signal -- testing events : 200; : Signal -- training and testing events: 1000; : Background -- training events : 800; : Background -- testing events : 200; : Background -- training and testing events: 1000; : ; Factory : Booking method: ␛[1mTMVA_DNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:2777,Modifiability,variab,variable,2777,"yout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : Layout: ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:5856,Modifiability,variab,variable,5856,"OOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|16|16"" [The Layout of the input]; : Layout: ""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,Tes",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:8781,Modifiability,config,configuration,8781,"hape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); Non-trainable params: 20 (80.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: model_cnn.h5; Factory : Booking method: ␛[1mPyTorch␛[0m; : ; : Using PyTorch - setting special configuration options ; : Using PyTorch version 2; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchModelCNN.pt; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 800 bkg: 800; : #events: (unweighted) sig: 800 bkg: 800; : Training 200 Decision Trees ... patience please; : Elapsed time for training with 1600 events: 0.877 sec ; BDT : [dataset] : Evaluation of BDT on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0172 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11598,Modifiability,config,configuration,11598," Layer 3 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 4 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11708,Modifiability,config,configuration,11708," ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/wei",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11818,Modifiability,config,configuration,11818,": Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11927,Modifiability,config,configuration,11927,"00 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: T",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:12036,Modifiability,config,configuration,12036,"Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.881238 0.852851 0.181896 0.0160504 7235.64 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.658204 0.827727 0.182292 0.0161751 7223.84 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.58547 0.792475 0.182478 0.0158424 7201.32 0; : 4 Minimum Test error found - save the configuration ; : 4 | 0.508772 0.760461 0.18258 0.0158465 7197.11 0; : 5 Minimum Test error found - save the configuration ; : 5 | 0.445197 0.705323 0.183204 0.015935 7174.07 0; : 6 | 0.409018 0.708257 0.18274 0.0151517 7160.42 1; : 7 | 0.369062 0.747523 0.18295 0.0151377 7150.84 2; : 8 | 0.31294 0.740134 0.183339 0.015769 7161.2 3; : 9 | 0.256184 0.756239 0.181088 0.0154708 7245.63 4; : 10 | 0.224257 0.809455 0.18089 0.0153446 7248.78 5; : ; : Elapsed time for training with 1600 events: 1.86 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_CNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14424,Modifiability,config,configuration,14424," = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 3 POOL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : E",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14531,Modifiability,config,configuration,14531,"OL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluati",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14639,Modifiability,config,configuration,14639,"ESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14799,Modifiability,config,configuration,14799,"nction = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:14907,Modifiability,config,configuration,14907," = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: P",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:15014,Modifiability,config,configuration,15014," data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:15173,Modifiability,config,configuration,15173,---------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the t,MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:15282,Modifiability,config,configuration,15282,"nts/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0.721714 1.40039 0.112743 931.931 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.744775 0.695682 1.39558 0.112793 935.465 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.694124 0.686492 1.40809 0.112179 925.986 0; : 4 | 0.679848 0.699399 1.40006 0.109116 929.552 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.670651 0.682024 1.40299 0.112234 929.685 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.652355 0.665731 1.40537 0.11341 928.823 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.647198 0.663594 1.40991 0.11661 927.858 0; : 8 | 0.624112 0.674678 1.40336 0.112656 929.728 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.597489 0.647076 1.41653 0.119736 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:29502,Modifiability,variab,variables,29502,"aloading, preprocessing and evaluation; : can be done within the TMVA system. To use this PyTorch; : interface, you need to generatea model with PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; BDT : Ranking result (top variable is best ranked); : --------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------; : 1 : vars : 1.153e-02; : 2 : vars : 1.059e-02; : 3 : vars : 1.011e-02; : 4 : vars : 9.992e-03; : 5 : vars : 9.753e-03; : 6 : vars : 9.676e-03; : 7 : vars : 9.360e-03; : 8 : vars : 9.253e-03; : 9 : vars : 9.032e-03; : 10 : vars : 8.946e-03; : 11 : vars : 8.936e-03; : 12 : vars : 8.767e-03; : 13 : vars : 8.623e-03; : 14 : vars : 8.396e-03; : 15 : vars : 8.239e-03; : 16 : vars : 8.232e-03; : 17 : vars : 8.138e-03; : 18 : vars : 8.131e-03; : 19 : vars : 7.983e-03; : 20 : vars : 7.979e-03; : 21 : vars : 7.827e-03; : 22 : vars : 7.605e-03; : 23 : vars : 7.531e-03; : 24 : vars : 7.436e-03; : 25 : vars : 7.368e-03; : 26 : vars : 7.314e-03; : 27 : vars : 7.253e-03; : 28 : vars : 7.197e-03; : 29 : vars : 7.186e-03; : 30 : vars : 7.176e-03; : 31 : vars : 7.095e-03; : 32 : vars",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:29560,Modifiability,variab,variable,29560,"ith PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; BDT : Ranking result (top variable is best ranked); : --------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------; : 1 : vars : 1.153e-02; : 2 : vars : 1.059e-02; : 3 : vars : 1.011e-02; : 4 : vars : 9.992e-03; : 5 : vars : 9.753e-03; : 6 : vars : 9.676e-03; : 7 : vars : 9.360e-03; : 8 : vars : 9.253e-03; : 9 : vars : 9.032e-03; : 10 : vars : 8.946e-03; : 11 : vars : 8.936e-03; : 12 : vars : 8.767e-03; : 13 : vars : 8.623e-03; : 14 : vars : 8.396e-03; : 15 : vars : 8.239e-03; : 16 : vars : 8.232e-03; : 17 : vars : 8.138e-03; : 18 : vars : 8.131e-03; : 19 : vars : 7.983e-03; : 20 : vars : 7.979e-03; : 21 : vars : 7.827e-03; : 22 : vars : 7.605e-03; : 23 : vars : 7.531e-03; : 24 : vars : 7.436e-03; : 25 : vars : 7.368e-03; : 26 : vars : 7.314e-03; : 27 : vars : 7.253e-03; : 28 : vars : 7.197e-03; : 29 : vars : 7.186e-03; : 30 : vars : 7.176e-03; : 31 : vars : 7.095e-03; : 32 : vars : 7.084e-03; : 33 : vars : 6.782e-03; : 34 : vars : 6.773e-03; : 35 : vars : 6.700e-03; : 36 : vars : 6.631e-03; : 37 : vars : 6.491e-03; : ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:36306,Modifiability,variab,variable,36306,"000e+00; : 224 : vars : 0.000e+00; : 225 : vars : 0.000e+00; : 226 : vars : 0.000e+00; : 227 : vars : 0.000e+00; : 228 : vars : 0.000e+00; : 229 : vars : 0.000e+00; : 230 : vars : 0.000e+00; : 231 : vars : 0.000e+00; : 232 : vars : 0.000e+00; : 233 : vars : 0.000e+00; : 234 : vars : 0.000e+00; : 235 : vars : 0.000e+00; : 236 : vars : 0.000e+00; : 237 : vars : 0.000e+00; : 238 : vars : 0.000e+00; : 239 : vars : 0.000e+00; : 240 : vars : 0.000e+00; : 241 : vars : 0.000e+00; : 242 : vars : 0.000e+00; : 243 : vars : 0.000e+00; : 244 : vars : 0.000e+00; : 245 : vars : 0.000e+00; : 246 : vars : 0.000e+00; : 247 : vars : 0.000e+00; : 248 : vars : 0.000e+00; : 249 : vars : 0.000e+00; : 250 : vars : 0.000e+00; : 251 : vars : 0.000e+00; : 252 : vars : 0.000e+00; : 253 : vars : 0.000e+00; : 254 : vars : 0.000e+00; : 255 : vars : 0.000e+00; : 256 : vars : 0.000e+00; : --------------------------------------; : No variable ranking supplied by classifier: TMVA_DNN_CPU; : No variable ranking supplied by classifier: TMVA_CNN_CPU; : No variable ranking supplied by classifier: PyKeras; : No variable ranking supplied by classifier: PyTorch; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_trainingError, Entries= 0, Total sum= 4.65034; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_valError, Entries= 0, Total sum= 7.70045; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_trainingError, Entries= 0, Total sum= 7.15224; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_valError, Entries= 0, Total sum= 6.76684; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 6.25938; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 7.32887; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 5.40937; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.w",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:36366,Modifiability,variab,variable,36366,"000e+00; : 224 : vars : 0.000e+00; : 225 : vars : 0.000e+00; : 226 : vars : 0.000e+00; : 227 : vars : 0.000e+00; : 228 : vars : 0.000e+00; : 229 : vars : 0.000e+00; : 230 : vars : 0.000e+00; : 231 : vars : 0.000e+00; : 232 : vars : 0.000e+00; : 233 : vars : 0.000e+00; : 234 : vars : 0.000e+00; : 235 : vars : 0.000e+00; : 236 : vars : 0.000e+00; : 237 : vars : 0.000e+00; : 238 : vars : 0.000e+00; : 239 : vars : 0.000e+00; : 240 : vars : 0.000e+00; : 241 : vars : 0.000e+00; : 242 : vars : 0.000e+00; : 243 : vars : 0.000e+00; : 244 : vars : 0.000e+00; : 245 : vars : 0.000e+00; : 246 : vars : 0.000e+00; : 247 : vars : 0.000e+00; : 248 : vars : 0.000e+00; : 249 : vars : 0.000e+00; : 250 : vars : 0.000e+00; : 251 : vars : 0.000e+00; : 252 : vars : 0.000e+00; : 253 : vars : 0.000e+00; : 254 : vars : 0.000e+00; : 255 : vars : 0.000e+00; : 256 : vars : 0.000e+00; : --------------------------------------; : No variable ranking supplied by classifier: TMVA_DNN_CPU; : No variable ranking supplied by classifier: TMVA_CNN_CPU; : No variable ranking supplied by classifier: PyKeras; : No variable ranking supplied by classifier: PyTorch; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_trainingError, Entries= 0, Total sum= 4.65034; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_valError, Entries= 0, Total sum= 7.70045; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_trainingError, Entries= 0, Total sum= 7.15224; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_valError, Entries= 0, Total sum= 6.76684; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 6.25938; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 7.32887; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 5.40937; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.w",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:36426,Modifiability,variab,variable,36426,"000e+00; : 224 : vars : 0.000e+00; : 225 : vars : 0.000e+00; : 226 : vars : 0.000e+00; : 227 : vars : 0.000e+00; : 228 : vars : 0.000e+00; : 229 : vars : 0.000e+00; : 230 : vars : 0.000e+00; : 231 : vars : 0.000e+00; : 232 : vars : 0.000e+00; : 233 : vars : 0.000e+00; : 234 : vars : 0.000e+00; : 235 : vars : 0.000e+00; : 236 : vars : 0.000e+00; : 237 : vars : 0.000e+00; : 238 : vars : 0.000e+00; : 239 : vars : 0.000e+00; : 240 : vars : 0.000e+00; : 241 : vars : 0.000e+00; : 242 : vars : 0.000e+00; : 243 : vars : 0.000e+00; : 244 : vars : 0.000e+00; : 245 : vars : 0.000e+00; : 246 : vars : 0.000e+00; : 247 : vars : 0.000e+00; : 248 : vars : 0.000e+00; : 249 : vars : 0.000e+00; : 250 : vars : 0.000e+00; : 251 : vars : 0.000e+00; : 252 : vars : 0.000e+00; : 253 : vars : 0.000e+00; : 254 : vars : 0.000e+00; : 255 : vars : 0.000e+00; : 256 : vars : 0.000e+00; : --------------------------------------; : No variable ranking supplied by classifier: TMVA_DNN_CPU; : No variable ranking supplied by classifier: TMVA_CNN_CPU; : No variable ranking supplied by classifier: PyKeras; : No variable ranking supplied by classifier: PyTorch; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_trainingError, Entries= 0, Total sum= 4.65034; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_valError, Entries= 0, Total sum= 7.70045; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_trainingError, Entries= 0, Total sum= 7.15224; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_valError, Entries= 0, Total sum= 6.76684; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 6.25938; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 7.32887; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 5.40937; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.w",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:36481,Modifiability,variab,variable,36481,"000e+00; : 224 : vars : 0.000e+00; : 225 : vars : 0.000e+00; : 226 : vars : 0.000e+00; : 227 : vars : 0.000e+00; : 228 : vars : 0.000e+00; : 229 : vars : 0.000e+00; : 230 : vars : 0.000e+00; : 231 : vars : 0.000e+00; : 232 : vars : 0.000e+00; : 233 : vars : 0.000e+00; : 234 : vars : 0.000e+00; : 235 : vars : 0.000e+00; : 236 : vars : 0.000e+00; : 237 : vars : 0.000e+00; : 238 : vars : 0.000e+00; : 239 : vars : 0.000e+00; : 240 : vars : 0.000e+00; : 241 : vars : 0.000e+00; : 242 : vars : 0.000e+00; : 243 : vars : 0.000e+00; : 244 : vars : 0.000e+00; : 245 : vars : 0.000e+00; : 246 : vars : 0.000e+00; : 247 : vars : 0.000e+00; : 248 : vars : 0.000e+00; : 249 : vars : 0.000e+00; : 250 : vars : 0.000e+00; : 251 : vars : 0.000e+00; : 252 : vars : 0.000e+00; : 253 : vars : 0.000e+00; : 254 : vars : 0.000e+00; : 255 : vars : 0.000e+00; : 256 : vars : 0.000e+00; : --------------------------------------; : No variable ranking supplied by classifier: TMVA_DNN_CPU; : No variable ranking supplied by classifier: TMVA_CNN_CPU; : No variable ranking supplied by classifier: PyKeras; : No variable ranking supplied by classifier: PyTorch; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_trainingError, Entries= 0, Total sum= 4.65034; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_valError, Entries= 0, Total sum= 7.70045; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_trainingError, Entries= 0, Total sum= 7.15224; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_valError, Entries= 0, Total sum= 6.76684; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 6.25938; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 7.32887; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 5.40937; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.w",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:41675,Modifiability,variab,variable,41675,"n loss: 0.436; [8] val loss: 0.758; [9, 4] train loss: 0.381; [9, 8] train loss: 0.435; [9, 12] train loss: 0.456; [9] val loss: 1.218; [10, 4] train loss: 0.438; [10, 8] train loss: 0.436; [10, 12] train loss: 0.444; [10] val loss: 0.632; Finished Training on 10 Epochs!; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events a",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:41723,Modifiability,variab,variables,41723,"n loss: 0.436; [8] val loss: 0.758; [9, 4] train loss: 0.381; [9, 8] train loss: 0.435; [9, 12] train loss: 0.456; [9] val loss: 1.218; [10, 4] train loss: 0.438; [10, 8] train loss: 0.436; [10, 12] train loss: 0.444; [10] val loss: 0.632; Finished Training on 10 Epochs!; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events a",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42011,Modifiability,variab,variable,42011,"g the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : ---------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42059,Modifiability,variab,variables,42059,"g the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : ---------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42347,Modifiability,variab,variable,42347,"le (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42395,Modifiability,variab,variables,42395,"le (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42599,Modifiability,variab,variable,42599," produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from tra",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42647,Modifiability,variab,variables,42647," produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from tra",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42851,Modifiability,variab,variable,42851,"riable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset PyTorch : 0.055 (0.225) 0.430 (0.620) 0.755 (0.832); : dataset TMVA_CNN_CPU : 0.040 (0.140) 0.345 (0.430) 0.699 (0.669); : datase",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42899,Modifiability,variab,variables,42899,"riable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset PyTorch : 0.055 (0.225) 0.430 (0.620) 0.755 (0.832); : dataset TMVA_CNN_CPU : 0.040 (0.140) 0.345 (0.430) 0.699 (0.669); : datase",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:49791,Modifiability,config,configuration,49791," to avoid conflict with tbb; gSystem->Setenv(""OMP_NUM_THREADS"", ""1"");; ; // do enable MT running; if (num_threads >= 0) {; ROOT::EnableImplicitMT(num_threads);; }; #endif; ; TMVA::Tools::Instance();; ; ; std::cout << ""Running with nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKerasCNN = false;; usePyTorchCNN = false;; #endif; ; TFile *outputFile = nullptr;; if (writeOutputFile); outputFile = TFile::Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:50018,Modifiability,variab,variables,50018,"ionOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; **/; ; TMVA::DataLoader loader(""dataset"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:50079,Modifiability,variab,variables,50079,"ionOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; **/; ; TMVA::DataLoader loader(""dataset"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:50360,Modifiability,variab,variables,50360,"ionOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; **/; ; TMVA::DataLoader loader(""dataset"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:50390,Modifiability,variab,variables,50390,"ionOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; **/; ; TMVA::DataLoader loader(""dataset"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:50468,Modifiability,variab,variable,50468,"ionOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; **/; ; TMVA::DataLoader loader(""dataset"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:51857,Modifiability,variab,variables,51857,"inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of train",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:51920,Modifiability,variab,variable,51920,"ut file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:52039,Modifiability,variab,variables,52039,"roundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; T",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:52947,Modifiability,variab,variables,52947," variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(; ""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"",; nTrainSig, nTrainBkg);; ; loader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions);; ; /***; ; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; ; ; ; **/; ; /****; # Booking Methods; ; Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; **/; ; // Boosted Decision Trees; if (useTMVABDT) {; factory.BookMethod(&loader, TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:55754,Modifiability,layers,layers,55754,"genceSteps=5,BatchSize=100,TestRepetitions=1,""; ""MaxEpochs=10,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."");; ; TString trainingStrategyString(""TrainingStrategy="");; trainingStrategyString += trainingString1; // + ""|"" + trainingString2 + ....; ; // Build now the full DNN Option string; ; TString dnnOptions(""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append("":"");; dnnOptions.Append(layoutString);; dnnOptions.Append("":"");; dnnOptions.Append(trainingStrategyString);; ; TString dnnMethodName = ""TMVA_DNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""TMVA_DNN_GPU"";; #elif defined(R__HAS_TMVACPU); dnnOptions += "":Architecture=CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; /***; ### Book Convolutional Neural Network in TMVA; ; For building a CNN one needs to define; ; - Input Layout : number of channels (in this case = 1) | image height | image width; - Batch Layout : batch size | number of channels | image size = (height*width); ; Then one add Convolutional layers and MaxPool layers.; ; - For Convolutional layer the option string has to be:; - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; width | activation function; ; - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; conv layer equal to the input; ; - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; convergence; ; - For the MaxPool layer:; - MAXPOOL | pool height | pool width | stride height | stride width; ; The RESHAPE layer is needed to flatten the output before the Dense layer; ; ; Note that to run the CNN is required to have CPU or GPU support; ; ***/; ; if (useTMVACNN) {; ; TString inputLayoutString(""InputLayout=1|16|16"");; ; /",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:55773,Modifiability,layers,layers,55773,"genceSteps=5,BatchSize=100,TestRepetitions=1,""; ""MaxEpochs=10,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."");; ; TString trainingStrategyString(""TrainingStrategy="");; trainingStrategyString += trainingString1; // + ""|"" + trainingString2 + ....; ; // Build now the full DNN Option string; ; TString dnnOptions(""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append("":"");; dnnOptions.Append(layoutString);; dnnOptions.Append("":"");; dnnOptions.Append(trainingStrategyString);; ; TString dnnMethodName = ""TMVA_DNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""TMVA_DNN_GPU"";; #elif defined(R__HAS_TMVACPU); dnnOptions += "":Architecture=CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; /***; ### Book Convolutional Neural Network in TMVA; ; For building a CNN one needs to define; ; - Input Layout : number of channels (in this case = 1) | image height | image width; - Batch Layout : batch size | number of channels | image size = (height*width); ; Then one add Convolutional layers and MaxPool layers.; ; - For Convolutional layer the option string has to be:; - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; width | activation function; ; - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; conv layer equal to the input; ; - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; convergence; ; - For the MaxPool layer:; - MAXPOOL | pool height | pool width | stride height | stride width; ; The RESHAPE layer is needed to flatten the output before the Dense layer; ; ; Note that to run the CNN is required to have CPU or GPU support; ; ***/; ; if (useTMVACNN) {; ; TString inputLayoutString(""InputLayout=1|16|16"");; ; /",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:58584,Modifiability,layers,layers,58584,"MVAGPU; cnnOptions += "":Architecture=GPU"";; cnnMethodName = ""TMVA_CNN_GPU"";; #else; cnnOptions += "":Architecture=CPU"";; cnnMethodName = ""TMVA_CNN_CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, cnnMethodName, cnnOptions);; }; ; /**; ### Book Convolutional Neural Network in Keras using a generated model; ; **/; ; #ifdef R__HAS_PYMVA; // The next section uses Python packages, execute it only if PyMVA is available; TString tmva_python_exe{TMVA::Python_Executable()};; TString python_exe = tmva_python_exe.IsNull() ? ""python"" : tmva_python_exe;; ; if (useKerasCNN) {; ; Info(""TMVA_CNN_Classification"", ""Building convolutional keras model"");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(; ""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((16, 16, 1), input_shape = (256, )))"");; m.AddLine(""model.add(Conv2D(10, kernel_size = (3, 3), kernel_initializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(Conv2D(10, kernel_size = (3, 3), kernel_initializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; // m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(MaxPooling2D(pool_size = (2, 2), strides = (1,1))) "");; m.AddLine(""model.add(Flatten())"");; m.AddLine(""model.add(Dense(256, activation = 'relu')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('model_cnn.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:65448,Modifiability,variab,variable,65448,"the given mean and sigm...Definition TRandom.cxx:275; TRandom::SetSeedvirtual void SetSeed(ULong_t seed=0)Set the random generator seed.Definition TRandom.cxx:615; TRandom::Uniformvirtual Double_t Uniform(Double_t x1=1)Returns a uniform deviate on the interval (0, x1).Definition TRandom.cxx:682; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::IsNullBool_t IsNull() constDefinition TString.h:414; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TTree::GetEntriesvirtual Long64_t GetEntries() constDefinition TTree.h:463; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_CNN_ClassificationDefinition TMVA_CNN_Classification.py:1; TMVA::Python_ExecutableTString Python_Executable()Function to find current Python executable used by ROOT If ""Python3"" is installed,...Definition PyMethodBase.cxx:43; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; AuthorLorenzo Moneta ;",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:2802,Performance,perform,performed,2802,"yout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : Layout: ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:3503,Performance,perform,perform,3503,"5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : Layout: ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : InputLayout: ""0|0|0"" [The Layout of the input]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Defaul",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:4053,Performance,perform,performance,4053,"is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : Layout: ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : InputLayout: ""0|0|0"" [The Layout of the input]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_CNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architectu",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:5881,Performance,perform,performed,5881,"OOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|16|16"" [The Layout of the input]; : Layout: ""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,Tes",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:6656,Performance,perform,perform,6656,"ion=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|16|16"" [The Layout of the input]; : Layout: ""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture wit",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:7207,Performance,perform,performance,7207," method-specific help message]; : InputLayout: ""1|16|16"" [The Layout of the input]; : Layout: ""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:9198,Performance,optimiz,optimizer,9198,"=========================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); Non-trainable params: 20 (80.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: model_cnn.h5; Factory : Booking method: ␛[1mPyTorch␛[0m; : ; : Using PyTorch - setting special configuration options ; : Using PyTorch version 2; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchModelCNN.pt; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 800 bkg: 800; : #events: (unweighted) sig: 800 bkg: 800; : Training 200 Decision Trees ... patience please; : Elapsed time for training with 1600 events: 0.877 sec ; BDT : [dataset] : Evaluation of BDT on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0172 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.class.C␛[0m; : TMVA_CNN_ClassificationOutput.root:/dataset/Method_BDT/BDT; Factory : Training finished; : ; Factory : Train method: TMVA_DNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 8 ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16428,Performance,load,loaded,16428," neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (No",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16736,Performance,load,loading,16736,": 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); N",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16754,Performance,optimiz,optimizer,16754,": 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); N",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:28675,Performance,load,loaded,28675,"Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.261 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyTorch for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyTorch ] :␛[0m; : ; : PyTorch is a scientific computing package supporting; : automatic differentiation. This method wraps the training; : and predictions steps of the PyTorch Python package for; : TMVA, so that dataloading, preprocessing and evaluation; : can be done within the TMVA system. To use this PyTorch; : interface, you need to generatea model with PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; BDT : Ranking result (top variable is best ranked); : --------------------------------------; : Rank : Variable : Variable Importance; : ----",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:37881,Performance,perform,performance,37881,"t Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 6.25938; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 7.32887; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 5.40937; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; :",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:38080,Performance,perform,performance,38080,"Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.175 sec ; Factory : Test method: PyTorch for Classification performance; : ; : Setup P",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:38368,Performance,perform,performance,38368,weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.175 sec ; Factory : Test method: PyTorch for Classification performance; : ; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; RecursiveScriptModule(; original_name=Sequential; (0): RecursiveScriptModule(original_name=Re,MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:38651,Performance,perform,performance,38651,6mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.175 sec ; Factory : Test method: PyTorch for Classification performance; : ; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; RecursiveScriptModule(; original_name=Sequential; (0): RecursiveScriptModule(original_name=Reshape); (1): RecursiveScriptModule(original_name=Conv2d); (2): RecursiveScriptModule(original_name=ReLU); (3): RecursiveScriptModule(original_name=BatchNorm2d); (4): RecursiveScriptModule(ori,MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:39132,Performance,perform,performance,39132,"ataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.175 sec ; Factory : Test method: PyTorch for Classification performance; : ; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; RecursiveScriptModule(; original_name=Sequential; (0): RecursiveScriptModule(original_name=Reshape); (1): RecursiveScriptModule(original_name=Conv2d); (2): RecursiveScriptModule(original_name=ReLU); (3): RecursiveScriptModule(original_name=BatchNorm2d); (4): RecursiveScriptModule(original_name=Conv2d); (5): RecursiveScriptModule(original_name=ReLU); (6): RecursiveScriptModule(original_name=MaxPool2d); (7): RecursiveScriptModule(original_name=Flatten); (8): RecursiveScriptModule(original_name=Linear); (9): RecursiveScriptModule(original_name=ReLU); (10): RecursiveScriptModule(original_name=Linear); (11): RecursiveScriptModule(original_name=Sigmoid); ); [1, 4] train loss: 1.164; [1, 8] train loss: 0.756; [1, 12] train loss: 0.696; [1] val loss: 0.693; [2, 4] train loss: 0.692; [2, 8] train loss: 0.690; [2, 12] train loss: 0",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:41212,Performance,optimiz,optimizer,41212,"in loss: 0.679; [3, 12] train loss: 0.679; [3] val loss: 0.692; [4, 4] train loss: 0.663; [4, 8] train loss: 0.668; [4, 12] train loss: 0.650; [4] val loss: 0.708; [5, 4] train loss: 0.642; [5, 8] train loss: 0.635; [5, 12] train loss: 0.633; [5] val loss: 0.707; [6, 4] train loss: 0.607; [6, 8] train loss: 0.588; [6, 12] train loss: 0.585; [6] val loss: 0.700; [7, 4] train loss: 0.553; [7, 8] train loss: 0.564; [7, 12] train loss: 0.542; [7] val loss: 0.637; [8, 4] train loss: 0.474; [8, 8] train loss: 0.446; [8, 12] train loss: 0.436; [8] val loss: 0.758; [9, 4] train loss: 0.381; [9, 8] train loss: 0.435; [9, 12] train loss: 0.456; [9] val loss: 1.218; [10, 4] train loss: 0.438; [10, 8] train loss: 0.436; [10, 12] train loss: 0.444; [10] val loss: 0.632; Finished Training on 10 Epochs!; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ;",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:48381,Performance,multi-thread,multi-thread,48381,"ssification example; /// @param nevts : number of signal/background events. Use by default a low value (1000); /// but increase to at least 5000 to get a good result; /// @param opt : vector of bool with method used (default all on if available). The order is:; /// - TMVA CNN; /// - Keras CNN; /// - TMVA DNN; /// - TMVA BDT; /// - PyTorch CNN; void TMVA_CNN_Classification(int nevts = 1000, std::vector<bool> opt = {1, 1, 1, 1, 1}); {; ; int imgSize = 16 * 16;; TString inputFileName = ""images_data_16x16.root"";; ; bool fileExist = !gSystem->AccessPathName(inputFileName);; ; // if file does not exists create it; if (!fileExist) {; MakeImagesTree(nevts, 16, 16);; }; ; bool useTMVACNN = (opt.size() > 0) ? opt[0] : false;; bool useKerasCNN = (opt.size() > 1) ? opt[1] : false;; bool useTMVADNN = (opt.size() > 2) ? opt[2] : false;; bool useTMVABDT = (opt.size() > 3) ? opt[3] : false;; bool usePyTorchCNN = (opt.size() > 4) ? opt[4] : false;; #ifndef R__HAS_TMVACPU; #ifndef R__HAS_TMVAGPU; Warning(""TMVA_CNN_Classification"",; ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for CNN"");; useTMVACNN = false;; #endif; #endif; ; bool writeOutputFile = true;; ; #ifdef R__USE_IMT; int num_threads = 4; // use by default 4 threads if value is not set before; // switch off MT in OpenBLAS to avoid conflict with tbb; gSystem->Setenv(""OMP_NUM_THREADS"", ""1"");; ; // do enable MT running; if (num_threads >= 0) {; ROOT::EnableImplicitMT(num_threads);; }; #endif; ; TMVA::Tools::Instance();; ; ; std::cout << ""Running with nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKerasCNN = false;; usePyTorchCNN = false;; #endif; ; TFile *outputFile = nullptr;; if (writeOutputFile); outputFile = TFile::Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later yo",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:49349,Performance,perform,performance,49349,"TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for CNN"");; useTMVACNN = false;; #endif; #endif; ; bool writeOutputFile = true;; ; #ifdef R__USE_IMT; int num_threads = 4; // use by default 4 threads if value is not set before; // switch off MT in OpenBLAS to avoid conflict with tbb; gSystem->Setenv(""OMP_NUM_THREADS"", ""1"");; ; // do enable MT running; if (num_threads >= 0) {; ROOT::EnableImplicitMT(num_threads);; }; #endif; ; TMVA::Tools::Instance();; ; ; std::cout << ""Running with nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKerasCNN = false;; usePyTorchCNN = false;; #endif; ; TFile *outputFile = nullptr;; if (writeOutputFile); outputFile = TFile::Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:50684,Performance,load,loader,50684,"l configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; **/; ; TMVA::DataLoader loader(""dataset"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.Add",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:51731,Performance,load,loader,51731,"t"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:51780,Performance,load,loader,51780,"d signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( m",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:51956,Performance,load,loader,51956,"ut file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:52239,Performance,load,loader,52239,"roundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; T",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:52727,Performance,load,loader,52727,"; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(; ""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"",; nTrainSig, nTrainBkg);; ; loader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions);; ; /***; ; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; ; ; ; **/; ; /****; # Booking Methods; ; Here we bo",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:53286,Performance,load,loader,53286,");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(; ""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"",; nTrainSig, nTrainBkg);; ; loader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions);; ; /***; ; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; ; ; ; **/; ; /****; # Booking Methods; ; Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; **/; ; // Boosted Decision Trees; if (useTMVABDT) {; factory.BookMethod(&loader, TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:""; ""UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"");; }; /**; ; #### Booking Deep Neural Network; ; Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; options; ; **/; ; if (useTMVADNN)",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:53769,Performance,load,loader,53769,"// It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(; ""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"",; nTrainSig, nTrainBkg);; ; loader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions);; ; /***; ; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; ; ; ; **/; ; /****; # Booking Methods; ; Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; **/; ; // Boosted Decision Trees; if (useTMVABDT) {; factory.BookMethod(&loader, TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:""; ""UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"");; }; /**; ; #### Booking Deep Neural Network; ; Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; options; ; **/; ; if (useTMVADNN) {; ; TString layoutString(; ""Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"");; ; // Training strategies; // one can catenate several training strings with different parameters (e.g. learning rates or regularizations; // parameters) The training string must be concatenates with the `|` delimiter; TString trainingString1(""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""MaxEpochs=10,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."");; ; TString trainingStrategyString(""TrainingStrategy="");; traini",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:55403,Performance,load,loader,55403,"genceSteps=5,BatchSize=100,TestRepetitions=1,""; ""MaxEpochs=10,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."");; ; TString trainingStrategyString(""TrainingStrategy="");; trainingStrategyString += trainingString1; // + ""|"" + trainingString2 + ....; ; // Build now the full DNN Option string; ; TString dnnOptions(""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append("":"");; dnnOptions.Append(layoutString);; dnnOptions.Append("":"");; dnnOptions.Append(trainingStrategyString);; ; TString dnnMethodName = ""TMVA_DNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""TMVA_DNN_GPU"";; #elif defined(R__HAS_TMVACPU); dnnOptions += "":Architecture=CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; /***; ### Book Convolutional Neural Network in TMVA; ; For building a CNN one needs to define; ; - Input Layout : number of channels (in this case = 1) | image height | image width; - Batch Layout : batch size | number of channels | image size = (height*width); ; Then one add Convolutional layers and MaxPool layers.; ; - For Convolutional layer the option string has to be:; - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; width | activation function; ; - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; conv layer equal to the input; ; - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; convergence; ; - For the MaxPool layer:; - MAXPOOL | pool height | pool width | stride height | stride width; ; The RESHAPE layer is needed to flatten the output before the Dense layer; ; ; Note that to run the CNN is required to have CPU or GPU support; ; ***/; ; if (useTMVACNN) {; ; TString inputLayoutString(""InputLayout=1|16|16"");; ; /",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:57818,Performance,load,loader,57818,"opConfig=0.0+0.0+0.0+0.0"");; ; TString trainingStrategyString(""TrainingStrategy="");; trainingStrategyString +=; trainingString1; // + ""|"" + trainingString2 + ""|"" + trainingString3; for concatenating more training strings; ; // Build full CNN Options.; TString cnnOptions(""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:""; ""WeightInitialization=XAVIER"");; ; cnnOptions.Append("":"");; cnnOptions.Append(inputLayoutString);; cnnOptions.Append("":"");; cnnOptions.Append(layoutString);; cnnOptions.Append("":"");; cnnOptions.Append(trainingStrategyString);; ; //// New DL (CNN); TString cnnMethodName = ""TMVA_CNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; cnnOptions += "":Architecture=GPU"";; cnnMethodName = ""TMVA_CNN_GPU"";; #else; cnnOptions += "":Architecture=CPU"";; cnnMethodName = ""TMVA_CNN_CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, cnnMethodName, cnnOptions);; }; ; /**; ### Book Convolutional Neural Network in Keras using a generated model; ; **/; ; #ifdef R__HAS_PYMVA; // The next section uses Python packages, execute it only if PyMVA is available; TString tmva_python_exe{TMVA::Python_Executable()};; TString python_exe = tmva_python_exe.IsNull() ? ""python"" : tmva_python_exe;; ; if (useKerasCNN) {; ; Info(""TMVA_CNN_Classification"", ""Building convolutional keras model"");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(; ""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((16, 16, 1), input_shape = (256, )))"");; m.AddLine(""model.add(Conv2D(10, kernel_size = (3, 3), kernel_initializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; m.AddLine(""model.add(BatchNormaliza",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:58522,Performance,optimiz,optimizers,58522,"/// New DL (CNN); TString cnnMethodName = ""TMVA_CNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; cnnOptions += "":Architecture=GPU"";; cnnMethodName = ""TMVA_CNN_GPU"";; #else; cnnOptions += "":Architecture=CPU"";; cnnMethodName = ""TMVA_CNN_CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, cnnMethodName, cnnOptions);; }; ; /**; ### Book Convolutional Neural Network in Keras using a generated model; ; **/; ; #ifdef R__HAS_PYMVA; // The next section uses Python packages, execute it only if PyMVA is available; TString tmva_python_exe{TMVA::Python_Executable()};; TString python_exe = tmva_python_exe.IsNull() ? ""python"" : tmva_python_exe;; ; if (useKerasCNN) {; ; Info(""TMVA_CNN_Classification"", ""Building convolutional keras model"");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(; ""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((16, 16, 1), input_shape = (256, )))"");; m.AddLine(""model.add(Conv2D(10, kernel_size = (3, 3), kernel_initializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(Conv2D(10, kernel_size = (3, 3), kernel_initializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; // m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(MaxPooling2D(pool_size = (2, 2), strides = (1,1))) "");; m.AddLine(""model.add(Flatten())"");; m.AddLine(""model.add(Dense(256, activation = 'relu')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accura",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:59469,Performance,optimiz,optimizer,59469,"import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(; ""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((16, 16, 1), input_shape = (256, )))"");; m.AddLine(""model.add(Conv2D(10, kernel_size = (3, 3), kernel_initializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(Conv2D(10, kernel_size = (3, 3), kernel_initializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; // m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(MaxPooling2D(pool_size = (2, 2), strides = (1,1))) "");; m.AddLine(""model.add(Flatten())"");; m.AddLine(""model.add(Dense(256, activation = 'relu')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('model_cnn.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_cnn_model.py"");; // execute; gSystem->Exec(python_exe + "" make_cnn_model.py"");; ; if (gSystem->AccessPathName(""model_cnn.h5"")) {; Warning(""TMVA_CNN_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_CNN_Classification"", ""Booking tf.Keras CNN model"");; factory.BookMethod(; &loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=model_cnn.h5:tf.keras:""; ""FilenameTrainedModel=trained_model_cnn.h5:NumEpochs=10:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; if (usePyTorchCNN) {; ; Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model"");; TString pyTorchFileName = gROOT->GetTutorialDir() + TString(""/",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:60019,Performance,load,loader,60019,"tializer = 'glorot_normal',activation = ""; ""'relu', padding = 'same'))"");; // m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(MaxPooling2D(pool_size = (2, 2), strides = (1,1))) "");; m.AddLine(""model.add(Flatten())"");; m.AddLine(""model.add(Dense(256, activation = 'relu')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('model_cnn.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_cnn_model.py"");; // execute; gSystem->Exec(python_exe + "" make_cnn_model.py"");; ; if (gSystem->AccessPathName(""model_cnn.h5"")) {; Warning(""TMVA_CNN_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_CNN_Classification"", ""Booking tf.Keras CNN model"");; factory.BookMethod(; &loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=model_cnn.h5:tf.keras:""; ""FilenameTrainedModel=trained_model_cnn.h5:NumEpochs=10:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; if (usePyTorchCNN) {; ; Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model"");; TString pyTorchFileName = gROOT->GetTutorialDir() + TString(""/tmva/PyTorch_Generate_CNN_Model.py"");; // check that pytorch can be imported and file defining the model and used later when booking the method is; // existing; if (gSystem->Exec(python_exe + "" -c 'import torch'"") || gSystem->AccessPathName(pyTorchFileName)) {; Warning(""TMVA_CNN_Classification"", ""PyTorch is not installed or model building file is not existing - skip using PyTorch"");; } else {; // book PyTorch method only if PyTorch model could be created; Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model"");; TString methodOpt = ""H:!V:VarTransform=None:FilenameModel=PyTorchMo",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:61224,Performance,load,loader,61224,"oid TF allocates all GPU memory; }; }; ; if (usePyTorchCNN) {; ; Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model"");; TString pyTorchFileName = gROOT->GetTutorialDir() + TString(""/tmva/PyTorch_Generate_CNN_Model.py"");; // check that pytorch can be imported and file defining the model and used later when booking the method is; // existing; if (gSystem->Exec(python_exe + "" -c 'import torch'"") || gSystem->AccessPathName(pyTorchFileName)) {; Warning(""TMVA_CNN_Classification"", ""PyTorch is not installed or model building file is not existing - skip using PyTorch"");; } else {; // book PyTorch method only if PyTorch model could be created; Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model"");; TString methodOpt = ""H:!V:VarTransform=None:FilenameModel=PyTorchModelCNN.pt:""; ""FilenameTrainedModel=PyTorchTrainedModelCNN.pt:NumEpochs=10:BatchSize=100"";; methodOpt += TString("":UserCode="") + pyTorchFileName;; factory.BookMethod(&loader, TMVA::Types::kPyTorch, ""PyTorch"", methodOpt);; }; }; #endif; ; //// ## Train Methods; ; factory.TrainAllMethods();; ; /// ## Test and Evaluate Methods; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// ## Plot ROC Curve; ; auto c1 = factory.GetROCCurve(&loader);; c1->Draw();; ; // close outputfile to save output file; outputFile->Close();; }; f#define f(i)Definition RSha256.hxx:104; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; gROOT#define g",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:61504,Performance,load,loader,61504,"ing the model and used later when booking the method is; // existing; if (gSystem->Exec(python_exe + "" -c 'import torch'"") || gSystem->AccessPathName(pyTorchFileName)) {; Warning(""TMVA_CNN_Classification"", ""PyTorch is not installed or model building file is not existing - skip using PyTorch"");; } else {; // book PyTorch method only if PyTorch model could be created; Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model"");; TString methodOpt = ""H:!V:VarTransform=None:FilenameModel=PyTorchModelCNN.pt:""; ""FilenameTrainedModel=PyTorchTrainedModelCNN.pt:NumEpochs=10:BatchSize=100"";; methodOpt += TString("":UserCode="") + pyTorchFileName;; factory.BookMethod(&loader, TMVA::Types::kPyTorch, ""PyTorch"", methodOpt);; }; }; #endif; ; //// ## Train Methods; ; factory.TrainAllMethods();; ; /// ## Test and Evaluate Methods; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// ## Plot ROC Curve; ; auto c1 = factory.GetROCCurve(&loader);; c1->Draw();; ; // close outputfile to save output file; outputFile->Close();; }; f#define f(i)Definition RSha256.hxx:104; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; gROOT#define gROOTDefinition TROOT.h:406; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TF1::SetParametersvirtual void SetParameters(const Double_t *p",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:65860,Performance,multi-thread,multi-threading,65860,"ble_t Uniform(Double_t x1=1)Returns a uniform deviate on the interval (0, x1).Definition TRandom.cxx:682; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::IsNullBool_t IsNull() constDefinition TString.h:414; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TTree::GetEntriesvirtual Long64_t GetEntries() constDefinition TTree.h:463; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_CNN_ClassificationDefinition TMVA_CNN_Classification.py:1; TMVA::Python_ExecutableTString Python_Executable()Function to find current Python executable used by ROOT If ""Python3"" is installed,...Definition PyMethodBase.cxx:43; mTMarker mDefinition textangle.C:8; lTLine lDefinition textangle.C:4; AuthorLorenzo Moneta ; Definition in file TMVA_CNN_Classification.C. tutorialstmvaTMVA_CNN_Classification.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:9262,Safety,predict,predict,9262,"=========================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); Non-trainable params: 20 (80.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: model_cnn.h5; Factory : Booking method: ␛[1mPyTorch␛[0m; : ; : Using PyTorch - setting special configuration options ; : Using PyTorch version 2; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchModelCNN.pt; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 800 bkg: 800; : #events: (unweighted) sig: 800 bkg: 800; : Training 200 Decision Trees ... patience please; : Elapsed time for training with 1600 events: 0.877 sec ; BDT : [dataset] : Evaluation of BDT on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0172 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.class.C␛[0m; : TMVA_CNN_ClassificationOutput.root:/dataset/Method_BDT/BDT; Factory : Training finished; : ; Factory : Train method: TMVA_DNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 8 ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16174,Safety,predict,predictions,16174,"6 925.361 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.561268 0.630455 1.40233 0.116045 932.919 0; : ; : Elapsed time for training with 1600 events: 14.2 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16897,Safety,predict,predict,16897,"ons steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); Non-trainable params: 20 (80.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/10; ; 1/13 [=>............................] - ETA: 10s - loss: 0.9275 - accuracy: 0.4600␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 4/13 [========>.....................] - ETA: 0s - loss: 2.4547 - accuracy: 0.4675 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 7/13 [===============>..",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:28416,Safety,predict,predictions,28416," : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.261 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyTorch for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyTorch ] :␛[0m; : ; : PyTorch is a scientific computing package supporting; : automatic differentiation. This method wraps the training; : and predictions steps of the PyTorch Python package for; : TMVA, so that dataloading, preprocessing and evaluation; : can be done within the TMVA system. To use this PyTorch; : interface, you need to generatea model with PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:41276,Safety,predict,predict,41276,"in loss: 0.679; [3, 12] train loss: 0.679; [3] val loss: 0.692; [4, 4] train loss: 0.663; [4, 8] train loss: 0.668; [4, 12] train loss: 0.650; [4] val loss: 0.708; [5, 4] train loss: 0.642; [5, 8] train loss: 0.635; [5, 12] train loss: 0.633; [5] val loss: 0.707; [6, 4] train loss: 0.607; [6, 8] train loss: 0.588; [6, 12] train loss: 0.585; [6] val loss: 0.700; [7, 4] train loss: 0.553; [7, 8] train loss: 0.564; [7, 12] train loss: 0.542; [7] val loss: 0.637; [8, 4] train loss: 0.474; [8, 8] train loss: 0.446; [8, 12] train loss: 0.436; [8] val loss: 0.758; [9, 4] train loss: 0.381; [9, 8] train loss: 0.435; [9, 12] train loss: 0.456; [9] val loss: 1.218; [10, 4] train loss: 0.438; [10, 8] train loss: 0.436; [10, 12] train loss: 0.444; [10] val loss: 0.632; Finished Training on 10 Epochs!; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ;",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:48644,Safety,avoid,avoid,48644,"a_16x16.root"";; ; bool fileExist = !gSystem->AccessPathName(inputFileName);; ; // if file does not exists create it; if (!fileExist) {; MakeImagesTree(nevts, 16, 16);; }; ; bool useTMVACNN = (opt.size() > 0) ? opt[0] : false;; bool useKerasCNN = (opt.size() > 1) ? opt[1] : false;; bool useTMVADNN = (opt.size() > 2) ? opt[2] : false;; bool useTMVABDT = (opt.size() > 3) ? opt[3] : false;; bool usePyTorchCNN = (opt.size() > 4) ? opt[4] : false;; #ifndef R__HAS_TMVACPU; #ifndef R__HAS_TMVAGPU; Warning(""TMVA_CNN_Classification"",; ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for CNN"");; useTMVACNN = false;; #endif; #endif; ; bool writeOutputFile = true;; ; #ifdef R__USE_IMT; int num_threads = 4; // use by default 4 threads if value is not set before; // switch off MT in OpenBLAS to avoid conflict with tbb; gSystem->Setenv(""OMP_NUM_THREADS"", ""1"");; ; // do enable MT running; if (num_threads >= 0) {; ROOT::EnableImplicitMT(num_threads);; }; #endif; ; TMVA::Tools::Instance();; ; ; std::cout << ""Running with nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKerasCNN = false;; usePyTorchCNN = false;; #endif; ; TFile *outputFile = nullptr;; if (writeOutputFile); outputFile = TFile::Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TM",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:50035,Safety,avoid,avoid,50035,"ionOutput.root"", ""RECREATE"");; ; /***; ## Create TMVA Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weight files in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; option string; ; - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; input variables; ***/; ; TMVA::Factory factory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; **/; ; TMVA::DataLoader loader(""dataset"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:60268,Safety,avoid,avoid,60268,"ation = 'relu')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('model_cnn.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_cnn_model.py"");; // execute; gSystem->Exec(python_exe + "" make_cnn_model.py"");; ; if (gSystem->AccessPathName(""model_cnn.h5"")) {; Warning(""TMVA_CNN_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_CNN_Classification"", ""Booking tf.Keras CNN model"");; factory.BookMethod(; &loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=model_cnn.h5:tf.keras:""; ""FilenameTrainedModel=trained_model_cnn.h5:NumEpochs=10:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; if (usePyTorchCNN) {; ; Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model"");; TString pyTorchFileName = gROOT->GetTutorialDir() + TString(""/tmva/PyTorch_Generate_CNN_Model.py"");; // check that pytorch can be imported and file defining the model and used later when booking the method is; // existing; if (gSystem->Exec(python_exe + "" -c 'import torch'"") || gSystem->AccessPathName(pyTorchFileName)) {; Warning(""TMVA_CNN_Classification"", ""PyTorch is not installed or model building file is not existing - skip using PyTorch"");; } else {; // book PyTorch method only if PyTorch model could be created; Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model"");; TString methodOpt = ""H:!V:VarTransform=None:FilenameModel=PyTorchModelCNN.pt:""; ""FilenameTrainedModel=PyTorchTrainedModelCNN.pt:NumEpochs=10:BatchSize=100"";; methodOpt += TString("":UserCode="") + pyTorchFileName;; factory.BookMethod(&loader, TMVA::Types::kPyTorch, ""PyTorch"", methodOpt);; }; }; #endif; ; //// ## Train Methods; ; factory.Train",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:4325,Security,validat,validation,4325,"is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : Layout: ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : InputLayout: ""0|0|0"" [The Layout of the input]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_CNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architectu",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:4393,Security,validat,validation,4393,"ication).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : InputLayout: ""0|0|0"" [The Layout of the input]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_CNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:7429,Security,validat,validation,7429," method-specific help message]; : InputLayout: ""1|16|16"" [The Layout of the input]; : Layout: ""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:7497,Security,validat,validation,7497,"ror (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); Non-trainable params: 20 (80.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11166,Security,validat,validation,11166,"nts: 0.877 sec ; BDT : [dataset] : Evaluation of BDT on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0172 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.class.C␛[0m; : TMVA_CNN_ClassificationOutput.root:/dataset/Method_BDT/BDT; Factory : Training finished; : ; Factory : Train method: TMVA_DNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 8 Input = ( 1, 1, 256 ) Batch size = 100 Loss function = C; Layer 0 DENSE Layer: ( Input = 256 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 1 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 2 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 3 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 4 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the co",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:13992,Security,validat,validation,13992,"etwork on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_CNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 7 Input = ( 1, 16, 16 ) Batch size = 100 Loss function = C; Layer 0 CONV LAYER: ( W = 16 , H = 16 , D = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 1 BATCH NORM Layer: Input/Output = ( 10 , 256 , 100 ) Norm dim = 10 axis = 1; ; Layer 2 CONV LAYER: ( W = 16 , H = 16 , D = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 3 POOL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:16672,Security,validat,validation,16672,": 0.629 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Training Model Summary; custom objects for loading model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); N",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:17779,Security,validat,validation,17779,"ing model : {'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': BCELoss(), 'train_func': <function fit at 0x7fe43c5d1b80>, 'predict_func': <function predict at 0x7fe43c5d1ca0>}; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2) 514 ; ; =================================================================; Total params: 577820 (2.20 MB); Trainable params: 577800 (2.20 MB); Non-trainable params: 20 (80.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/10; ; 1/13 [=>............................] - ETA: 10s - loss: 0.9275 - accuracy: 0.4600␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 4/13 [========>.....................] - ETA: 0s - loss: 2.4547 - accuracy: 0.4675 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 7/13 [===============>..............] - ETA: 0s - loss: 1.8505 - accuracy: 0.4886␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 10/13 [======================>.......] - ETA: 0s - loss: 1.5967 - accuracy: 0.5030␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 13/13 [==============================] - ETA: 0s - loss: 1.4175 - accuracy: 0.4922; Epoch 1: val_loss improved from inf to 0.86968, saving model to trained_model_cnn.h5; ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:28919,Security,validat,validation,28919,"t/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyTorch for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyTorch ] :␛[0m; : ; : PyTorch is a scientific computing package supporting; : automatic differentiation. This method wraps the training; : and predictions steps of the PyTorch Python package for; : TMVA, so that dataloading, preprocessing and evaluation; : can be done within the TMVA system. To use this PyTorch; : interface, you need to generatea model with PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; BDT : Ranking result (top variable is best ranked); : --------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------; : 1 : vars : 1.153e-02; : 2 : vars : 1.059e-02; : 3 : vars : 1.011e-02; : 4 : vars : 9.992e-03; : 5 : vars : 9.753e-03; : 6 : vars : 9.676e-03; : 7 : vars : 9.360e-03; : 8 : vars : 9.253e-03; : 9 ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:29031,Security,validat,validation,29031,"t/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyTorch for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyTorch ] :␛[0m; : ; : PyTorch is a scientific computing package supporting; : automatic differentiation. This method wraps the training; : and predictions steps of the PyTorch Python package for; : TMVA, so that dataloading, preprocessing and evaluation; : can be done within the TMVA system. To use this PyTorch; : interface, you need to generatea model with PyTorch first.; : Then, this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 1280 training events and 320 validation events; : Print Training Model Architecture; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; : Elapsed time for training with 1600 events: 23.1 sec ; PyTorch : [dataset] : Evaluation of PyTorch on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.434 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; BDT : Ranking result (top variable is best ranked); : --------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------; : 1 : vars : 1.153e-02; : 2 : vars : 1.059e-02; : 3 : vars : 1.011e-02; : 4 : vars : 9.992e-03; : 5 : vars : 9.753e-03; : 6 : vars : 9.676e-03; : 7 : vars : 9.360e-03; : 8 : vars : 9.253e-03; : 9 ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:65286,Security,access,access,65286,"n TMacro.h:31; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random number from the standard Normal (Gaussian) Distribution with the given mean and sigm...Definition TRandom.cxx:275; TRandom::SetSeedvirtual void SetSeed(ULong_t seed=0)Set the random generator seed.Definition TRandom.cxx:615; TRandom::Uniformvirtual Double_t Uniform(Double_t x1=1)Returns a uniform deviate on the interval (0, x1).Definition TRandom.cxx:682; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::IsNullBool_t IsNull() constDefinition TString.h:414; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TTree::GetEntriesvirtual Long64_t GetEntries() constDefinition TTree.h:463; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_CNN_ClassificationDefinition TMVA_CNN_Classification.py:1; TMVA::Python_ExecutableTString Python_Executable()Function to find current Python executable used",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:65320,Security,access,access,65320,"n TMacro.h:31; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random number from the standard Normal (Gaussian) Distribution with the given mean and sigm...Definition TRandom.cxx:275; TRandom::SetSeedvirtual void SetSeed(ULong_t seed=0)Set the random generator seed.Definition TRandom.cxx:615; TRandom::Uniformvirtual Double_t Uniform(Double_t x1=1)Returns a uniform deviate on the interval (0, x1).Definition TRandom.cxx:682; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::IsNullBool_t IsNull() constDefinition TString.h:414; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TTree::GetEntriesvirtual Long64_t GetEntries() constDefinition TTree.h:463; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_CNN_ClassificationDefinition TMVA_CNN_Classification.py:1; TMVA::Python_ExecutableTString Python_Executable()Function to find current Python executable used",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:1197,Testability,test,testing,1197,"Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_CNN_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Convolutional Neural Network ; This is an example of using a CNN in TMVA. We do classification using a toy image data set that is generated when running the example macro. ; Running with nthreads = 4; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 1000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 1000 events; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Using variable vars[0] from array expression vars of size 256; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; : Using variable vars[0] from array expression vars of size 256; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 800; : Signal -- testing events : 200; : Signal -- training and testing events: 1000; : Background -- training events : 800; : Background -- testing events : 200; : Background -- training and testing events: 1000; : ; Factory : Booking method: ␛[1mTMVA_DNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:1339,Testability,test,testing,1339,"Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_CNN_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Convolutional Neural Network ; This is an example of using a CNN in TMVA. We do classification using a toy image data set that is generated when running the example macro. ; Running with nthreads = 4; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 1000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 1000 events; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Using variable vars[0] from array expression vars of size 256; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; : Using variable vars[0] from array expression vars of size 256; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 800; : Signal -- testing events : 200; : Signal -- training and testing events: 1000; : Background -- training events : 800; : Background -- testing events : 200; : Background -- training and testing events: 1000; : ; Factory : Booking method: ␛[1mTMVA_DNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:1386,Testability,test,testing,1386,"Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_CNN_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Convolutional Neural Network ; This is an example of using a CNN in TMVA. We do classification using a toy image data set that is generated when running the example macro. ; Running with nthreads = 4; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 1000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 1000 events; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Using variable vars[0] from array expression vars of size 256; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; : Using variable vars[0] from array expression vars of size 256; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 800; : Signal -- testing events : 200; : Signal -- training and testing events: 1000; : Background -- training events : 800; : Background -- testing events : 200; : Background -- training and testing events: 1000; : ; Factory : Booking method: ␛[1mTMVA_DNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:1463,Testability,test,testing,1463,"Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_CNN_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Convolutional Neural Network ; This is an example of using a CNN in TMVA. We do classification using a toy image data set that is generated when running the example macro. ; Running with nthreads = 4; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 1000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 1000 events; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Using variable vars[0] from array expression vars of size 256; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; : Using variable vars[0] from array expression vars of size 256; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 800; : Signal -- testing events : 200; : Signal -- training and testing events: 1000; : Background -- training events : 800; : Background -- testing events : 200; : Background -- training and testing events: 1000; : ; Factory : Booking method: ␛[1mTMVA_DNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:1514,Testability,test,testing,1514,"Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_CNN_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Convolutional Neural Network ; This is an example of using a CNN in TMVA. We do classification using a toy image data set that is generated when running the example macro. ; Running with nthreads = 4; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 1000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 1000 events; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Using variable vars[0] from array expression vars of size 256; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; : Using variable vars[0] from array expression vars of size 256; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 800; : Signal -- testing events : 200; : Signal -- training and testing events: 1000; : Background -- training events : 800; : Background -- testing events : 200; : Background -- training and testing events: 1000; : ; Factory : Booking method: ␛[1mTMVA_DNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:4041,Testability,test,testing,4041,"is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : Layout: ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : InputLayout: ""0|0|0"" [The Layout of the input]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_CNN_CPU␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:InputLayout=1|16|16:Layout=CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architectu",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:7195,Testability,test,testing,7195," method-specific help message]; : InputLayout: ""1|16|16"" [The Layout of the input]; : Layout: ""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,MaxEpochs=10,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 16, 16, 1) 0 ; ; conv2d (Conv2D) (None, 16, 16, 10) 100 ; ; batch_normalization (Batch (None, 16, 16, 10) 40 ; Normalization) ; ; conv2d_1 (Conv2D) (None, 16, 16, 10) 910 ; ; max_pooling2d (MaxPooling2 (None, 15, 15, 10) 0 ; D) ; ; flatten (Flatten) (None, 2250) 0 ; ; dense (Dense) (None, 256) 576256 ; ; dense_1 (Dense) (None, 2",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:11127,Testability,test,testing,11127,"nts: 0.877 sec ; BDT : [dataset] : Evaluation of BDT on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0172 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.class.C␛[0m; : TMVA_CNN_ClassificationOutput.root:/dataset/Method_BDT/BDT; Factory : Training finished; : ; Factory : Train method: TMVA_DNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 8 Input = ( 1, 1, 256 ) Batch size = 100 Loss function = C; Layer 0 DENSE Layer: ( Input = 256 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 1 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 2 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 3 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 4 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 5 BATCH NORM Layer: Input/Output = ( 100 , 100 , 1 ) Norm dim = 100 axis = -1; ; Layer 6 DENSE Layer: ( Input = 100 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 7 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 119.005; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the co",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:13953,Testability,test,testing,13953,"etwork on CPU using batches with size = 100; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on training sample (1600 events); : Elapsed time for evaluation of 1600 events: 0.0796 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_CNN_CPU for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 7 Input = ( 1, 16, 16 ) Batch size = 100 Loss function = C; Layer 0 CONV LAYER: ( W = 16 , H = 16 , D = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 1 BATCH NORM Layer: Input/Output = ( 10 , 256 , 100 ) Norm dim = 10 axis = 1; ; Layer 2 CONV LAYER: ( W = 16 , H = 16 , D = 10 ) Filter ( W = 3 , H = 3 ) Output = ( 100 , 10 , 10 , 256 ) Activation Function = Relu; Layer 3 POOL Layer: ( W = 15 , H = 15 , D = 10 ) Filter ( W = 2 , H = 2 ) Output = ( 100 , 10 , 10 , 225 ) ; Layer 4 RESHAPE Layer Input = ( 10 , 15 , 15 ) Output = ( 1 , 100 , 2250 ) ; Layer 5 DENSE Layer: ( Input = 2250 , Width = 100 ) Output = ( 1 , 100 , 100 ) Activation Function = Relu; Layer 6 DENSE Layer: ( Input = 100 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 1280 events for training and 320 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 13.2274; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 1.28042 0",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:37301,Testability,test,testing,37301,"iable ranking supplied by classifier: TMVA_DNN_CPU; : No variable ranking supplied by classifier: TMVA_CNN_CPU; : No variable ranking supplied by classifier: PyKeras; : No variable ranking supplied by classifier: PyTorch; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_trainingError, Entries= 0, Total sum= 4.65034; TH1.Print Name = TrainingHistory_TMVA_DNN_CPU_valError, Entries= 0, Total sum= 7.70045; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_trainingError, Entries= 0, Total sum= 7.15224; TH1.Print Name = TrainingHistory_TMVA_CNN_CPU_valError, Entries= 0, Total sum= 6.76684; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 6.25938; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 7.32887; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 5.40937; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 se",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:37937,Testability,test,testing,37937,"t Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 6.25938; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 7.32887; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 5.40937; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; :",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:38227,Testability,test,testing,38227,"Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 7.21788; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.175 sec ; Factory : Test method: PyTorch for Classification performance; : ; : Setup P",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:38515,Testability,test,testing,38515,weights/TMVA_CNN_Classification_TMVA_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_TMVA_CNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_CNN_Classification_PyTorch.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.175 sec ; Factory : Test method: PyTorch for Classification performance; : ; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; RecursiveScriptModule(; original_name=Sequential; (0): RecursiveScriptModule(original_name=Re,MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:38996,Testability,test,testing,38996,s: 0.00573 sec ; Factory : Test method: TMVA_DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_DNN_CPU : [dataset] : Evaluation of TMVA_DNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.023 sec ; Factory : Test method: TMVA_CNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 400; : ; TMVA_CNN_CPU : [dataset] : Evaluation of TMVA_CNN_CPU on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.149 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_cnn.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.175 sec ; Factory : Test method: PyTorch for Classification performance; : ; : Setup PyTorch Model for training; : Executing user initialization code from /home/sftnight/build/workspace/root-makedoc-master/rootspi/rdoc/src/master.build/tutorials/tmva/PyTorch_Generate_CNN_Model.py; RecursiveScriptModule(; original_name=Sequential; (0): RecursiveScriptModule(original_name=Reshape); (1): RecursiveScriptModule(original_name=Conv2d); (2): RecursiveScriptModule(original_name=ReLU); (3): RecursiveScriptModule(original_name=BatchNorm2d); (4): RecursiveScriptModule(original_name=Conv2d); (5): RecursiveScriptModule(original_name=ReLU); (6): RecursiveScriptModule(original_name=MaxPool2d); (7): RecursiveScriptModule(original_name=Flatten); (8): RecursiveScriptModule(original_name=Linear); (9): RecursiveScriptModule(original_name=ReLU); (10): RecursiveScriptModule(original_name=Linear); (11): RecursiveScriptModule(original_name=Sigmoid),MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:41396,Testability,test,testing,41396,".708; [5, 4] train loss: 0.642; [5, 8] train loss: 0.635; [5, 12] train loss: 0.633; [5] val loss: 0.707; [6, 4] train loss: 0.607; [6, 8] train loss: 0.588; [6, 12] train loss: 0.585; [6] val loss: 0.700; [7, 4] train loss: 0.553; [7, 8] train loss: 0.564; [7, 12] train loss: 0.542; [7] val loss: 0.637; [8, 4] train loss: 0.474; [8, 8] train loss: 0.446; [8, 12] train loss: 0.436; [8] val loss: 0.758; [9, 4] train loss: 0.381; [9, 8] train loss: 0.435; [9, 12] train loss: 0.456; [9] val loss: 1.218; [10, 4] train loss: 0.438; [10, 8] train loss: 0.436; [10, 12] train loss: 0.444; [10] val loss: 0.632; Finished Training on 10 Epochs!; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 2",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:41589,Testability,test,test,41589,"0.588; [6, 12] train loss: 0.585; [6] val loss: 0.700; [7, 4] train loss: 0.553; [7, 8] train loss: 0.564; [7, 12] train loss: 0.542; [7] val loss: 0.637; [8, 4] train loss: 0.474; [8, 8] train loss: 0.446; [8, 12] train loss: 0.436; [8] val loss: 0.758; [9, 4] train loss: 0.381; [9, 8] train loss: 0.435; [9, 12] train loss: 0.456; [9] val loss: 1.218; [10, 4] train loss: 0.438; [10, 8] train loss: 0.436; [10, 12] train loss: 0.444; [10] val loss: 0.632; Finished Training on 10 Epochs!; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifie",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:41851,Testability,test,test,41851,"n loss: 0.436; [8] val loss: 0.758; [9, 4] train loss: 0.381; [9, 8] train loss: 0.435; [9, 12] train loss: 0.456; [9] val loss: 1.218; [10, 4] train loss: 0.438; [10, 8] train loss: 0.436; [10, 12] train loss: 0.444; [10] val loss: 0.632; Finished Training on 10 Epochs!; running Torch code defining the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events a",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42187,Testability,test,test,42187,"g the model....; The PyTorch CNN model is created and saved as PyTorchModelCNN.pt; : Loaded pytorch train function: ; : Loaded pytorch optimizer: ; : Loaded pytorch loss function: ; : Loaded pytorch predict function: ; : Loaded model from file: PyTorchTrainedModelCNN.pt; PyTorch : [dataset] : Evaluation of PyTorch on testing sample (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : ---------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42513,Testability,test,test,42513,"le (400 events); : Elapsed time for evaluation of 400 events: 0.12 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:42765,Testability,test,test,42765," produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN_CPU; : ; TMVA_DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: TMVA_CNN_CPU; : ; TMVA_CNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from tra",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:43676,Testability,test,test,43676,"and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; Factory : Evaluate classifier: PyTorch; : ; PyTorch : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 256 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyTorch : 0.790; : dataset TMVA_CNN_CPU : 0.751; : dataset BDT : 0.750; : dataset TMVA_DNN_CPU : 0.688; : dataset PyKeras : 0.666; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset PyTorch : 0.055 (0.225) 0.430 (0.620) 0.755 (0.832); : dataset TMVA_CNN_CPU : 0.040 (0.140) 0.345 (0.430) 0.699 (0.669); : dataset BDT : 0.105 (0.310) 0.360 (0.658) 0.678 (0.860); : dataset TMVA_DNN_CPU : 0.055 (0.135) 0.263 (0.441) 0.560 (0.701); : dataset PyKeras : 0.065 (0.070) 0.222 (0.348) 0.522 (0.629); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 400 events; : ; Dataset:dataset : Created tree 'TrainTree' with 1600 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, pl",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:51026,Testability,test,test,51026,"ory(; ""TMVA_CNN_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification:Transformations=None:!Correlations"");; ; /***; ; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; **/; ; TMVA::DataLoader loader(""dataset"");; ; /***; ; ## Setup Dataset(s); ; Define input data file and signal and background trees; ; **/; ; std::unique_ptr<TFile> inputFile{TFile::Open(inputFileName)};; if (!inputFile) {; Error(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; // --- Register the training and test trees; ; auto signalTree = inputFile->Get<TTree>(""sig_tree"");; auto backgroundTree = inputFile->Get<TTree>(""bkg_tree"");; ; if (!signalTree) {; Error(""TMVA_CNN_Classification"", ""Could not find signal tree in file '%s'"", inputFileName.Data());; return;; }; if (!backgroundTree) {; Error(""TMVA_CNN_Classification"", ""Could not find background tree in file '%s'"", inputFileName.Data());; return;; }; ; int nEventsSig = signalTree->GetEntries();; int nEventsBkg = backgroundTree->GetEntries();; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSigna",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:52575,Testability,test,testing,52575,"; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(; ""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"",; nTrainSig, nTrainBkg);; ; loader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions);; ; /***; ; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; ; ; ; **/; ; /****; # Booking Methods; ; Here we bo",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:52714,Testability,test,testing,52714,"; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight);; loader.AddBackgroundTree(backgroundTree, backgroundWeight);; ; /// add event variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(; ""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"",; nTrainSig, nTrainBkg);; ; loader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions);; ; /***; ; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; ; ; ; **/; ; /****; # Booking Methods; ; Here we bo",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:52856,Testability,test,testing,52856," variables (image); /// use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize);; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; // loader.SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // It is possible also to specify the number of training and testing events,; // note we disable the computation of the correlation matrix of the input variables; ; int nTrainSig = 0.8 * nEventsSig;; int nTrainBkg = 0.8 * nEventsBkg;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(; ""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"",; nTrainSig, nTrainBkg);; ; loader.PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions);; ; /***; ; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; ; ; ; **/; ; /****; # Booking Methods; ; Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; **/; ; // Boosted Decision Trees; if (useTMVABDT) {; factory.BookMethod(&loader, TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8C.html:54390,Usability,learn,learning,54390,"nfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; ; ; ; **/; ; /****; # Booking Methods; ; Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; **/; ; // Boosted Decision Trees; if (useTMVABDT) {; factory.BookMethod(&loader, TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:""; ""UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"");; }; /**; ; #### Booking Deep Neural Network; ; Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; options; ; **/; ; if (useTMVADNN) {; ; TString layoutString(; ""Layout=DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR"");; ; // Training strategies; // one can catenate several training strings with different parameters (e.g. learning rates or regularizations; // parameters) The training string must be concatenates with the `|` delimiter; TString trainingString1(""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""MaxEpochs=10,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0."");; ; TString trainingStrategyString(""TrainingStrategy="");; trainingStrategyString += trainingString1; // + ""|"" + trainingString2 + ....; ; // Build now the full DNN Option string; ; TString dnnOptions(""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append("":"");; dnnOptions.Append(layoutString);; dnnOptions.Append("":"");; dnnOptions.Append(trainingStrategyString);; ; TString dnnMethodName = ""TMVA_DNN_CPU"";; // use GPU if available; #ifdef R__HAS_TMVAGPU; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""TMVA_DNN_GPU"";; #elif defined(R__HAS_TMVACPU); dnnOptions += "":Architecture=CPU"";; #endif; ; factory.BookMethod(&loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; /***; ### B",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8C.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:10591,Availability,avail,available,10591,".5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; ; #### Booking Deep Neural Network; ; # Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; # options; ; if useTMVADNN:; layoutString = ROOT.TString(; ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR""; ); ; # Training strategies; # one can catenate several training strings with different parameters (e.g. learning rates or regularizations; # parameters) The training string must be concatenated with the `|` delimiter; trainingString1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.""; ) # + ""|"" + trainingString2 + ...; trainingString1 += "",MaxEpochs="" + str(max_epochs); ; # Build now the full DNN Option string; dnnMethodName = ""TMVA_DNN_CPU""; ; # use GPU if available; dnnOptions = ""CPU""; if hasGPU :; dnnOptions = ""GPU""; dnnMethodName = ""TMVA_DNN_GPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=None,; WeightInitialization=""XAVIER"",; Layout=layoutString,; TrainingStrategy=trainingString1,; Architecture=dnnOptions; ); ; ; ### Book Convolutional Neural Network in TMVA; ; # For building a CNN one needs to define; ; # - Input Layout : number of channels (in this case = 1) | image height | image width; # - Batch Layout : batch size | number of channels | image size = (height*width); ; # Then one add Convolutional layers and MaxPool layers.; ; # - For Convolutional layer the option string has to be:; # - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; # width | activation function; ; # - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimen",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:12409,Availability,avail,available,12409,"h | stride height | stride width | padding height | paddig; # width | activation function; ; # - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; # conv layer equal to the input; ; # - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; # convergence; ; # - For the MaxPool layer:; # - MAXPOOL | pool height | pool width | stride height | stride width; ; # The RESHAPE layer is needed to flatten the output before the Dense layer; ; # Note that to run the CNN is required to have CPU or GPU support; ; ; if useTMVACNN:; # Training strategies.; trainingString1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0""; ); trainingString1 += "",MaxEpochs="" + str(max_epochs); ; ## New DL (CNN); cnnMethodName = ""TMVA_CNN_CPU""; cnnOptions = ""CPU""; # use GPU if available; if hasGPU:; cnnOptions = ""GPU""; cnnMethodName = ""TMVA_CNN_GPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; cnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=None,; WeightInitialization=""XAVIER"",; InputLayout=""1|16|16"",; Layout=""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"",; TrainingStrategy=trainingString1,; Architecture=cnnOptions,; ); ; ; ### Book Convolutional Neural Network in Keras using a generated model; ; ; if usePyTorchCNN:; ROOT.Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model""); pyTorchFileName = str(ROOT.gROOT.GetTutorialDir()); pyTorchFileName += ""/tmva/PyTorch_Generate_CNN_Model.py""; # check that pytorch can be imported and file defining the model exists; torch_spec = importlib.util.find_spec(""torch""); if torch_spec is not None and os.path.exists(pyTorchFileName):; #cmd = str(ROOT.TMVA.Python_Executable()) + "" ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:1363,Deployability,install,installed,1363,"tion using a toy image data set that is generated when running the example macro; ; ; # TMVA Classification Example Using a Convolutional Neural Network; ; ; ## Helper function to create input images data; ## we create a signal and background 2D histograms from 2d gaussians; ## with a location (means in X and Y) different for each event; ## The difference between signal and background is in the gaussian width.; ## The width for the background gaussian is slightly larger than the signal width by few % values; ; import os; import importlib.util; ; opt = [1, 1, 1, 1, 1]; useTMVACNN = opt[0] if len(opt) > 0 else False; useKerasCNN = opt[1] if len(opt) > 1 else False; useTMVADNN = opt[2] if len(opt) > 2 else False; useTMVABDT = opt[3] if len(opt) > 3 else False; usePyTorchCNN = opt[4] if len(opt) > 4 else False; ; tf_spec = importlib.util.find_spec(""tensorflow""); if tf_spec is None:; useKerasCNN = False; print(""TMVA_CNN_Classificaton"",""Skip using Keras since tensorflow is not installed""); else:; import tensorflow; ; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; torch_spec = importlib.util.find_spec(""torch""); if torch_spec is None:; usePyTorchCNN = False; print(""TMVA_CNN_Classificaton"",""Skip using PyTorch since torch is not installed""); else:; import torch; ; ; import ROOT; ; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; def MakeImagesTree(n, nh, nw):; # image size (nh x nw); ntot = nh * nw; fileOutName = ""images_data_16x16.root""; nRndmEvts = 10000 # number of events we use to fill each image; delta_sigma = 0.1 # 5% difference in the sigma; pixelNoise = 5; ; sX1 = 3; sY1 = 3; sX2 = sX1 + delta_sigma; sY2 = sY1 - delta_sigma; h1 = ROOT.TH2D(""h1"", ""h1"", nh, 0, 10, nw, 0, 10); h2 = ROOT.TH2D(""h2"", ""h2"", nh, 0, 10, nw, 0, 10); f1 = ROOT.TF2(""f1"", ""xygaus""); f2 = ROOT.TF2(""f2"", ""xygaus""); sgn = ROOT.TTree(""sig_tree"", ""sig",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:1753,Deployability,install,installed,1753,"# The difference between signal and background is in the gaussian width.; ## The width for the background gaussian is slightly larger than the signal width by few % values; ; import os; import importlib.util; ; opt = [1, 1, 1, 1, 1]; useTMVACNN = opt[0] if len(opt) > 0 else False; useKerasCNN = opt[1] if len(opt) > 1 else False; useTMVADNN = opt[2] if len(opt) > 2 else False; useTMVABDT = opt[3] if len(opt) > 3 else False; usePyTorchCNN = opt[4] if len(opt) > 4 else False; ; tf_spec = importlib.util.find_spec(""tensorflow""); if tf_spec is None:; useKerasCNN = False; print(""TMVA_CNN_Classificaton"",""Skip using Keras since tensorflow is not installed""); else:; import tensorflow; ; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; torch_spec = importlib.util.find_spec(""torch""); if torch_spec is None:; usePyTorchCNN = False; print(""TMVA_CNN_Classificaton"",""Skip using PyTorch since torch is not installed""); else:; import torch; ; ; import ROOT; ; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; def MakeImagesTree(n, nh, nw):; # image size (nh x nw); ntot = nh * nw; fileOutName = ""images_data_16x16.root""; nRndmEvts = 10000 # number of events we use to fill each image; delta_sigma = 0.1 # 5% difference in the sigma; pixelNoise = 5; ; sX1 = 3; sY1 = 3; sX2 = sX1 + delta_sigma; sY2 = sY1 - delta_sigma; h1 = ROOT.TH2D(""h1"", ""h1"", nh, 0, 10, nw, 0, 10); h2 = ROOT.TH2D(""h2"", ""h2"", nh, 0, 10, nw, 0, 10); f1 = ROOT.TF2(""f1"", ""xygaus""); f2 = ROOT.TF2(""f2"", ""xygaus""); sgn = ROOT.TTree(""sig_tree"", ""signal_tree""); bkg = ROOT.TTree(""bkg_tree"", ""background_tree""); ; f = TFile(fileOutName, ""RECREATE""); x1 = ROOT.std.vector[""float""](ntot); x2 = ROOT.std.vector[""float""](ntot); ; # create signal and background trees with a single branch; # an std::vector<float> of size nh x nw containing the image data; bkg.Branch(""vars"", ""std::vector<float>""",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:5638,Deployability,configurat,configuration,5638," Deep Learning for CNN"",; ); ; writeOutputFile = True; ; num_threads = 4 # use max 4 threads; max_epochs = 10 # maximum number of epochs used for training; ; ; # do enable MT running; if ""imt"" in ROOT.gROOT.GetConfigFeatures():; ROOT.EnableImplicitMT(num_threads); ROOT.gSystem.Setenv(""OMP_NUM_THREADS"", ""1"") # switch OFF MT in OpenBLAS; print(""Running with nthreads = {}"".format(ROOT.GetThreadPoolSize())); else:; print(""Running in serial mode since ROOT does not support MT""); ; ; ; ; outputFile = None; if writeOutputFile:; outputFile = TFile.Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE""); ; ; ## Create TMVA Factory; ; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; ; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:13851,Deployability,install,installed,13851,"ook Convolutional Neural Network in Keras using a generated model; ; ; if usePyTorchCNN:; ROOT.Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model""); pyTorchFileName = str(ROOT.gROOT.GetTutorialDir()); pyTorchFileName += ""/tmva/PyTorch_Generate_CNN_Model.py""; # check that pytorch can be imported and file defining the model exists; torch_spec = importlib.util.find_spec(""torch""); if torch_spec is not None and os.path.exists(pyTorchFileName):; #cmd = str(ROOT.TMVA.Python_Executable()) + "" "" + pyTorchFileName; #os.system(cmd); #import PyTorch_Generate_CNN_Model; ROOT.Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model""); factory.BookMethod(; loader,; TMVA.Types.kPyTorch,; ""PyTorch"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""PyTorchModelCNN.pt"",; FilenameTrainedModel=""PyTorchTrainedModelCNN.pt"",; NumEpochs=max_epochs,; BatchSize=100,; UserCode=str(pyTorchFileName); ); else:; ROOT.Warning(; ""TMVA_CNN_Classification"",; ""PyTorch is not installed or model building file is not existing - skip using PyTorch"",; ); ; if useKerasCNN:; ROOT.Info(""TMVA_CNN_Classification"", ""Building convolutional keras model""); # create python script which can be executed; # create 2 conv2d layer + maxpool + dense; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; ; # from keras.initializers import TruncatedNormal; # from keras import initializations; from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape; ; # from keras.callbacks import ReduceLROnPlateau; model = Sequential(); model.add(Reshape((16, 16, 1), input_shape=(256,))); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); # stride for maxpool is equal to pool size; model.add(MaxPooling2D(pool_size=(2, 2))); model.add(Flatten",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:15708,Energy Efficiency,allocate,allocates,15708,"kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); # stride for maxpool is equal to pool size; model.add(MaxPooling2D(pool_size=(2, 2))); model.add(Flatten()); model.add(Dense(64, activation=""tanh"")); # model.add(Dropout(0.2)); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_cnn.h5""); model.summary(); ; if not os.path.exists(""model_cnn.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_CNN_Classification"", ""Booking convolutional keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_cnn.h5"",; FilenameTrainedModel=""trained_model_cnn.h5"",; NumEpochs=max_epochs,; BatchSize=100,; GpuOptions=""allow_growth=True"",; ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ; ## Train Methods; ; factory.TrainAllMethods(); ; ## Test and Evaluate Methods; ; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; ## Plot ROC Curve; ; c1 = factory.GetROCCurve(loader); c1.Draw(); ; # close outputfile to save output file; outputFile.Close(); lenOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char cha",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:5638,Modifiability,config,configuration,5638," Deep Learning for CNN"",; ); ; writeOutputFile = True; ; num_threads = 4 # use max 4 threads; max_epochs = 10 # maximum number of epochs used for training; ; ; # do enable MT running; if ""imt"" in ROOT.gROOT.GetConfigFeatures():; ROOT.EnableImplicitMT(num_threads); ROOT.gSystem.Setenv(""OMP_NUM_THREADS"", ""1"") # switch OFF MT in OpenBLAS; print(""Running with nthreads = {}"".format(ROOT.GetThreadPoolSize())); else:; print(""Running in serial mode since ROOT does not support MT""); ; ; ; ; outputFile = None; if writeOutputFile:; outputFile = TFile.Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE""); ; ; ## Create TMVA Factory; ; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; ; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:5871,Modifiability,variab,variables,5871,"; print(""Running with nthreads = {}"".format(ROOT.GetThreadPoolSize())); else:; print(""Running in serial mode since ROOT does not support MT""); ; ; ; ; outputFile = None; if writeOutputFile:; outputFile = TFile.Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE""); ; ; ## Create TMVA Factory; ; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; ; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:5934,Modifiability,variab,variables,5934,"; print(""Running with nthreads = {}"".format(ROOT.GetThreadPoolSize())); else:; print(""Running in serial mode since ROOT does not support MT""); ; ; ; ; outputFile = None; if writeOutputFile:; outputFile = TFile.Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE""); ; ; ## Create TMVA Factory; ; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; ; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:6248,Modifiability,variab,variables,6248,"ctory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = sign",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:6280,Modifiability,variab,variables,6280,"ctory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = sign",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:6360,Modifiability,variab,variable,6360,"ctory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = sign",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:7609,Modifiability,variab,variables,7609,"tup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # no",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:7671,Modifiability,variab,variable,7671,"e = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrai",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:7788,Modifiability,variab,variables,7788,"""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:8669,Modifiability,variab,variables,8669,"ckgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random"",; SplitSeed=100,; NormMode=""NumEvents"",; V=False,; CalcCorrelations=False,; ); ; ; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; ; # signalTree.Print();; ; # Booking Methods; ; # Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; ; # Boosted Decision Trees; if useTMVABDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=400,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:11236,Modifiability,layers,layers,11236,"egies; # one can catenate several training strings with different parameters (e.g. learning rates or regularizations; # parameters) The training string must be concatenated with the `|` delimiter; trainingString1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.""; ) # + ""|"" + trainingString2 + ...; trainingString1 += "",MaxEpochs="" + str(max_epochs); ; # Build now the full DNN Option string; dnnMethodName = ""TMVA_DNN_CPU""; ; # use GPU if available; dnnOptions = ""CPU""; if hasGPU :; dnnOptions = ""GPU""; dnnMethodName = ""TMVA_DNN_GPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=None,; WeightInitialization=""XAVIER"",; Layout=layoutString,; TrainingStrategy=trainingString1,; Architecture=dnnOptions; ); ; ; ### Book Convolutional Neural Network in TMVA; ; # For building a CNN one needs to define; ; # - Input Layout : number of channels (in this case = 1) | image height | image width; # - Batch Layout : batch size | number of channels | image size = (height*width); ; # Then one add Convolutional layers and MaxPool layers.; ; # - For Convolutional layer the option string has to be:; # - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; # width | activation function; ; # - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; # conv layer equal to the input; ; # - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; # convergence; ; # - For the MaxPool layer:; # - MAXPOOL | pool height | pool width | stride height | stride width; ; # The RESHAPE layer is needed to flatten the output before the Dense layer; ; # Note that to run the CNN is required to have CPU or GPU su",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:11255,Modifiability,layers,layers,11255,"egies; # one can catenate several training strings with different parameters (e.g. learning rates or regularizations; # parameters) The training string must be concatenated with the `|` delimiter; trainingString1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.""; ) # + ""|"" + trainingString2 + ...; trainingString1 += "",MaxEpochs="" + str(max_epochs); ; # Build now the full DNN Option string; dnnMethodName = ""TMVA_DNN_CPU""; ; # use GPU if available; dnnOptions = ""CPU""; if hasGPU :; dnnOptions = ""GPU""; dnnMethodName = ""TMVA_DNN_GPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=None,; WeightInitialization=""XAVIER"",; Layout=layoutString,; TrainingStrategy=trainingString1,; Architecture=dnnOptions; ); ; ; ### Book Convolutional Neural Network in TMVA; ; # For building a CNN one needs to define; ; # - Input Layout : number of channels (in this case = 1) | image height | image width; # - Batch Layout : batch size | number of channels | image size = (height*width); ; # Then one add Convolutional layers and MaxPool layers.; ; # - For Convolutional layer the option string has to be:; # - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; # width | activation function; ; # - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; # conv layer equal to the input; ; # - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; # convergence; ; # - For the MaxPool layer:; # - MAXPOOL | pool height | pool width | stride height | stride width; ; # The RESHAPE layer is needed to flatten the output before the Dense layer; ; # Note that to run the CNN is required to have CPU or GPU su",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:14335,Modifiability,layers,layers,14335," + pyTorchFileName; #os.system(cmd); #import PyTorch_Generate_CNN_Model; ROOT.Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model""); factory.BookMethod(; loader,; TMVA.Types.kPyTorch,; ""PyTorch"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""PyTorchModelCNN.pt"",; FilenameTrainedModel=""PyTorchTrainedModelCNN.pt"",; NumEpochs=max_epochs,; BatchSize=100,; UserCode=str(pyTorchFileName); ); else:; ROOT.Warning(; ""TMVA_CNN_Classification"",; ""PyTorch is not installed or model building file is not existing - skip using PyTorch"",; ); ; if useKerasCNN:; ROOT.Info(""TMVA_CNN_Classification"", ""Building convolutional keras model""); # create python script which can be executed; # create 2 conv2d layer + maxpool + dense; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; ; # from keras.initializers import TruncatedNormal; # from keras import initializations; from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape; ; # from keras.callbacks import ReduceLROnPlateau; model = Sequential(); model.add(Reshape((16, 16, 1), input_shape=(256,))); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); # stride for maxpool is equal to pool size; model.add(MaxPooling2D(pool_size=(2, 2))); model.add(Flatten()); model.add(Dense(64, activation=""tanh"")); # model.add(Dropout(0.2)); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_cnn.h5""); model.summary(); ; if not os.path.exists(""model_cnn.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_CNN_Classification"", ""Booking c",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:4445,Performance,multi-thread,multi-thread,4445," k in range(nh):; for l in range(nw):; m = k * nw + l; # add some noise in each bin; x1[m] = h1.GetBinContent(k + 1, l + 1) + ROOT.gRandom.Gaus(0, pixelNoise); x2[m] = h2.GetBinContent(k + 1, l + 1) + ROOT.gRandom.Gaus(0, pixelNoise); ; sgn.Fill(); bkg.Fill(); ; sgn.Write(); bkg.Write(); ; print(""Signal and background tree with images data written to the file %s"", f.GetName()); sgn.Print(); bkg.Print(); f.Close(); ; hasGPU = ""tmva-gpu"" in ROOT.gROOT.GetConfigFeatures(); hasCPU = ""tmva-cpu"" in ROOT.gROOT.GetConfigFeatures(); ; nevt = 1000 # use a larger value to get better results; ; if (not hasCPU and not hasGPU) :; ROOT.Warning(""TMVA_CNN_Classificaton"",""ROOT is not supporting tmva-cpu and tmva-gpu skip using TMVA-DNN and TMVA-CNN""); useTMVACNN = False; useTMVADNN = False; ; if not ""tmva-pymva"" in ROOT.gROOT.GetConfigFeatures():; useKerasCNN = False; usePyTorchCNN = False; else:; TMVA.PyMethodBase.PyInitialize(); ; if not useTMVACNN:; ROOT.Warning(; ""TMVA_CNN_Classificaton"",; ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for CNN"",; ); ; writeOutputFile = True; ; num_threads = 4 # use max 4 threads; max_epochs = 10 # maximum number of epochs used for training; ; ; # do enable MT running; if ""imt"" in ROOT.gROOT.GetConfigFeatures():; ROOT.EnableImplicitMT(num_threads); ROOT.gSystem.Setenv(""OMP_NUM_THREADS"", ""1"") # switch OFF MT in OpenBLAS; print(""Running with nthreads = {}"".format(ROOT.GetThreadPoolSize())); else:; print(""Running in serial mode since ROOT does not support MT""); ; ; ; ; outputFile = None; if writeOutputFile:; outputFile = TFile.Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE""); ; ; ## Create TMVA Factory; ; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; ; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight fi",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:5184,Performance,perform,performance,5184,"; useTMVADNN = False; ; if not ""tmva-pymva"" in ROOT.gROOT.GetConfigFeatures():; useKerasCNN = False; usePyTorchCNN = False; else:; TMVA.PyMethodBase.PyInitialize(); ; if not useTMVACNN:; ROOT.Warning(; ""TMVA_CNN_Classificaton"",; ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for CNN"",; ); ; writeOutputFile = True; ; num_threads = 4 # use max 4 threads; max_epochs = 10 # maximum number of epochs used for training; ; ; # do enable MT running; if ""imt"" in ROOT.gROOT.GetConfigFeatures():; ROOT.EnableImplicitMT(num_threads); ROOT.gSystem.Setenv(""OMP_NUM_THREADS"", ""1"") # switch OFF MT in OpenBLAS; print(""Running with nthreads = {}"".format(ROOT.GetThreadPoolSize())); else:; print(""Running in serial mode since ROOT does not support MT""); ; ; ; ; outputFile = None; if writeOutputFile:; outputFile = TFile.Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE""); ; ; ## Create TMVA Factory; ; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; ; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The n",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:6554,Performance,load,loader,6554,"tput file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); lo",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:7486,Performance,load,loader,7486,"e input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # load",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:7534,Performance,load,loader,7534,"pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:7707,Performance,load,loader,7707,"e = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrai",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:7985,Performance,load,loader,7985,"""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:8451,Performance,load,loader,8451,"vent-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random"",; SplitSeed=100,; NormMode=""NumEvents"",; V=False,; CalcCorrelations=False,; ); ; ; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; ; # signalTree.Print();; ; # Booking Methods; ; # Here we book the TMVA methods. We book a Boosted Decisio",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:8817,Performance,load,loader,8817,"weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random"",; SplitSeed=100,; NormMode=""NumEvents"",; V=False,; CalcCorrelations=False,; ); ; ; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; ; # signalTree.Print();; ; # Booking Methods; ; # Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; ; # Boosted Decision Trees; if useTMVABDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=400,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; ; #### Booking Deep Neural Network; ; # Here we book the DNN of TMVA. See the example TMVA_Higgs_Classif",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:9443,Performance,load,loader,9443,"ing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random"",; SplitSeed=100,; NormMode=""NumEvents"",; V=False,; CalcCorrelations=False,; ); ; ; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; ; # signalTree.Print();; ; # Booking Methods; ; # Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; ; # Boosted Decision Trees; if useTMVABDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=400,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; ; #### Booking Deep Neural Network; ; # Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; # options; ; if useTMVADNN:; layoutString = ROOT.TString(; ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR""; ); ; # Training strategies; # one can catenate several training strings with different parameters (e.g. learning rates or regularizations; # parameters) The training string must be concatenated with the `|` delimiter; trainingString1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.""; ) # + ""|"" + trainingString2 +",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:10710,Performance,load,loader,10710," Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; # options; ; if useTMVADNN:; layoutString = ROOT.TString(; ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR""; ); ; # Training strategies; # one can catenate several training strings with different parameters (e.g. learning rates or regularizations; # parameters) The training string must be concatenated with the `|` delimiter; trainingString1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.""; ) # + ""|"" + trainingString2 + ...; trainingString1 += "",MaxEpochs="" + str(max_epochs); ; # Build now the full DNN Option string; dnnMethodName = ""TMVA_DNN_CPU""; ; # use GPU if available; dnnOptions = ""CPU""; if hasGPU :; dnnOptions = ""GPU""; dnnMethodName = ""TMVA_DNN_GPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=None,; WeightInitialization=""XAVIER"",; Layout=layoutString,; TrainingStrategy=trainingString1,; Architecture=dnnOptions; ); ; ; ### Book Convolutional Neural Network in TMVA; ; # For building a CNN one needs to define; ; # - Input Layout : number of channels (in this case = 1) | image height | image width; # - Batch Layout : batch size | number of channels | image size = (height*width); ; # Then one add Convolutional layers and MaxPool layers.; ; # - For Convolutional layer the option string has to be:; # - CONV | number of units | filter height | filter width | stride height | stride width | padding height | paddig; # width | activation function; ; # - note in this case we are using a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; # conv layer equal to the input; ; # - note we use after the first convolutional layer a batch normalization layer. This seem",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:12507,Performance,load,loader,12507," a filer 3x3 and padding=1 and stride=1 so we get the output dimension of the; # conv layer equal to the input; ; # - note we use after the first convolutional layer a batch normalization layer. This seems to help significantly the; # convergence; ; # - For the MaxPool layer:; # - MAXPOOL | pool height | pool width | stride height | stride width; ; # The RESHAPE layer is needed to flatten the output before the Dense layer; ; # Note that to run the CNN is required to have CPU or GPU support; ; ; if useTMVACNN:; # Training strategies.; trainingString1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0""; ); trainingString1 += "",MaxEpochs="" + str(max_epochs); ; ## New DL (CNN); cnnMethodName = ""TMVA_CNN_CPU""; cnnOptions = ""CPU""; # use GPU if available; if hasGPU:; cnnOptions = ""GPU""; cnnMethodName = ""TMVA_CNN_GPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; cnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=None,; WeightInitialization=""XAVIER"",; InputLayout=""1|16|16"",; Layout=""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"",; TrainingStrategy=trainingString1,; Architecture=cnnOptions,; ); ; ; ### Book Convolutional Neural Network in Keras using a generated model; ; ; if usePyTorchCNN:; ROOT.Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model""); pyTorchFileName = str(ROOT.gROOT.GetTutorialDir()); pyTorchFileName += ""/tmva/PyTorch_Generate_CNN_Model.py""; # check that pytorch can be imported and file defining the model exists; torch_spec = importlib.util.find_spec(""torch""); if torch_spec is not None and os.path.exists(pyTorchFileName):; #cmd = str(ROOT.TMVA.Python_Executable()) + "" "" + pyTorchFileName; #os.system(cmd); #import PyTorch_Generate_CNN_Model; ROOT.Info(""TMVA_CNN_Classification"", ""Booking PyTorch ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:13542,Performance,load,loader,13542,"Name,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=None,; WeightInitialization=""XAVIER"",; InputLayout=""1|16|16"",; Layout=""CONV|10|3|3|1|1|1|1|RELU,BNORM,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|100|RELU,DENSE|1|LINEAR"",; TrainingStrategy=trainingString1,; Architecture=cnnOptions,; ); ; ; ### Book Convolutional Neural Network in Keras using a generated model; ; ; if usePyTorchCNN:; ROOT.Info(""TMVA_CNN_Classification"", ""Using Convolutional PyTorch Model""); pyTorchFileName = str(ROOT.gROOT.GetTutorialDir()); pyTorchFileName += ""/tmva/PyTorch_Generate_CNN_Model.py""; # check that pytorch can be imported and file defining the model exists; torch_spec = importlib.util.find_spec(""torch""); if torch_spec is not None and os.path.exists(pyTorchFileName):; #cmd = str(ROOT.TMVA.Python_Executable()) + "" "" + pyTorchFileName; #os.system(cmd); #import PyTorch_Generate_CNN_Model; ROOT.Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model""); factory.BookMethod(; loader,; TMVA.Types.kPyTorch,; ""PyTorch"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""PyTorchModelCNN.pt"",; FilenameTrainedModel=""PyTorchTrainedModelCNN.pt"",; NumEpochs=max_epochs,; BatchSize=100,; UserCode=str(pyTorchFileName); ); else:; ROOT.Warning(; ""TMVA_CNN_Classification"",; ""PyTorch is not installed or model building file is not existing - skip using PyTorch"",; ); ; if useKerasCNN:; ROOT.Info(""TMVA_CNN_Classification"", ""Building convolutional keras model""); # create python script which can be executed; # create 2 conv2d layer + maxpool + dense; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; ; # from keras.initializers import TruncatedNormal; # from keras import initializations; from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape; ; # from keras.callbacks import ReduceLROnPlateau; model = Sequential(); model.add(Reshape((16, 16, 1), input_shape=(256,))); model",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:14200,Performance,optimiz,optimizers,14200,"s; torch_spec = importlib.util.find_spec(""torch""); if torch_spec is not None and os.path.exists(pyTorchFileName):; #cmd = str(ROOT.TMVA.Python_Executable()) + "" "" + pyTorchFileName; #os.system(cmd); #import PyTorch_Generate_CNN_Model; ROOT.Info(""TMVA_CNN_Classification"", ""Booking PyTorch CNN model""); factory.BookMethod(; loader,; TMVA.Types.kPyTorch,; ""PyTorch"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""PyTorchModelCNN.pt"",; FilenameTrainedModel=""PyTorchTrainedModelCNN.pt"",; NumEpochs=max_epochs,; BatchSize=100,; UserCode=str(pyTorchFileName); ); else:; ROOT.Warning(; ""TMVA_CNN_Classification"",; ""PyTorch is not installed or model building file is not existing - skip using PyTorch"",; ); ; if useKerasCNN:; ROOT.Info(""TMVA_CNN_Classification"", ""Building convolutional keras model""); # create python script which can be executed; # create 2 conv2d layer + maxpool + dense; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; ; # from keras.initializers import TruncatedNormal; # from keras import initializations; from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape; ; # from keras.callbacks import ReduceLROnPlateau; model = Sequential(); model.add(Reshape((16, 16, 1), input_shape=(256,))); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); # stride for maxpool is equal to pool size; model.add(MaxPooling2D(pool_size=(2, 2))); model.add(Flatten()); model.add(Dense(64, activation=""tanh"")); # model.add(Dropout(0.2)); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_cnn.h5""); model.summary(); ; if not os.path.exists(""model_cnn.h5""):; raise FileNotFoundError(""Error c",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:15032,Performance,optimiz,optimizer,15032,"e python script which can be executed; # create 2 conv2d layer + maxpool + dense; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; ; # from keras.initializers import TruncatedNormal; # from keras import initializations; from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape; ; # from keras.callbacks import ReduceLROnPlateau; model = Sequential(); model.add(Reshape((16, 16, 1), input_shape=(256,))); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); # stride for maxpool is equal to pool size; model.add(MaxPooling2D(pool_size=(2, 2))); model.add(Flatten()); model.add(Dense(64, activation=""tanh"")); # model.add(Dropout(0.2)); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_cnn.h5""); model.summary(); ; if not os.path.exists(""model_cnn.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_CNN_Classification"", ""Booking convolutional keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_cnn.h5"",; FilenameTrainedModel=""trained_model_cnn.h5"",; NumEpochs=max_epochs,; BatchSize=100,; GpuOptions=""allow_growth=True"",; ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ; ## Train Methods; ; factory.TrainAllMethods(); ; ## Test and Evaluate Methods; ; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; ## Plot ROC Curve; ; c1 = factory.GetROCCurve(loader); c1.Draw(); ; # close outputfile to save output file; outputFile.Close(); lenOption_t Option_t ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:15430,Performance,load,loader,15430,"lbacks import ReduceLROnPlateau; model = Sequential(); model.add(Reshape((16, 16, 1), input_shape=(256,))); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); model.add(Conv2D(10, kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); # stride for maxpool is equal to pool size; model.add(MaxPooling2D(pool_size=(2, 2))); model.add(Flatten()); model.add(Dense(64, activation=""tanh"")); # model.add(Dropout(0.2)); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_cnn.h5""); model.summary(); ; if not os.path.exists(""model_cnn.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_CNN_Classification"", ""Booking convolutional keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_cnn.h5"",; FilenameTrainedModel=""trained_model_cnn.h5"",; NumEpochs=max_epochs,; BatchSize=100,; GpuOptions=""allow_growth=True"",; ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ; ## Train Methods; ; factory.TrainAllMethods(); ; ## Test and Evaluate Methods; ; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; ## Plot ROC Curve; ; c1 = factory.GetROCCurve(loader); c1.Draw(); ; # close outputfile to save output file; outputFile.Close(); lenOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid win",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:15927,Performance,load,loader,15927,".add(Dropout(0.2)); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_cnn.h5""); model.summary(); ; if not os.path.exists(""model_cnn.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_CNN_Classification"", ""Booking convolutional keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_cnn.h5"",; FilenameTrainedModel=""trained_model_cnn.h5"",; NumEpochs=max_epochs,; BatchSize=100,; GpuOptions=""allow_growth=True"",; ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ; ## Train Methods; ; factory.TrainAllMethods(); ; ## Test and Evaluate Methods; ; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; ## Plot ROC Curve; ; c1 = factory.GetROCCurve(loader); c1.Draw(); ; # close outputfile to save output file; outputFile.Close(); lenOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t UChar_t lenDefinition TGWin32VirtualXProxy.cxx:249; formatOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle Get",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:18438,Performance,multi-thread,multi-threading,18438,"t TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char char ret_data h unsigned char height h Atom_t Int_t ULong_t ULong_t unsigned char prop_list Atom_t Atom_t Atom_t Time_t formatDefinition TGWin32VirtualXProxy.cxx:249; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; AuthorHarshal Shende ; Definition in file TMVA_CNN_Classification.py. tutorialstmvaTMVA_CNN_Classification.py. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:1448,Safety,avoid,avoid,1448,"tion using a toy image data set that is generated when running the example macro; ; ; # TMVA Classification Example Using a Convolutional Neural Network; ; ; ## Helper function to create input images data; ## we create a signal and background 2D histograms from 2d gaussians; ## with a location (means in X and Y) different for each event; ## The difference between signal and background is in the gaussian width.; ## The width for the background gaussian is slightly larger than the signal width by few % values; ; import os; import importlib.util; ; opt = [1, 1, 1, 1, 1]; useTMVACNN = opt[0] if len(opt) > 0 else False; useKerasCNN = opt[1] if len(opt) > 1 else False; useTMVADNN = opt[2] if len(opt) > 2 else False; useTMVABDT = opt[3] if len(opt) > 3 else False; usePyTorchCNN = opt[4] if len(opt) > 4 else False; ; tf_spec = importlib.util.find_spec(""tensorflow""); if tf_spec is None:; useKerasCNN = False; print(""TMVA_CNN_Classificaton"",""Skip using Keras since tensorflow is not installed""); else:; import tensorflow; ; # PyTorch has to be imported before ROOT to avoid crashes because of clashing; # std::regexp symbols that are exported by cppyy.; # See also: https://github.com/wlav/cppyy/issues/227; torch_spec = importlib.util.find_spec(""torch""); if torch_spec is None:; usePyTorchCNN = False; print(""TMVA_CNN_Classificaton"",""Skip using PyTorch since torch is not installed""); else:; import torch; ; ; import ROOT; ; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; def MakeImagesTree(n, nh, nw):; # image size (nh x nw); ntot = nh * nw; fileOutName = ""images_data_16x16.root""; nRndmEvts = 10000 # number of events we use to fill each image; delta_sigma = 0.1 # 5% difference in the sigma; pixelNoise = 5; ; sX1 = 3; sY1 = 3; sX2 = sX1 + delta_sigma; sY2 = sY1 - delta_sigma; h1 = ROOT.TH2D(""h1"", ""h1"", nh, 0, 10, nw, 0, 10); h2 = ROOT.TH2D(""h2"", ""h2"", nh, 0, 10, nw, 0, 10); f1 = ROOT.TF2(""f1"", ""xygaus""); f2 = ROOT.TF2(""f2"", ""xygaus""); sgn = ROOT.TTree(""sig_tree"", ""sig",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:5888,Safety,avoid,avoid,5888,"; print(""Running with nthreads = {}"".format(ROOT.GetThreadPoolSize())); else:; print(""Running in serial mode since ROOT does not support MT""); ; ; ; ; outputFile = None; if writeOutputFile:; outputFile = TFile.Open(""TMVA_CNN_ClassificationOutput.root"", ""RECREATE""); ; ; ## Create TMVA Factory; ; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; ; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; # - The first argument is the base of the name of all the output; # weight files in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; ; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the; # option string; ; # - note that we disable any pre-transformation of the input variables and we avoid computing correlations between; # input variables; ; ; factory = TMVA.Factory(; ""TMVA_CNN_Classification"",; outputFile,; V=False,; ROC=True,; Silent=False,; Color=True,; AnalysisType=""Classification"",; Transformations=None,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:15699,Safety,avoid,avoid,15699,"kernel_size=(3, 3), kernel_initializer=""TruncatedNormal"", activation=""relu"", padding=""same"")); # stride for maxpool is equal to pool size; model.add(MaxPooling2D(pool_size=(2, 2))); model.add(Flatten()); model.add(Dense(64, activation=""tanh"")); # model.add(Dropout(0.2)); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_cnn.h5""); model.summary(); ; if not os.path.exists(""model_cnn.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_CNN_Classification"", ""Booking convolutional keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_cnn.h5"",; FilenameTrainedModel=""trained_model_cnn.h5"",; NumEpochs=max_epochs,; BatchSize=100,; GpuOptions=""allow_growth=True"",; ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ; ## Train Methods; ; factory.TrainAllMethods(); ; ## Test and Evaluate Methods; ; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; ## Plot ROC Curve; ; c1 = factory.GetROCCurve(loader); c1.Draw(); ; # close outputfile to save output file; outputFile.Close(); lenOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void char Point_t Rectangle_t WindowAttributes_t Float_t Float_t Float_t Int_t Int_t UInt_t UInt_t Rectangle_t Int_t Int_t Window_t TString Int_t GCValues_t GetPrimarySelectionOwner GetDisplay GetScreen GetColormap GetNativeEvent const char const char dpyName wid window const char font_name cursor keysym reg const char only_if_exist regb h Point_t winding char text const char depth char const char Int_t count const char ColorStruct_t color const char Pixmap_t Pixmap_t PictureAttributes_t attr const char cha",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:7117,Testability,test,test,7117,"ne,; Correlations=False,; ); ; ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; ; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; # In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree; ; loader = TMVA.DataLoader(""dataset""); ; ; ## Setup Dataset(s); ; # Define input data file and signal and background trees; ; ; imgSize = 16 * 16; inputFileName = ""images_data_16x16.root""; ; # if the input file does not exist create it; if ROOT.gSystem.AccessPathName(inputFileName):; MakeImagesTree(nevt, 16, 16); ; inputFile = TFile.Open(inputFileName); if inputFile is None:; ROOT.Warning(""TMVA_CNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data()); ; ; # inputFileName = ""tmva_class_example.root""; ; ; # --- Register the training and test trees; ; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); ; nEventsSig = signalTree.GetEntries(); nEventsBkg = backgroundTree.GetEntries(); ; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be differ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:8306,Testability,test,testing,8306,"vent-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random"",; SplitSeed=100,; NormMode=""NumEvents"",; V=False,; CalcCorrelations=False,; ); ; ; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; ; # signalTree.Print();; ; # Booking Methods; ; # Here we book the TMVA methods. We book a Boosted Decisio",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:8439,Testability,test,testing,8439,"vent-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; ; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random"",; SplitSeed=100,; NormMode=""NumEvents"",; V=False,; CalcCorrelations=False,; ); ; ; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; ; # signalTree.Print();; ; # Booking Methods; ; # Here we book the TMVA methods. We book a Boosted Decisio",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:8579,Testability,test,testing,8579,"ckgroundWeight); ; ## add event variables (image); ## use new method (from ROOT 6.20 to add a variable array for all image data); loader.AddVariablesArray(""vars"", imgSize); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = """" # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = """" # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader.PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # It is possible also to specify the number of training and testing events,; # note we disable the computation of the correlation matrix of the input variables; ; nTrainSig = 0.8 * nEventsSig; nTrainBkg = 0.8 * nEventsBkg; ; # build the string options for DataLoader::PrepareTrainingAndTestTree; ; loader.PrepareTrainingAndTestTree(; mycuts,; mycutb,; nTrain_Signal=nTrainSig,; nTrain_Background=nTrainBkg,; SplitMode=""Random"",; SplitSeed=100,; NormMode=""NumEvents"",; V=False,; CalcCorrelations=False,; ); ; ; # DataSetInfo : [dataset] : Added class ""Signal""; # : Add Tree sig_tree of type Signal with 10000 events; # DataSetInfo : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; ; # signalTree.Print();; ; # Booking Methods; ; # Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; ; # Boosted Decision Trees; if useTMVABDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=400,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; ",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__CNN__Classification_8py.html:10079,Usability,learn,learning,10079,"o : [dataset] : Added class ""Background""; # : Add Tree bkg_tree of type Background with 10000 events; ; # signalTree.Print();; ; # Booking Methods; ; # Here we book the TMVA methods. We book a Boosted Decision Tree method (BDT); ; ; # Boosted Decision Trees; if useTMVABDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=400,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; ; #### Booking Deep Neural Network; ; # Here we book the DNN of TMVA. See the example TMVA_Higgs_Classification.C for a detailed description of the; # options; ; if useTMVADNN:; layoutString = ROOT.TString(; ""DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,BNORM,DENSE|100|RELU,DENSE|1|LINEAR""; ); ; # Training strategies; # one can catenate several training strings with different parameters (e.g. learning rates or regularizations; # parameters) The training string must be concatenated with the `|` delimiter; trainingString1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,Repetitions=1,""; ""ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,""; ""WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.""; ) # + ""|"" + trainingString2 + ...; trainingString1 += "",MaxEpochs="" + str(max_epochs); ; # Build now the full DNN Option string; dnnMethodName = ""TMVA_DNN_CPU""; ; # use GPU if available; dnnOptions = ""CPU""; if hasGPU :; dnnOptions = ""GPU""; dnnMethodName = ""TMVA_DNN_GPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=None,; WeightInitialization=""XAVIER"",; Layout=layoutString,; TrainingStrategy=trainingString1,; Architecture=dnnOptions; ); ; ; ### Book Convolutional Neural Network in TMVA; ; # For building a CNN one needs to define; ; # - Input Layout : number of channels (in this case = 1) | image height | image width; # - Batch Layout : batch",MatchSource.WIKI,doc/master/TMVA__CNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__CNN__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:12134,Availability,error,error,12134,"Steps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24053,Availability,error,error,24053,"42343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 5 Input = ( 1, 1, 7 ) Batch size = 128 Loss function = C; Layer 0 DENSE Layer: ( Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.5886",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24321,Availability,error,error,24321,"Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configurat",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24430,Availability,error,error,24430," = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24540,Availability,error,error,24540," Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24704,Availability,error,error,24704," Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24868,Availability,error,error,24868,"ng; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; :",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:25302,Availability,error,error,25302,m Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; : 23 | 0.536715 0.589196 0.591864 0.0475573 20459.1 7; : 24 | 0.537796 0.579234 0.591655 0.0473491 20459.1 8; : 25 | 0.534347 0.586832 0.601681 0.047397 20090.8 9; : 26 | 0.531742 0.582802 0.613513 0.0496834 19750.6 10; : 27 | 0.532714 0.581886 0.617515 0.0480417 19554.9 11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Ev,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:25464,Availability,error,error,25464, 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; : 23 | 0.536715 0.589196 0.591864 0.0475573 20459.1 7; : 24 | 0.537796 0.579234 0.591655 0.0473491 20459.1 8; : 25 | 0.534347 0.586832 0.601681 0.047397 20090.8 9; : 26 | 0.531742 0.582802 0.613513 0.0496834 19750.6 10; : 27 | 0.532714 0.581886 0.617515 0.0480417 19554.9 11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.253 sec ; : Creating xml weight file: ␛[0;36mdataset,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59076,Availability,down,download,59076,"alse; // Multi Layer Perceptron (old TMVA NN implementation); bool useBDT = true; // Boosted Decision Tree; bool useDL = true; // TMVA Deep learning ( CPU or GPU); bool useKeras = true; // Keras Deep learning; bool usePyTorch = true; // PyTorch Deep learning; ; TMVA::Tools::Instance();; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKeras = false;; usePyTorch = false;; #endif; ; auto outputFile = TFile::Open(""Higgs_ClassificationOutput.root"", ""RECREATE"");; ; TMVA::Factory factory(""TMVA_Higgs_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification"" );; ; /**; ; ## Setup Dataset(s); ; Define now input data file and signal and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59340,Availability,down,downloaded,59340,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:64784,Availability,error,error,64784,"parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; One can then concatenate different training strategy with different parameters. The training strategy are separated by; the ``""|""`` separator.; ; - Optimizer; - Learning rate; - Momentum (valid for SGD and RMSPROP); - Regularization and Weight Decay; - Dropout; - Max number of epochs; - Convergence steps. if the test error will not decrease after that value the training will stop; - Batch size (This value must be the same specified in the input layout); - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; In addition to input layout, batch layout and training strategy we add now:; ; - Type of Loss function (e.g. CROSSENTROPY); - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); - Variable Transformation; - Type of Architecture (e.g. CPU, GPU, Standard); ; We can then book the DL method using the built option string; ; ***/; ; if (useDL) {; ; bool useDLGPU = false;; #ifdef R__HAS_TMVAGPU; useDLGPU = true;; #endif; ; // Define DNN layout; TString inputLayoutString = ""InputLayout=1|1|7"";; TString batchLayoutString= ""BatchLayout=1|128|7"";; TString layoutString (""Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:64970,Availability,error,error,64970,"parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; One can then concatenate different training strategy with different parameters. The training strategy are separated by; the ``""|""`` separator.; ; - Optimizer; - Learning rate; - Momentum (valid for SGD and RMSPROP); - Regularization and Weight Decay; - Dropout; - Max number of epochs; - Convergence steps. if the test error will not decrease after that value the training will stop; - Batch size (This value must be the same specified in the input layout); - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; In addition to input layout, batch layout and training strategy we add now:; ; - Type of Loss function (e.g. CROSSENTROPY); - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); - Variable Transformation; - Type of Architecture (e.g. CPU, GPU, Standard); ; We can then book the DL method using the built option string; ; ***/; ; if (useDL) {; ; bool useDLGPU = false;; #ifdef R__HAS_TMVAGPU; useDLGPU = true;; #endif; ; // Define DNN layout; TString inputLayoutString = ""InputLayout=1|1|7"";; TString batchLayoutString= ""BatchLayout=1|128|7"";; TString layoutString (""Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:69925,Availability,error,error,69925,"edModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Methods; ; Here we train all the previously booked methods.; ; */; ; factory.TrainAllMethods();; ; /**; ## Test all methods; ; Now we test and evaluate all methods using the test data set; */; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// after we get the ROC curve and we display; ; auto c1 = factory.GetROCCurve(loader);; c1->Draw();; ; /// at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; /// to display additional plots; ; outputFile->Close();; ; ; }; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; gROOT#define gROOTDefinition TROOT.h:406; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TDirectoryFile::GetTObject * Get(const char *namecycle) overrideReturn pointer to object identified by namecycle.Definition TDirectoryFile.cxx:937; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCa",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17055,Deployability,configurat,configuration,17055,"j : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed ti",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:20298,Deployability,configurat,configuration,20298,"optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0105 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0038 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 7000 bkg: 7000; : #eve",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24344,Deployability,configurat,configuration,24344,"Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configurat",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24453,Deployability,configurat,configuration,24453," = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24563,Deployability,configurat,configuration,24563," Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24727,Deployability,configurat,configuration,24727," Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24891,Deployability,configurat,configuration,24891,"ng; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; :",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:25325,Deployability,configurat,configuration,25325,m Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; : 23 | 0.536715 0.589196 0.591864 0.0475573 20459.1 7; : 24 | 0.537796 0.579234 0.591655 0.0473491 20459.1 8; : 25 | 0.534347 0.586832 0.601681 0.047397 20090.8 9; : 26 | 0.531742 0.582802 0.613513 0.0496834 19750.6 10; : 27 | 0.532714 0.581886 0.617515 0.0480417 19554.9 11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Ev,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:25487,Deployability,configurat,configuration,25487, 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; : 23 | 0.536715 0.589196 0.591864 0.0475573 20459.1 7; : 24 | 0.537796 0.579234 0.591655 0.0473491 20459.1 8; : 25 | 0.534347 0.586832 0.601681 0.047397 20090.8 9; : 26 | 0.531742 0.582802 0.613513 0.0496834 19750.6 10; : 27 | 0.532714 0.581886 0.617515 0.0480417 19554.9 11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.253 sec ; : Creating xml weight file: ␛[0;36mdataset,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:57583,Deployability,configurat,configuration,57583,": 0.132 (0.121) 0.404 (0.410) 0.669 (0.673); : dataset BDT : 0.098 (0.099) 0.393 (0.402) 0.657 (0.681); : dataset Likelihood : 0.085 (0.082) 0.355 (0.363) 0.580 (0.596); : dataset Fisher : 0.015 (0.015) 0.121 (0.131) 0.487 (0.506); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 6000 events; : ; Dataset:dataset : Created tree 'TrainTree' with 14000 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html␛[0m; ; /***; ## Declare Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; **/; ; void TMVA_Higgs_Classification() {; ; // options to control used methods; ; bool useLikelihood = true; // likelihood based discriminant; bool useLikelihoodKDE = false; // likelihood based discriminant; bool useFischer = true; // Fischer discriminant; bool useMLP = false; // Multi Layer Perceptron (old TMVA NN implementation); bool useBDT = true; // Boosted Decision Tree; bool useDL = true; // TMVA Deep learning ( CPU or GPU); bool useKeras = true; // Keras Deep learning; bool usePyTorch = true; // PyTorch Deep learning; ; TMVA::Tools::Instance();; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitializ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:63266,Deployability,configurat,configuration,63266,""" );; ; }; ; // Fisher discriminant (same as LD); if (useFischer) {; factory.BookMethod(loader, TMVA::Types::kFisher, ""Fisher"", ""H:!V:Fisher:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10"" );; }; ; //Boosted Decision Trees; if (useBDT) {; factory.BookMethod(loader,TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"" );; }; ; //Multi-Layer Perceptron (Neural Network); if (useMLP) {; factory.BookMethod(loader, TMVA::Types::kMLP, ""MLP"",; ""!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator"" );; }; ; ; /// Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; /***; ; ## Booking Deep Neural Network; ; Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; We define first the DNN layout:; ; - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:503,Energy Efficiency,energy,energy,503,". ROOT: tutorials/tmva/TMVA_Higgs_Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_Higgs_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; Classification example of TMVA based on public Higgs UCI dataset ; The UCI data set is a public HIGGS data set , see http://archive.ics.uci.edu/ml/datasets/HIGGS used in this paper: Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014). ; ******************************************************************************; *Tree :sig_tree : tree *; *Entries : 10000 : Total = 1177229 bytes File Size = 785298 *; * : : Tree compression factor = 1.48 *; ******************************************************************************; *Br 0 :Type : Type/F *; *Entries : 10000 : Total Size= 40556 bytes File Size = 307 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 130.54 *; *............................................................................*; *Br 1 :lepton_pT : lepton_pT/F *; *Entries : 10000 : Total Size= 40581 bytes File Size = 30464 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.32 *; *............................................................................*; *Br 2 :lepton_eta : lepton_eta/F *; *Entries : 10000 : Total Size= 40586 bytes File Size = 28650 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.40 *; *............................................................................*; *Br 3 :lepton_phi : lepton_phi/F *; *Entries : 10000 : Total Size= 40586 bytes File Size = 30508 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.31 *; *............................................................................*; *Br 4 :missing_energy_magnitude : missing_energy_magnitude/F *; *Entries : 10000 : Total Size= 40656 bytes File Size = 35749 *; *Baskets : 1 : Basket Size= 1500672 bytes Com",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:16906,Energy Efficiency,reduce,reduced,16906,"e : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17501,Energy Efficiency,adapt,adaptive,17501," reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m=====================================",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17589,Energy Efficiency,adapt,adaptive,17589," reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m=====================================",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:19617,Energy Efficiency,reduce,reduces,19617,"[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_j",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:19651,Energy Efficiency,power,power,19651,"[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_j",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:20213,Energy Efficiency,power,powerful,20213,"ity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0105 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0038 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:47805,Energy Efficiency,power,power,47805,"from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.275 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 15.0519; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 15.7615; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.4202; TH1.Print N",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:69057,Energy Efficiency,allocate,allocates,69057,"; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('Higgs_model.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_higgs_model.py"");; ; if (gSystem->AccessPathName(""Higgs_model.h5"")) {; Warning(""TMVA_Higgs_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_Higgs_Classification"", ""Booking tf.Keras Dense model"");; factory.BookMethod(; loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=Higgs_model.h5:tf.keras:""; ""FilenameTrainedModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Methods; ; Here we train all the previously booked methods.; ; */; ; factory.TrainAllMethods();; ; /**; ## Test all methods; ; Now we test and evaluate all methods using the test data set; */; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// after we get the ROC curve and we display; ; auto c1 = factory.GetROCCurve(loader);; c1->Draw();; ; /// at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; /// to display additional plots; ; outputFile->Close();; ; ; }; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in wa",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:11852,Integrability,message,message,11852," the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER:InputLayout=1|1|7:BatchLayout=1|128|7:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with n",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17712,Integrability,message,message,17712,"CA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:20361,Integrability,message,message,20361,"optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0105 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0038 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 7000 bkg: 7000; : #eve",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:26943,Integrability,wrap,wraps,26943,"681 0.047397 20090.8 9; : 26 | 0.531742 0.582802 0.613513 0.0496834 19750.6 10; : 27 | 0.532714 0.581886 0.617515 0.0480417 19554.9 11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.253 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trai",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:27135,Integrability,interface,interface,27135,"11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.253 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model wei",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:27274,Integrability,message,message,27274,"n_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/112 [..............................] - ETA: 1:13 - loss: 0.6918 - accuracy: 0.5600␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 24/112 [=====>........................] - ETA: 0s - loss: 0.6860 - accuracy: 0.5275 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 46/112 [===========>..................] - ETA: 0",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:69799,Integrability,message,messages,69799,"kMethod(; loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=Higgs_model.h5:tf.keras:""; ""FilenameTrainedModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Methods; ; Here we train all the previously booked methods.; ; */; ; factory.TrainAllMethods();; ; /**; ## Test all methods; ; Now we test and evaluate all methods using the test data set; */; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// after we get the ROC curve and we display; ; auto c1 = factory.GetROCCurve(loader);; c1->Draw();; ; /// at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; /// to display additional plots; ; outputFile->Close();; ; ; }; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; gROOT#define gROOTDefinition TROOT.h:406; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TDirectoryFile::GetTObject * Get(const char *namecycle) overrideReturn pointer to object identified by namecycle.Definition TDirectoryFile.cxx:937; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:11503,Modifiability,variab,variable,11503,"ANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER:InputLayout=1|1|7:BatchLayout=1|128|7:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetiti",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13342,Modifiability,variab,variable,13342,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13373,Modifiability,variab,variable,13373,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13400,Modifiability,variab,variable,13400,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13432,Modifiability,variab,variable,13432,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13460,Modifiability,variab,variable,13460,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13491,Modifiability,variab,variable,13491,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13518,Modifiability,variab,variable,13518,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13550,Modifiability,variab,variable,13550,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13578,Modifiability,variab,variable,13578,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13609,Modifiability,variab,variable,13609,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13636,Modifiability,variab,variable,13636,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13668,Modifiability,variab,variable,13668,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13696,Modifiability,variab,variable,13696,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13729,Modifiability,variab,variable,13729,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Tr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:14881,Modifiability,variab,variable,14881,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:14912,Modifiability,variab,variable,14912,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:14939,Modifiability,variab,variable,14939,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:14971,Modifiability,variab,variable,14971,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:14999,Modifiability,variab,variable,14999,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15030,Modifiability,variab,variable,15030,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15057,Modifiability,variab,variable,15057,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15089,Modifiability,variab,variable,15089,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15117,Modifiability,variab,variable,15117,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15148,Modifiability,variab,variable,15148,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15175,Modifiability,variab,variable,15175,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15207,Modifiability,variab,variable,15207,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15235,Modifiability,variab,variable,15235,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15268,Modifiability,variab,variable,15268,"(Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loading Keras Model ; : Loaded model from file: Higgs_model.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15789,Modifiability,variab,variables,15789,"taset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelat",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:15862,Modifiability,variab,variable,15862,"ariable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:16604,Modifiability,variab,variables,16604,"---------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune th",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:16642,Modifiability,variab,variables,16642,"6856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:16770,Modifiability,variab,variables,16770,"--------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:16959,Modifiability,variab,variables,16959,"e : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17006,Modifiability,variab,variables,17006,"e : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17055,Modifiability,config,configuration,17055,"j : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed ti",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17460,Modifiability,variab,variable,17460,"1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Fact",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17501,Modifiability,adapt,adaptive,17501," reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m=====================================",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17589,Modifiability,adapt,adaptive,17589," reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m=====================================",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17680,Modifiability,variab,variable,17680,"CA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:18808,Modifiability,variab,variable,18808," All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from thi",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:18983,Modifiability,variab,variables,18983,"ning sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributio",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:19398,Modifiability,variab,variable,19398,"ood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:19573,Modifiability,variab,variables,19573,"m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:19720,Modifiability,variab,variable,19720,"l and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:19945,Modifiability,variab,variables,19945,"iminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0105 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (140",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:19974,Modifiability,variab,variable,19974,"d background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0105 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0038 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[d",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:20204,Modifiability,variab,variable,20204,"ity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0105 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0038 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:20298,Modifiability,config,configuration,20298,"optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0105 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0038 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 7000 bkg: 7000; : #eve",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24344,Modifiability,config,configuration,24344,"Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configurat",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24453,Modifiability,config,configuration,24453," = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24563,Modifiability,config,configuration,24563," Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24727,Modifiability,config,configuration,24727," Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:24891,Modifiability,config,configuration,24891,"ng; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; :",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:25325,Modifiability,config,configuration,25325,m Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; : 23 | 0.536715 0.589196 0.591864 0.0475573 20459.1 7; : 24 | 0.537796 0.579234 0.591655 0.0473491 20459.1 8; : 25 | 0.534347 0.586832 0.601681 0.047397 20090.8 9; : 26 | 0.531742 0.582802 0.613513 0.0496834 19750.6 10; : 27 | 0.532714 0.581886 0.617515 0.0480417 19554.9 11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Ev,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:25487,Modifiability,config,configuration,25487, 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.581823 0.587355 0.586416 0.0468644 20639.4 0; : 4 | 0.575813 0.593823 0.586862 0.0466947 20615.8 1; : 5 Minimum Test error found - save the configuration ; : 5 | 0.572257 0.578309 0.587324 0.0469531 20608.1 0; : 6 | 0.567347 0.582461 0.586969 0.0467966 20615.6 1; : 7 Minimum Test error found - save the configuration ; : 7 | 0.564553 0.577241 0.588162 0.0470086 20578.3 0; : 8 | 0.56278 0.582039 0.588324 0.0468355 20565.5 1; : 9 | 0.559377 0.582454 0.588676 0.0469029 20554.7 2; : 10 | 0.55743 0.581686 0.590386 0.0469245 20490.9 3; : 11 | 0.555638 0.57854 0.591068 0.0469776 20467.2 4; : 12 | 0.556914 0.579893 0.589121 0.0470111 20542 5; : 13 | 0.553226 0.579356 0.590241 0.0470372 20500.6 6; : 14 Minimum Test error found - save the configuration ; : 14 | 0.551359 0.575116 0.590392 0.0472515 20503 0; : 15 | 0.550484 0.575412 0.59006 0.0471061 20510 1; : 16 Minimum Test error found - save the configuration ; : 16 | 0.548532 0.572184 0.591616 0.0473333 20460 0; : 17 | 0.544023 0.585132 0.591038 0.0471532 20474.9 1; : 18 | 0.543801 0.585958 0.59089 0.0471671 20481 2; : 19 | 0.543073 0.577544 0.59151 0.0472011 20459 3; : 20 | 0.540305 0.585505 0.591485 0.0472123 20460.3 4; : 21 | 0.541092 0.577737 0.591401 0.0472411 20464.6 5; : 22 | 0.537394 0.582432 0.592336 0.0472536 20429.9 6; : 23 | 0.536715 0.589196 0.591864 0.0475573 20459.1 7; : 24 | 0.537796 0.579234 0.591655 0.0473491 20459.1 8; : 25 | 0.534347 0.586832 0.601681 0.047397 20090.8 9; : 26 | 0.531742 0.582802 0.613513 0.0496834 19750.6 10; : 27 | 0.532714 0.581886 0.617515 0.0480417 19554.9 11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.253 sec ; : Creating xml weight file: ␛[0;36mdataset,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:47257,Modifiability,variab,variables,47257,uracy: 0.6941 - val_loss: 0.5930 - val_accuracy: 0.6779; : Getting training history for item:0 name = 'loss'; : Getting training history for item:1 name = 'accuracy'; : Getting training history for item:2 name = 'val_loss'; : Getting training history for item:3 name = 'val_accuracy'; : Elapsed time for training with 14000 events: 6.58 sec ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.275 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:47322,Modifiability,variab,variable,47322,tem:1 name = 'accuracy'; : Getting training history for item:2 name = 'val_loss'; : Getting training history for item:3 name = 'val_accuracy'; : Elapsed time for training with 14000 events: 6.58 sec ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.275 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:47715,Modifiability,variab,variable,47715,"s.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.275 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 15.0519; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, To",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:48085,Modifiability,variab,variable,48085,"ss: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 15.0519; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 15.7615; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.4202; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 11.959; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2436; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.1109; Factory : === Destroy and recreate all methods via weight",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:48462,Modifiability,variab,variable,48462,"-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 15.0519; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 15.7615; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.4202; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 11.959; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2436; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.1109; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:48517,Modifiability,variab,variable,48517,"-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 15.0519; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 15.7615; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.4202; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 11.959; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2436; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.1109; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:57583,Modifiability,config,configuration,57583,": 0.132 (0.121) 0.404 (0.410) 0.669 (0.673); : dataset BDT : 0.098 (0.099) 0.393 (0.402) 0.657 (0.681); : dataset Likelihood : 0.085 (0.082) 0.355 (0.363) 0.580 (0.596); : dataset Fisher : 0.015 (0.015) 0.121 (0.131) 0.487 (0.506); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 6000 events; : ; Dataset:dataset : Created tree 'TrainTree' with 14000 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html␛[0m; ; /***; ## Declare Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; **/; ; void TMVA_Higgs_Classification() {; ; // options to control used methods; ; bool useLikelihood = true; // likelihood based discriminant; bool useLikelihoodKDE = false; // likelihood based discriminant; bool useFischer = true; // Fischer discriminant; bool useMLP = false; // Multi Layer Perceptron (old TMVA NN implementation); bool useBDT = true; // Boosted Decision Tree; bool useDL = true; // TMVA Deep learning ( CPU or GPU); bool useKeras = true; // Keras Deep learning; bool usePyTorch = true; // PyTorch Deep learning; ; TMVA::Tools::Instance();; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitializ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59675,Modifiability,variab,variables,59675,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59705,Modifiability,variab,variables,59705,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59783,Modifiability,variab,variable,59783,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:60584,Modifiability,variab,variables,60584,"h input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation),",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:63266,Modifiability,config,configuration,63266,""" );; ; }; ; // Fisher discriminant (same as LD); if (useFischer) {; factory.BookMethod(loader, TMVA::Types::kFisher, ""Fisher"", ""H:!V:Fisher:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10"" );; }; ; //Boosted Decision Trees; if (useBDT) {; factory.BookMethod(loader,TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"" );; }; ; //Multi-Layer Perceptron (Neural Network); if (useMLP) {; factory.BookMethod(loader, TMVA::Types::kMLP, ""MLP"",; ""!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator"" );; }; ; ; /// Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; /***; ; ## Booking Deep Neural Network; ; Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; We define first the DNN layout:; ; - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:63602,Modifiability,variab,variables,63602,"T, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"" );; }; ; //Multi-Layer Perceptron (Neural Network); if (useMLP) {; factory.BookMethod(loader, TMVA::Types::kMLP, ""MLP"",; ""!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator"" );; }; ; ; /// Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; /***; ; ## Booking Deep Neural Network; ; Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; We define first the DNN layout:; ; - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; One can then concatenate different training strategy with different parameters. The training strategy are separated by; the ``""|""``",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:63804,Modifiability,variab,variables,63804,"cles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator"" );; }; ; ; /// Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; /***; ; ## Booking Deep Neural Network; ; Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; We define first the DNN layout:; ; - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; One can then concatenate different training strategy with different parameters. The training strategy are separated by; the ``""|""`` separator.; ; - Optimizer; - Learning rate; - Momentum (valid for SGD and RMSPROP); - Regularization and Weight Decay; - Dropout; - Max number of epochs; - Convergence steps. if the test error will not decrease after that value the training will stop; - Batch size (This value must be the same specified in the input layout); ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:64271,Modifiability,layers,layers,64271,"fined using a string. Note that whitespaces between characters are not allowed.; ; We define first the DNN layout:; ; - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; One can then concatenate different training strategy with different parameters. The training strategy are separated by; the ``""|""`` separator.; ; - Optimizer; - Learning rate; - Momentum (valid for SGD and RMSPROP); - Regularization and Weight Decay; - Dropout; - Max number of epochs; - Convergence steps. if the test error will not decrease after that value the training will stop; - Batch size (This value must be the same specified in the input layout); - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; In addition t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:67683,Modifiability,layers,layers,67683,"OSSENTROPY:VarTransform=G:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append ("":""); dnnOptions.Append (inputLayoutString);; dnnOptions.Append ("":""); dnnOptions.Append (batchLayoutString);; dnnOptions.Append ("":""); dnnOptions.Append (layoutString);; dnnOptions.Append ("":""); dnnOptions.Append (trainingStrategyString);; ; TString dnnMethodName = ""DNN_CPU"";; if (useDLGPU) {; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""DNN_GPU"";; } else {; dnnOptions += "":Architecture=CPU"";; }; ; factory.BookMethod(loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; // Keras deep learning; if (useKeras) {; ; Info(""TMVA_Higgs_Classification"", ""Building deep neural network with keras "");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Dense(64, activation='relu',input_dim=7))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(2, activation='sigmoid'))"");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('Higgs_model.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_higgs_model.py"");; ; if (gSystem->AccessPathName(""Higgs_model.h5"")) {; Warning(""TMVA_Higgs_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be cre",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:71923,Modifiability,variab,variable,71923,":string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSignalTreevoid AddSignalTree(TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:371; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kFisher@ kFisherDefinition Types.h:82; TMVA::Types::kPyKeras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMVA::Types::kLikelihood@ kLikelihoodDefinition Types.h:79; TMVA::Types::kMLP@ kMLPDefinition Types.h:90; TMacroClass supporting a collection of lines with C++ code.Definition TMacro.h:31; TStringBasic string class.Definition TString.h:139; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:73069,Modifiability,variab,variable,73069,"er of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kFisher@ kFisherDefinition Types.h:82; TMVA::Types::kPyKeras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMVA::Types::kLikelihood@ kLikelihoodDefinition Types.h:79; TMVA::Types::kMLP@ kMLPDefinition Types.h:90; TMacroClass supporting a collection of lines with C++ code.Definition TMacro.h:31; TStringBasic string class.Definition TString.h:139; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TTree::Printvoid Print(Option_t *option="""") const overridePrint a summary of the tree contents.Definition TTree.cxx:7219; c1return c1Definition legend1.C:41; TMVA_Higgs_ClassificationDefinition TMVA_Higgs_Classification.py:1; mTMarker mDefinition textangle.C:8; AuthorLorenzo Moneta ; Definition in file TMVA_Higgs_Classification.C. tutorialstmvaTMVA_Higgs_Classification.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:11528,Performance,perform,performed,11528,"ANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER:InputLayout=1|1|7:BatchLayout=1|128|7:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetiti",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:12309,Performance,perform,perform,12309,"=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with eve",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:12893,Performance,perform,performance,12893,"ic help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; __________",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:16735,Performance,perform,performance,16735,"--------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:17377,Performance,tune,tune,17377,"1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.117 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.02 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Fact",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:19475,Performance,perform,performance,19475,"m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:27220,Performance,load,loaded,27220," Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.253 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:49734,Performance,perform,performance,49734,"ningHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.4202; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 11.959; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2436; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.1109; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.9824",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:49941,Performance,perform,performance,49941,"= 13.2436; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.1109; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : ----------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:50200,Performance,perform,performance,50200,/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.101 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:50395,Performance,perform,performance,50395,: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.101 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; Py,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:51170,Performance,perform,performance,51170,ctory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.101 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.168 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:57142,Performance,perform,performance,57142,"ciency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset DNN_CPU : 0.147 (0.142) 0.404 (0.444) 0.678 (0.708); : dataset PyKeras : 0.132 (0.121) 0.404 (0.410) 0.669 (0.673); : dataset BDT : 0.098 (0.099) 0.393 (0.402) 0.657 (0.681); : dataset Likelihood : 0.085 (0.082) 0.355 (0.363) 0.580 (0.596); : dataset Fisher : 0.015 (0.015) 0.121 (0.131) 0.487 (0.506); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 6000 events; : ; Dataset:dataset : Created tree 'TrainTree' with 14000 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html␛[0m; ; /***; ## Declare Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; **/; ; void TMVA_Higgs_Classification() {; ; // options to control used methods; ; bool useLikelihood = true; // likelihood based discriminant; bool useLikelihoodKDE = false; // likelihood based discriminant; bool useFischer = true; // Fischer discriminant; bool useMLP = false; // Multi Layer Perceptron (old TMVA NN implementation); bool useBDT = true; // Boosted Decision Tree; bool useDL ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59887,Performance,load,loader,59887,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59932,Performance,load,loader,59932,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59962,Performance,load,loader,59962,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59993,Performance,load,loader,59993,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:60023,Performance,load,loader,60023,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:60054,Performance,load,loader,60054,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:60084,Performance,load,loader,60084,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:60115,Performance,load,loader,60115,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:60426,Performance,load,loader,60426,"h input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation),",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:60479,Performance,load,loader,60479,"h input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation),",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:60783,Performance,load,loader,60783,"h input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation),",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:61272,Performance,load,loader,61272,"t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFisc",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:61398,Performance,load,loader,61398,"t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFisc",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:61821,Performance,load,loader,61821,"le: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFischer) {; factory.BookMethod(loader, TMVA::Types::kFisher, ""Fisher"", ""H:!V:Fisher:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10"" );; }; ; //Boosted Decision Trees; if (useBDT) {; factory.BookMethod(loader,TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"" );; }; ; //Multi-Layer Perceptron (Neural Network); if (useMLP) {; factory.BookMethod(loader, TMVA::Types::kMLP, ""MLP"",; ""!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:Te",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:62100,Performance,load,loader,62100,"ts are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFischer) {; factory.BookMethod(loader, TMVA::Types::kFisher, ""Fisher"", ""H:!V:Fisher:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10"" );; }; ; //Boosted Decision Trees; if (useBDT) {; factory.BookMethod(loader,TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"" );; }; ; //Multi-Layer Perceptron (Neural Network); if (useMLP) {; factory.BookMethod(loader, TMVA::Types::kMLP, ""MLP"",; ""!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator"" );; }; ; ; /// Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; /***; ; ## Booking Deep Neural Network; ; Here we define ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:62359,Performance,load,loader,62359,"rain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFischer) {; factory.BookMethod(loader, TMVA::Types::kFisher, ""Fisher"", ""H:!V:Fisher:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10"" );; }; ; //Boosted Decision Trees; if (useBDT) {; factory.BookMethod(loader,TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"" );; }; ; //Multi-Layer Perceptron (Neural Network); if (useMLP) {; factory.BookMethod(loader, TMVA::Types::kMLP, ""MLP"",; ""!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator"" );; }; ; ; /// Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; /***; ; ## Booking Deep Neural Network; ; Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; We define first the DNN layout:; ; - **input layout** : this defines the input data for",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:62572,Performance,load,loader,62572," We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFischer) {; factory.BookMethod(loader, TMVA::Types::kFisher, ""Fisher"", ""H:!V:Fisher:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10"" );; }; ; //Boosted Decision Trees; if (useBDT) {; factory.BookMethod(loader,TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"" );; }; ; //Multi-Layer Perceptron (Neural Network); if (useMLP) {; factory.BookMethod(loader, TMVA::Types::kMLP, ""MLP"",; ""!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator"" );; }; ; ; /// Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; /***; ; ## Booking Deep Neural Network; ; Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; We define first the DNN layout:; ; - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; In case of a dense layer as first layer the input layout should be ``1 | 1 | number of in",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:62846,Performance,load,loader,62846,"=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFischer) {; factory.BookMethod(loader, TMVA::Types::kFisher, ""Fisher"", ""H:!V:Fisher:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10"" );; }; ; //Boosted Decision Trees; if (useBDT) {; factory.BookMethod(loader,TMVA::Types::kBDT, ""BDT"",; ""!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20"" );; }; ; //Multi-Layer Perceptron (Neural Network); if (useMLP) {; factory.BookMethod(loader, TMVA::Types::kMLP, ""MLP"",; ""!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator"" );; }; ; ; /// Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; /***; ; ## Booking Deep Neural Network; ; Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; We define first the DNN layout:; ; - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; note that in case of on",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:67210,Performance,load,loader,67210,"""; // ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; // ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; // ""Optimizer=SGD,DropConfig=0.0+0.0+0.0+0."");; ; TString trainingStrategyString (""TrainingStrategy="");; trainingStrategyString += training1; // + ""|"" + training2;; ; // General Options.; ; TString dnnOptions (""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append ("":""); dnnOptions.Append (inputLayoutString);; dnnOptions.Append ("":""); dnnOptions.Append (batchLayoutString);; dnnOptions.Append ("":""); dnnOptions.Append (layoutString);; dnnOptions.Append ("":""); dnnOptions.Append (trainingStrategyString);; ; TString dnnMethodName = ""DNN_CPU"";; if (useDLGPU) {; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""DNN_GPU"";; } else {; dnnOptions += "":Architecture=CPU"";; }; ; factory.BookMethod(loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; // Keras deep learning; if (useKeras) {; ; Info(""TMVA_Higgs_Classification"", ""Building deep neural network with keras "");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Dense(64, activation='relu',input_dim=7))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(2, activation='sigmoid'))"");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('Higgs_model.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->Pro",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:67623,Performance,optimiz,optimizers,67623,"General Options.; ; TString dnnOptions (""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append ("":""); dnnOptions.Append (inputLayoutString);; dnnOptions.Append ("":""); dnnOptions.Append (batchLayoutString);; dnnOptions.Append ("":""); dnnOptions.Append (layoutString);; dnnOptions.Append ("":""); dnnOptions.Append (trainingStrategyString);; ; TString dnnMethodName = ""DNN_CPU"";; if (useDLGPU) {; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""DNN_GPU"";; } else {; dnnOptions += "":Architecture=CPU"";; }; ; factory.BookMethod(loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; // Keras deep learning; if (useKeras) {; ; Info(""TMVA_Higgs_Classification"", ""Building deep neural network with keras "");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Dense(64, activation='relu',input_dim=7))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(2, activation='sigmoid'))"");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('Higgs_model.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_higgs_model.py"");; ; if (gSystem->AccessPathName(""Higgs_model.h5"")) {; Warning(""TMVA_Higgs_Classification"", ""Error creating Keras model file - skip using Keras"");; } e",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:68111,Performance,optimiz,optimizer,68111,";; dnnMethodName = ""DNN_GPU"";; } else {; dnnOptions += "":Architecture=CPU"";; }; ; factory.BookMethod(loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; // Keras deep learning; if (useKeras) {; ; Info(""TMVA_Higgs_Classification"", ""Building deep neural network with keras "");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Dense(64, activation='relu',input_dim=7))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(2, activation='sigmoid'))"");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('Higgs_model.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_higgs_model.py"");; ; if (gSystem->AccessPathName(""Higgs_model.h5"")) {; Warning(""TMVA_Higgs_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_Higgs_Classification"", ""Booking tf.Keras Dense model"");; factory.BookMethod(; loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=Higgs_model.h5:tf.keras:""; ""FilenameTrainedModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Metho",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:68795,Performance,load,loader,68795,".AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(2, activation='sigmoid'))"");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('Higgs_model.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_higgs_model.py"");; ; if (gSystem->AccessPathName(""Higgs_model.h5"")) {; Warning(""TMVA_Higgs_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_Higgs_Classification"", ""Booking tf.Keras Dense model"");; factory.BookMethod(; loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=Higgs_model.h5:tf.keras:""; ""FilenameTrainedModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Methods; ; Here we train all the previously booked methods.; ; */; ; factory.TrainAllMethods();; ; /**; ## Test all methods; ; Now we test and evaluate all methods using the test data set; */; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// after we get the ROC curve and we display; ; auto c1 = factory.GetROCCurve(loader);; c1->Draw();; ; /// at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; /// to display additional plots; ; outputFile->Close();; ; ; }; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:69441,Performance,load,loader,69441,"lassification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_Higgs_Classification"", ""Booking tf.Keras Dense model"");; factory.BookMethod(; loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=Higgs_model.h5:tf.keras:""; ""FilenameTrainedModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Methods; ; Here we train all the previously booked methods.; ; */; ; factory.TrainAllMethods();; ; /**; ## Test all methods; ; Now we test and evaluate all methods using the test data set; */; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// after we get the ROC curve and we display; ; auto c1 = factory.GetROCCurve(loader);; c1->Draw();; ; /// at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; /// to display additional plots; ; outputFile->Close();; ; ; }; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; gROOT#define gROOTDefinition TROOT.h:406; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TDirectoryFile::GetTObject * Get(const char *namecycle) overrideReturn pointer to object identified by namecycle.Definition TDirectoryFile.cxx:937; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFi",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:70860,Performance,cache,cacheDir,70860,"gfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; gROOT#define gROOTDefinition TROOT.h:406; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TDirectoryFile::GetTObject * Get(const char *namecycle) overrideReturn pointer to object identified by namecycle.Definition TDirectoryFile.cxx:937; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSignalTreevoid AddSignalTree(TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:371; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:70975,Performance,cache,cache,70975,"gfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; gROOT#define gROOTDefinition TROOT.h:406; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TDirectoryFile::GetTObject * Get(const char *namecycle) overrideReturn pointer to object identified by namecycle.Definition TDirectoryFile.cxx:937; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSignalTreevoid AddSignalTree(TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:371; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:26966,Safety,predict,predictions,26966,"681 0.047397 20090.8 9; : 26 | 0.531742 0.582802 0.613513 0.0496834 19750.6 10; : 27 | 0.532714 0.581886 0.617515 0.0480417 19554.9 11; : ; : Elapsed time for training with 14000 events: 16.1 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.253 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trai",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:69048,Safety,avoid,avoid,69048,"; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('Higgs_model.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_higgs_model.py"");; ; if (gSystem->AccessPathName(""Higgs_model.h5"")) {; Warning(""TMVA_Higgs_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_Higgs_Classification"", ""Booking tf.Keras Dense model"");; factory.BookMethod(; loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=Higgs_model.h5:tf.keras:""; ""FilenameTrainedModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Methods; ; Here we train all the previously booked methods.; ; */; ; factory.TrainAllMethods();; ; /**; ## Test all methods; ; Now we test and evaluate all methods using the test data set; */; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// after we get the ROC curve and we display; ; auto c1 = factory.GetROCCurve(loader);; c1->Draw();; ; /// at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; /// to display additional plots; ; outputFile->Close();; ; ; }; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in wa",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13065,Security,validat,validation,13065,"ic help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; __________",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:13133,Security,validat,validation,13133,": Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (D",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:23914,Security,validat,validation,23914,"-----------------------------------------------------------; : Start of deep neural network training on CPU using MT, nthreads = 1; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5.7307 ]; : m_lv: 0.0054053 1.0003 [ -3.2810 5.7307 ]; : m_jlv: 0.0044637 0.99837 [ -3.2803 5.7307 ]; : m_bb: 0.0043676 0.99847 [ -3.2797 5.7307 ]; : m_wbb: 0.0042343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 5 Input = ( 1, 1, 7 ) Batch size = 128 Loss function = C; Layer 0 DENSE Layer: ( Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error fou",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:27466,Security,validat,validation,27466,"n_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/112 [..............................] - ETA: 1:13 - loss: 0.6918 - accuracy: 0.5600␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 24/112 [=====>........................] - ETA: 0s - loss: 0.6860 - accuracy: 0.5275 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 46/112 [===========>..................] - ETA: 0",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:28172,Security,validat,validation,28172," : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/112 [..............................] - ETA: 1:13 - loss: 0.6918 - accuracy: 0.5600␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 24/112 [=====>........................] - ETA: 0s - loss: 0.6860 - accuracy: 0.5275 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 46/112 [===========>..................] - ETA: 0s - loss: 0.6806 - accuracy: 0.5572␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 67/112 [================>.............] - ETA: 0s - loss: 0.6763 - accuracy: 0.5693␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 88/112 [======================>.......] - ETA: 0s - loss: 0.6711 - accuracy: 0.5783; Epoch 1: val_loss improved from inf to 0.65278, saving model to Higgs_trained_model.h5; ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:72907,Security,access,access,72907,"er of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kFisher@ kFisherDefinition Types.h:82; TMVA::Types::kPyKeras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMVA::Types::kLikelihood@ kLikelihoodDefinition Types.h:79; TMVA::Types::kMLP@ kMLPDefinition Types.h:90; TMacroClass supporting a collection of lines with C++ code.Definition TMacro.h:31; TStringBasic string class.Definition TString.h:139; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TTree::Printvoid Print(Option_t *option="""") const overridePrint a summary of the tree contents.Definition TTree.cxx:7219; c1return c1Definition legend1.C:41; TMVA_Higgs_ClassificationDefinition TMVA_Higgs_Classification.py:1; mTMarker mDefinition textangle.C:8; AuthorLorenzo Moneta ; Definition in file TMVA_Higgs_Classification.C. tutorialstmvaTMVA_Higgs_Classification.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:72941,Security,access,access,72941,"er of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kFisher@ kFisherDefinition Types.h:82; TMVA::Types::kPyKeras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMVA::Types::kLikelihood@ kLikelihoodDefinition Types.h:79; TMVA::Types::kMLP@ kMLPDefinition Types.h:90; TMacroClass supporting a collection of lines with C++ code.Definition TMacro.h:31; TStringBasic string class.Definition TString.h:139; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; TTree::Printvoid Print(Option_t *option="""") const overridePrint a summary of the tree contents.Definition TTree.cxx:7219; c1return c1Definition legend1.C:41; TMVA_Higgs_ClassificationDefinition TMVA_Higgs_Classification.py:1; mTMarker mDefinition textangle.C:8; AuthorLorenzo Moneta ; Definition in file TMVA_Higgs_Classification.C. tutorialstmvaTMVA_Higgs_Classification.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:8524,Testability,test,testing,8524,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:8667,Testability,test,testing,8667,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:8715,Testability,test,testing,8715,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:8794,Testability,test,testing,8794,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:8846,Testability,test,testing,8846,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:12881,Testability,test,testing,12881,"ic help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=30,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; __________",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:23875,Testability,test,testing,23875,"-----------------------------------------------------------; : Start of deep neural network training on CPU using MT, nthreads = 1; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5.7307 ]; : m_lv: 0.0054053 1.0003 [ -3.2810 5.7307 ]; : m_jlv: 0.0044637 0.99837 [ -3.2803 5.7307 ]; : m_bb: 0.0043676 0.99847 [ -3.2797 5.7307 ]; : m_wbb: 0.0042343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 5 Input = ( 1, 1, 7 ) Batch size = 128 Loss function = C; Layer 0 DENSE Layer: ( Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 1.171; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.66619 0.628056 0.587341 0.0468945 20605.2 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.605139 0.593298 0.586597 0.0468674 20632.5 0; : 3 Minimum Test error fou",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:49145,Testability,test,testing,49145,"ank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 15.0519; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 15.7615; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.4202; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 11.959; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2436; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.1109; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Fact",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:49804,Testability,test,testing,49804,"ningHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.4202; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 11.959; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2436; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.1109; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.9824",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:50003,Testability,test,testing,50003,"= 13.2436; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.1109; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : ----------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:50138,Testability,test,testing,50138,/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.101 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:50256,Testability,test,testing,50256,/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.012 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.101 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:51032,Testability,test,testing,51032,ample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00146 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0456 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.101 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.168 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:51517,Testability,test,testing,51517,Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.101 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.168 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: Fisher; : ; Fisher : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Also filling probability and rarity histograms (on request)...; TFHandler_Fisher : Variable Mean RMS [ Min Max,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:51727,Testability,test,test,51727,: m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.101 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.168 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: Fisher; : ; Fisher : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Also filling probability and rarity histograms (on request)...; TFHandler_Fisher : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_l,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:52354,Testability,test,test,52354, gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: Higgs_trained_model.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.168 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: Fisher; : ; Fisher : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Also filling probability and rarity histograms (on request)...; TFHandler_Fisher : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_BDT : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:53037,Testability,test,test,53037,: m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: Fisher; : ; Fisher : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Also filling probability and rarity histograms (on request)...; TFHandler_Fisher : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_BDT : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: DNN_CPU; : ; DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:53659,Testability,test,test,53659,.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_BDT : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: DNN_CPU; : ; DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5.7307 ]; : m_lv: 0.0054053 1.0003 [ -3.2810 5.7307 ]; : m_jlv: 0.0044637 0.99837 [ -3.2803 5.7307 ]; : m_bb: 0.0043676 0.99847 [ -3.2797 5.7307 ]; : m_wbb: 0.0042343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 ,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:54876,Testability,test,test,54876, RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5.7307 ]; : m_lv: 0.0054053 1.0003 [ -3.2810 5.7307 ]; : m_jlv: 0.0044637 0.99837 [ -3.2803 5.7307 ]; : m_bb: 0.0043676 0.99847 [ -3.2797 5.7307 ]; : m_wbb: 0.0042343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_PyKeras : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset DNN_CPU : 0.762; : dataset PyKeras : 0.757; : dataset BDT : 0.754; : dataset Likelihood : 0.698; : dataset Fisher : 0.642; : -------------------------------,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:56153,Testability,test,test,56153,"---------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset DNN_CPU : 0.762; : dataset PyKeras : 0.757; : dataset BDT : 0.754; : dataset Likelihood : 0.698; : dataset Fisher : 0.642; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset DNN_CPU : 0.147 (0.142) 0.404 (0.444) 0.678 (0.708); : dataset PyKeras : 0.132 (0.121) 0.404 (0.410) 0.669 (0.673); : dataset BDT : 0.098 (0.099) 0.393 (0.402) 0.657 (0.681); : dataset Likelihood : 0.085 (0.082) 0.355 (0.363) 0.580 (0.596); : dataset Fisher : 0.015 (0.015) 0.121 (0.131) 0.487 (0.506); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 6000 events; : ; Dataset:dataset : Created tree 'TrainTree' with 14000 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please v",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:59412,Testability,test,test,59412,"al and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download file from Cernbox location; Info(""TMVA_Higgs_Classification"",""Download Higgs_data.root file"");; TFile::SetCacheFileDir(""."");; inputFile = TFile::Open(inputFileLink, ""CACHEREAD"");; if (!inputFile) {; Error(""TMVA_Higgs_Classification"",""Input file cannot be downloaded - exit"");; return;; }; }; ; // --- Register the training and test trees; ; TTree *signalTree = (TTree*)inputFile->Get(""sig_tree"");; TTree *backgroundTree = (TTree*)inputFile->Get(""bkg_tree"");; ; signalTree->Print();; ; /***; ## Declare DataLoader(s); ; The next step is to declare the DataLoader class that deals with input variables; ; Define the input variables that shall be used for the MVA training; note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; ; ***/; ; TMVA::DataLoader * loader = new TMVA::DataLoader(""dataset"");; ; loader->AddVariable(""m_jj"");; loader->AddVariable(""m_jjj"");; loader->AddVariable(""m_lv"");; loader->AddVariable(""m_jlv"");; loader->AddVariable(""m_bb"");; loader->AddVariable(""m_wbb"");; loader->AddVariable(""m_wwbb"");; ; /// We set now the input data trees in the TMVA DataLoader class; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:61120,Testability,test,testing,61120,"t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFisc",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:61259,Testability,test,testing,61259,"t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFisc",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:61374,Testability,test,testing,61374,"t backgroundWeight = 1.0;; ; // You can add an arbitrary number of signal or background trees; loader->AddSignalTree ( signalTree, signalWeight );; loader->AddBackgroundTree( backgroundTree, backgroundWeight );; ; ; // Set individual event weights (the variables must exist in the original TTree); // for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; // for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; //loader->SetBackgroundWeightExpression( ""weight"" );; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """"; // for example: TCut mycutb = ""abs(var1)<0.5"";; ; // Tell the factory how to use the training and testing events; //; // If no numbers of events are given, half of the events in the tree are used; // for training, and the other half for testing:; // loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; // To also specify the number of testing events, use:; ; loader->PrepareTrainingAndTestTree( mycuts, mycutb,; ""nTrain_Signal=7000:nTrain_Background=7000:SplitMode=Random:NormMode=NumEvents:!V"" );; ; /***; ## Booking Methods; ; Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; and a shallow neural network; ; */; ; ; // Likelihood (""naive Bayes estimator""); if (useLikelihood) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""Likelihood"",; ""H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50"" );; }; // Use a kernel density estimator to approximate the PDFs; if (useLikelihoodKDE) {; factory.BookMethod(loader, TMVA::Types::kLikelihood, ""LikelihoodKDE"",; ""!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50"" );; ; }; ; // Fisher discriminant (same as LD); if (useFisc",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:64779,Testability,test,test,64779,"parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; One can then concatenate different training strategy with different parameters. The training strategy are separated by; the ``""|""`` separator.; ; - Optimizer; - Learning rate; - Momentum (valid for SGD and RMSPROP); - Regularization and Weight Decay; - Dropout; - Max number of epochs; - Convergence steps. if the test error will not decrease after that value the training will stop; - Batch size (This value must be the same specified in the input layout); - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; In addition to input layout, batch layout and training strategy we add now:; ; - Type of Loss function (e.g. CROSSENTROPY); - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); - Variable Transformation; - Type of Architecture (e.g. CPU, GPU, Standard); ; We can then book the DL method using the built option string; ; ***/; ; if (useDL) {; ; bool useDLGPU = false;; #ifdef R__HAS_TMVAGPU; useDLGPU = true;; #endif; ; // Define DNN layout; TString inputLayoutString = ""InputLayout=1|1|7"";; TString batchLayoutString= ""BatchLayout=1|128|7"";; TString layoutString (""Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:64965,Testability,test,test,64965,"parameters for DNN layout)*; ; note that in case of only dense layer the input layout could be omitted but it is required when defining more; complex architectures; ; - **layer layout** string defining the layer architecture. The syntax is; - layer type (e.g. DENSE, CONV, RNN); - layer parameters (e.g. number of units); - activation function (e.g TANH, RELU,...); ; *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; One can then concatenate different training strategy with different parameters. The training strategy are separated by; the ``""|""`` separator.; ; - Optimizer; - Learning rate; - Momentum (valid for SGD and RMSPROP); - Regularization and Weight Decay; - Dropout; - Max number of epochs; - Convergence steps. if the test error will not decrease after that value the training will stop; - Batch size (This value must be the same specified in the input layout); - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; In addition to input layout, batch layout and training strategy we add now:; ; - Type of Loss function (e.g. CROSSENTROPY); - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); - Variable Transformation; - Type of Architecture (e.g. CPU, GPU, Standard); ; We can then book the DL method using the built option string; ; ***/; ; if (useDL) {; ; bool useDLGPU = false;; #ifdef R__HAS_TMVAGPU; useDLGPU = true;; #endif; ; // Define DNN layout; TString inputLayoutString = ""InputLayout=1|1|7"";; TString batchLayoutString= ""BatchLayout=1|128|7"";; TString layoutString (""Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:69239,Testability,test,test,69239,"odel.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_higgs_model.py"");; ; if (gSystem->AccessPathName(""Higgs_model.h5"")) {; Warning(""TMVA_Higgs_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_Higgs_Classification"", ""Booking tf.Keras Dense model"");; factory.BookMethod(; loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=Higgs_model.h5:tf.keras:""; ""FilenameTrainedModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Methods; ; Here we train all the previously booked methods.; ; */; ; factory.TrainAllMethods();; ; /**; ## Test all methods; ; Now we test and evaluate all methods using the test data set; */; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// after we get the ROC curve and we display; ; auto c1 = factory.GetROCCurve(loader);; c1->Draw();; ; /// at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; /// to display additional plots; ; outputFile->Close();; ; ; }; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; gROOT#define gROOTDefinition TROOT.h:406; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:69279,Testability,test,test,69279,"odel.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_higgs_model.py"");; ; if (gSystem->AccessPathName(""Higgs_model.h5"")) {; Warning(""TMVA_Higgs_Classification"", ""Error creating Keras model file - skip using Keras"");; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_Higgs_Classification"", ""Booking tf.Keras Dense model"");; factory.BookMethod(; loader, TMVA::Types::kPyKeras, ""PyKeras"",; ""H:!V:VarTransform=None:FilenameModel=Higgs_model.h5:tf.keras:""; ""FilenameTrainedModel=Higgs_trained_model.h5:NumEpochs=20:BatchSize=100:""; ""GpuOptions=allow_growth=True""); // needed for RTX NVidia card and to avoid TF allocates all GPU memory; }; }; ; /**; ## Train Methods; ; Here we train all the previously booked methods.; ; */; ; factory.TrainAllMethods();; ; /**; ## Test all methods; ; Now we test and evaluate all methods using the test data set; */; ; factory.TestAllMethods();; ; factory.EvaluateAllMethods();; ; /// after we get the ROC curve and we display; ; auto c1 = factory.GetROCCurve(loader);; c1->Draw();; ; /// at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; /// to display additional plots; ; outputFile->Close();; ; ; }; Double_tdouble Double_tDefinition RtypesCore.h:59; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; gROOT#define gROOTDefinition TROOT.h:406; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCutA specialized string object used for TTr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:71424,Testability,test,test,71424,"namecycle.Definition TDirectoryFile.cxx:937; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSignalTreevoid AddSignalTree(TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:371; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kFisher@ kFisherDefinition Types.h:82; TMVA::Types::kPyKeras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinitio",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:16976,Usability,simpl,simply,16976,"e : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:20156,Usability,simpl,simple,20156,"ity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0105 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0038 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:58162,Usability,learn,learning,58162," Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; **/; ; void TMVA_Higgs_Classification() {; ; // options to control used methods; ; bool useLikelihood = true; // likelihood based discriminant; bool useLikelihoodKDE = false; // likelihood based discriminant; bool useFischer = true; // Fischer discriminant; bool useMLP = false; // Multi Layer Perceptron (old TMVA NN implementation); bool useBDT = true; // Boosted Decision Tree; bool useDL = true; // TMVA Deep learning ( CPU or GPU); bool useKeras = true; // Keras Deep learning; bool usePyTorch = true; // PyTorch Deep learning; ; TMVA::Tools::Instance();; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKeras = false;; usePyTorch = false;; #endif; ; auto outputFile = TFile::Open(""Higgs_ClassificationOutput.root"", ""RECREATE"");; ; TMVA::Factory factory(""TMVA_Higgs_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification"" );; ; /**; ; ## Setup Dataset(s); ; Define now input data file and signal and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download f",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:58222,Usability,learn,learning,58222," Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; **/; ; void TMVA_Higgs_Classification() {; ; // options to control used methods; ; bool useLikelihood = true; // likelihood based discriminant; bool useLikelihoodKDE = false; // likelihood based discriminant; bool useFischer = true; // Fischer discriminant; bool useMLP = false; // Multi Layer Perceptron (old TMVA NN implementation); bool useBDT = true; // Boosted Decision Tree; bool useDL = true; // TMVA Deep learning ( CPU or GPU); bool useKeras = true; // Keras Deep learning; bool usePyTorch = true; // PyTorch Deep learning; ; TMVA::Tools::Instance();; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKeras = false;; usePyTorch = false;; #endif; ; auto outputFile = TFile::Open(""Higgs_ClassificationOutput.root"", ""RECREATE"");; ; TMVA::Factory factory(""TMVA_Higgs_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification"" );; ; /**; ; ## Setup Dataset(s); ; Define now input data file and signal and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download f",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:58272,Usability,learn,learning,58272," Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; **/; ; void TMVA_Higgs_Classification() {; ; // options to control used methods; ; bool useLikelihood = true; // likelihood based discriminant; bool useLikelihoodKDE = false; // likelihood based discriminant; bool useFischer = true; // Fischer discriminant; bool useMLP = false; // Multi Layer Perceptron (old TMVA NN implementation); bool useBDT = true; // Boosted Decision Tree; bool useDL = true; // TMVA Deep learning ( CPU or GPU); bool useKeras = true; // Keras Deep learning; bool usePyTorch = true; // PyTorch Deep learning; ; TMVA::Tools::Instance();; ; #ifdef R__HAS_PYMVA; gSystem->Setenv(""KERAS_BACKEND"", ""tensorflow"");; // for using Keras; TMVA::PyMethodBase::PyInitialize();; #else; useKeras = false;; usePyTorch = false;; #endif; ; auto outputFile = TFile::Open(""Higgs_ClassificationOutput.root"", ""RECREATE"");; ; TMVA::Factory factory(""TMVA_Higgs_Classification"", outputFile,; ""!V:ROC:!Silent:Color:AnalysisType=Classification"" );; ; /**; ; ## Setup Dataset(s); ; Define now input data file and signal and background trees; ; **/; ; TString inputFileName = ""Higgs_data.root"";; TString inputFileLink = ""http://root.cern/files/"" + inputFileName;; ; TFile *inputFile = nullptr;; ; if (!gSystem->AccessPathName(inputFileName)) {; // file exists; inputFile = TFile::Open( inputFileName );; }; ; if (!inputFile) {; // download f",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html:67284,Usability,learn,learning,67284,"""; // ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; // ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; // ""Optimizer=SGD,DropConfig=0.0+0.0+0.0+0."");; ; TString trainingStrategyString (""TrainingStrategy="");; trainingStrategyString += training1; // + ""|"" + training2;; ; // General Options.; ; TString dnnOptions (""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:""; ""WeightInitialization=XAVIER"");; dnnOptions.Append ("":""); dnnOptions.Append (inputLayoutString);; dnnOptions.Append ("":""); dnnOptions.Append (batchLayoutString);; dnnOptions.Append ("":""); dnnOptions.Append (layoutString);; dnnOptions.Append ("":""); dnnOptions.Append (trainingStrategyString);; ; TString dnnMethodName = ""DNN_CPU"";; if (useDLGPU) {; dnnOptions += "":Architecture=GPU"";; dnnMethodName = ""DNN_GPU"";; } else {; dnnOptions += "":Architecture=CPU"";; }; ; factory.BookMethod(loader, TMVA::Types::kDL, dnnMethodName, dnnOptions);; }; ; // Keras deep learning; if (useKeras) {; ; Info(""TMVA_Higgs_Classification"", ""Building deep neural network with keras "");; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Dense(64, activation='relu',input_dim=7))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(64, activation='relu'))"");; m.AddLine(""model.add(Dense(2, activation='sigmoid'))"");; m.AddLine(""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(""model.save('Higgs_model.h5')"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_higgs_model.py"");; // execute; auto ret = (TString *)gROOT->Pro",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8C.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:12147,Availability,error,error,12147,"Steps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:23990,Availability,error,error,23990,"42343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 5 Input = ( 1, 1, 7 ) Batch size = 128 Loss function = C; Layer 0 DENSE Layer: ( Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24261,Availability,error,error,24261,"ut = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24371,Availability,error,error,24371,"4 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuratio",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24533,Availability,error,error,24533,"ion = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24747,Availability,error,error,24747,"= Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 1",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25019,Availability,error,error,25019,-----------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mda,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25130,Availability,error,error,25130,Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25349,Availability,error,error,25349, 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:57922,Availability,avail,available,57922,"be created with the; ## method parameters; ; ## - The second argument is the output file for the training results; ; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; import ROOT; import os; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; # options to control used methods; useLikelihood = True # likelihood based discriminant; useLikelihoodKDE = False # likelihood based discriminant; useFischer = True # Fischer discriminant; useMLP = False # Multi Layer Perceptron (old TMVA NN implementation); useBDT = True # Boosted Decision Tree; useDL = True # TMVA Deep learning ( CPU or GPU); useKeras = True # Use Keras Deep Learning via PyMVA; ; if ROOT.gSystem.GetFromPipe(""root-config --has-tmva-pymva"") == ""yes"":; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False # cannot use Keras if PYMVA is not available; ; if useKeras:; try:; import tensorflow; except:; ROOT.Warning(""TMVA_Higgs_Classification"", ""Skip using Keras since tensorflow is not available""); useKeras = False; ; outputFile = TFile.Open(""Higgs_ClassificationOutput.root"", ""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTre",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:58067,Availability,avail,available,58067,"ment is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; import ROOT; import os; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; # options to control used methods; useLikelihood = True # likelihood based discriminant; useLikelihoodKDE = False # likelihood based discriminant; useFischer = True # Fischer discriminant; useMLP = False # Multi Layer Perceptron (old TMVA NN implementation); useBDT = True # Boosted Decision Tree; useDL = True # TMVA Deep learning ( CPU or GPU); useKeras = True # Use Keras Deep Learning via PyMVA; ; if ROOT.gSystem.GetFromPipe(""root-config --has-tmva-pymva"") == ""yes"":; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False # cannot use Keras if PYMVA is not available; ; if useKeras:; try:; import tensorflow; except:; ROOT.Warning(""TMVA_Higgs_Classification"", ""Skip using Keras since tensorflow is not available""); useKeras = False; ; outputFile = TFile.Open(""Higgs_ClassificationOutput.root"", ""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:58779,Availability,down,downloaded,58779,"if ROOT.gSystem.GetFromPipe(""root-config --has-tmva-pymva"") == ""yes"":; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False # cannot use Keras if PYMVA is not available; ; if useKeras:; try:; import tensorflow; except:; ROOT.Warning(""TMVA_Higgs_Classification"", ""Skip using Keras since tensorflow is not available""); useKeras = False; ; outputFile = TFile.Open(""Higgs_ClassificationOutput.root"", ""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary numb",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:64319,Availability,error,error,64319,"e that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, RELU,...); ; # *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; # We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; # One can then concatenate different training strategy with different parameters. The training strategy are separated by; # the ``""|""`` separator.; ; # - Optimizer; # - Learning rate; # - Momentum (valid for SGD and RMSPROP); # - Regularization and Weight Decay; # - Dropout; # - Max number of epochs; # - Convergence steps. if the test error will not decrease after that value the training will stop; # - Batch size (This value must be the same specified in the input layout); # - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; # We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; # Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; # In addition to input layout, batch layout and training strategy we add now:; ; # - Type of Loss function (e.g. CROSSENTROPY); # - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); # - Variable Transformation; # - Type of Architecture (e.g. CPU, GPU, Standard); ; # We can then book the DL method using the built option string; if useDL:; useDLGPU = ROOT.gSystem.GetFromPipe(""root-config --has-tmva-gpu"") == ""yes""; ; # Define DNN layout; # Define Training strategies; # one can catenate several training strategies; training1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,""; ""ConvergenceSteps=10,",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:64509,Availability,error,error,64509,"e that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, RELU,...); ; # *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; # We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; # One can then concatenate different training strategy with different parameters. The training strategy are separated by; # the ``""|""`` separator.; ; # - Optimizer; # - Learning rate; # - Momentum (valid for SGD and RMSPROP); # - Regularization and Weight Decay; # - Dropout; # - Max number of epochs; # - Convergence steps. if the test error will not decrease after that value the training will stop; # - Batch size (This value must be the same specified in the input layout); # - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; # We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; # Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; # In addition to input layout, batch layout and training strategy we add now:; ; # - Type of Loss function (e.g. CROSSENTROPY); # - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); # - Variable Transformation; # - Type of Architecture (e.g. CPU, GPU, Standard); ; # We can then book the DL method using the built option string; if useDL:; useDLGPU = ROOT.gSystem.GetFromPipe(""root-config --has-tmva-gpu"") == ""yes""; ; # Define DNN layout; # Define Training strategies; # one can catenate several training strategies; training1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,""; ""ConvergenceSteps=10,",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16990,Deployability,configurat,configuration,16990,"j : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed ti",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:20235,Deployability,configurat,configuration,20235,"optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0116 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.00388 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 7000 bkg: 7000; : #ev",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24284,Deployability,configurat,configuration,24284,"ut = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24394,Deployability,configurat,configuration,24394,"4 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuratio",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24556,Deployability,configurat,configuration,24556,"ion = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24770,Deployability,configurat,configuration,24770,"= Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 1",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25042,Deployability,configurat,configuration,25042,-----------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mda,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25153,Deployability,configurat,configuration,25153,Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25372,Deployability,configurat,configuration,25372, 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:57099,Deployability,configurat,configuration,57099,".393 (0.402) 0.657 (0.681); : dataset PyKeras : 0.138 (0.111) 0.408 (0.410) 0.656 (0.661); : dataset Likelihood : 0.085 (0.082) 0.355 (0.363) 0.580 (0.596); : dataset Fisher : 0.015 (0.015) 0.121 (0.131) 0.487 (0.506); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 6000 events; : ; Dataset:dataset : Created tree 'TrainTree' with 14000 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html␛[0m; ; ## Declare Factory; ; ; ## Create the Factory class. Later you can choose the methods; ## whose performance you'd like to investigate.; ; ## The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; ## - The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ; ## - The second argument is the output file for the training results; ; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; import ROOT; import os; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; # options to control used methods; useLikelihood = True # likelihood based discriminant; useLikelihoodKDE = False # likelihood based discriminant; useFischer = True # Fischer discriminant; useMLP = False # Multi Layer Perceptron (old TMVA NN implementation); useBDT = True # Boosted Decision Tree; useDL = True # TMVA Deep learning ( CPU or GPU); useKeras = True # Use Keras Deep Learning via PyMVA; ; if ROOT.gSystem.GetFromPipe(""root-config --has-tmva-pymva"") == ""yes"":; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False # cannot use Keras if PYMVA is not available; ; if use",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:62755,Deployability,configurat,configuration,62755,"(; loader,; TMVA.Types.kFisher,; ""Fisher"",; H=True,; V=False,; Fisher=True,; VarTransform=None,; CreateMVAPdfs=True,; PDFInterpolMVAPdf=""Spline2"",; NbinsMVAPdf=50,; NsmoothMVAPdf=10,; ); ; # Boosted Decision Trees; if useBDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=200,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; # Multi-Layer Perceptron (Neural Network); if useMLP:; factory.BookMethod(; loader,; TMVA.Types.kMLP,; ""MLP"",; H=False,; V=False,; NeuronType=""tanh"",; VarTransform=""N"",; NCycles=100,; HiddenLayers=""N+5"",; TestRate=5,; UseRegulator=False,; ); ; ## Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; ## Booking Deep Neural Network; ; # Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; # The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; # We define first the DNN layout:; ; # - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; # In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); # - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; # If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; # *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; # note that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, REL",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:518,Energy Efficiency,energy,energy,518,". ROOT: tutorials/tmva/TMVA_Higgs_Classification.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; TMVA_Higgs_Classification.py File ReferenceTutorials » TMVA tutorials. Detailed Description; Classification example of TMVA based on public Higgs UCI dataset ; The UCI data set is a public HIGGS data set , see http://archive.ics.uci.edu/ml/datasets/HIGGS used in this paper: Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014). ******************************************************************************; *Tree :sig_tree : tree *; *Entries : 10000 : Total = 1177229 bytes File Size = 785298 *; * : : Tree compression factor = 1.48 *; ******************************************************************************; *Br 0 :Type : Type/F *; *Entries : 10000 : Total Size= 40556 bytes File Size = 307 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 130.54 *; *............................................................................*; *Br 1 :lepton_pT : lepton_pT/F *; *Entries : 10000 : Total Size= 40581 bytes File Size = 30464 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.32 *; *............................................................................*; *Br 2 :lepton_eta : lepton_eta/F *; *Entries : 10000 : Total Size= 40586 bytes File Size = 28650 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.40 *; *............................................................................*; *Br 3 :lepton_phi : lepton_phi/F *; *Entries : 10000 : Total Size= 40586 bytes File Size = 30508 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.31 *; *............................................................................*; *Br 4 :missing_energy_magnitude : missing_energy_magnitude/F *; *Entries : 10000 : Total Size= 40656 bytes File Size = 35749 *; *Baskets : 1 : Basket Size= 1500",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16841,Energy Efficiency,reduce,reduced,16841,"e : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:17436,Energy Efficiency,adapt,adaptive,17436," reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m===================================",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:17524,Energy Efficiency,adapt,adaptive,17524," reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m===================================",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:19554,Energy Efficiency,reduce,reduces,19554,"[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_j",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:19588,Energy Efficiency,power,power,19588,"[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_j",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:20150,Energy Efficiency,power,powerful,20150,"ity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0116 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.00388 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m;",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:47354,Energy Efficiency,power,power,47354," from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.26 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 11.248; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 11.788; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.3463; TH1.Print Nam",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:67656,Energy Efficiency,allocate,allocates,67656,"orflow.keras.layers import Input, Dense; ; model = Sequential(); model.add(Dense(64, activation=""relu"", input_dim=7)); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we display; ; c1 = factory.GetROCCurve(loader); c1.Draw(); # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots; ; outputFile.Close(); TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:11865,Integrability,message,message,11865," the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER:InputLayout=1|1|7:BatchLayout=1|128|7:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with n",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:17647,Integrability,message,message,17647,"CA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distribution",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:20298,Integrability,message,message,20298,"optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0116 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.00388 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 7000 bkg: 7000; : #ev",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:26503,Integrability,wrap,wraps,26503,"9647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trai",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:26695,Integrability,interface,interface,26695," 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model wei",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:26834,Integrability,message,message,26834,"n_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/112 [..............................] - ETA: 1:09 - loss: 0.6923 - accuracy: 0.6000␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 21/112 [====>.........................] - ETA: 0s - loss: 0.6891 - accuracy: 0.5348 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 42/112 [==========>...................] - ETA: 0",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:11516,Modifiability,variab,variable,11516,"ANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER:InputLayout=1|1|7:BatchLayout=1|128|7:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetiti",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13355,Modifiability,variab,variable,13355,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13386,Modifiability,variab,variable,13386,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13413,Modifiability,variab,variable,13413,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13445,Modifiability,variab,variable,13445,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13473,Modifiability,variab,variable,13473,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13504,Modifiability,variab,variable,13504,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13531,Modifiability,variab,variable,13531,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13563,Modifiability,variab,variable,13563,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13591,Modifiability,variab,variable,13591,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13622,Modifiability,variab,variable,13622,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13649,Modifiability,variab,variable,13649,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13681,Modifiability,variab,variable,13681,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13709,Modifiability,variab,variable,13709,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13742,Modifiability,variab,variable,13742,"ghtsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:14816,Modifiability,variab,variable,14816,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:14847,Modifiability,variab,variable,14847,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:14874,Modifiability,variab,variable,14874,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:14906,Modifiability,variab,variable,14906,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:14934,Modifiability,variab,variable,14934,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:14965,Modifiability,variab,variable,14965,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:14992,Modifiability,variab,variable,14992,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15024,Modifiability,variab,variable,15024,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15052,Modifiability,variab,variable,15052,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15083,Modifiability,variab,variable,15083,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15110,Modifiability,variab,variable,15110,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15142,Modifiability,variab,variable,15142,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15170,Modifiability,variab,variable,15170,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15203,Modifiability,variab,variable,15203,"nse_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; Factory : Booking method: ␛[1mPyKeras␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Loading Keras Model ; : Loaded model from file: model_higgs.h5; Factory : ␛[1mTrain all methods␛[0m; Factory : [dataset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15724,Modifiability,variab,variables,15724,"taset] : Create Transformation ""I"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelat",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:15797,Modifiability,variab,variable,15797,"ariable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; TFHandler_Factory : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16539,Modifiability,variab,variables,16539,"---------------; : m_jj: 1.0318 0.65629 [ 0.15106 16.132 ]; : m_jjj: 1.0217 0.37420 [ 0.34247 8.9401 ]; : m_lv: 1.0507 0.16678 [ 0.26679 3.6823 ]; : m_jlv: 1.0161 0.40288 [ 0.38441 6.5831 ]; : m_bb: 0.97707 0.53961 [ 0.080986 8.2551 ]; : m_wbb: 1.0358 0.36856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune th",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16577,Modifiability,variab,variables,16577,"6856 [ 0.38503 6.4013 ]; : m_wwbb: 0.96265 0.31608 [ 0.43228 4.5350 ]; : -----------------------------------------------------------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16705,Modifiability,variab,variables,16705,"--------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16894,Modifiability,variab,variables,16894,"e : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16941,Modifiability,variab,variables,16941,"e : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16990,Modifiability,config,configuration,16990,"j : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed ti",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:17395,Modifiability,variab,variable,17395,"1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Fa",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:17436,Modifiability,adapt,adaptive,17436," reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m===================================",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:17524,Modifiability,adapt,adaptive,17524," reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m===================================",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:17615,Modifiability,variab,variable,17615,"CA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distribution",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:18745,Modifiability,variab,variable,18745,"ll tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from thi",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:18920,Modifiability,variab,variables,18920,"ng sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributio",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:19335,Modifiability,variab,variable,19335,"ood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Factory : Training finished; : ; Factory : Train method: Fisher for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:19510,Modifiability,variab,variables,19510,"m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:19657,Modifiability,variab,variable,19657,"l and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:19882,Modifiability,variab,variables,19882,"iminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0116 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (140",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:19911,Modifiability,variab,variable,19911,"d background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0116 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.00388 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:20141,Modifiability,variab,variable,20141,"ity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0116 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.00388 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m;",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:20235,Modifiability,config,configuration,20235,"optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0116 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.00388 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDT for Classification; : ; BDT : #events: (reweighted) sig: 7000 bkg: 7000; : #ev",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24284,Modifiability,config,configuration,24284,"ut = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24394,Modifiability,config,configuration,24394,"4 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuratio",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24556,Modifiability,config,configuration,24556,"ion = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:24770,Modifiability,config,configuration,24770,"= Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 1",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25042,Modifiability,config,configuration,25042,-----------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mda,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25153,Modifiability,config,configuration,25153,Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:25372,Modifiability,config,configuration,25372, 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906 0.587335 0.047007 20609.7 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.57382 0.588714 0.592175 0.0469679 20425.3 0; : 5 | 0.569519 0.59535 0.590168 0.0468081 20494.7 1; : 6 | 0.567869 0.590023 0.589038 0.0469022 20541 2; : 7 Minimum Test error found - save the configuration ; : 7 | 0.561012 0.586993 0.590467 0.0471223 20495.3 0; : 8 | 0.559195 0.590522 0.590271 0.0471869 20505.1 1; : 9 | 0.558998 0.588766 0.598449 0.0469787 20193.3 2; : 10 | 0.555383 0.590075 0.591595 0.0470733 20451 3; : 11 Minimum Test error found - save the configuration ; : 11 | 0.555591 0.582449 0.595129 0.047292 20327.2 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.550993 0.581236 0.592181 0.0472746 20436.5 0; : 13 | 0.548186 0.58351 0.592876 0.0471057 20404.2 1; : 14 | 0.547427 0.581875 0.592374 0.0471592 20425 2; : 15 Minimum Test error found - save the configuration ; : 15 | 0.545473 0.580893 0.592763 0.0477516 20432.6 0; : 16 | 0.544618 0.582994 0.591861 0.0472249 20446.7 1; : 17 | 0.542847 0.586139 0.592563 0.047232 20420.6 2; : 18 | 0.542463 0.586829 0.59647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:46806,Modifiability,variab,variables,46806,===================] - 0s 2ms/step - loss: 0.5754 - accuracy: 0.6892 - val_loss: 0.5898 - val_accuracy: 0.6757; : Getting training history for item:0 name = 'loss'; : Getting training history for item:1 name = 'accuracy'; : Getting training history for item:2 name = 'val_loss'; : Getting training history for item:3 name = 'val_accuracy'; : Elapsed time for training with 14000 events: 6.33 sec ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.26 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:46871,Modifiability,variab,variable,46871, item:0 name = 'loss'; : Getting training history for item:1 name = 'accuracy'; : Getting training history for item:2 name = 'val_loss'; : Getting training history for item:3 name = 'val_accuracy'; : Elapsed time for training with 14000 events: 6.33 sec ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.26 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:47264,Modifiability,variab,variable,47264,"ensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.26 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 11.248; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Tot",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:47634,Modifiability,variab,variable,47634,"ss: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.class.C␛[0m; Factory : Training finished; : ; : Ranking input variables (method specific)...; Likelihood : Ranking result (top variable is best ranked); : -------------------------------------; : Rank : Variable : Delta Separation; : -------------------------------------; : 1 : m_bb : 3.688e-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 11.248; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 11.788; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.3463; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 12.0693; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2125; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.2158; Factory : === Destroy and recreate all methods via weight ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:48011,Modifiability,variab,variable,48011,"-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 11.248; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 11.788; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.3463; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 12.0693; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2125; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.2158; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:48066,Modifiability,variab,variable,48066,"-02; : 2 : m_wbb : 3.307e-02; : 3 : m_wwbb : 2.885e-02; : 4 : m_jjj : -1.155e-03; : 5 : m_jj : -1.436e-03; : 6 : m_lv : -5.963e-03; : 7 : m_jlv : -9.884e-03; : -------------------------------------; Fisher : Ranking result (top variable is best ranked); : ---------------------------------; : Rank : Variable : Discr. power; : ---------------------------------; : 1 : m_bb : 1.279e-02; : 2 : m_wwbb : 9.131e-03; : 3 : m_wbb : 2.668e-03; : 4 : m_jlv : 9.145e-04; : 5 : m_jjj : 1.769e-04; : 6 : m_lv : 6.617e-05; : 7 : m_jj : 6.707e-06; : ---------------------------------; BDT : Ranking result (top variable is best ranked); : ----------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 11.248; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 11.788; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.3463; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 12.0693; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2125; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.2158; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:57099,Modifiability,config,configuration,57099,".393 (0.402) 0.657 (0.681); : dataset PyKeras : 0.138 (0.111) 0.408 (0.410) 0.656 (0.661); : dataset Likelihood : 0.085 (0.082) 0.355 (0.363) 0.580 (0.596); : dataset Fisher : 0.015 (0.015) 0.121 (0.131) 0.487 (0.506); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 6000 events; : ; Dataset:dataset : Created tree 'TrainTree' with 14000 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html␛[0m; ; ## Declare Factory; ; ; ## Create the Factory class. Later you can choose the methods; ## whose performance you'd like to investigate.; ; ## The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; ## - The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ; ## - The second argument is the output file for the training results; ; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; import ROOT; import os; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; # options to control used methods; useLikelihood = True # likelihood based discriminant; useLikelihoodKDE = False # likelihood based discriminant; useFischer = True # Fischer discriminant; useMLP = False # Multi Layer Perceptron (old TMVA NN implementation); useBDT = True # Boosted Decision Tree; useDL = True # TMVA Deep learning ( CPU or GPU); useKeras = True # Use Keras Deep Learning via PyMVA; ; if ROOT.gSystem.GetFromPipe(""root-config --has-tmva-pymva"") == ""yes"":; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False # cannot use Keras if PYMVA is not available; ; if use",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:57792,Modifiability,config,config,57792," The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ; ## - The second argument is the output file for the training results; ; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; import ROOT; import os; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; # options to control used methods; useLikelihood = True # likelihood based discriminant; useLikelihoodKDE = False # likelihood based discriminant; useFischer = True # Fischer discriminant; useMLP = False # Multi Layer Perceptron (old TMVA NN implementation); useBDT = True # Boosted Decision Tree; useDL = True # TMVA Deep learning ( CPU or GPU); useKeras = True # Use Keras Deep Learning via PyMVA; ; if ROOT.gSystem.GetFromPipe(""root-config --has-tmva-pymva"") == ""yes"":; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False # cannot use Keras if PYMVA is not available; ; if useKeras:; try:; import tensorflow; except:; ROOT.Warning(""TMVA_Higgs_Classification"", ""Skip using Keras since tensorflow is not available""); useKeras = False; ; outputFile = TFile.Open(""Higgs_ClassificationOutput.root"", ""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59116,Modifiability,variab,variables,59116,"""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeig",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59146,Modifiability,variab,variables,59146,"""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeig",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59226,Modifiability,variab,variable,59226,"""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeig",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59938,Modifiability,variab,variables,59938,"eclare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:62755,Modifiability,config,configuration,62755,"(; loader,; TMVA.Types.kFisher,; ""Fisher"",; H=True,; V=False,; Fisher=True,; VarTransform=None,; CreateMVAPdfs=True,; PDFInterpolMVAPdf=""Spline2"",; NbinsMVAPdf=50,; NsmoothMVAPdf=10,; ); ; # Boosted Decision Trees; if useBDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=200,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; # Multi-Layer Perceptron (Neural Network); if useMLP:; factory.BookMethod(; loader,; TMVA.Types.kMLP,; ""MLP"",; H=False,; V=False,; NeuronType=""tanh"",; VarTransform=""N"",; NCycles=100,; HiddenLayers=""N+5"",; TestRate=5,; UseRegulator=False,; ); ; ## Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; ## Booking Deep Neural Network; ; # Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; # The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; # We define first the DNN layout:; ; # - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; # In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); # - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; # If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; # *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; # note that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, REL",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:63097,Modifiability,variab,variables,63097,"epth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; # Multi-Layer Perceptron (Neural Network); if useMLP:; factory.BookMethod(; loader,; TMVA.Types.kMLP,; ""MLP"",; H=False,; V=False,; NeuronType=""tanh"",; VarTransform=""N"",; NCycles=100,; HiddenLayers=""N+5"",; TestRate=5,; UseRegulator=False,; ); ; ## Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; ## Booking Deep Neural Network; ; # Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; # The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; # We define first the DNN layout:; ; # - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; # In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); # - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; # If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; # *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; # note that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, RELU,...); ; # *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; # We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; # One can then concatenate different training strategy with different parameters. The training strategy are s",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:63303,Modifiability,variab,variables,63303,"nLayers=""N+5"",; TestRate=5,; UseRegulator=False,; ); ; ## Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; ## Booking Deep Neural Network; ; # Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; # The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; # We define first the DNN layout:; ; # - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; # In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); # - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; # If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; # *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; # note that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, RELU,...); ; # *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; # We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; # One can then concatenate different training strategy with different parameters. The training strategy are separated by; # the ``""|""`` separator.; ; # - Optimizer; # - Learning rate; # - Momentum (valid for SGD and RMSPROP); # - Regularization and Weight Decay; # - Dropout; # - Max number of epochs; # - Convergence steps. if the test error will not decrease after that value the training will stop; # - Batch size (This value must be the sa",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:63786,Modifiability,layers,layers,63786,"e that whitespaces between characters are not allowed.; ; # We define first the DNN layout:; ; # - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; # In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); # - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; # If the first layer is dense it should be ``1 | batch size ! number of variables`` (features); ; # *(note the use of the character `|` as separator of input parameters for DNN layout)*; ; # note that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, RELU,...); ; # *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; # We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; # One can then concatenate different training strategy with different parameters. The training strategy are separated by; # the ``""|""`` separator.; ; # - Optimizer; # - Learning rate; # - Momentum (valid for SGD and RMSPROP); # - Regularization and Weight Decay; # - Dropout; # - Max number of epochs; # - Convergence steps. if the test error will not decrease after that value the training will stop; # - Batch size (This value must be the same specified in the input layout); # - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; # We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; # Note we use the ``"":""`` separator to separate the different higher level options, as in the other",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:65208,Modifiability,config,config,65208," epochs; # - Convergence steps. if the test error will not decrease after that value the training will stop; # - Batch size (This value must be the same specified in the input layout); # - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; # We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; # Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; # In addition to input layout, batch layout and training strategy we add now:; ; # - Type of Loss function (e.g. CROSSENTROPY); # - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); # - Variable Transformation; # - Type of Architecture (e.g. CPU, GPU, Standard); ; # We can then book the DL method using the built option string; if useDL:; useDLGPU = ROOT.gSystem.GetFromPipe(""root-config --has-tmva-gpu"") == ""yes""; ; # Define DNN layout; # Define Training strategies; # one can catenate several training strategies; training1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,""; ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,"" # ADAM default parameters; ""DropConfig=0.0+0.0+0.0+0.""; ); # training2 = ROOT.TString(""LearningRate=1e-3,Momentum=0.9""; # ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; # ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; # ""Optimizer=SGD,DropConfig=0.0+0.0+0.0+0.""); ; # General Options.; dnnMethodName = ROOT.TString(""DNN_CPU""); ; if useDLGPU:; arch = ""GPU""; dnnMethodName = ""DNN_GPU""; else:; arch = ""CPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=""G"",; WeightInitialization=""XAVIER"",; InputLayout=""1|1|7"",; BatchLayout=""1|128|7"",; Layout=""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TAN",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:66481,Modifiability,layers,layers,66481,"chs=20,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,"" # ADAM default parameters; ""DropConfig=0.0+0.0+0.0+0.""; ); # training2 = ROOT.TString(""LearningRate=1e-3,Momentum=0.9""; # ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; # ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; # ""Optimizer=SGD,DropConfig=0.0+0.0+0.0+0.""); ; # General Options.; dnnMethodName = ROOT.TString(""DNN_CPU""); ; if useDLGPU:; arch = ""GPU""; dnnMethodName = ""DNN_GPU""; else:; arch = ""CPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=""G"",; WeightInitialization=""XAVIER"",; InputLayout=""1|1|7"",; BatchLayout=""1|128|7"",; Layout=""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"",; TrainingStrategy=training1,; Architecture=arch,; ); ; # Keras DL; if useKeras:; ROOT.Info(""TMVA_Higgs_Classification"", ""Building Deep Learning keras model""); # create Keras model with 4 layers of 64 units and relu activations; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; from tensorflow.keras.layers import Input, Dense; ; model = Sequential(); model.add(Dense(64, activation=""relu"", input_dim=7)); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameMode",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:66657,Modifiability,layers,layers,66657,"omentum=0.9""; # ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; # ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; # ""Optimizer=SGD,DropConfig=0.0+0.0+0.0+0.""); ; # General Options.; dnnMethodName = ROOT.TString(""DNN_CPU""); ; if useDLGPU:; arch = ""GPU""; dnnMethodName = ""DNN_GPU""; else:; arch = ""CPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=""G"",; WeightInitialization=""XAVIER"",; InputLayout=""1|1|7"",; BatchLayout=""1|128|7"",; Layout=""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"",; TrainingStrategy=training1,; Architecture=arch,; ); ; # Keras DL; if useKeras:; ROOT.Info(""TMVA_Higgs_Classification"", ""Building Deep Learning keras model""); # create Keras model with 4 layers of 64 units and relu activations; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; from tensorflow.keras.layers import Input, Dense; ; model = Sequential(); model.add(Dense(64, activation=""relu"", input_dim=7)); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:11541,Performance,perform,performed,11541,"ANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER:InputLayout=1|1|7:BatchLayout=1|128|7:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetiti",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:12322,Performance,perform,perform,12322,"=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""G"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with eve",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:12906,Performance,perform,performance,12906,"ic help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; __________",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16670,Performance,perform,performance,16670,"--------; : Ranking input variables (method unspecific)...; IdTransformation : Ranking result (top variable is best ranked); : -------------------------------; : Rank : Variable : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:17312,Performance,tune,tune,17312,"1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for training with 14000 events: 0.118 sec ; Likelihood : [dataset] : Evaluation of Likelihood on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.0223 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.class.C␛[0m; : Higgs_ClassificationOutput.root:/dataset/Method_Likelihood/Likelihood; Fa",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:19412,Performance,perform,performance,19412,"m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Fisher ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Fisher discriminants select events by distinguishing the mean ; : values of the signal and background distributions in a trans- ; : formed variable space where linear correlations are removed.; : ; : (More precisely: the ""linear discriminator"" determines; : an axis in the (correlated) hyperspace of the input ; : variables such that, when projecting the output classes ; : (signal and background) upon this axis, they are pushed ; : as far as possible away from each other, while events; : of a same class are confined in a close vicinity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:26780,Performance,load,loaded,26780," Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:49282,Performance,perform,performance,49282,"ingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.3463; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 12.0693; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2125; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.2158; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.982",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:49490,Performance,perform,performance,49490," 13.2125; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.2158; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : ---------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:49749,Performance,perform,performance,49749,TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0995 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.kera,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:49944,Performance,perform,performance,49944, ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0995 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing ,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:50720,Performance,perform,performance,50720,tory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0995 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.156 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:56640,Performance,perform,performance,56640,"iency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset DNN_CPU : 0.129 (0.138) 0.392 (0.444) 0.662 (0.711); : dataset BDT : 0.098 (0.099) 0.393 (0.402) 0.657 (0.681); : dataset PyKeras : 0.138 (0.111) 0.408 (0.410) 0.656 (0.661); : dataset Likelihood : 0.085 (0.082) 0.355 (0.363) 0.580 (0.596); : dataset Fisher : 0.015 (0.015) 0.121 (0.131) 0.487 (0.506); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 6000 events; : ; Dataset:dataset : Created tree 'TrainTree' with 14000 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html␛[0m; ; ## Declare Factory; ; ; ## Create the Factory class. Later you can choose the methods; ## whose performance you'd like to investigate.; ; ## The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; ## - The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ; ## - The second argument is the output file for the training results; ; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; import ROOT; import os; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; # options to control used methods; useLikelihood = True # likelihood based discriminant; useLikelihoodKDE = False # likelihood based discriminant; useFischer = True # Fischer discriminant; useMLP = False # Multi Layer Perceptron (old TMVA NN implementation); useBDT = True # Booste",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59301,Performance,load,loader,59301,"""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeig",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59340,Performance,load,loader,59340,"ataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(va",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59368,Performance,load,loader,59368,"t data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59397,Performance,load,loader,59397,"ackground trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59425,Performance,load,loader,59425,"ame = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59454,Performance,load,loader,59454,"utFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the fac",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59482,Performance,load,loader,59482,"n.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59511,Performance,load,loader,59511,"; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59792,Performance,load,loader,59792,"ut file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.Pr",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:59840,Performance,load,loader,59840,"nputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:60135,Performance,load,loader,60135,"eclare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:60628,Performance,load,loader,60628,"Loader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDE",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:60753,Performance,load,loader,60753,"Loader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDE",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:61172,Performance,load,loader,61172,"( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDEtype=""Gauss"",; KDEiter=""Adaptive"",; KDEFineFactor=0.3,; KDEborder=None,; NAvEvtPerBin=50,; ); ; # Fisher discriminant (same as LD); if useFischer:; factory.BookMethod(; loader,; TMVA.Types.kFisher,; ""Fisher"",; H=True,; V=False,; Fisher=True,; VarTransform=None,; CreateMVAPdfs=True,; PDFInterpolMVAPdf=""Spline2"",; NbinsMVAPdf=50,; NsmoothMVAPdf=10,; ); ; # Boosted Decision Trees; if useBDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=200,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSamp",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:61474,Performance,load,loader,61474,"g and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDEtype=""Gauss"",; KDEiter=""Adaptive"",; KDEFineFactor=0.3,; KDEborder=None,; NAvEvtPerBin=50,; ); ; # Fisher discriminant (same as LD); if useFischer:; factory.BookMethod(; loader,; TMVA.Types.kFisher,; ""Fisher"",; H=True,; V=False,; Fisher=True,; VarTransform=None,; CreateMVAPdfs=True,; PDFInterpolMVAPdf=""Spline2"",; NbinsMVAPdf=50,; NsmoothMVAPdf=10,; ); ; # Boosted Decision Trees; if useBDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=200,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; # Multi-Layer Perceptron (Neural Network); if useMLP:; factory.BookMethod(; loader,; TMVA.Types.kMLP,; ""MLP"",; H=False,; V=False,; NeuronType=""tanh"",; VarTransform=""N"",; NCycles=100,; HiddenLayers=""N+5"",; TestRate=5,; UseRegulator=False,; )",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:61762,Performance,load,loader,61762,"epareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDEtype=""Gauss"",; KDEiter=""Adaptive"",; KDEFineFactor=0.3,; KDEborder=None,; NAvEvtPerBin=50,; ); ; # Fisher discriminant (same as LD); if useFischer:; factory.BookMethod(; loader,; TMVA.Types.kFisher,; ""Fisher"",; H=True,; V=False,; Fisher=True,; VarTransform=None,; CreateMVAPdfs=True,; PDFInterpolMVAPdf=""Spline2"",; NbinsMVAPdf=50,; NsmoothMVAPdf=10,; ); ; # Boosted Decision Trees; if useBDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=200,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; # Multi-Layer Perceptron (Neural Network); if useMLP:; factory.BookMethod(; loader,; TMVA.Types.kMLP,; ""MLP"",; H=False,; V=False,; NeuronType=""tanh"",; VarTransform=""N"",; NCycles=100,; HiddenLayers=""N+5"",; TestRate=5,; UseRegulator=False,; ); ; ## Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; ## Booking Deep Neural Network; ; # Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; # The DNN configur",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:62007,Performance,load,loader,62007," Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDEtype=""Gauss"",; KDEiter=""Adaptive"",; KDEFineFactor=0.3,; KDEborder=None,; NAvEvtPerBin=50,; ); ; # Fisher discriminant (same as LD); if useFischer:; factory.BookMethod(; loader,; TMVA.Types.kFisher,; ""Fisher"",; H=True,; V=False,; Fisher=True,; VarTransform=None,; CreateMVAPdfs=True,; PDFInterpolMVAPdf=""Spline2"",; NbinsMVAPdf=50,; NsmoothMVAPdf=10,; ); ; # Boosted Decision Trees; if useBDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=200,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; # Multi-Layer Perceptron (Neural Network); if useMLP:; factory.BookMethod(; loader,; TMVA.Types.kMLP,; ""MLP"",; H=False,; V=False,; NeuronType=""tanh"",; VarTransform=""N"",; NCycles=100,; HiddenLayers=""N+5"",; TestRate=5,; UseRegulator=False,; ); ; ## Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; ## Booking Deep Neural Network; ; # Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; # The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; # We define first the DNN layout:; ; # - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; # In c",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:62311,Performance,load,loader,62311,"[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDEtype=""Gauss"",; KDEiter=""Adaptive"",; KDEFineFactor=0.3,; KDEborder=None,; NAvEvtPerBin=50,; ); ; # Fisher discriminant (same as LD); if useFischer:; factory.BookMethod(; loader,; TMVA.Types.kFisher,; ""Fisher"",; H=True,; V=False,; Fisher=True,; VarTransform=None,; CreateMVAPdfs=True,; PDFInterpolMVAPdf=""Spline2"",; NbinsMVAPdf=50,; NsmoothMVAPdf=10,; ); ; # Boosted Decision Trees; if useBDT:; factory.BookMethod(; loader,; TMVA.Types.kBDT,; ""BDT"",; V=False,; NTrees=200,; MinNodeSize=""2.5%"",; MaxDepth=2,; BoostType=""AdaBoost"",; AdaBoostBeta=0.5,; UseBaggedBoost=True,; BaggedSampleFraction=0.5,; SeparationType=""GiniIndex"",; nCuts=20,; ); ; # Multi-Layer Perceptron (Neural Network); if useMLP:; factory.BookMethod(; loader,; TMVA.Types.kMLP,; ""MLP"",; H=False,; V=False,; NeuronType=""tanh"",; VarTransform=""N"",; NCycles=100,; HiddenLayers=""N+5"",; TestRate=5,; UseRegulator=False,; ); ; ## Here we book the new DNN of TMVA if we have support in ROOT. We will use GPU version if ROOT is enabled with GPU; ; ; ## Booking Deep Neural Network; ; # Here we define the option string for building the Deep Neural network model.; ; #### 1. Define DNN layout; ; # The DNN configuration is defined using a string. Note that whitespaces between characters are not allowed.; ; # We define first the DNN layout:; ; # - **input layout** : this defines the input data format for the DNN as ``input depth | height | width``.; # In case of a dense layer as first layer the input layout should be ``1 | 1 | number of input variables`` (features); # - **batch layout** : this defines how are the input batch. It is related to input layout but not the same.; # If the first layer is dense it should be ``1 | batch size ! number of variables",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:66024,Performance,load,loader,66024,"nsformation; # - Type of Architecture (e.g. CPU, GPU, Standard); ; # We can then book the DL method using the built option string; if useDL:; useDLGPU = ROOT.gSystem.GetFromPipe(""root-config --has-tmva-gpu"") == ""yes""; ; # Define DNN layout; # Define Training strategies; # one can catenate several training strategies; training1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,""; ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; ""Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,"" # ADAM default parameters; ""DropConfig=0.0+0.0+0.0+0.""; ); # training2 = ROOT.TString(""LearningRate=1e-3,Momentum=0.9""; # ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; # ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; # ""Optimizer=SGD,DropConfig=0.0+0.0+0.0+0.""); ; # General Options.; dnnMethodName = ROOT.TString(""DNN_CPU""); ; if useDLGPU:; arch = ""GPU""; dnnMethodName = ""DNN_GPU""; else:; arch = ""CPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=""G"",; WeightInitialization=""XAVIER"",; InputLayout=""1|1|7"",; BatchLayout=""1|128|7"",; Layout=""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"",; TrainingStrategy=training1,; Architecture=arch,; ); ; # Keras DL; if useKeras:; ROOT.Info(""TMVA_Higgs_Classification"", ""Building Deep Learning keras model""); # create Keras model with 4 layers of 64 units and relu activations; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; from tensorflow.keras.layers import Input, Dense; ; model = Sequential(); model.add(Dense(64, activation=""relu"", input_dim=7)); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:66611,Performance,optimiz,optimizers,66611,"0.""; ); # training2 = ROOT.TString(""LearningRate=1e-3,Momentum=0.9""; # ""ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,""; # ""MaxEpochs=20,WeightDecay=1e-4,Regularization=None,""; # ""Optimizer=SGD,DropConfig=0.0+0.0+0.0+0.""); ; # General Options.; dnnMethodName = ROOT.TString(""DNN_CPU""); ; if useDLGPU:; arch = ""GPU""; dnnMethodName = ""DNN_GPU""; else:; arch = ""CPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=""G"",; WeightInitialization=""XAVIER"",; InputLayout=""1|1|7"",; BatchLayout=""1|128|7"",; Layout=""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"",; TrainingStrategy=training1,; Architecture=arch,; ); ; # Keras DL; if useKeras:; ROOT.Info(""TMVA_Higgs_Classification"", ""Building Deep Learning keras model""); # create Keras model with 4 layers of 64 units and relu activations; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; from tensorflow.keras.layers import Input, Dense; ; model = Sequential(); model.add(Dense(64, activation=""relu"", input_dim=7)); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVi",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:66971,Performance,optimiz,optimizer,66971," ""DNN_GPU""; else:; arch = ""CPU""; ; factory.BookMethod(; loader,; TMVA.Types.kDL,; dnnMethodName,; H=False,; V=True,; ErrorStrategy=""CROSSENTROPY"",; VarTransform=""G"",; WeightInitialization=""XAVIER"",; InputLayout=""1|1|7"",; BatchLayout=""1|128|7"",; Layout=""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"",; TrainingStrategy=training1,; Architecture=arch,; ); ; # Keras DL; if useKeras:; ROOT.Info(""TMVA_Higgs_Classification"", ""Building Deep Learning keras model""); # create Keras model with 4 layers of 64 units and relu activations; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; from tensorflow.keras.layers import Input, Dense; ; model = Sequential(); model.add(Dense(64, activation=""relu"", input_dim=7)); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:67375,Performance,load,loader,67375,"ROOT.Info(""TMVA_Higgs_Classification"", ""Building Deep Learning keras model""); # create Keras model with 4 layers of 64 units and relu activations; import tensorflow; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; from tensorflow.keras.layers import Input, Dense; ; model = Sequential(); model.add(Dense(64, activation=""relu"", input_dim=7)); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we display; ; c1 = factory.GetROCCurve(loader); c1.Draw(); # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots; ; outputFile.Close(); TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t net",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:68006,Performance,load,loader,68006,", weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we display; ; c1 = factory.GetROCCurve(loader); c1.Draw(); # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots; ; outputFile.Close(); TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; AuthorHarshal Shende ; Definition in",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:68499,Performance,cache,cacheDir,68499," skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we display; ; c1 = factory.GetROCCurve(loader); c1.Draw(); # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots; ; outputFile.Close(); TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; AuthorHarshal Shende ; Definition in file TMVA_Higgs_Classification.py. tutorialstmvaTMVA_Higgs_Classification.py. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:68614,Performance,cache,cache,68614," skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we display; ; c1 = factory.GetROCCurve(loader); c1.Draw(); # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots; ; outputFile.Close(); TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; AuthorHarshal Shende ; Definition in file TMVA_Higgs_Classification.py. tutorialstmvaTMVA_Higgs_Classification.py. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:26526,Safety,predict,predictions,26526,"9647 0.0472837 20277.3 3; : 19 | 0.540337 0.582774 0.592992 0.0472829 20406.5 4; : 20 | 0.538876 0.589547 0.592937 0.0473047 20409.4 5; : ; : Elapsed time for training with 14000 events: 11.9 sec ; : Evaluate deep neural network on CPU using batches with size = 128; : ; DNN_CPU : [dataset] : Evaluation of DNN_CPU on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.247 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trai",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:67647,Safety,avoid,avoid,67647,"orflow.keras.layers import Input, Dense; ; model = Sequential(); model.add(Dense(64, activation=""relu"", input_dim=7)); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we display; ; c1 = factory.GetROCCurve(loader); c1.Draw(); # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots; ; outputFile.Close(); TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition ",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13078,Security,validat,validation,13078,"ic help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; __________",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:13146,Security,validat,validation,13146,": Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (D",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:23851,Security,validat,validation,23851,"-----------------------------------------------------------; : Start of deep neural network training on CPU using MT, nthreads = 1; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5.7307 ]; : m_lv: 0.0054053 1.0003 [ -3.2810 5.7307 ]; : m_jlv: 0.0044637 0.99837 [ -3.2803 5.7307 ]; : m_bb: 0.0043676 0.99847 [ -3.2797 5.7307 ]; : m_wbb: 0.0042343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 5 Input = ( 1, 1, 7 ) Batch size = 128 Loss function = C; Layer 0 DENSE Layer: ( Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:27026,Security,validat,validation,27026,"n_DNN_CPU.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/112 [..............................] - ETA: 1:09 - loss: 0.6923 - accuracy: 0.6000␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 21/112 [====>.........................] - ETA: 0s - loss: 0.6891 - accuracy: 0.5348 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 42/112 [==========>...................] - ETA: 0",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:27732,Security,validat,validation,27732," : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 11200 training events and 2800 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; dense (Dense) (None, 64) 512 ; ; dense_1 (Dense) (None, 64) 4160 ; ; dense_2 (Dense) (None, 64) 4160 ; ; dense_3 (Dense) (None, 64) 4160 ; ; dense_4 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 13122 (51.26 KB); Trainable params: 13122 (51.26 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/112 [..............................] - ETA: 1:09 - loss: 0.6923 - accuracy: 0.6000␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 21/112 [====>.........................] - ETA: 0s - loss: 0.6891 - accuracy: 0.5348 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 42/112 [==========>...................] - ETA: 0s - loss: 0.6834 - accuracy: 0.5626␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 63/112 [===============>..............] - ETA: 0s - loss: 0.6773 - accuracy: 0.5776␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 86/112 [======================>.......] - ETA: 0s - loss: 0.6765 - accuracy: 0.5751; Epoch 1: val_loss improved from inf to 0.65844, saving model to trained_model_higgs.h5; ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:8537,Testability,test,testing,8537,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:8680,Testability,test,testing,8680,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:8728,Testability,test,testing,8728,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:8807,Testability,test,testing,8807,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:8859,Testability,test,testing,8859,"le Size = 34493 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; *Br 28 :m_wwbb : m_wwbb/F *; *Entries : 10000 : Total Size= 40566 bytes File Size = 34410 *; *Baskets : 1 : Basket Size= 1500672 bytes Compression= 1.16 *; *............................................................................*; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sig_tree of type Signal with 10000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg_tree of type Background with 10000 events; Factory : Booking method: ␛[1mLikelihood␛[0m; : ; Factory : Booking method: ␛[1mFisher␛[0m; : ; Factory : Booking method: ␛[1mBDT␛[0m; : ; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sig_tree; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg_tree; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 7000; : Signal -- testing events : 3000; : Signal -- training and testing events: 10000; : Background -- training events : 7000; : Background -- testing events : 3000; : Background -- training and testing events: 10000; : ; DataSetInfo : Correlation matrix (Signal):; : ----------------------------------------------------------------; : m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb; : m_jj: +1.000 +0.774 -0.004 +0.096 +0.024 +0.512 +0.533; : m_jjj: +0.774 +1.000 -0.010 +0.073 +0.152 +0.674 +0.668; : m_lv: -0.004 -0.010 +1.000 +0.121 -0.027 +0.009 +0.021; : m_jlv: +0.096 +0.073 +0.121 +1.000 +0.313 +0.544 +0.552; : m_bb: +0.024 +0.152 -0.027 +0.313 +1.000 +0.445 +0.333; : m_wbb: +0.512 +0.674 +0.009 +0.544 +0.445 +1.000 +0.915; : m_wwbb: +0.533 +0.668 +0.021 +0.552 +0.333 +0.915 +1.",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:12894,Testability,test,testing,12894,"ic help message]; : InputLayout: ""1|1|7"" [The Layout of the input]; : BatchLayout: ""1|128|7"" [The Layout of the batch]; : Layout: ""DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.9,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,ADAM_beta1=0.9,ADAM_beta2=0.999,ADAM_eps=1.E-7,DropConfig=0.0+0.0+0.0+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; DNN_CPU : [dataset] : Create Transformation ""G"" with events from all classes.; : ; : Transformation, Variable selection : ; : Input : variable 'm_jj' <---> Output : variable 'm_jj'; : Input : variable 'm_jjj' <---> Output : variable 'm_jjj'; : Input : variable 'm_lv' <---> Output : variable 'm_lv'; : Input : variable 'm_jlv' <---> Output : variable 'm_jlv'; : Input : variable 'm_bb' <---> Output : variable 'm_bb'; : Input : variable 'm_wbb' <---> Output : variable 'm_wbb'; : Input : variable 'm_wwbb' <---> Output : variable 'm_wwbb'; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; __________",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:23812,Testability,test,testing,23812,"-----------------------------------------------------------; : Start of deep neural network training on CPU using MT, nthreads = 1; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5.7307 ]; : m_lv: 0.0054053 1.0003 [ -3.2810 5.7307 ]; : m_jlv: 0.0044637 0.99837 [ -3.2803 5.7307 ]; : m_bb: 0.0043676 0.99847 [ -3.2797 5.7307 ]; : m_wbb: 0.0042343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 5 Input = ( 1, 1, 7 ) Batch size = 128 Loss function = C; Layer 0 DENSE Layer: ( Input = 7 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 128 , 64 ) Activation Function = Tanh; Layer 4 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 128 , 1 ) Activation Function = Identity; : Using 11200 events for training and 2800 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.868652; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.659892 0.620582 0.589558 0.0469541 20523.3 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.604286 0.594787 0.586926 0.0471912 20632.4 0; : 3 | 0.58117 0.603906",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:48693,Testability,test,testing,48693,"Rank : Variable : Variable Importance; : ----------------------------------------; : 1 : m_bb : 2.089e-01; : 2 : m_wwbb : 1.673e-01; : 3 : m_wbb : 1.568e-01; : 4 : m_jlv : 1.560e-01; : 5 : m_jjj : 1.421e-01; : 6 : m_jj : 1.052e-01; : 7 : m_lv : 6.369e-02; : ----------------------------------------; : No variable ranking supplied by classifier: DNN_CPU; : No variable ranking supplied by classifier: PyKeras; TH1.Print Name = TrainingHistory_DNN_CPU_trainingError, Entries= 0, Total sum= 11.248; TH1.Print Name = TrainingHistory_DNN_CPU_valError, Entries= 0, Total sum= 11.788; TH1.Print Name = TrainingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.3463; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 12.0693; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2125; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.2158; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Fac",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:49352,Testability,test,testing,49352,"ingHistory_PyKeras_'accuracy', Entries= 0, Total sum= 13.3463; TH1.Print Name = TrainingHistory_PyKeras_'loss', Entries= 0, Total sum= 12.0693; TH1.Print Name = TrainingHistory_PyKeras_'val_accuracy', Entries= 0, Total sum= 13.2125; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.2158; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.982",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:49552,Testability,test,testing,49552," 13.2125; TH1.Print Name = TrainingHistory_PyKeras_'val_loss', Entries= 0, Total sum= 12.2158; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : ---------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:49687,Testability,test,testing,49687,TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0995 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.kera,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:49805,Testability,test,testing,49805,TMVA_Higgs_Classification_Likelihood.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_BDT.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_DNN_CPU.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_PyKeras.weights.xml␛[0m; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: Likelihood for Classification performance; : ; Likelihood : [dataset] : Evaluation of Likelihood on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0107 sec ; Factory : Test method: Fisher for Classification performance; : ; Fisher : [dataset] : Evaluation of Fisher on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0995 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.kera,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:50581,Testability,test,testing,50581,ample (6000 events); : Elapsed time for evaluation of 6000 events: 0.00276 sec ; : Dataset[dataset] : Evaluation of Fisher on testing sample; Factory : Test method: BDT for Classification performance; : ; BDT : [dataset] : Evaluation of BDT on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0472 sec ; Factory : Test method: DNN_CPU for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0995 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.156 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ ,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:51013,Testability,test,testing,51013,000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0995 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.156 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: Fisher; : ; Fisher : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Also filling probability and rarity histograms (on request)...; TFHandler_Fisher : Variable Mean RMS [ Min Max,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:51223,Testability,test,test,51223,4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; DNN_CPU : [dataset] : Evaluation of DNN_CPU on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.0995 sec ; Factory : Test method: PyKeras for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.156 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: Fisher; : ; Fisher : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Also filling probability and rarity histograms (on request)...; TFHandler_Fisher : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_l,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:51850,Testability,test,test,51850,ion from TensorFlow : tf.keras; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_higgs.h5; PyKeras : [dataset] : Evaluation of PyKeras on testing sample (6000 events); : Elapsed time for evaluation of 6000 events: 0.156 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: Likelihood; : ; Likelihood : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_Likelihood : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: Fisher; : ; Fisher : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Also filling probability and rarity histograms (on request)...; TFHandler_Fisher : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_BDT : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:52533,Testability,test,test,52533,: m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: Fisher; : ; Fisher : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Also filling probability and rarity histograms (on request)...; TFHandler_Fisher : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_BDT : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: DNN_CPU; : ; DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:53155,Testability,test,test,53155,.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: BDT; : ; BDT : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_BDT : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: DNN_CPU; : ; DNN_CPU : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5.7307 ]; : m_lv: 0.0054053 1.0003 [ -3.2810 5.7307 ]; : m_jlv: 0.0044637 0.99837 [ -3.2803 5.7307 ]; : m_bb: 0.0043676 0.99847 [ -3.2797 5.7307 ]; : m_wbb: 0.0042343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 ,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:54372,Testability,test,test,54372, RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.0043655 0.99836 [ -3.2801 5.7307 ]; : m_jjj: 0.0044371 0.99827 [ -3.2805 5.7307 ]; : m_lv: 0.0054053 1.0003 [ -3.2810 5.7307 ]; : m_jlv: 0.0044637 0.99837 [ -3.2803 5.7307 ]; : m_bb: 0.0043676 0.99847 [ -3.2797 5.7307 ]; : m_wbb: 0.0042343 0.99744 [ -3.2803 5.7307 ]; : m_wwbb: 0.0046014 0.99948 [ -3.2802 5.7307 ]; : -----------------------------------------------------------; TFHandler_DNN_CPU : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 0.017919 1.0069 [ -3.3498 3.4247 ]; : m_jjj: 0.020352 1.0044 [ -3.2831 3.3699 ]; : m_lv: 0.016356 0.99266 [ -3.2339 3.3958 ]; : m_jlv: -0.018431 0.98242 [ -3.0632 5.7307 ]; : m_bb: 0.0069564 0.98851 [ -2.9734 3.3513 ]; : m_wbb: -0.010633 0.99340 [ -3.2442 3.2244 ]; : m_wwbb: -0.012669 0.99259 [ -3.1871 5.7307 ]; : -----------------------------------------------------------; Factory : Evaluate classifier: PyKeras; : ; PyKeras : [dataset] : Loop over test events and fill histograms with classifier response...; : ; TFHandler_PyKeras : Variable Mean RMS [ Min Max ]; : -----------------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset DNN_CPU : 0.759; : dataset BDT : 0.754; : dataset PyKeras : 0.752; : dataset Likelihood : 0.698; : dataset Fisher : 0.642; : -------------------------------,MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:55649,Testability,test,test,55649,"---------------------------------------------------; : m_jj: 1.0447 0.66216 [ 0.14661 10.222 ]; : m_jjj: 1.0275 0.37015 [ 0.34201 5.6016 ]; : m_lv: 1.0500 0.15582 [ 0.29757 2.8989 ]; : m_jlv: 1.0053 0.39478 [ 0.41660 5.8799 ]; : m_bb: 0.97464 0.52138 [ 0.10941 5.5163 ]; : m_wbb: 1.0296 0.35719 [ 0.38878 3.9747 ]; : m_wwbb: 0.95617 0.30368 [ 0.44118 4.0728 ]; : -----------------------------------------------------------; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset DNN_CPU : 0.759; : dataset BDT : 0.754; : dataset PyKeras : 0.752; : dataset Likelihood : 0.698; : dataset Fisher : 0.642; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset DNN_CPU : 0.129 (0.138) 0.392 (0.444) 0.662 (0.711); : dataset BDT : 0.098 (0.099) 0.393 (0.402) 0.657 (0.681); : dataset PyKeras : 0.138 (0.111) 0.408 (0.410) 0.656 (0.661); : dataset Likelihood : 0.085 (0.082) 0.355 (0.363) 0.580 (0.596); : dataset Fisher : 0.015 (0.015) 0.121 (0.131) 0.487 (0.506); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 6000 events; : ; Dataset:dataset : Created tree 'TrainTree' with 14000 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please v",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:58897,Testability,test,test,58897," # cannot use Keras if PYMVA is not available; ; if useKeras:; try:; import tensorflow; except:; ROOT.Warning(""TMVA_Higgs_Classification"", ""Skip using Keras since tensorflow is not available""); useKeras = False; ; outputFile = TFile.Open(""Higgs_ClassificationOutput.root"", ""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName):; ROOT.Info(""TMVA_Higgs_Classification"", ""Download Higgs_data.root file""); TFile.SetCacheFileDir("".""); inputFile = TFile.Open(inputFileLink, ""CACHEREAD""); if inputFile is None:; raise FileNotFoundError(""Input file cannot be downloaded - exit""); else:; # file exists; inputFile = TFile.Open(inputFileName); ; ; # --- Register the training and test trees; signalTree = inputFile.Get(""sig_tree""); backgroundTree = inputFile.Get(""bkg_tree""); signalTree.Print(); ; ## Declare DataLoader(s); ; # The next step is to declare the DataLoader class that deals with input variables; # Define the input variables that shall be used for the MVA training; # note that you may also use variable expressions, which can be parsed by TTree::Draw( ""expression"" )]; loader = TMVA.DataLoader(""dataset""); ; loader.AddVariable(""m_jj""); loader.AddVariable(""m_jjj""); loader.AddVariable(""m_lv""); loader.AddVariable(""m_jlv""); loader.AddVariable(""m_bb""); loader.AddVariable(""m_wbb""); loader.AddVariable(""m_wwbb""); ; # We set now the input data trees in the TMVA DataLoader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backg",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:60480,Testability,test,testing,60480,"Loader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDE",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:60616,Testability,test,testing,60616,"Loader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDE",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:60729,Testability,test,testing,60729,"Loader class; # global event weights per tree (see below for setting event-wise weights); signalWeight = 1.0; backgroundWeight = 1.0; # You can add an arbitrary number of signal or background trees; loader.AddSignalTree(signalTree, signalWeight); loader.AddBackgroundTree(backgroundTree, backgroundWeight); ; # Set individual event weights (the variables must exist in the original TTree); # for signal : factory->SetSignalWeightExpression (""weight1*weight2"");; # for background: factory->SetBackgroundWeightExpression(""weight1*weight2"");; # loader->SetBackgroundWeightExpression( ""weight"" );; ; ; # Apply additional cuts on the signal and background samples (can be different); mycuts = ROOT.TCut("""") # for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; mycutb = ROOT.TCut("""") # for example: TCut mycutb = ""abs(var1)<0.5"";; ; # Tell the factory how to use the training and testing events; #; # If no numbers of events are given, half of the events in the tree are used; # for training, and the other half for testing:; # loader->PrepareTrainingAndTestTree( mycut, ""SplitMode=random:!V"" );; # To also specify the number of testing events, use:; ; loader.PrepareTrainingAndTestTree(; mycuts, mycutb, nTrain_Signal=7000, nTrain_Background=7000, SplitMode=""Random"", NormMode=""NumEvents"", V=False; ); ; ## Booking Methods; ; # Here we book the TMVA methods. We book first a Likelihood based on KDE (Kernel Density Estimation), a Fischer discriminant, a BDT; # and a shallow neural network; # Likelihood (""naive Bayes estimator""); if useLikelihood:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""Likelihood"",; H=True,; V=False,; TransformOutput=True,; PDFInterpol=""Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10"",; NSmooth=1,; NAvEvtPerBin=50,; ); ; # Use a kernel density estimator to approximate the PDFs; if useLikelihoodKDE:; factory.BookMethod(; loader,; TMVA.Types.kLikelihood,; ""LikelihoodKDE"",; H=False,; V=False,; TransformOutput=False,; PDFInterpol=""KDE"",; KDE",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:64314,Testability,test,test,64314,"e that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, RELU,...); ; # *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; # We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; # One can then concatenate different training strategy with different parameters. The training strategy are separated by; # the ``""|""`` separator.; ; # - Optimizer; # - Learning rate; # - Momentum (valid for SGD and RMSPROP); # - Regularization and Weight Decay; # - Dropout; # - Max number of epochs; # - Convergence steps. if the test error will not decrease after that value the training will stop; # - Batch size (This value must be the same specified in the input layout); # - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; # We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; # Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; # In addition to input layout, batch layout and training strategy we add now:; ; # - Type of Loss function (e.g. CROSSENTROPY); # - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); # - Variable Transformation; # - Type of Architecture (e.g. CPU, GPU, Standard); ; # We can then book the DL method using the built option string; if useDL:; useDLGPU = ROOT.gSystem.GetFromPipe(""root-config --has-tmva-gpu"") == ""yes""; ; # Define DNN layout; # Define Training strategies; # one can catenate several training strategies; training1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,""; ""ConvergenceSteps=10,",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:64504,Testability,test,test,64504,"e that in case of only dense layer the input layout could be omitted but it is required when defining more; # complex architectures; ; # - **layer layout** string defining the layer architecture. The syntax is; # - layer type (e.g. DENSE, CONV, RNN); # - layer parameters (e.g. number of units); # - activation function (e.g TANH, RELU,...); ; # *the different layers are separated by the ``"",""`` *; ; #### 2. Define Training Strategy; ; # We define here the training strategy parameters for the DNN. The parameters are separated by the ``"",""`` separator.; # One can then concatenate different training strategy with different parameters. The training strategy are separated by; # the ``""|""`` separator.; ; # - Optimizer; # - Learning rate; # - Momentum (valid for SGD and RMSPROP); # - Regularization and Weight Decay; # - Dropout; # - Max number of epochs; # - Convergence steps. if the test error will not decrease after that value the training will stop; # - Batch size (This value must be the same specified in the input layout); # - Test Repetitions (the interval when the test error will be computed); ; ; #### 3. Define general DNN options; ; # We define the general DNN options concatenating in the final string the previously defined layout and training strategy.; # Note we use the ``"":""`` separator to separate the different higher level options, as in the other TMVA methods.; # In addition to input layout, batch layout and training strategy we add now:; ; # - Type of Loss function (e.g. CROSSENTROPY); # - Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL ); # - Variable Transformation; # - Type of Architecture (e.g. CPU, GPU, Standard); ; # We can then book the DL method using the built option string; if useDL:; useDLGPU = ROOT.gSystem.GetFromPipe(""root-config --has-tmva-gpu"") == ""yes""; ; # Define DNN layout; # Define Training strategies; # one can catenate several training strategies; training1 = ROOT.TString(; ""LearningRate=1e-3,Momentum=0.9,""; ""ConvergenceSteps=10,",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:67819,Testability,test,test,67819,"ctivation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we display; ; c1 = factory.GetROCCurve(loader); c1.Draw(); # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots; ; outputFile.Close(); TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyI",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:67859,Testability,test,test,67859,"ctivation=""relu"")); model.add(Dense(64, activation=""relu"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(""model_higgs.h5""); model.summary(); ; if not os.path.exists(""model_higgs.h5""):; raise FileNotFoundError(""Error creating Keras model file - skip using Keras""); else:; # book PyKeras method only if Keras model could be created; ROOT.Info(""TMVA_Higgs_Classification"", ""Booking Deep Learning keras model""); factory.BookMethod(; loader,; TMVA.Types.kPyKeras,; ""PyKeras"",; H=True,; V=False,; VarTransform=None,; FilenameModel=""model_higgs.h5"",; FilenameTrainedModel=""trained_model_higgs.h5"",; NumEpochs=20,; BatchSize=100,; ); # GpuOptions=""allow_growth=True"",; # ) # needed for RTX NVidia card and to avoid TF allocates all GPU memory; ; ; ## Train Methods; ; # Here we train all the previously booked methods.; ; factory.TrainAllMethods(); ## Test all methods; ; # Now we test and evaluate all methods using the test data set; factory.TestAllMethods(); ; factory.EvaluateAllMethods(); ; # after we get the ROC curve and we display; ; c1 = factory.GetROCCurve(loader); c1.Draw(); # at the end we close the output file which contains the evaluation result of all methods and it can be used by TMVAGUI; # to display additional plots; ; outputFile.Close(); TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::PyMethodBase::PyInitializestatic void PyI",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:16911,Usability,simpl,simply,16911,"e : Separation; : -------------------------------; : 1 : m_bb : 9.511e-02; : 2 : m_wbb : 4.268e-02; : 3 : m_wwbb : 4.178e-02; : 4 : m_jjj : 2.825e-02; : 5 : m_jlv : 1.999e-02; : 6 : m_jj : 3.834e-03; : 7 : m_lv : 3.699e-03; : -------------------------------; Factory : Train method: Likelihood for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ Likelihood ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : The maximum-likelihood classifier models the data with probability ; : density functions (PDF) reproducing the signal and background; : distributions of the input variables. Correlations among the ; : variables are ignored.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Required for good performance are decorrelated input variables; : (PCA transformation via the option ""VarTransform=Decorrelate""; : may be tried). Irreducible non-linear correlations may be reduced; : by precombining strongly correlated input variables, or by simply; : removing one of the variables.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : High fidelity PDF estimates are mandatory, i.e., sufficient training ; : statistics is required to populate the tails of the distributions; : It would be a surprise if the default Spline or KDE kernel parameters; : provide a satisfying fit to the data. The user is advised to properly; : tune the events per bin and smooth options in the spline cases; : individually per variable. If the KDE kernel is used, the adaptive; : Gaussian kernel may lead to artefacts, so please always also try; : the non-adaptive one.; : ; : All tuning parameters must be adjusted individually for each input; : variable!; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Filling reference histograms; : Building PDF out of reference histograms; : Elapsed time for t",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:20093,Usability,simpl,simple,20093,"ity. The ; : linearity property of this classifier is reflected in the ; : metric with which ""far apart"" and ""close vicinity"" are ; : determined: the covariance matrix of the discriminating; : variable space.); : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : Optimal performance for Fisher discriminants is obtained for ; : linearly correlated Gaussian-distributed variables. Any deviation; : from this ideal reduces the achievable separation power. In ; : particular, no discrimination at all is achieved for a variable; : that has the same sample mean for signal and background, even if ; : the shapes of the distributions are very different. Thus, Fisher ; : discriminants often benefit from suitable transformations of the ; : input variables. For example, if a variable x in [-1,1] has a ; : a parabolic signal distributions, and a uniform background; : distributions, their mean value is zero in both cases, leading ; : to no separation. The simple transformation x -> |x| renders this ; : variable powerful for the use in a Fisher discriminant.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : <None>; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; Fisher : Results for Fisher coefficients:; : -----------------------; : Variable: Coefficient:; : -----------------------; : m_jj: -0.051; : m_jjj: +0.192; : m_lv: +0.045; : m_jlv: +0.059; : m_bb: -0.211; : m_wbb: +0.549; : m_wwbb: -0.778; : (offset): +0.136; : -----------------------; : Elapsed time for training with 14000 events: 0.0116 sec ; Fisher : [dataset] : Evaluation of Fisher on training sample (14000 events); : Elapsed time for evaluation of 14000 events: 0.00388 sec ; : <CreateMVAPdfs> Separation from histogram (PDF): 0.090 (0.000); : Dataset[dataset] : Evaluation of Fisher on training sample; : Creating xml weight file: ␛[0;36mdataset/weights/TMVA_Higgs_Classification_Fisher.weights.xml␛[0m;",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html:57679,Usability,learn,learning,57679," Factory; ; ; ## Create the Factory class. Later you can choose the methods; ## whose performance you'd like to investigate.; ; ## The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass; ; ## - The first argument is the base of the name of all the output; ## weightfiles in the directory weight/ that will be created with the; ## method parameters; ; ## - The second argument is the output file for the training results; ; ## - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in the option string; ; import ROOT; import os; ; TMVA = ROOT.TMVA; TFile = ROOT.TFile; ; TMVA.Tools.Instance(); ; # options to control used methods; useLikelihood = True # likelihood based discriminant; useLikelihoodKDE = False # likelihood based discriminant; useFischer = True # Fischer discriminant; useMLP = False # Multi Layer Perceptron (old TMVA NN implementation); useBDT = True # Boosted Decision Tree; useDL = True # TMVA Deep learning ( CPU or GPU); useKeras = True # Use Keras Deep Learning via PyMVA; ; if ROOT.gSystem.GetFromPipe(""root-config --has-tmva-pymva"") == ""yes"":; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False # cannot use Keras if PYMVA is not available; ; if useKeras:; try:; import tensorflow; except:; ROOT.Warning(""TMVA_Higgs_Classification"", ""Skip using Keras since tensorflow is not available""); useKeras = False; ; outputFile = TFile.Open(""Higgs_ClassificationOutput.root"", ""RECREATE""); factory = TMVA.Factory(; ""TMVA_Higgs_Classification"", outputFile, V=False, ROC=True, Silent=False, Color=True, AnalysisType=""Classification""; ); ; ; ## Setup Dataset(s); ; # Define now input data file and signal and background trees; ; inputFileName = ""Higgs_data.root""; inputFileLink = ""http://root.cern.ch/files/"" + inputFileName; ; ; if ROOT.gSystem.AccessPathName(inputFileName)",MatchSource.WIKI,doc/master/TMVA__Higgs__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__Higgs__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:7026,Availability,error,error,7026,"|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are i",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:9929,Availability,error,error,9929,"00:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of t",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15259,Availability,error,error,15259,"- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.60",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15529,Availability,error,error,15529,"** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the conf",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15800,Availability,error,error,15800," 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15910,Availability,error,error,15910," = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.58963",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16020,Availability,error,error,16020,"ty; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.5995",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16183,Availability,error,error,16183,"0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.5",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16294,Availability,error,error,16294,----------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16405,Availability,error,error,16405,: --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for traini,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16516,Availability,error,error,16516, error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LS,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16628,Availability,error,error,16628,4394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 320,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16740,Availability,error,error,16740, 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16851,Availability,error,error,16851,0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : T,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16963,Availability,error,error,16963,616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network tr,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:17128,Availability,error,error,17128,"0791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:17240,Availability,error,error,17240,"68 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:18781,Availability,error,error,18781,"andalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19050,Availability,error,error,19050,"L NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 143",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19160,Availability,error,error,19160,"= 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19270,Availability,error,error,19270," Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19536,Availability,error,error,19536,"Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19700,Availability,error,error,19700,"eta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural netw",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19866,Availability,error,error,19866, Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 ev,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19978,Availability,error,error,19978,------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:20419,Availability,error,error,20419,"01998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; ========================",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:20531,Availability,error,error,20531,"ound - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 164",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:60694,Availability,avail,available,60694,"ut << std::endl;; sgn.Fill();; bkg.Fill();; ; if (n == 1) {; auto c1 = new TCanvas();; c1->Divide(ntime, 2);; for (int j = 0; j < ntime; ++j) {; c1->cd(j + 1);; v1[j]->Draw();; }; for (int j = 0; j < ntime; ++j) {; c1->cd(ntime + j + 1);; v2[j]->Draw();; }; gPad->Update();; }; }; if (n > 1) {; sgn.Write();; bkg.Write();; sgn.Print();; bkg.Print();; f.Close();; }; }; /// macro for performing a classification using a Recurrent Neural Network; /// @param nevts = 2000 Number of events used. (increase for better classification results); /// @param use_type; /// use_type = 0 use Simple RNN network; /// use_type = 1 use LSTM network; /// use_type = 2 use GRU; /// use_type = 3 build 3 different networks with RNN, LSTM and GRU; ; void TMVA_RNN_Classification(int nevts = 2000, int use_type = 1); {; ; const int ninput = 30;; const int ntime = 10;; const int batchSize = 100;; const int maxepochs = 20;; ; int nTotEvts = nevts; // total events to be generated for signal or background; ; bool useKeras = true;; ; ; bool useTMVA_RNN = true;; bool useTMVA_DNN = true;; bool useTMVA_BDT = false;; ; std::vector<std::string> rnn_types = {""RNN"", ""LSTM"", ""GRU""};; std::vector<bool> use_rnn_type = {1, 1, 1};; if (use_type >=0 && use_type < 3) {; use_rnn_type = {0,0,0};; use_rnn_type[use_type] = 1;; }; bool useGPU = true; // use GPU for TMVA if available; ; #ifndef R__HAS_TMVAGPU; useGPU = false;; #ifndef R__HAS_TMVACPU; Warning(""TMVA_RNN_Classification"", ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for RNN"");; useTMVA_RNN = false;; #endif; #endif; ; ; TString archString = (useGPU) ? ""GPU"" : ""CPU"";; ; bool writeOutputFile = true;; ; ; ; const char *rnn_type = ""RNN"";; ; #ifdef R__HAS_PYMVA; TMVA::PyMethodBase::PyInitialize();; #else; useKeras = false;; #endif; ; #ifdef R__USE_IMT; int num_threads = 4; // use max 4 threads; // switch off MT in OpenBLAS to avoid conflict with tbb; gSystem->Setenv(""OMP_NUM_THREADS"", ""1"");; ; // do enable MT running; if (n",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:71898,Availability,error,error,71898,"; ## Book TMVA BDT; **/; ; if (useTMVA_BDT) {; ; factory->BookMethod(dataloader, TMVA::Types::kBDT, ""BDTG"",; ""!H:!V:NTrees=100:MinNodeSize=2.5%:BoostType=Grad:Shrinkage=0.10:UseBaggedBoost:""; ""BaggedSampleFraction=0.5:nCuts=20:""; ""MaxDepth=2"");; ; }; ; /// Train all methods; factory->TrainAllMethods();; ; std::cout << ""nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; // ---- Evaluate all MVAs using the set of test events; factory->TestAllMethods();; ; // ----- Evaluate and compare performance of all configured MVAs; factory->EvaluateAllMethods();; ; // check method; ; // plot ROC curve; auto c1 = factory->GetROCCurve(dataloader);; c1->Draw();; ; if (outputFile) outputFile->Close();; }; DataLoader.h; DataSetInfo.h; MethodDL.h; f#define f(i)Definition RSha256.hxx:104; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; TFile.h; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; Formchar * Form(const char *fmt,...)Formats a string in a circular formatting buffer.Definition TString.cxx:2489; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TTree.h; gPad#define gPadDefinition TVirtualPad.h:308; TCanvasThe Canvas class.Definition TCanvas.h:23; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TF11-Dim function classDefinition TF1.h:233; TF1::SetParametersvirtual void SetParameters(const Double_t *params)Definition TF1.h:677; TFileA ROOT file is an on-disk fi",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15552,Deployability,configurat,configuration,15552,"** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the conf",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15823,Deployability,configurat,configuration,15823," 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15933,Deployability,configurat,configuration,15933," = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.58963",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16043,Deployability,configurat,configuration,16043,"ty; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.5995",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16206,Deployability,configurat,configuration,16206,"0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.5",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16317,Deployability,configurat,configuration,16317,----------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16428,Deployability,configurat,configuration,16428,: --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for traini,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16539,Deployability,configurat,configuration,16539, error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LS,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16651,Deployability,configurat,configuration,16651,4394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 320,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16763,Deployability,configurat,configuration,16763, 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16874,Deployability,configurat,configuration,16874,0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : T,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16986,Deployability,configurat,configuration,16986,616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network tr,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:17151,Deployability,configurat,configuration,17151,"0791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:17263,Deployability,configurat,configuration,17263,"68 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19073,Deployability,configurat,configuration,19073,"L NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 143",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19183,Deployability,configurat,configuration,19183,"= 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19293,Deployability,configurat,configuration,19293," Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19559,Deployability,configurat,configuration,19559,"Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19723,Deployability,configurat,configuration,19723,"eta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural netw",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19889,Deployability,configurat,configuration,19889, Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 ev,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:20001,Deployability,configurat,configuration,20001,------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:20442,Deployability,configurat,configuration,20442,"01998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; ========================",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:20554,Deployability,configurat,configuration,20554,"ound - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 164",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:62828,Deployability,configurat,configuration,62828,"sts create it; if (!fileExist) {; MakeTimeData(nTotEvts,ntime, ninput);; }; ; ; auto inputFile = TFile::Open(inputFileName);; if (!inputFile) {; Error(""TMVA_RNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; ; std::cout << ""--- RNNClassification : Using input file: "" << inputFile->GetName() << std::endl;; ; // Create a ROOT output file where TMVA will store ntuples, histograms, etc.; TString outfileName(TString::Format(""data_RNN_%s.root"", archString.Data()));; TFile *outputFile = nullptr;; if (writeOutputFile) outputFile = TFile::Open(outfileName, ""RECREATE"");; ; /**; ## Declare Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; the option string; ; **/; ; // Creating the factory object; TMVA::Factory *factory = new TMVA::Factory(""TMVAClassification"", outputFile,; ""!V:!Silent:Color:DrawProgressBar:Transformations=None:!Correlations:""; ""AnalysisType=Classification:ModelPersistence"");; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; TTree *signalTree = (TTree *)inputFile->Get(""sgn"");; TTree *background = (TTree *)inputFile->Get(""bkg"");; ; const int nvar = ninput * ntime;; ; /// add variables - use new AddVariablesArray function; for (auto i = 0; i < ntime; i++) {; dataloader->AddVariablesArray(Form(""vars_time%d"", i), ninput);; }; ; dataloader->AddSignalTree(signalTree, 1.0);; dataloader->AddBackground",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:380,Integrability,depend,dependent,380,". ROOT: tutorials/tmva/TMVA_RNN_Classification.C File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA_RNN_Classification.C File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Recurrent Neural Network ; This is an example of using a RNN in TMVA. We do classification using a toy time dependent data set that is generated when running this example macro. ; Running with nthreads = 4; --- RNNClassification : Using input file: time_data_t10_d30.root; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sgn of type Signal with 2000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg of type Background with 2000 events; number of variables is 300; vars_time0[0],vars_time0[1],vars_time0[2],vars_time0[3],vars_time0[4],vars_time0[5],vars_time0[6],vars_time0[7],vars_time0[8],vars_time0[9],vars_time0[10],vars_time0[11],vars_time0[12],vars_time0[13],vars_time0[14],vars_time0[15],vars_time0[16],vars_time0[17],vars_time0[18],vars_time0[19],vars_time0[20],vars_time0[21],vars_time0[22],vars_time0[23],vars_time0[24],vars_time0[25],vars_time0[26],vars_time0[27],vars_time0[28],vars_time0[29],vars_time1[0],vars_time1[1],vars_time1[2],vars_time1[3],vars_time1[4],vars_time1[5],vars_time1[6],vars_time1[7],vars_time1[8],vars_time1[9],vars_time1[10],vars_time1[11],vars_time1[12],vars_time1[13],vars_time1[14],vars_time1[15],vars_time1[16],vars_time1[17],vars_time1[18],vars_time1[19],vars_time1[20],vars_time1[21],vars_time1[22],vars_time1[23],vars_time1[24],vars_time1[25],vars_time1[26],vars_time1[27],vars_time1[28],vars_time1[29],vars_time2[0],vars_time2[1],vars_time2[2],vars_time2[3],vars_time2[4],vars_time2[5],vars_time2[6],vars_time2[7],vars_time2[8],vars_time2[9],vars_time2[10],vars_time2[11],vars_time2[12],vars_time2[13],vars_time2[14],vars_time2[15],vars_time2[16],vars_time2[17],vars_time2[18],vars_time2[19],vars_time2[20],vars_time2[21],vars_time2[22],vars_time2[23",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:6815,Integrability,message,message,6815,"er:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:9719,Integrability,message,message,9719," : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [E",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:69010,Integrability,depend,depending,69010,"odels; ; Book the different types of recurrent models in Keras (SimpleRNN, LSTM or GRU); ; **/; ; if (useKeras) {; ; for (int i = 0; i < 3; i++) {; ; if (use_rnn_type[i]) {; ; TString modelName = TString::Format(""model_%s.h5"", rnn_types[i].c_str());; TString trainedModelName = TString::Format(""trained_model_%s.h5"", rnn_types[i].c_str());; ; Info(""TMVA_RNN_Classification"", ""Building recurrent keras model using a %s layer"", rnn_types[i].c_str());; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, SimpleRNN, GRU, LSTM, Reshape, ""; ""BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((10, 30), input_shape = (10*30, )))"");; // add recurrent neural network depending on type / Use option to return the full output; if (rnn_types[i] == ""LSTM""); m.AddLine(""model.add(LSTM(units=10, return_sequences=True) )"");; else if (rnn_types[i] == ""GRU""); m.AddLine(""model.add(GRU(units=10, return_sequences=True) )"");; else; m.AddLine(""model.add(SimpleRNN(units=10, return_sequences=True) )"");; ; // m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(Flatten())""); // needed if returning the full time output sequence; m.AddLine(""model.add(Dense(64, activation = 'tanh')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(; ""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(TString::Format(""modelName = '%s'"", modelName.Data()));; m.AddLine(""model.save(modelName)"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_rnn_model.py"");; // execute python script to make the model; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:71772,Integrability,message,messages,71772,"atchSize));; }; }; }; }; ; // use BDT in case not using Keras or TMVA DL; if (!useKeras || !useTMVA_BDT); useTMVA_BDT = true;; ; /**; ## Book TMVA BDT; **/; ; if (useTMVA_BDT) {; ; factory->BookMethod(dataloader, TMVA::Types::kBDT, ""BDTG"",; ""!H:!V:NTrees=100:MinNodeSize=2.5%:BoostType=Grad:Shrinkage=0.10:UseBaggedBoost:""; ""BaggedSampleFraction=0.5:nCuts=20:""; ""MaxDepth=2"");; ; }; ; /// Train all methods; factory->TrainAllMethods();; ; std::cout << ""nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; // ---- Evaluate all MVAs using the set of test events; factory->TestAllMethods();; ; // ----- Evaluate and compare performance of all configured MVAs; factory->EvaluateAllMethods();; ; // check method; ; // plot ROC curve; auto c1 = factory->GetROCCurve(dataloader);; c1->Draw();; ; if (outputFile) outputFile->Close();; }; DataLoader.h; DataSetInfo.h; MethodDL.h; f#define f(i)Definition RSha256.hxx:104; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; TFile.h; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; gRandomR__EXTERN TRandom * gRandomDefinition TRandom.h:62; Formchar * Form(const char *fmt,...)Formats a string in a circular formatting buffer.Definition TString.cxx:2489; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TTree.h; gPad#define gPadDefinition TVirtualPad.h:308; TCanvasThe Canvas class.Definition TCanvas.h:23; TCutA specialized string object used for TTree selections.Definition TCut.h:25; TF11-Dim function classDefinition T",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:755,Modifiability,variab,variables,755,"root; DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sgn of type Signal with 2000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg of type Background with 2000 events; number of variables is 300; vars_time0[0],vars_time0[1],vars_time0[2],vars_time0[3],vars_time0[4],vars_time0[5],vars_time0[6],vars_time0[7],vars_time0[8],vars_time0[9],vars_time0[10],vars_time0[11],vars_time0[12],vars_time0[13],vars_time0[14],vars_time0[15],vars_time0[16],vars_time0[17],vars_time0[18],vars_time0[19],vars_time0[20],vars_time0[21],vars_time0[22],vars_time0[23],vars_time0[24],vars_time0[25],vars_time0[26],vars_time0[27],vars_time0[28],vars_time0[29],vars_time1[0],vars_time1[1],vars_time1[2],vars_time1[3],vars_time1[4],vars_time1[5],vars_time1[6],vars_time1[7],vars_time1[8],vars_time1[9],vars_time1[10],vars_time1[11],vars_time1[12],vars_time1[13],vars_time1[14],vars_time1[15],vars_time1[16],vars_time1[17],vars_time1[18],vars_time1[19],vars_time1[20],vars_time1[21],vars_time1[22],vars_time1[23],vars_time1[24],vars_time1[25],vars_time1[26],vars_time1[27],vars_time1[28],vars_time1[29],vars_time2[0],vars_time2[1],vars_time2[2],vars_time2[3],vars_time2[4],vars_time2[5],vars_time2[6],vars_time2[7],vars_time2[8],vars_time2[9],vars_time2[10],vars_time2[11],vars_time2[12],vars_time2[13],vars_time2[14],vars_time2[15],vars_time2[16],vars_time2[17],vars_time2[18],vars_time2[19],vars_time2[20],vars_time2[21],vars_time2[22],vars_time2[23",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:6466,Modifiability,variab,variable,6466,"=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:9370,Modifiability,variab,variable,9370,"ROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12311,Modifiability,variab,variable,12311,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12387,Modifiability,variab,variable,12387,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12463,Modifiability,variab,variable,12463,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12539,Modifiability,variab,variable,12539,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12615,Modifiability,variab,variable,12615,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12691,Modifiability,variab,variable,12691,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12767,Modifiability,variab,variable,12767,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12843,Modifiability,variab,variable,12843,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12919,Modifiability,variab,variable,12919,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:12995,Modifiability,variab,variable,12995,ing method: ␛[1mBDTG␛[0m; : ; : the option NegWeightTreatment=InverseBoostNegWeights does not exist for BoostType=Grad; : --> change to new default NegWeightTreatment=Pray; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sgn; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : ,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13176,Modifiability,variab,variable,13176,ding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sgn; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events ,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13252,Modifiability,variab,variable,13252,dataset] : create input formulas for tree sgn; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- traini,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13328,Modifiability,variab,variable,13328,] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- trai,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13404,Modifiability,variab,variable,13404,] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Fact,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13480,Modifiability,variab,variable,13480,] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13556,Modifiability,variab,variable,13556,"] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning N",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13632,Modifiability,variab,variable,13632,"] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch siz",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13708,Modifiability,variab,variable,13708,"] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NT",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13784,Modifiability,variab,variable,13784,"] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 1",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:13860,Modifiability,variab,variable,13860,"] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , W",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15552,Modifiability,config,configuration,15552,"** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the conf",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15823,Modifiability,config,configuration,15823," 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15933,Modifiability,config,configuration,15933," = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.58963",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16043,Modifiability,config,configuration,16043,"ty; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.5995",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16206,Modifiability,config,configuration,16206,"0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.5",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16317,Modifiability,config,configuration,16317,----------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16428,Modifiability,config,configuration,16428,: --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for traini,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16539,Modifiability,config,configuration,16539, error found - save the configuration ; : 1 | 0.70053 0.692869 0.629389 0.0419042 4255.43 0; : 2 | 0.694153 0.694394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LS,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16651,Modifiability,config,configuration,16651,4394 0.642643 0.0421501 4163.24 1; : 3 | 0.689625 0.697522 0.642119 0.0416746 4163.58 2; : 4 | 0.681203 0.694822 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 320,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16763,Modifiability,config,configuration,16763, 0.637781 0.0412238 4190.71 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16874,Modifiability,config,configuration,16874,0.623882 0.0409529 4288.69 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : T,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:16986,Modifiability,config,configuration,16986,616834 0.0410715 4342.07 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.607427 0.0404816 4409.59 0; : 8 | 0.616397 0.638352 0.60791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network tr,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:17151,Modifiability,config,configuration,17151,"0791 0.0403928 4405.16 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.606268 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:17263,Modifiability,config,configuration,17263,"68 0.0407993 4421.11 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.604906 0.0402508 4427.48 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.568589 0.592822 0.598106 0.0406425 4484.6 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.559657 0.589421 0.600393 0.0404151 4464.46 0; : 13 Minimum Test error found - save the configuration ; : 13 | 0.552318 0.583187 0.601709 0.0405559 4455.11 0; : 14 Minimum Test error found - save the configuration ; : 14 | 0.540541 0.58147 0.593964 0.0401674 4514.29 0; : 15 Minimum Test error found - save the configuration ; : 15 | 0.532596 0.581192 0.589633 0.0399271 4547.88 0; : 16 Minimum Test error found - save the configuration ; : 16 | 0.529842 0.56732 0.599541 0.040336 4470.63 0; : 17 | 0.521737 0.574812 0.603666 0.0406979 4440.75 1; : 18 Minimum Test error found - save the configuration ; : 18 | 0.517445 0.565391 0.598324 0.0405273 4481.92 0; : 19 Minimum Test error found - save the configuration ; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19073,Modifiability,config,configuration,19073,"L NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 143",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19183,Modifiability,config,configuration,19183,"= 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19293,Modifiability,config,configuration,19293," Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19559,Modifiability,config,configuration,19559,"Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19723,Modifiability,config,configuration,19723,"eta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural netw",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:19889,Modifiability,config,configuration,19889, Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 ev,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:20001,Modifiability,config,configuration,20001,------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error found - save the configuration ; : 3 | 0.6901 0.69615 0.192163 0.0158037 14515.8 0; : 4 | 0.683586 0.700826 0.19294 0.0151654 14400.2 1; : 5 | 0.682085 0.701998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:20442,Modifiability,config,configuration,20442,"01998 0.192893 0.0153949 14422.7 2; : 6 | 0.6867 0.696275 0.191869 0.0152227 14492.2 3; : 7 Minimum Test error found - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; ========================",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:20554,Modifiability,config,configuration,20554,"ound - save the configuration ; : 7 | 0.692231 0.691692 0.197632 0.0164904 14132.6 0; : 8 | 0.683383 0.694963 0.194299 0.0156318 14328.3 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.682898 0.684979 0.192122 0.0157399 14513.9 0; : 10 | 0.670592 0.688549 0.192796 0.0152906 14422.1 1; : 11 Minimum Test error found - save the configuration ; : 11 | 0.670202 0.684295 0.191832 0.0154715 14515.7 0; : 12 Minimum Test error found - save the configuration ; : 12 | 0.675447 0.682107 0.194058 0.0158608 14366.1 0; : 13 | 0.678105 0.691356 0.193716 0.0152633 14345.5 1; : 14 | 0.684972 0.687408 0.191852 0.0154194 14509.8 2; : 15 | 0.676423 0.689156 0.193176 0.0153404 14395.3 3; : 16 | 0.683185 0.689966 0.191807 0.0149908 14478.3 4; : 17 | 0.672717 0.689982 0.192316 0.0149466 14433.2 5; : 18 | 0.670449 0.68412 0.190877 0.0151753 14570.1 6; : 19 Minimum Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 164",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:41476,Modifiability,variab,variables,41476,: [dataset] : Evaluation of PyKeras_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.273 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDTG for Classification; : ; BDTG : #events: (reweighted) sig: 1600 bkg: 1600; : #events: (unweighted) sig: 1600 bkg: 1600; : Training 100 Decision Trees ... patience please; : Elapsed time for training with 3200 events: 1.67 sec ; BDTG : [dataset] : Evaluation of BDTG on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0185 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_BDTG.class.C␛[0m; : data_RNN_CPU.root:/dataset/Method_BDT/BDTG; Factory : Training finished; : ; : Ranking input variables (method specific)...; : No variable ranking supplied by classifier: TMVA_LSTM; : No variable ranking supplied by classifier: TMVA_DNN; : No variable ranking supplied by classifier: PyKeras_LSTM; BDTG : Ranking result (top variable is best ranked); : --------------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------------; : 1 : vars_time9 : 2.245e-02; : 2 : vars_time8 : 2.023e-02; : 3 : vars_time7 : 1.907e-02; : 4 : vars_time7 : 1.907e-02; : 5 : vars_time8 : 1.847e-02; : 6 : vars_time7 : 1.771e-02; : 7 : vars_time9 : 1.708e-02; : 8 : vars_time6 : 1.653e-02; : 9 : vars_time0 : 1.650e-02; : 10 : vars_time7 : 1.626e-02; : 11 : vars_time8 : 1.623e-02; : 12 : vars_time6 : 1.532e-02; : 13 : vars_time8 : 1.530e-02; : 14 : vars_time9 : 1.520e-02; : 15 : vars_time8 : 1.487e-02; : 16 : vars_time6 : 1.421e-02; : 17 : vars_time6 : 1.421e-02; : 18 : vars_time8 : 1.381e-02; : 19 : vars_time8 : 1.379e-02;,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:41513,Modifiability,variab,variable,41513,alone class: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDTG for Classification; : ; BDTG : #events: (reweighted) sig: 1600 bkg: 1600; : #events: (unweighted) sig: 1600 bkg: 1600; : Training 100 Decision Trees ... patience please; : Elapsed time for training with 3200 events: 1.67 sec ; BDTG : [dataset] : Evaluation of BDTG on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0185 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_BDTG.class.C␛[0m; : data_RNN_CPU.root:/dataset/Method_BDT/BDTG; Factory : Training finished; : ; : Ranking input variables (method specific)...; : No variable ranking supplied by classifier: TMVA_LSTM; : No variable ranking supplied by classifier: TMVA_DNN; : No variable ranking supplied by classifier: PyKeras_LSTM; BDTG : Ranking result (top variable is best ranked); : --------------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------------; : 1 : vars_time9 : 2.245e-02; : 2 : vars_time8 : 2.023e-02; : 3 : vars_time7 : 1.907e-02; : 4 : vars_time7 : 1.907e-02; : 5 : vars_time8 : 1.847e-02; : 6 : vars_time7 : 1.771e-02; : 7 : vars_time9 : 1.708e-02; : 8 : vars_time6 : 1.653e-02; : 9 : vars_time0 : 1.650e-02; : 10 : vars_time7 : 1.626e-02; : 11 : vars_time8 : 1.623e-02; : 12 : vars_time6 : 1.532e-02; : 13 : vars_time8 : 1.530e-02; : 14 : vars_time9 : 1.520e-02; : 15 : vars_time8 : 1.487e-02; : 16 : vars_time6 : 1.421e-02; : 17 : vars_time6 : 1.421e-02; : 18 : vars_time8 : 1.381e-02; : 19 : vars_time8 : 1.379e-02; : 20 : vars_time9 : 1.331e-02; : 21 : vars_time0 : 1.313e-02; : 22 : vars_time8 : 1.307e-02; : 23 : vars_time5 : 1.291e-02; : 24 : vars_time9 : 1.261e-02; : 25 : vars_time7 : 1.243e-02; : 26 : vars_time9 : 1.230e-02; : 27 : vars_time6 : 1.229e-02; ,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:41570,Modifiability,variab,variable,41570,alone class: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDTG for Classification; : ; BDTG : #events: (reweighted) sig: 1600 bkg: 1600; : #events: (unweighted) sig: 1600 bkg: 1600; : Training 100 Decision Trees ... patience please; : Elapsed time for training with 3200 events: 1.67 sec ; BDTG : [dataset] : Evaluation of BDTG on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0185 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_BDTG.class.C␛[0m; : data_RNN_CPU.root:/dataset/Method_BDT/BDTG; Factory : Training finished; : ; : Ranking input variables (method specific)...; : No variable ranking supplied by classifier: TMVA_LSTM; : No variable ranking supplied by classifier: TMVA_DNN; : No variable ranking supplied by classifier: PyKeras_LSTM; BDTG : Ranking result (top variable is best ranked); : --------------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------------; : 1 : vars_time9 : 2.245e-02; : 2 : vars_time8 : 2.023e-02; : 3 : vars_time7 : 1.907e-02; : 4 : vars_time7 : 1.907e-02; : 5 : vars_time8 : 1.847e-02; : 6 : vars_time7 : 1.771e-02; : 7 : vars_time9 : 1.708e-02; : 8 : vars_time6 : 1.653e-02; : 9 : vars_time0 : 1.650e-02; : 10 : vars_time7 : 1.626e-02; : 11 : vars_time8 : 1.623e-02; : 12 : vars_time6 : 1.532e-02; : 13 : vars_time8 : 1.530e-02; : 14 : vars_time9 : 1.520e-02; : 15 : vars_time8 : 1.487e-02; : 16 : vars_time6 : 1.421e-02; : 17 : vars_time6 : 1.421e-02; : 18 : vars_time8 : 1.381e-02; : 19 : vars_time8 : 1.379e-02; : 20 : vars_time9 : 1.331e-02; : 21 : vars_time0 : 1.313e-02; : 22 : vars_time8 : 1.307e-02; : 23 : vars_time5 : 1.291e-02; : 24 : vars_time9 : 1.261e-02; : 25 : vars_time7 : 1.243e-02; : 26 : vars_time9 : 1.230e-02; : 27 : vars_time6 : 1.229e-02; ,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:41626,Modifiability,variab,variable,41626,alone class: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDTG for Classification; : ; BDTG : #events: (reweighted) sig: 1600 bkg: 1600; : #events: (unweighted) sig: 1600 bkg: 1600; : Training 100 Decision Trees ... patience please; : Elapsed time for training with 3200 events: 1.67 sec ; BDTG : [dataset] : Evaluation of BDTG on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0185 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_BDTG.class.C␛[0m; : data_RNN_CPU.root:/dataset/Method_BDT/BDTG; Factory : Training finished; : ; : Ranking input variables (method specific)...; : No variable ranking supplied by classifier: TMVA_LSTM; : No variable ranking supplied by classifier: TMVA_DNN; : No variable ranking supplied by classifier: PyKeras_LSTM; BDTG : Ranking result (top variable is best ranked); : --------------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------------; : 1 : vars_time9 : 2.245e-02; : 2 : vars_time8 : 2.023e-02; : 3 : vars_time7 : 1.907e-02; : 4 : vars_time7 : 1.907e-02; : 5 : vars_time8 : 1.847e-02; : 6 : vars_time7 : 1.771e-02; : 7 : vars_time9 : 1.708e-02; : 8 : vars_time6 : 1.653e-02; : 9 : vars_time0 : 1.650e-02; : 10 : vars_time7 : 1.626e-02; : 11 : vars_time8 : 1.623e-02; : 12 : vars_time6 : 1.532e-02; : 13 : vars_time8 : 1.530e-02; : 14 : vars_time9 : 1.520e-02; : 15 : vars_time8 : 1.487e-02; : 16 : vars_time6 : 1.421e-02; : 17 : vars_time6 : 1.421e-02; : 18 : vars_time8 : 1.381e-02; : 19 : vars_time8 : 1.379e-02; : 20 : vars_time9 : 1.331e-02; : 21 : vars_time0 : 1.313e-02; : 22 : vars_time8 : 1.307e-02; : 23 : vars_time5 : 1.291e-02; : 24 : vars_time9 : 1.261e-02; : 25 : vars_time7 : 1.243e-02; : 26 : vars_time9 : 1.230e-02; : 27 : vars_time6 : 1.229e-02; ,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:41708,Modifiability,variab,variable,41708,alone class: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDTG for Classification; : ; BDTG : #events: (reweighted) sig: 1600 bkg: 1600; : #events: (unweighted) sig: 1600 bkg: 1600; : Training 100 Decision Trees ... patience please; : Elapsed time for training with 3200 events: 1.67 sec ; BDTG : [dataset] : Evaluation of BDTG on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0185 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_BDTG.class.C␛[0m; : data_RNN_CPU.root:/dataset/Method_BDT/BDTG; Factory : Training finished; : ; : Ranking input variables (method specific)...; : No variable ranking supplied by classifier: TMVA_LSTM; : No variable ranking supplied by classifier: TMVA_DNN; : No variable ranking supplied by classifier: PyKeras_LSTM; BDTG : Ranking result (top variable is best ranked); : --------------------------------------------; : Rank : Variable : Variable Importance; : --------------------------------------------; : 1 : vars_time9 : 2.245e-02; : 2 : vars_time8 : 2.023e-02; : 3 : vars_time7 : 1.907e-02; : 4 : vars_time7 : 1.907e-02; : 5 : vars_time8 : 1.847e-02; : 6 : vars_time7 : 1.771e-02; : 7 : vars_time9 : 1.708e-02; : 8 : vars_time6 : 1.653e-02; : 9 : vars_time0 : 1.650e-02; : 10 : vars_time7 : 1.626e-02; : 11 : vars_time8 : 1.623e-02; : 12 : vars_time6 : 1.532e-02; : 13 : vars_time8 : 1.530e-02; : 14 : vars_time9 : 1.520e-02; : 15 : vars_time8 : 1.487e-02; : 16 : vars_time6 : 1.421e-02; : 17 : vars_time6 : 1.421e-02; : 18 : vars_time8 : 1.381e-02; : 19 : vars_time8 : 1.379e-02; : 20 : vars_time9 : 1.331e-02; : 21 : vars_time0 : 1.313e-02; : 22 : vars_time8 : 1.307e-02; : 23 : vars_time5 : 1.291e-02; : 24 : vars_time9 : 1.261e-02; : 25 : vars_time7 : 1.243e-02; : 26 : vars_time9 : 1.230e-02; : 27 : vars_time6 : 1.229e-02; ,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54145,Modifiability,variab,variable,54145,"STM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : ---------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54193,Modifiability,variab,variables,54193,"STM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : ---------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54473,Modifiability,variab,variable,54473,"TM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : ------------------------------------------------------------------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54521,Modifiability,variab,variables,54521,"TM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : ------------------------------------------------------------------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54735,Modifiability,variab,variable,54735,"ation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method:",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54783,Modifiability,variab,variables,54783,"ation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method:",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54981,Modifiability,variab,variable,54981,"variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset PyKeras_LSTM : 0.225 (0.215) 0.600 (0.609) 0.839 (0.855); : dataset BDTG : 0.135 (0.272) 0.502 (0.648) 0.835 (0.886); : dataset TMVA_LSTM : 0.058 (0.097) 0.445 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:55029,Modifiability,variab,variables,55029,"variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset PyKeras_LSTM : 0.225 (0.215) 0.600 (0.609) 0.839 (0.855); : dataset BDTG : 0.135 (0.272) 0.502 (0.648) 0.835 (0.886); : dataset TMVA_LSTM : 0.058 (0.097) 0.445 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:62828,Modifiability,config,configuration,62828,"sts create it; if (!fileExist) {; MakeTimeData(nTotEvts,ntime, ninput);; }; ; ; auto inputFile = TFile::Open(inputFileName);; if (!inputFile) {; Error(""TMVA_RNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; ; std::cout << ""--- RNNClassification : Using input file: "" << inputFile->GetName() << std::endl;; ; // Create a ROOT output file where TMVA will store ntuples, histograms, etc.; TString outfileName(TString::Format(""data_RNN_%s.root"", archString.Data()));; TFile *outputFile = nullptr;; if (writeOutputFile) outputFile = TFile::Open(outfileName, ""RECREATE"");; ; /**; ## Declare Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; the option string; ; **/; ; // Creating the factory object; TMVA::Factory *factory = new TMVA::Factory(""TMVAClassification"", outputFile,; ""!V:!Silent:Color:DrawProgressBar:Transformations=None:!Correlations:""; ""AnalysisType=Classification:ModelPersistence"");; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; TTree *signalTree = (TTree *)inputFile->Get(""sgn"");; TTree *background = (TTree *)inputFile->Get(""bkg"");; ; const int nvar = ninput * ntime;; ; /// add variables - use new AddVariablesArray function; for (auto i = 0; i < ntime; i++) {; dataloader->AddVariablesArray(Form(""vars_time%d"", i), ninput);; }; ; dataloader->AddSignalTree(signalTree, 1.0);; dataloader->AddBackground",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:63454,Modifiability,variab,variables,63454,"le::Open(outfileName, ""RECREATE"");; ; /**; ## Declare Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; the option string; ; **/; ; // Creating the factory object; TMVA::Factory *factory = new TMVA::Factory(""TMVAClassification"", outputFile,; ""!V:!Silent:Color:DrawProgressBar:Transformations=None:!Correlations:""; ""AnalysisType=Classification:ModelPersistence"");; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; TTree *signalTree = (TTree *)inputFile->Get(""sgn"");; TTree *background = (TTree *)inputFile->Get(""bkg"");; ; const int nvar = ninput * ntime;; ; /// add variables - use new AddVariablesArray function; for (auto i = 0; i < ntime; i++) {; dataloader->AddVariablesArray(Form(""vars_time%d"", i), ninput);; }; ; dataloader->AddSignalTree(signalTree, 1.0);; dataloader->AddBackgroundTree(background, 1.0);; ; // check given input; auto &datainfo = dataloader->GetDataSetInfo();; auto vars = datainfo.GetListOfVariables();; std::cout << ""number of variables is "" << vars.size() << std::endl;; for (auto &v : vars); std::cout << v << "","";; std::cout << std::endl;; ; int nTrainSig = 0.8 * nTotEvts;; int nTrainBkg = 0.8 * nTotEvts;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"", nTr",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:63841,Modifiability,variab,variables,63841,"onfiguration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; the option string; ; **/; ; // Creating the factory object; TMVA::Factory *factory = new TMVA::Factory(""TMVAClassification"", outputFile,; ""!V:!Silent:Color:DrawProgressBar:Transformations=None:!Correlations:""; ""AnalysisType=Classification:ModelPersistence"");; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; TTree *signalTree = (TTree *)inputFile->Get(""sgn"");; TTree *background = (TTree *)inputFile->Get(""bkg"");; ; const int nvar = ninput * ntime;; ; /// add variables - use new AddVariablesArray function; for (auto i = 0; i < ntime; i++) {; dataloader->AddVariablesArray(Form(""vars_time%d"", i), ninput);; }; ; dataloader->AddSignalTree(signalTree, 1.0);; dataloader->AddBackgroundTree(background, 1.0);; ; // check given input; auto &datainfo = dataloader->GetDataSetInfo();; auto vars = datainfo.GetListOfVariables();; std::cout << ""number of variables is "" << vars.size() << std::endl;; for (auto &v : vars); std::cout << v << "","";; std::cout << std::endl;; ; int nTrainSig = 0.8 * nTotEvts;; int nTrainBkg = 0.8 * nTotEvts;; ; // build the string options for DataLoader::PrepareTrainingAndTestTree; TString prepareOptions = TString::Format(""nTrain_Signal=%d:nTrain_Background=%d:SplitMode=Random:SplitSeed=100:NormMode=NumEvents:!V:!CalcCorrelations"", nTrainSig, nTrainBkg);; ; // Apply additional cuts on the signal and background samples (can be different); TCut mycuts = """"; // for example: TCut mycuts = ""abs(var1)<0.5 && abs(var2-0.5)<1"";; TCut mycutb = """";; ; dataloader->PrepareTrainingAndTestTree(mycuts, mycutb, prepareOptions);; ; std::cout << ""prepared DATA LOADER "" << std::endl;; ; /**; ## Book TMVA recurrent models; ; Book the different types of recurrent models in TMVA (SimpleRNN, LSTM or GRU); ; **/; ; if (useTMVA_RNN) {; ; for (int i = 0; i < 3; ++i) {; ; if (!use_rnn_type[i]); continue;; ; const char *rnn_t",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:68752,Modifiability,layers,layers,68752,"tring);; dnnOptions.Append("":"");; dnnOptions.Append(archString);; ; TString dnnName = ""TMVA_DNN"";; factory->BookMethod(dataloader, TMVA::Types::kDL, dnnName, dnnOptions);; }; ; /**; ## Book Keras recurrent models; ; Book the different types of recurrent models in Keras (SimpleRNN, LSTM or GRU); ; **/; ; if (useKeras) {; ; for (int i = 0; i < 3; i++) {; ; if (use_rnn_type[i]) {; ; TString modelName = TString::Format(""model_%s.h5"", rnn_types[i].c_str());; TString trainedModelName = TString::Format(""trained_model_%s.h5"", rnn_types[i].c_str());; ; Info(""TMVA_RNN_Classification"", ""Building recurrent keras model using a %s layer"", rnn_types[i].c_str());; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, SimpleRNN, GRU, LSTM, Reshape, ""; ""BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((10, 30), input_shape = (10*30, )))"");; // add recurrent neural network depending on type / Use option to return the full output; if (rnn_types[i] == ""LSTM""); m.AddLine(""model.add(LSTM(units=10, return_sequences=True) )"");; else if (rnn_types[i] == ""GRU""); m.AddLine(""model.add(GRU(units=10, return_sequences=True) )"");; else; m.AddLine(""model.add(SimpleRNN(units=10, return_sequences=True) )"");; ; // m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(Flatten())""); // needed if returning the full time output sequence; m.AddLine(""model.add(Dense(64, activation = 'tanh')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(; ""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(TString::Format(""modelName = '%s'"", modelName.Data()));; m.AddLine",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:71405,Modifiability,config,configured,71405,"fo(""TMVA_RNN_Classification"", ""Booking Keras %s model"", rnn_types[i].c_str());; factory->BookMethod(dataloader, TMVA::Types::kPyKeras,; TString::Format(""PyKeras_%s"", rnn_types[i].c_str()),; TString::Format(""!H:!V:VarTransform=None:FilenameModel=%s:tf.keras:""; ""FilenameTrainedModel=%s:GpuOptions=allow_growth=True:""; ""NumEpochs=%d:BatchSize=%d"",; modelName.Data(), trainedModelName.Data(), maxepochs, batchSize));; }; }; }; }; ; // use BDT in case not using Keras or TMVA DL; if (!useKeras || !useTMVA_BDT); useTMVA_BDT = true;; ; /**; ## Book TMVA BDT; **/; ; if (useTMVA_BDT) {; ; factory->BookMethod(dataloader, TMVA::Types::kBDT, ""BDTG"",; ""!H:!V:NTrees=100:MinNodeSize=2.5%:BoostType=Grad:Shrinkage=0.10:UseBaggedBoost:""; ""BaggedSampleFraction=0.5:nCuts=20:""; ""MaxDepth=2"");; ; }; ; /// Train all methods; factory->TrainAllMethods();; ; std::cout << ""nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; // ---- Evaluate all MVAs using the set of test events; factory->TestAllMethods();; ; // ----- Evaluate and compare performance of all configured MVAs; factory->EvaluateAllMethods();; ; // check method; ; // plot ROC curve; auto c1 = factory->GetROCCurve(dataloader);; c1->Draw();; ; if (outputFile) outputFile->Close();; }; DataLoader.h; DataSetInfo.h; MethodDL.h; f#define f(i)Definition RSha256.hxx:104; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; TFile.h; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; gRandomR__EXTERN TRandom * gRandomDefinition TRand",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:74240,Modifiability,variab,variables,74240," open a file.Definition TFile.cxx:4089; TFile::Closevoid Close(Option_t *option="""") overrideClose a file.Definition TFile.cxx:950; TGraph::Drawvoid Draw(Option_t *chopt="""") overrideDraw this graph with its current attributes.Definition TGraph.cxx:831; TH1D1-D histogram with a double per channel (see TH1 documentation)Definition TH1.h:670; TH1F::Resetvoid Reset(Option_t *option="""") overrideReset.Definition TH1.cxx:10310; TH1::FillRandomvirtual void FillRandom(const char *fname, Int_t ntimes=5000, TRandom *rng=nullptr)Fill histogram following distribution in function fname.Definition TH1.cxx:3519; TH1::GetBinContentvirtual Double_t GetBinContent(Int_t bin) constReturn content of bin number bin.Definition TH1.cxx:5061; TMVA::Config::Instancestatic Config & Instance()static function: returns TMVA instanceDefinition Config.cxx:98; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddVariablesArrayvoid AddVariablesArray(const TString &expression, int size, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating array of variables in data set info in case input tree provides an array ...Definition DataLoader.cxx:504; TMVA::DataLoader::AddSignalTreevoid AddSignalTree(TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:371; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataSetInfo::GetListOfVariablesstd::vector< TString > GetListOfVariables() c",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:75201,Modifiability,variab,variablesDefinition,75201," type='F', Double_t min=0, Double_t max=0)user inserts discriminating array of variables in data set info in case input tree provides an array ...Definition DataLoader.cxx:504; TMVA::DataLoader::AddSignalTreevoid AddSignalTree(TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:371; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataSetInfo::GetListOfVariablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=T",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:77636,Modifiability,variab,variable,77636," of lines with C++ code.Definition TMacro.h:31; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random number from the standard Normal (Gaussian) Distribution with the given mean and sigm...Definition TRandom.cxx:275; TRandom::SetSeedvirtual void SetSeed(ULong_t seed=0)Set the random generator seed.Definition TRandom.cxx:615; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; ROOT::VecOps::cosRVec< PromoteType< T > > cos(const RVec< T > &v)Definition RVec.hxx:1852; ROOT::VecOps::sinRVec< PromoteType< T > > sin(const RVec< T > &v)Definition RVec.hxx:1851; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_RNN_ClassificationDefinition TMVA_RNN_Classification.py:1; TMVA_SOFIE_GNN_Parser.h2h2Definition TMVA_SOFIE_GNN_Parser.py:188; TMath::Piconstexpr Double_t Pi()Definition TMath.h:37; v2@ v2Definition rootcling_impl",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:6491,Performance,perform,performed,6491,"=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:7500,Performance,perform,perform,7500,"formations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_DNN␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:8048,Performance,perform,performance,8048," squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_DNN␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:9395,Performance,perform,performed,9395,"ROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:10188,Performance,perform,perform,10188,"WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:10736,Performance,perform,performance,10736,"med)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 8234 (32.16 KB); Trainable params: 8234 (32.16 KB); Non-trainable params: 0 (0.00 Byte); ________________________________________________",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:52670,Performance,perform,performance,52670,"or, Entries= 0, Total sum= 13.7968; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'accuracy', Entries= 0, Total sum= 14.2691; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'loss', Entries= 0, Total sum= 10.8696; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'val_accuracy', Entries= 0, Total sum= 14.125; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'val_loss', Entries= 0, Total sum= 11.0066; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; nthreads = 4; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: TMVA_LSTM for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification perf",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:52949,Performance,perform,performance,52949,"5; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'val_loss', Entries= 0, Total sum= 11.0066; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; nthreads = 4; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: TMVA_LSTM for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with clas",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:53230,Performance,perform,performance,53230,"weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; nthreads = 4; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: TMVA_LSTM for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:53719,Performance,perform,performance,53719," ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:59737,Performance,perform,performing,59737,"ime));; }; for (int i = 0; i < n; ++i) {; ; if (i % 1000 == 0); std::cout << ""Generating event ... "" << i << std::endl;; ; for (int j = 0; j < ntime; ++j) {; auto h1 = v1[j];; auto h2 = v2[j];; h1->Reset();; h2->Reset();; ; f1->SetParameters(1, mean1[j], sigma1[j]);; f2->SetParameters(1, mean2[j], sigma2[j]);; ; h1->FillRandom(""f1"", 1000);; h2->FillRandom(""f2"", 1000);; ; for (int k = 0; k < ndim; ++k) {; // std::cout << j*10+k << "" "";; x1[j][k] = h1->GetBinContent(k + 1) + gRandom->Gaus(0, 10);; x2[j][k] = h2->GetBinContent(k + 1) + gRandom->Gaus(0, 10);; }; }; // std::cout << std::endl;; sgn.Fill();; bkg.Fill();; ; if (n == 1) {; auto c1 = new TCanvas();; c1->Divide(ntime, 2);; for (int j = 0; j < ntime; ++j) {; c1->cd(j + 1);; v1[j]->Draw();; }; for (int j = 0; j < ntime; ++j) {; c1->cd(ntime + j + 1);; v2[j]->Draw();; }; gPad->Update();; }; }; if (n > 1) {; sgn.Write();; bkg.Write();; sgn.Print();; bkg.Print();; f.Close();; }; }; /// macro for performing a classification using a Recurrent Neural Network; /// @param nevts = 2000 Number of events used. (increase for better classification results); /// @param use_type; /// use_type = 0 use Simple RNN network; /// use_type = 1 use LSTM network; /// use_type = 2 use GRU; /// use_type = 3 build 3 different networks with RNN, LSTM and GRU; ; void TMVA_RNN_Classification(int nevts = 2000, int use_type = 1); {; ; const int ninput = 30;; const int ntime = 10;; const int batchSize = 100;; const int maxepochs = 20;; ; int nTotEvts = nevts; // total events to be generated for signal or background; ; bool useKeras = true;; ; ; bool useTMVA_RNN = true;; bool useTMVA_DNN = true;; bool useTMVA_BDT = false;; ; std::vector<std::string> rnn_types = {""RNN"", ""LSTM"", ""GRU""};; std::vector<bool> use_rnn_type = {1, 1, 1};; if (use_type >=0 && use_type < 3) {; use_rnn_type = {0,0,0};; use_rnn_type[use_type] = 1;; }; bool useGPU = true; // use GPU for TMVA if available; ; #ifndef R__HAS_TMVAGPU; useGPU = false;; #ifndef R__HAS_TMVACPU; Warn",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:60842,Performance,multi-thread,multi-thread,60842,"ut << std::endl;; sgn.Fill();; bkg.Fill();; ; if (n == 1) {; auto c1 = new TCanvas();; c1->Divide(ntime, 2);; for (int j = 0; j < ntime; ++j) {; c1->cd(j + 1);; v1[j]->Draw();; }; for (int j = 0; j < ntime; ++j) {; c1->cd(ntime + j + 1);; v2[j]->Draw();; }; gPad->Update();; }; }; if (n > 1) {; sgn.Write();; bkg.Write();; sgn.Print();; bkg.Print();; f.Close();; }; }; /// macro for performing a classification using a Recurrent Neural Network; /// @param nevts = 2000 Number of events used. (increase for better classification results); /// @param use_type; /// use_type = 0 use Simple RNN network; /// use_type = 1 use LSTM network; /// use_type = 2 use GRU; /// use_type = 3 build 3 different networks with RNN, LSTM and GRU; ; void TMVA_RNN_Classification(int nevts = 2000, int use_type = 1); {; ; const int ninput = 30;; const int ntime = 10;; const int batchSize = 100;; const int maxepochs = 20;; ; int nTotEvts = nevts; // total events to be generated for signal or background; ; bool useKeras = true;; ; ; bool useTMVA_RNN = true;; bool useTMVA_DNN = true;; bool useTMVA_BDT = false;; ; std::vector<std::string> rnn_types = {""RNN"", ""LSTM"", ""GRU""};; std::vector<bool> use_rnn_type = {1, 1, 1};; if (use_type >=0 && use_type < 3) {; use_rnn_type = {0,0,0};; use_rnn_type[use_type] = 1;; }; bool useGPU = true; // use GPU for TMVA if available; ; #ifndef R__HAS_TMVAGPU; useGPU = false;; #ifndef R__HAS_TMVACPU; Warning(""TMVA_RNN_Classification"", ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for RNN"");; useTMVA_RNN = false;; #endif; #endif; ; ; TString archString = (useGPU) ? ""GPU"" : ""CPU"";; ; bool writeOutputFile = true;; ; ; ; const char *rnn_type = ""RNN"";; ; #ifdef R__HAS_PYMVA; TMVA::PyMethodBase::PyInitialize();; #else; useKeras = false;; #endif; ; #ifdef R__USE_IMT; int num_threads = 4; // use max 4 threads; // switch off MT in OpenBLAS to avoid conflict with tbb; gSystem->Setenv(""OMP_NUM_THREADS"", ""1"");; ; // do enable MT running; if (n",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:62386,Performance,perform,performance,62386,"leImplicitMT(num_threads);; }; #endif; ; TMVA::Config::Instance();; ; std::cout << ""Running with nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; TString inputFileName = ""time_data_t10_d30.root"";; ; bool fileExist = !gSystem->AccessPathName(inputFileName);; ; // if file does not exists create it; if (!fileExist) {; MakeTimeData(nTotEvts,ntime, ninput);; }; ; ; auto inputFile = TFile::Open(inputFileName);; if (!inputFile) {; Error(""TMVA_RNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; ; std::cout << ""--- RNNClassification : Using input file: "" << inputFile->GetName() << std::endl;; ; // Create a ROOT output file where TMVA will store ntuples, histograms, etc.; TString outfileName(TString::Format(""data_RNN_%s.root"", archString.Data()));; TFile *outputFile = nullptr;; if (writeOutputFile) outputFile = TFile::Open(outfileName, ""RECREATE"");; ; /**; ## Declare Factory; ; Create the Factory class. Later you can choose the methods; whose performance you'd like to investigate.; ; The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; pass; ; - The first argument is the base of the name of all the output; weightfiles in the directory weight/ that will be created with the; method parameters; ; - The second argument is the output file for the training results; ; - The third argument is a string option defining some general configuration for the TMVA session.; For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; the option string; ; **/; ; // Creating the factory object; TMVA::Factory *factory = new TMVA::Factory(""TMVAClassification"", outputFile,; ""!V:!Silent:Color:DrawProgressBar:Transformations=None:!Correlations:""; ""AnalysisType=Classification:ModelPersistence"");; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; TTree *signalTree = (TTree *)inputFile->Get(""sgn"");; TTree *background = (TTree *)i",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:68692,Performance,optimiz,optimizers,68692,""");; dnnOptions.Append(layoutString);; dnnOptions.Append("":"");; dnnOptions.Append(trainingStrategyString);; dnnOptions.Append("":"");; dnnOptions.Append(archString);; ; TString dnnName = ""TMVA_DNN"";; factory->BookMethod(dataloader, TMVA::Types::kDL, dnnName, dnnOptions);; }; ; /**; ## Book Keras recurrent models; ; Book the different types of recurrent models in Keras (SimpleRNN, LSTM or GRU); ; **/; ; if (useKeras) {; ; for (int i = 0; i < 3; i++) {; ; if (use_rnn_type[i]) {; ; TString modelName = TString::Format(""model_%s.h5"", rnn_types[i].c_str());; TString trainedModelName = TString::Format(""trained_model_%s.h5"", rnn_types[i].c_str());; ; Info(""TMVA_RNN_Classification"", ""Building recurrent keras model using a %s layer"", rnn_types[i].c_str());; // create python script which can be executed; // create 2 conv2d layer + maxpool + dense; TMacro m;; m.AddLine(""import tensorflow"");; m.AddLine(""from tensorflow.keras.models import Sequential"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, SimpleRNN, GRU, LSTM, Reshape, ""; ""BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((10, 30), input_shape = (10*30, )))"");; // add recurrent neural network depending on type / Use option to return the full output; if (rnn_types[i] == ""LSTM""); m.AddLine(""model.add(LSTM(units=10, return_sequences=True) )"");; else if (rnn_types[i] == ""GRU""); m.AddLine(""model.add(GRU(units=10, return_sequences=True) )"");; else; m.AddLine(""model.add(SimpleRNN(units=10, return_sequences=True) )"");; ; // m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(Flatten())""); // needed if returning the full time output sequence; m.AddLine(""model.add(Dense(64, activation = 'tanh')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(; ""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metric",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:69651,Performance,optimiz,optimizer,69651,"ntial"");; m.AddLine(""from tensorflow.keras.optimizers import Adam"");; m.AddLine(""from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, SimpleRNN, GRU, LSTM, Reshape, ""; ""BatchNormalization"");; m.AddLine("""");; m.AddLine(""model = Sequential() "");; m.AddLine(""model.add(Reshape((10, 30), input_shape = (10*30, )))"");; // add recurrent neural network depending on type / Use option to return the full output; if (rnn_types[i] == ""LSTM""); m.AddLine(""model.add(LSTM(units=10, return_sequences=True) )"");; else if (rnn_types[i] == ""GRU""); m.AddLine(""model.add(GRU(units=10, return_sequences=True) )"");; else; m.AddLine(""model.add(SimpleRNN(units=10, return_sequences=True) )"");; ; // m.AddLine(""model.add(BatchNormalization())"");; m.AddLine(""model.add(Flatten())""); // needed if returning the full time output sequence; m.AddLine(""model.add(Dense(64, activation = 'tanh')) "");; m.AddLine(""model.add(Dense(2, activation = 'sigmoid')) "");; m.AddLine(; ""model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.001), weighted_metrics = ['accuracy'])"");; m.AddLine(TString::Format(""modelName = '%s'"", modelName.Data()));; m.AddLine(""model.save(modelName)"");; m.AddLine(""model.summary()"");; ; m.SaveSource(""make_rnn_model.py"");; // execute python script to make the model; auto ret = (TString *)gROOT->ProcessLine(""TMVA::Python_Executable()"");; TString python_exe = (ret) ? *(ret) : ""python"";; gSystem->Exec(python_exe + "" make_rnn_model.py"");; ; if (gSystem->AccessPathName(modelName)) {; Warning(""TMVA_RNN_Classification"", ""Error creating Keras recurrent model file - Skip using Keras"");; useKeras = false;; } else {; // book PyKeras method only if Keras model could be created; Info(""TMVA_RNN_Classification"", ""Booking Keras %s model"", rnn_types[i].c_str());; factory->BookMethod(dataloader, TMVA::Types::kPyKeras,; TString::Format(""PyKeras_%s"", rnn_types[i].c_str()),; TString::Format(""!H:!V:VarTransform=None:FilenameModel=%s:tf.keras:""; ""FilenameTrainedModel=%s:GpuOption",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:71386,Performance,perform,performance,71386,"fo(""TMVA_RNN_Classification"", ""Booking Keras %s model"", rnn_types[i].c_str());; factory->BookMethod(dataloader, TMVA::Types::kPyKeras,; TString::Format(""PyKeras_%s"", rnn_types[i].c_str()),; TString::Format(""!H:!V:VarTransform=None:FilenameModel=%s:tf.keras:""; ""FilenameTrainedModel=%s:GpuOptions=allow_growth=True:""; ""NumEpochs=%d:BatchSize=%d"",; modelName.Data(), trainedModelName.Data(), maxepochs, batchSize));; }; }; }; }; ; // use BDT in case not using Keras or TMVA DL; if (!useKeras || !useTMVA_BDT); useTMVA_BDT = true;; ; /**; ## Book TMVA BDT; **/; ; if (useTMVA_BDT) {; ; factory->BookMethod(dataloader, TMVA::Types::kBDT, ""BDTG"",; ""!H:!V:NTrees=100:MinNodeSize=2.5%:BoostType=Grad:Shrinkage=0.10:UseBaggedBoost:""; ""BaggedSampleFraction=0.5:nCuts=20:""; ""MaxDepth=2"");; ; }; ; /// Train all methods; factory->TrainAllMethods();; ; std::cout << ""nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; // ---- Evaluate all MVAs using the set of test events; factory->TestAllMethods();; ; // ----- Evaluate and compare performance of all configured MVAs; factory->EvaluateAllMethods();; ; // check method; ; // plot ROC curve; auto c1 = factory->GetROCCurve(dataloader);; c1->Draw();; ; if (outputFile) outputFile->Close();; }; DataLoader.h; DataSetInfo.h; MethodDL.h; f#define f(i)Definition RSha256.hxx:104; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; TFile.h; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; gRandomR__EXTERN TRandom * gRandomDefinition TRand",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:75513,Performance,load,loader,75513,"n DataLoader.cxx:371; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataSetInfo::GetListOfVariablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Types::kPyKeras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDL",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:76067,Performance,load,loader,76067,"iablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Types::kPyKeras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMacroClass supporting a collection of lines with C++ code.Definition TMacro.h:31; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random number from the standard Normal (Gaussian) Distribution with the given mean and sigm...Definition TRandom.cxx:275; TRandom::SetSeedvirtual void SetSeed(ULong_t seed=0)Set the random generator seed.Definition TRandom.cxx:615; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::Formatstatic TString Format(const char *fmt,...)Static ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:78154,Performance,multi-thread,multi-threading,78154,"ingBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; ROOT::VecOps::cosRVec< PromoteType< T > > cos(const RVec< T > &v)Definition RVec.hxx:1852; ROOT::VecOps::sinRVec< PromoteType< T > > sin(const RVec< T > &v)Definition RVec.hxx:1851; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_RNN_ClassificationDefinition TMVA_RNN_Classification.py:1; TMVA_SOFIE_GNN_Parser.h2h2Definition TMVA_SOFIE_GNN_Parser.py:188; TMath::Piconstexpr Double_t Pi()Definition TMath.h:37; v2@ v2Definition rootcling_impl.cxx:3702; v@ vDefinition rootcling_impl.cxx:3699; v1@ v1Definition rootcling_impl.cxx:3701; mTMarker mDefinition textangle.C:8; Config.h; Factory.h; AuthorLorenzo Moneta ; Definition in file TMVA_RNN_Classification.C. tutorialstmvaTMVA_RNN_Classification.C. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:61255,Safety,avoid,avoid,61255,"= 100;; const int maxepochs = 20;; ; int nTotEvts = nevts; // total events to be generated for signal or background; ; bool useKeras = true;; ; ; bool useTMVA_RNN = true;; bool useTMVA_DNN = true;; bool useTMVA_BDT = false;; ; std::vector<std::string> rnn_types = {""RNN"", ""LSTM"", ""GRU""};; std::vector<bool> use_rnn_type = {1, 1, 1};; if (use_type >=0 && use_type < 3) {; use_rnn_type = {0,0,0};; use_rnn_type[use_type] = 1;; }; bool useGPU = true; // use GPU for TMVA if available; ; #ifndef R__HAS_TMVAGPU; useGPU = false;; #ifndef R__HAS_TMVACPU; Warning(""TMVA_RNN_Classification"", ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for RNN"");; useTMVA_RNN = false;; #endif; #endif; ; ; TString archString = (useGPU) ? ""GPU"" : ""CPU"";; ; bool writeOutputFile = true;; ; ; ; const char *rnn_type = ""RNN"";; ; #ifdef R__HAS_PYMVA; TMVA::PyMethodBase::PyInitialize();; #else; useKeras = false;; #endif; ; #ifdef R__USE_IMT; int num_threads = 4; // use max 4 threads; // switch off MT in OpenBLAS to avoid conflict with tbb; gSystem->Setenv(""OMP_NUM_THREADS"", ""1"");; ; // do enable MT running; if (num_threads >= 0) {; ROOT::EnableImplicitMT(num_threads);; }; #endif; ; TMVA::Config::Instance();; ; std::cout << ""Running with nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; TString inputFileName = ""time_data_t10_d30.root"";; ; bool fileExist = !gSystem->AccessPathName(inputFileName);; ; // if file does not exists create it; if (!fileExist) {; MakeTimeData(nTotEvts,ntime, ninput);; }; ; ; auto inputFile = TFile::Open(inputFileName);; if (!inputFile) {; Error(""TMVA_RNN_Classification"", ""Error opening input file %s - exit"", inputFileName.Data());; return;; }; ; ; std::cout << ""--- RNNClassification : Using input file: "" << inputFile->GetName() << std::endl;; ; // Create a ROOT output file where TMVA will store ntuples, histograms, etc.; TString outfileName(TString::Format(""data_RNN_%s.root"", archString.Data()));; TFile *outputFile = nullptr;; if (w",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:7312,Security,validat,validation,7312,"ollowing options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_DNN␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSEN",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:7380,Security,validat,validation,7380,"(short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_DNN␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:Inpu",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:10874,Security,validat,validation,10874,"med)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 8234 (32.16 KB); Trainable params: 8234 (32.16 KB); Non-trainable params: 0 (0.00 Byte); ________________________________________________",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:10942,Security,validat,validation,10942," error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 8234 (32.16 KB); Trainable params: 8234 (32.16 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; (TString) ""python3""[7]; Factory : Booking method: ␛[1mPyKeras_LSTM␛[0m; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Loadin",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15120,Security,validat,validation,15120,"ars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:18642,Security,validat,validation,18642,"; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error f",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:21239,Security,validat,validation,21239,"um Test error found - save the configuration ; : 19 | 0.661329 0.670317 0.192098 0.0154074 14488.6 0; : 20 Minimum Test error found - save the configuration ; : 20 | 0.658253 0.661402 0.192014 0.0155677 14508.7 0; : ; : Elapsed time for training with 3200 events: 3.87 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.102 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 8234 (32.16 KB); Trainable params: 8234 (32.16 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/26 [>.............................] - ETA: 44s - loss: 0.8260 - accuracy: 0.5100␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 5/26 [====>.........................] - ETA: 0s - loss: 0.7468 - accuracy: 0.5420 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 11/26 [===========>..................] - ETA: 0s - loss: 0.7192 - accuracy: 0.5400␈␈␈␈",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:21947,Security,validat,validation,21947," xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 8234 (32.16 KB); Trainable params: 8234 (32.16 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/20; ; 1/26 [>.............................] - ETA: 44s - loss: 0.8260 - accuracy: 0.5100␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 5/26 [====>.........................] - ETA: 0s - loss: 0.7468 - accuracy: 0.5420 ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 11/26 [===========>..................] - ETA: 0s - loss: 0.7192 - accuracy: 0.5400␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 16/26 [=================>............] - ETA: 0s - loss: 0.7120 - accuracy: 0.5494␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈; 22/26 [========================>.....] - ETA: 0s - loss: 0.7048 - accuracy: 0.5509; Epoch 1: val_loss improved from inf to 0.69266, saving model to trained_model_LSTM.h5; ␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:77474,Security,access,access,77474,"ras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMacroClass supporting a collection of lines with C++ code.Definition TMacro.h:31; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random number from the standard Normal (Gaussian) Distribution with the given mean and sigm...Definition TRandom.cxx:275; TRandom::SetSeedvirtual void SetSeed(ULong_t seed=0)Set the random generator seed.Definition TRandom.cxx:615; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; ROOT::VecOps::cosRVec< PromoteType< T > > cos(const RVec< T > &v)Definition RVec.hxx:1852; ROOT::VecOps::sinRVec< PromoteType< T > > sin(const RVec< T > &v)Definition RVec.hxx:1851; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_RNN_ClassificationDefinition TMVA_RNN_Classificati",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:77508,Security,access,access,77508,"ras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMacroClass supporting a collection of lines with C++ code.Definition TMacro.h:31; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random number from the standard Normal (Gaussian) Distribution with the given mean and sigm...Definition TRandom.cxx:275; TRandom::SetSeedvirtual void SetSeed(ULong_t seed=0)Set the random generator seed.Definition TRandom.cxx:615; TStringBasic string class.Definition TString.h:139; TString::Dataconst char * Data() constDefinition TString.h:376; TString::Formatstatic TString Format(const char *fmt,...)Static method which formats a string using a printf style format descriptor and return a TString.Definition TString.cxx:2378; TSystem::Execvirtual Int_t Exec(const char *shellcmd)Execute a command.Definition TSystem.cxx:653; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TSystem::Setenvvirtual void Setenv(const char *name, const char *value)Set environment variable.Definition TSystem.cxx:1649; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; ROOT::VecOps::cosRVec< PromoteType< T > > cos(const RVec< T > &v)Definition RVec.hxx:1852; ROOT::VecOps::sinRVec< PromoteType< T > > sin(const RVec< T > &v)Definition RVec.hxx:1851; c1return c1Definition legend1.C:41; nconst Int_t nDefinition legend1.C:16; h1TH1F * h1Definition legend1.C:5; f1TF1 * f1Definition legend1.C:11; ROOT::EnableImplicitMTvoid EnableImplicitMT(UInt_t numthreads=0)Enable ROOT's implicit multi-threading for all objects and methods that provide an internal paralleli...Definition TROOT.cxx:539; ROOT::GetThreadPoolSizeUInt_t GetThreadPoolSize()Returns the size of ROOT's thread pool.Definition TROOT.cxx:577; TMVA_RNN_ClassificationDefinition TMVA_RNN_Classificati",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:8036,Testability,test,testing,8036," squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=20,Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : Will now use the CPU architecture with BLAS and IMT support !; Factory : Booking method: ␛[1mTMVA_DNN␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:10724,Testability,test,testing,10724,"med)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Will now use the CPU architecture with BLAS and IMT support !; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 8234 (32.16 KB); Trainable params: 8234 (32.16 KB); Non-trainable params: 0 (0.00 Byte); ________________________________________________",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:14023,Testability,test,testing,14023,"y expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = I",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:14166,Testability,test,testing,14166,"Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimiz",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:14213,Testability,test,testing,14213,"ars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:14291,Testability,test,testing,14291,"ars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:14342,Testability,test,testing,14342,"ars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:15081,Testability,test,testing,15081,"ars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:18603,Testability,test,testing,18603,"; : 19 | 0.509836 0.557055 0.617073 0.042121 4348.19 0; : 20 | 0.510257 0.574511 0.617241 0.0402784 4333.04 1; : ; : Elapsed time for training with 3200 events: 12.3 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.22 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.7616; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.731202 0.712689 0.193045 0.0157486 14439.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.701214 0.698536 0.191577 0.0154545 14535.3 0; : 3 Minimum Test error f",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:52186,Testability,test,testing,52186,"00e+00; : 296 : vars_time9 : 0.000e+00; : 297 : vars_time9 : 0.000e+00; : 298 : vars_time9 : 0.000e+00; : 299 : vars_time9 : 0.000e+00; : 300 : vars_time9 : 0.000e+00; : --------------------------------------------; TH1.Print Name = TrainingHistory_TMVA_LSTM_trainingError, Entries= 0, Total sum= 11.8838; TH1.Print Name = TrainingHistory_TMVA_LSTM_valError, Entries= 0, Total sum= 12.3797; TH1.Print Name = TrainingHistory_TMVA_DNN_trainingError, Entries= 0, Total sum= 13.6351; TH1.Print Name = TrainingHistory_TMVA_DNN_valError, Entries= 0, Total sum= 13.7968; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'accuracy', Entries= 0, Total sum= 14.2691; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'loss', Entries= 0, Total sum= 10.8696; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'val_accuracy', Entries= 0, Total sum= 14.125; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'val_loss', Entries= 0, Total sum= 11.0066; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; nthreads = 4; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: TMVA_LSTM for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test metho",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:52811,Testability,test,testing,52811,"or, Entries= 0, Total sum= 13.7968; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'accuracy', Entries= 0, Total sum= 14.2691; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'loss', Entries= 0, Total sum= 10.8696; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'val_accuracy', Entries= 0, Total sum= 14.125; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'val_loss', Entries= 0, Total sum= 11.0066; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; nthreads = 4; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: TMVA_LSTM for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification perf",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:53088,Testability,test,testing,53088,"5; TH1.Print Name = TrainingHistory_PyKeras_LSTM_'val_loss', Entries= 0, Total sum= 11.0066; Factory : === Destroy and recreate all methods via weight files for testing ===; : ; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_PyKeras_LSTM.weights.xml␛[0m; : Reading weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; nthreads = 4; Factory : ␛[1mTest all methods␛[0m; Factory : Test method: TMVA_LSTM for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with clas",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:53586,Testability,test,testing,53586,"[1mTest all methods␛[0m; Factory : Test method: TMVA_LSTM for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:53777,Testability,test,testing,53777," ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0529 sec ; Factory : Test method: TMVA_DNN for Classification performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:53985,Testability,test,test,53985,"n performance; : ; : Evaluate deep neural network on CPU using batches with size = 800; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.0218 sec ; Factory : Test method: PyKeras_LSTM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier respo",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54313,Testability,test,test,54313,"STM for Classification performance; : ; : Setting up tf.keras; : Using TensorFlow version 2; : Use Keras version from TensorFlow : tf.keras; : Applying GPU option: gpu_options.allow_growth=True; : Disabled TF eager execution when evaluating model ; : Loading Keras Model ; : Loaded model from file: trained_model_LSTM.h5; PyKeras_LSTM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : ---------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54649,Testability,test,test,54649,"TM : [dataset] : Evaluation of PyKeras_LSTM on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.249 sec ; Factory : Test method: BDTG for Classification performance; : ; BDTG : [dataset] : Evaluation of BDTG on testing sample (800 events); : Elapsed time for evaluation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : ------------------------------------------------------------------------------------------------------------------",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:54895,Testability,test,test,54895,"ation of 800 events: 0.00679 sec ; Factory : ␛[1mEvaluate all methods␛[0m; Factory : Evaluate classifier: TMVA_LSTM; : ; TMVA_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: TMVA_DNN; : ; TMVA_DNN : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Evaluate deep neural network on CPU using batches with size = 1000; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: PyKeras_LSTM; : ; PyKeras_LSTM : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method:",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:55778,Testability,test,test,55778,"[dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; Factory : Evaluate classifier: BDTG; : ; BDTG : [dataset] : Loop over test events and fill histograms with classifier response...; : ; : Dataset[dataset] : variable plots are not produces ! The number of variables is 300 , it is larger than 200; : ; : Evaluation results ranked by best signal efficiency and purity (area); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA ; : Name: Method: ROC-integ; : dataset PyKeras_LSTM : 0.857; : dataset BDTG : 0.843; : dataset TMVA_LSTM : 0.783; : dataset TMVA_DNN : 0.654; : -------------------------------------------------------------------------------------------------------------------; : ; : Testing efficiency compared to training efficiency (overtraining check); : -------------------------------------------------------------------------------------------------------------------; : DataSet MVA Signal efficiency: from test sample (from training sample) ; : Name: Method: @B=0.01 @B=0.10 @B=0.30 ; : -------------------------------------------------------------------------------------------------------------------; : dataset PyKeras_LSTM : 0.225 (0.215) 0.600 (0.609) 0.839 (0.855); : dataset BDTG : 0.135 (0.272) 0.502 (0.648) 0.835 (0.886); : dataset TMVA_LSTM : 0.058 (0.097) 0.445 (0.478) 0.716 (0.752); : dataset TMVA_DNN : 0.035 (0.028) 0.239 (0.223) 0.505 (0.504); : -------------------------------------------------------------------------------------------------------------------; : ; Dataset:dataset : Created tree 'TestTree' with 800 events; : ; Dataset:dataset : Created tree 'TrainTree' with 3200 events; : ; Factory : ␛[1mThank you for using TMVA!␛[0m; : ␛[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html␛[0m; /***; ; # TMVA",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:71313,Testability,test,test,71313,"fo(""TMVA_RNN_Classification"", ""Booking Keras %s model"", rnn_types[i].c_str());; factory->BookMethod(dataloader, TMVA::Types::kPyKeras,; TString::Format(""PyKeras_%s"", rnn_types[i].c_str()),; TString::Format(""!H:!V:VarTransform=None:FilenameModel=%s:tf.keras:""; ""FilenameTrainedModel=%s:GpuOptions=allow_growth=True:""; ""NumEpochs=%d:BatchSize=%d"",; modelName.Data(), trainedModelName.Data(), maxepochs, batchSize));; }; }; }; }; ; // use BDT in case not using Keras or TMVA DL; if (!useKeras || !useTMVA_BDT); useTMVA_BDT = true;; ; /**; ## Book TMVA BDT; **/; ; if (useTMVA_BDT) {; ; factory->BookMethod(dataloader, TMVA::Types::kBDT, ""BDTG"",; ""!H:!V:NTrees=100:MinNodeSize=2.5%:BoostType=Grad:Shrinkage=0.10:UseBaggedBoost:""; ""BaggedSampleFraction=0.5:nCuts=20:""; ""MaxDepth=2"");; ; }; ; /// Train all methods; factory->TrainAllMethods();; ; std::cout << ""nthreads = "" << ROOT::GetThreadPoolSize() << std::endl;; ; // ---- Evaluate all MVAs using the set of test events; factory->TestAllMethods();; ; // ----- Evaluate and compare performance of all configured MVAs; factory->EvaluateAllMethods();; ; // check method; ; // plot ROC curve; auto c1 = factory->GetROCCurve(dataloader);; c1->Draw();; ; if (outputFile) outputFile->Close();; }; DataLoader.h; DataSetInfo.h; MethodDL.h; f#define f(i)Definition RSha256.hxx:104; Infovoid Info(const char *location, const char *msgfmt,...)Use this function for informational messages.Definition TError.cxx:218; Errorvoid Error(const char *location, const char *msgfmt,...)Use this function in case an error occurred.Definition TError.cxx:185; Warningvoid Warning(const char *location, const char *msgfmt,...)Use this function in warning situations.Definition TError.cxx:229; TFile.h; x2Option_t Option_t TPoint TPoint const char x2Definition TGWin32VirtualXProxy.cxx:70; x1Option_t Option_t TPoint TPoint const char x1Definition TGWin32VirtualXProxy.cxx:70; TROOT.h; gROOT#define gROOTDefinition TROOT.h:406; gRandomR__EXTERN TRandom * gRandomDefinition TRand",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:74697,Testability,test,test,74697,"ame, Int_t ntimes=5000, TRandom *rng=nullptr)Fill histogram following distribution in function fname.Definition TH1.cxx:3519; TH1::GetBinContentvirtual Double_t GetBinContent(Int_t bin) constReturn content of bin number bin.Definition TH1.cxx:5061; TMVA::Config::Instancestatic Config & Instance()static function: returns TMVA instanceDefinition Config.cxx:98; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddVariablesArrayvoid AddVariablesArray(const TString &expression, int size, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating array of variables in data set info in case input tree provides an array ...Definition DataLoader.cxx:504; TMVA::DataLoader::AddSignalTreevoid AddSignalTree(TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:371; TMVA::DataLoader::PrepareTrainingAndTestTreevoid PrepareTrainingAndTestTree(const TCut &cut, const TString &splitOpt)prepare the training and test trees -> same cuts for signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataSetInfo::GetListOfVariablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8C.html:75739,Testability,test,testing,75739," signal and backgroundDefinition DataLoader.cxx:632; TMVA::DataLoader::AddBackgroundTreevoid AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)number of signal events (used to compute significance)Definition DataLoader.cxx:402; TMVA::DataLoader::GetDataSetInfoDataSetInfo & GetDataSetInfo()Definition DataLoader.cxx:137; TMVA::DataSetInfo::GetListOfVariablesstd::vector< TString > GetListOfVariables() constreturns list of variablesDefinition DataSetInfo.cxx:406; TMVA::FactoryThis is the main MVA steering class.Definition Factory.h:80; TMVA::Factory::TrainAllMethodsvoid TrainAllMethods()Iterates through all booked methods and calls training.Definition Factory.cxx:1114; TMVA::Factory::BookMethodMethodBase * BookMethod(DataLoader *loader, TString theMethodName, TString methodTitle, TString theOption="""")Book a classifier or regression method.Definition Factory.cxx:352; TMVA::Factory::TestAllMethodsvoid TestAllMethods()Evaluates all booked methods on the testing data and adds the output to the Results in the corresponi...Definition Factory.cxx:1271; TMVA::Factory::EvaluateAllMethodsvoid EvaluateAllMethods(void)Iterates over all MVAs that have been booked, and calls their evaluation methods.Definition Factory.cxx:1376; TMVA::Factory::GetROCCurveTGraph * GetROCCurve(DataLoader *loader, TString theMethodName, Bool_t setTitles=kTRUE, UInt_t iClass=0, Types::ETreeType type=Types::kTesting)Argument iClass specifies the class to generate the ROC curve in a multiclass setting.Definition Factory.cxx:912; TMVA::PyMethodBase::PyInitializestatic void PyInitialize()Initialize Python interpreter.Definition PyMethodBase.cxx:153; TMVA::Types::kPyKeras@ kPyKerasDefinition Types.h:103; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kDL@ kDLDefinition Types.h:99; TMacroClass supporting a collection of lines with C++ code.Definition TMacro.h:31; TRandom::Gausvirtual Double_t Gaus(Double_t mean=0, Double_t sigma=1)Samples a random",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8C.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8C.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:2491,Availability,error,error,2491,"0|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=10Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=10Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are in",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:5393,Availability,error,error,5393,"00:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]; : BatchLayout: ""0|0|0"" [The Layout of the batch]; : ValidationSize: ""20%"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of t",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:15594,Availability,error,error,15594,"- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.61",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:15864,Availability,error,error,15864,"** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16134,Availability,error,error,16134,", 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TM",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16244,Availability,error,error,16244,"n = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16354,Availability,error,error,16354,"ity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep L",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16518,Availability,error,error,16518,"0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Ou",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16629,Availability,error,error,16629,"----------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = (",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:18116,Availability,error,error,18116,"andalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:18387,Availability,error,error,18387,"NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:18497,Availability,error,error,18497,"300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:18660,Availability,error,error,18660,"tion = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 5; : 17 | 0.677004 0.674528 0.190369 0.0146787 14571.1 6; : 18 Minimum Test error found - save the configuration ; : 18 | 0.670403 0.662 0.188594 0.0151297 14758.1",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:19035,Availability,error,error,19035,"eta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 5; : 17 | 0.677004 0.674528 0.190369 0.0146787 14571.1 6; : 18 Minimum Test error found - save the configuration ; : 18 | 0.670403 0.662 0.188594 0.0151297 14758.1 0; : 19 | 0.673327 0.684121 0.189356 0.0146185 14650.5 1; : 20 | 0.68729 0.678527 0.188558 0.0148948 14741.2 2; : ; : Elapsed time for training with 3200 events: 3.82 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:19146,Availability,error,error,19146,----------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 5; : 17 | 0.677004 0.674528 0.190369 0.0146787 14571.1 6; : 18 Minimum Test error found - save the configuration ; : 18 | 0.670403 0.662 0.188594 0.0151297 14758.1 0; : 19 | 0.673327 0.684121 0.189356 0.0146185 14650.5 1; : 20 | 0.68729 0.678527 0.188558 0.0148948 14741.2 2; : ; : Elapsed time for training with 3200 events: 3.82 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0999 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Cr,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:19586,Availability,error,error,19586,91864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 5; : 17 | 0.677004 0.674528 0.190369 0.0146787 14571.1 6; : 18 Minimum Test error found - save the configuration ; : 18 | 0.670403 0.662 0.188594 0.0151297 14758.1 0; : 19 | 0.673327 0.684121 0.189356 0.0146185 14650.5 1; : 20 | 0.68729 0.678527 0.188558 0.0148948 14741.2 2; : ; : Elapsed time for training with 3200 events: 3.82 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0999 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras_LSTM ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predi,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:33045,Availability,error,errors,33045,"ber ; : of events ""nEventsMin"" as given in earlier versions; : If this number is too large, detailed features ; : in the parameter space are hard to be modelled. If it is too small, ; : the risk to overtrain rises and boosting seems to be less effective; : typical values from our current experience for best performance ; : are between 0.5(%) and 10(%) ; : ; : The default minimal number is currently set to ; : max(20, (N_training_events / N_variables^2 / 10)) ; : and can be changed by the user.; : ; : The other crucial parameter, the pruning strength (""PruneStrength""),; : is also related to overtraining. It is a regularisation parameter ; : that is used when determining after the training which splits ; : are considered statistically insignificant and are removed. The; : user is advised to carefully watch the BDT screen output for; : the comparison between efficiencies obtained on the training and; : the independent test sample. They should be equal within statistical; : errors, in order to minimize statistical fluctuations in different samples.; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; BDTG : #events: (reweighted) sig: 1600 bkg: 1600; : #events: (unweighted) sig: 1600 bkg: 1600; : Training 100 Decision Trees ... patience please; : Elapsed time for training with 3200 events: 1.7 sec ; BDTG : [dataset] : Evaluation of BDTG on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0183 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_BDTG.class.C␛[0m; : data_RNN_CPU.root:/dataset/Method_BDT/BDTG; Factory : Training finished; : ; : Ranking input variables (method specific)...; : No variable ranking supplied by classifier: TMVA_LSTM; : No variable ranking supplied by classifier: TMVA_DNN; : No variable ranking supplied by",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:52984,Availability,avail,available,52984," = ROOT.TCanvas(); c1.Divide(ntime, 2); for j in range(ntime):; c1.cd(j + 1); v1[j].Draw(); for j in range(ntime):; c1.cd(ntime + j + 1); v2[j].Draw(); ; ROOT.gPad.Update(); ; if n > 1:; sgn.Write(); bkg.Write(); sgn.Print(); bkg.Print(); f.Close(); ; ; ## macro for performing a classification using a Recurrent Neural Network; ## @param use_type; ## use_type = 0 use Simple RNN network; ## use_type = 1 use LSTM network; ## use_type = 2 use GRU; ## use_type = 3 build 3 different networks with RNN, LSTM and GRU; ; ; use_type = 1; ninput = 30; ntime = 10; batchSize = 100; maxepochs = 10; ; nTotEvts = 2000 # total events to be generated for signal or background; ; useKeras = True; ; useTMVA_RNN = True; useTMVA_DNN = True; useTMVA_BDT = False; ; tf_spec = importlib.util.find_spec(""tensorflow""); if tf_spec is None:; useKeras = False; ROOT.Warning(""TMVA_RNN_Classificaton"",""Skip using Keras since tensorflow is not installed""); ; ; rnn_types = [""RNN"", ""LSTM"", ""GRU""]; use_rnn_type = [1, 1, 1]; ; if 0 <= use_type < 3:; use_rnn_type = [0, 0, 0]; use_rnn_type[use_type] = 1; ; useGPU = True # use GPU for TMVA if available; ; useGPU = ""tmva-gpu"" in ROOT.gROOT.GetConfigFeatures(); useTMVA_RNN = (""tmva-cpu"" in ROOT.gROOT.GetConfigFeatures()) or useGPU; ; if useTMVA_RNN:; ROOT.Warning(; ""TMVA_RNN_Classification"",; ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for RNN"",; ); ; archString = ""GPU"" if useGPU else ""CPU""; ; writeOutputFile = True; ; rnn_type = ""RNN""; ; if ""tmva-pymva"" in ROOT.gROOT.GetConfigFeatures():; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False; ; ; ; inputFileName = ""time_data_t10_d30.root""; ; fileDoesNotExist = ROOT.gSystem.AccessPathName(inputFileName); ; # if file does not exists create it; if fileDoesNotExist:; MakeTimeData(nTotEvts, ntime, ninput); ; ; inputFile = TFile.Open(inputFileName); if inputFile is None:; raise ROOT.Error(""Error opening input file %s - exit"", inputFileName.Data()); ; ; print(""--- RNNClass",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:15887,Deployability,configurat,configuration,15887,"** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16157,Deployability,configurat,configuration,16157,", 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TM",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16267,Deployability,configurat,configuration,16267,"n = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16377,Deployability,configurat,configuration,16377,"ity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep L",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16541,Deployability,configurat,configuration,16541,"0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Ou",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16652,Deployability,configurat,configuration,16652,"----------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: TMVA_DNN for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = (",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:18410,Deployability,configurat,configuration,18410,"NETWORK: Depth = 4 Input = ( 1, 1, 300 ) Batch size = 256 Loss function = C; Layer 0 DENSE Layer: ( Input = 300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:18520,Deployability,configurat,configuration,18520,"300 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 1 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:18683,Deployability,configurat,configuration,18683,"tion = Tanh; Layer 2 DENSE Layer: ( Input = 64 , Width = 64 ) Output = ( 1 , 256 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 256 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 5; : 17 | 0.677004 0.674528 0.190369 0.0146787 14571.1 6; : 18 Minimum Test error found - save the configuration ; : 18 | 0.670403 0.662 0.188594 0.0151297 14758.1",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:19058,Deployability,configurat,configuration,19058,"eta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.805395; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 5; : 17 | 0.677004 0.674528 0.190369 0.0146787 14571.1 6; : 18 Minimum Test error found - save the configuration ; : 18 | 0.670403 0.662 0.188594 0.0151297 14758.1 0; : 19 | 0.673327 0.684121 0.189356 0.0146185 14650.5 1; : 20 | 0.68729 0.678527 0.188558 0.0148948 14741.2 2; : ; : Elapsed time for training with 3200 events: 3.82 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:19169,Deployability,configurat,configuration,19169,----------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.725372 0.686272 0.190597 0.0152439 14599.1 0; : 2 Minimum Test error found - save the configuration ; : 2 | 0.695636 0.683324 0.189128 0.015082 14708.8 0; : 3 | 0.691864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 5; : 17 | 0.677004 0.674528 0.190369 0.0146787 14571.1 6; : 18 Minimum Test error found - save the configuration ; : 18 | 0.670403 0.662 0.188594 0.0151297 14758.1 0; : 19 | 0.673327 0.684121 0.189356 0.0146185 14650.5 1; : 20 | 0.68729 0.678527 0.188558 0.0148948 14741.2 2; : ; : Elapsed time for training with 3200 events: 3.82 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0999 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Cr,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:19609,Deployability,configurat,configuration,19609,91864 0.689576 0.189646 0.0148842 14648.6 1; : 4 Minimum Test error found - save the configuration ; : 4 | 0.689168 0.677551 0.190642 0.015026 14577.3 0; : 5 | 0.686885 0.678678 0.189865 0.0148775 14629.7 1; : 6 | 0.68905 0.681357 0.189104 0.014885 14694.1 2; : 7 | 0.687138 0.683485 0.19023 0.0152893 14633.6 3; : 8 | 0.686429 0.685529 0.189996 0.0147979 14612 4; : 9 | 0.685179 0.683467 0.190317 0.0152552 14623.4 5; : 10 Minimum Test error found - save the configuration ; : 10 | 0.684262 0.673795 0.18992 0.0153518 14664.7 0; : 11 Minimum Test error found - save the configuration ; : 11 | 0.682907 0.671521 0.194162 0.0155002 14328.7 0; : 12 | 0.693473 0.677553 0.188937 0.0147664 14698.2 1; : 13 | 0.695581 0.671789 0.190501 0.0149406 14581.9 2; : 14 | 0.68024 0.680521 0.188432 0.0146998 14735.3 3; : 15 | 0.68435 0.688062 0.189615 0.0147881 14643.1 4; : 16 | 0.686476 0.675588 0.190015 0.0152195 14645.7 5; : 17 | 0.677004 0.674528 0.190369 0.0146787 14571.1 6; : 18 Minimum Test error found - save the configuration ; : 18 | 0.670403 0.662 0.188594 0.0151297 14758.1 0; : 19 | 0.673327 0.684121 0.189356 0.0146185 14650.5 1; : 20 | 0.68729 0.678527 0.188558 0.0148948 14741.2 2; : ; : Elapsed time for training with 3200 events: 3.82 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0999 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras_LSTM ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predi,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:31812,Deployability,configurat,configuration,31812,"ng of the following tree.; : ; : Decision trees are a sequence of binary splits of the data sample; : using a single discriminant variable at a time. A test event ; : ending up after the sequence of left-right splits in a final ; : (""leaf"") node is classified as either signal or background; : depending on the majority type of training events in that node.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : By the nature of the binary splits performed on the individual; : variables, decision trees do not deal well with linear correlations; : between variables (they need to approximate the linear split in; : the two dimensional space by a sequence of splits on the two ; : variables individually). Hence decorrelation could be useful ; : to optimise the BDT performance.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : The two most important parameters in the configuration are the ; : minimal number of events requested by a leaf node as percentage of the ; : number of training events (option ""MinNodeSize"" replacing the actual number ; : of events ""nEventsMin"" as given in earlier versions; : If this number is too large, detailed features ; : in the parameter space are hard to be modelled. If it is too small, ; : the risk to overtrain rises and boosting seems to be less effective; : typical values from our current experience for best performance ; : are between 0.5(%) and 10(%) ; : ; : The default minimal number is currently set to ; : max(20, (N_training_events / N_variables^2 / 10)) ; : and can be changed by the user.; : ; : The other crucial parameter, the pruning strength (""PruneStrength""),; : is also related to overtraining. It is a regularisation parameter ; : that is used when determining after the training which splits ; : are considered statistically insignificant and are removed. The; : user is advised to carefully watch the BDT screen output for; : the comparison between efficiencies obtained on the training and; : the independent test sam",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:31887,Deployability,configurat,configuration,31887,"ng of the following tree.; : ; : Decision trees are a sequence of binary splits of the data sample; : using a single discriminant variable at a time. A test event ; : ending up after the sequence of left-right splits in a final ; : (""leaf"") node is classified as either signal or background; : depending on the majority type of training events in that node.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : By the nature of the binary splits performed on the individual; : variables, decision trees do not deal well with linear correlations; : between variables (they need to approximate the linear split in; : the two dimensional space by a sequence of splits on the two ; : variables individually). Hence decorrelation could be useful ; : to optimise the BDT performance.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : The two most important parameters in the configuration are the ; : minimal number of events requested by a leaf node as percentage of the ; : number of training events (option ""MinNodeSize"" replacing the actual number ; : of events ""nEventsMin"" as given in earlier versions; : If this number is too large, detailed features ; : in the parameter space are hard to be modelled. If it is too small, ; : the risk to overtrain rises and boosting seems to be less effective; : typical values from our current experience for best performance ; : are between 0.5(%) and 10(%) ; : ; : The default minimal number is currently set to ; : max(20, (N_training_events / N_variables^2 / 10)) ; : and can be changed by the user.; : ; : The other crucial parameter, the pruning strength (""PruneStrength""),; : is also related to overtraining. It is a regularisation parameter ; : that is used when determining after the training which splits ; : are considered statistically insignificant and are removed. The; : user is advised to carefully watch the BDT screen output for; : the comparison between efficiencies obtained on the training and; : the independent test sam",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:52788,Deployability,install,installed,52788," = ROOT.TCanvas(); c1.Divide(ntime, 2); for j in range(ntime):; c1.cd(j + 1); v1[j].Draw(); for j in range(ntime):; c1.cd(ntime + j + 1); v2[j].Draw(); ; ROOT.gPad.Update(); ; if n > 1:; sgn.Write(); bkg.Write(); sgn.Print(); bkg.Print(); f.Close(); ; ; ## macro for performing a classification using a Recurrent Neural Network; ## @param use_type; ## use_type = 0 use Simple RNN network; ## use_type = 1 use LSTM network; ## use_type = 2 use GRU; ## use_type = 3 build 3 different networks with RNN, LSTM and GRU; ; ; use_type = 1; ninput = 30; ntime = 10; batchSize = 100; maxepochs = 10; ; nTotEvts = 2000 # total events to be generated for signal or background; ; useKeras = True; ; useTMVA_RNN = True; useTMVA_DNN = True; useTMVA_BDT = False; ; tf_spec = importlib.util.find_spec(""tensorflow""); if tf_spec is None:; useKeras = False; ROOT.Warning(""TMVA_RNN_Classificaton"",""Skip using Keras since tensorflow is not installed""); ; ; rnn_types = [""RNN"", ""LSTM"", ""GRU""]; use_rnn_type = [1, 1, 1]; ; if 0 <= use_type < 3:; use_rnn_type = [0, 0, 0]; use_rnn_type[use_type] = 1; ; useGPU = True # use GPU for TMVA if available; ; useGPU = ""tmva-gpu"" in ROOT.gROOT.GetConfigFeatures(); useTMVA_RNN = (""tmva-cpu"" in ROOT.gROOT.GetConfigFeatures()) or useGPU; ; if useTMVA_RNN:; ROOT.Warning(; ""TMVA_RNN_Classification"",; ""TMVA is not build with GPU or CPU multi-thread support. Cannot use TMVA Deep Learning for RNN"",; ); ; archString = ""GPU"" if useGPU else ""CPU""; ; writeOutputFile = True; ; rnn_type = ""RNN""; ; if ""tmva-pymva"" in ROOT.gROOT.GetConfigFeatures():; TMVA.PyMethodBase.PyInitialize(); else:; useKeras = False; ; ; ; inputFileName = ""time_data_t10_d30.root""; ; fileDoesNotExist = ROOT.gSystem.AccessPathName(inputFileName); ; # if file does not exists create it; if fileDoesNotExist:; MakeTimeData(nTotEvts, ntime, ninput); ; ; inputFile = TFile.Open(inputFileName); if inputFile is None:; raise ROOT.Error(""Error opening input file %s - exit"", inputFileName.Data()); ; ; print(""--- RNNClass",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:54709,Deployability,configurat,configuration,54709,"xist = ROOT.gSystem.AccessPathName(inputFileName); ; # if file does not exists create it; if fileDoesNotExist:; MakeTimeData(nTotEvts, ntime, ninput); ; ; inputFile = TFile.Open(inputFileName); if inputFile is None:; raise ROOT.Error(""Error opening input file %s - exit"", inputFileName.Data()); ; ; print(""--- RNNClassification : Using input file: {}"".format(inputFile.GetName())); ; # Create a ROOT output file where TMVA will store ntuples, histograms, etc.; outfileName = ""data_RNN_"" + archString + "".root""; outputFile = None; ; ; if writeOutputFile:; outputFile = TFile.Open(outfileName, ""RECREATE""); ; ; ## Declare Factory; ; # Create the Factory class. Later you can choose the methods; # whose performance you'd like to investigate.; ; # The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to; # pass; ; # - The first argument is the base of the name of all the output; # weightfiles in the directory weight/ that will be created with the; # method parameters; ; # - The second argument is the output file for the training results; #; # - The third argument is a string option defining some general configuration for the TMVA session.; # For example all TMVA output can be suppressed by removing the ""!"" (not) in front of the ""Silent"" argument in; # the option string; ; ; # // Creating the factory object; factory = TMVA.Factory(; ""TMVAClassification"",; outputFile,; V=False,; Silent=False,; Color=True,; DrawProgressBar=True,; Transformations=None,; Correlations=False,; AnalysisType=""Classification"",; ModelPersistence=True,; ); dataloader = TMVA.DataLoader(""dataset""); ; signalTree = inputFile.Get(""sgn""); background = inputFile.Get(""bkg""); ; nvar = ninput * ntime; ; ## add variables - use new AddVariablesArray function; for i in range(ntime):; dataloader.AddVariablesArray(""vars_time"" + str(i), ninput); ; ; dataloader.AddSignalTree(signalTree, 1.0); dataloader.AddBackgroundTree(background, 1.0); ; # check given input; datainfo = dat",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:395,Integrability,depend,dependent,395,". ROOT: tutorials/tmva/TMVA_RNN_Classification.py File Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Namespaces ; TMVA_RNN_Classification.py File ReferenceTutorials » TMVA tutorials. Detailed Description; TMVA Classification Example Using a Recurrent Neural Network ; This is an example of using a RNN in TMVA. We do classification using a toy time dependent data set that is generated when running this example macro. DataSetInfo : [dataset] : Added class ""Signal""; : Add Tree sgn of type Signal with 2000 events; DataSetInfo : [dataset] : Added class ""Background""; : Add Tree bkg of type Background with 2000 events; Factory : Booking method: ␛[1mTMVA_LSTM␛[0m; : ; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=10Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=10Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Backgroun",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:2280,Integrability,message,message,2280,"ser:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=10Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=10Optimizer=ADAM,DropConfig=0.0+0.+0.+0."" [Defines ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:5183,Integrability,message,message,5183," : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM"" [Defines the training strategies.]; : - Default:; : VerbosityLevel: ""Default"" [Verbosity level]; : CreateMVAPdfs: ""False"" [Create PDFs for classifier outputs (signal and background)]; : IgnoreNegWeightsInTraining: ""False"" [E",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:20571,Integrability,wrap,wraps,20571,".188594 0.0151297 14758.1 0; : 19 | 0.673327 0.684121 0.189356 0.0146185 14650.5 1; : 20 | 0.68729 0.678527 0.188558 0.0148948 14741.2 2; : ; : Elapsed time for training with 3200 events: 3.82 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0999 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras_LSTM ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; saved recurrent model model_LSTM.h5; Booking Keras model LSTM; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total param",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:20763,Integrability,interface,interface,20763,".2 2; : ; : Elapsed time for training with 3200 events: 3.82 sec ; : Evaluate deep neural network on CPU using batches with size = 256; : ; TMVA_DNN : [dataset] : Evaluation of TMVA_DNN on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0999 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras_LSTM ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; saved recurrent model model_LSTM.h5; Booking Keras model LSTM; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 8234 (32.16 KB); Trainable params: 8234 (32.16 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:20902,Integrability,message,message,20902,"luation of 3200 events: 0.0999 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_TMVA_DNN.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: PyKeras_LSTM for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ PyKeras_LSTM ] :␛[0m; : ; : Keras is a high-level API for the Theano and Tensorflow packages.; : This method wraps the training and predictions steps of the Keras; : Python package for TMVA, so that dataloading, preprocessing and; : evaluation can be done within the TMVA system. To use this Keras; : interface, you have to generate a model with Keras first. Then,; : this model can be loaded and trained in TMVA.; : ; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; : Split TMVA training data in 2560 training events and 640 validation events; : Training Model Summary; saved recurrent model model_LSTM.h5; Booking Keras model LSTM; Model: ""sequential""; _________________________________________________________________; Layer (type) Output Shape Param # ; =================================================================; reshape (Reshape) (None, 10, 30) 0 ; ; lstm (LSTM) (None, 10, 10) 1640 ; ; flatten (Flatten) (None, 100) 0 ; ; dense (Dense) (None, 64) 6464 ; ; dense_1 (Dense) (None, 2) 130 ; ; =================================================================; Total params: 8234 (32.16 KB); Trainable params: 8234 (32.16 KB); Non-trainable params: 0 (0.00 Byte); _________________________________________________________________; : Option SaveBestOnly: Only model weights with smallest validation loss will be stored; Epoch 1/10; ; 1/26 [>.............................] - ETA: 42s - loss: 0.7326 - accuracy: 0.4600␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈␈",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:31291,Integrability,depend,depending,31291,"mdataset/weights/TMVAClassification_PyKeras_LSTM.class.C␛[0m; Factory : Training finished; : ; Factory : Train method: BDTG for Classification; : ; : ; : ␛[1m================================================================␛[0m; : ␛[1mH e l p f o r M V A m e t h o d [ BDTG ] :␛[0m; : ; : ␛[1m--- Short description:␛[0m; : ; : Boosted Decision Trees are a collection of individual decision; : trees which form a multivariate classifier by (weighted) majority ; : vote of the individual trees. Consecutive decision trees are ; : trained using the original training data set with re-weighted ; : events. By default, the AdaBoost method is employed, which gives ; : events that were misclassified in the previous tree a larger ; : weight in the training of the following tree.; : ; : Decision trees are a sequence of binary splits of the data sample; : using a single discriminant variable at a time. A test event ; : ending up after the sequence of left-right splits in a final ; : (""leaf"") node is classified as either signal or background; : depending on the majority type of training events in that node.; : ; : ␛[1m--- Performance optimisation:␛[0m; : ; : By the nature of the binary splits performed on the individual; : variables, decision trees do not deal well with linear correlations; : between variables (they need to approximate the linear split in; : the two dimensional space by a sequence of splits on the two ; : variables individually). Hence decorrelation could be useful ; : to optimise the BDT performance.; : ; : ␛[1m--- Performance tuning via configuration options:␛[0m; : ; : The two most important parameters in the configuration are the ; : minimal number of events requested by a leaf node as percentage of the ; : number of training events (option ""MinNodeSize"" replacing the actual number ; : of events ""nEventsMin"" as given in earlier versions; : If this number is too large, detailed features ; : in the parameter space are hard to be modelled. If it is too small, ; : the ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:33143,Integrability,message,message,33143,"ertrain rises and boosting seems to be less effective; : typical values from our current experience for best performance ; : are between 0.5(%) and 10(%) ; : ; : The default minimal number is currently set to ; : max(20, (N_training_events / N_variables^2 / 10)) ; : and can be changed by the user.; : ; : The other crucial parameter, the pruning strength (""PruneStrength""),; : is also related to overtraining. It is a regularisation parameter ; : that is used when determining after the training which splits ; : are considered statistically insignificant and are removed. The; : user is advised to carefully watch the BDT screen output for; : the comparison between efficiencies obtained on the training and; : the independent test sample. They should be equal within statistical; : errors, in order to minimize statistical fluctuations in different samples.; : ; : <Suppress this message by specifying ""!H"" in the booking option>; : ␛[1m================================================================␛[0m; : ; BDTG : #events: (reweighted) sig: 1600 bkg: 1600; : #events: (unweighted) sig: 1600 bkg: 1600; : Training 100 Decision Trees ... patience please; : Elapsed time for training with 3200 events: 1.7 sec ; BDTG : [dataset] : Evaluation of BDTG on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.0183 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_BDTG.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TMVAClassification_BDTG.class.C␛[0m; : data_RNN_CPU.root:/dataset/Method_BDT/BDTG; Factory : Training finished; : ; : Ranking input variables (method specific)...; : No variable ranking supplied by classifier: TMVA_LSTM; : No variable ranking supplied by classifier: TMVA_DNN; : No variable ranking supplied by classifier: PyKeras_LSTM; BDTG : Ranking result (top variable is best ranked); : --------------------------------------------; : Rank : Variable : Variable Importance; : ----------------------------",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:59571,Integrability,depend,depending,59571,"RandomSeed=0,; InputLayout=""1|1|"" + str(ntime * ninput),; Layout=""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"",; TrainingStrategy=trainingString1; ); ; ; ## Book Keras recurrent models; ; # Book the different types of recurrent models in Keras (SimpleRNN, LSTM or GRU); ; ; if useKeras:; for i in range(3):; if use_rnn_type[i]:; modelName = ""model_"" + rnn_types[i] + "".h5""; trainedModelName = ""trained_"" + modelName; print(""Building recurrent keras model using a"", rnn_types[i], ""layer""); # create python script which can be executed; # create 2 conv2d layer + maxpool + dense; from tensorflow.keras.models import Sequential; from tensorflow.keras.optimizers import Adam; ; # from keras.initializers import TruncatedNormal; # from keras import initializations; from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, SimpleRNN, GRU, LSTM, Reshape, BatchNormalization; ; model = Sequential(); model.add(Reshape((10, 30), input_shape=(10 * 30,))); # add recurrent neural network depending on type / Use option to return the full output; if rnn_types[i] == ""LSTM"":; model.add(LSTM(units=10, return_sequences=True)); elif rnn_types[i] == ""GRU"":; model.add(GRU(units=10, return_sequences=True)); else:; model.add(SimpleRNN(units=10, return_sequences=True)); # m.AddLine(""model.add(BatchNormalization())"");; model.add(Flatten()) # needed if returning the full time output sequence; model.add(Dense(64, activation=""tanh"")); model.add(Dense(2, activation=""sigmoid"")); model.compile(loss=""binary_crossentropy"", optimizer=Adam(learning_rate=0.001), weighted_metrics=[""accuracy""]); model.save(modelName); model.summary(); print(""saved recurrent model"", modelName); ; if not os.path.exists(modelName):; useKeras = False; print(""Error creating Keras recurrent model file - Skip using Keras""); else:; # book PyKeras method only if Keras model could be created; print(""Booking Keras model "", rnn_types[i]); factory.BookMethod(; dataloader,; TMVA.Types.kPyKeras,; ""PyKeras_"" + rnn_types[i],; H=True,;",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:1931,Modifiability,variab,variable,1931,"ed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=10Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:ValidationSize=0.2:RandomSeed=1234:InputLayout=10|30:Layout=LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=5,BatchSize=100,TestRepetitions=1,WeightDecay=1e-2,Regularization=None,MaxEpochs=10Optimizer=ADAM,DropConfig=0.0+0.+0.+0.:Architecture=CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""10|30"" [The Layout of the input]; : Layout: ""LSTM|10|30|10|0|1,RESHAPE|FLAT,DENSE|64|TANH,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIERUNIFORM"" [Weight initialization strategy]; : RandomSeed: ""1234"" [Random seed used for weight initialization and batch shuffling]; : ValidationSize: ""0.2"" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set.",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:4834,Modifiability,variab,variable,4834,"ROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : <none>; : - Default:; : Boost_num: ""0"" [Number of times the classifier will be boosted]; : Parsing option string: ; : ... ""!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIER:RandomSeed=0:InputLayout=1|1|300:Layout=DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=10,BatchSize=256,TestRepetitions=1,WeightDecay=1e-4,Regularization=None,MaxEpochs=20DropConfig=0.0+0.+0.+0.,Optimizer=ADAM:CPU""; : The following options are set:; : - By User:; : V: ""True"" [Verbose output (short form of ""VerbosityLevel"" below - overrides the latter one)]; : VarTransform: ""None"" [List of variable transformations performed before training, e.g., ""D_Background,P_Signal,G,N_AllClasses"" for: ""Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)""]; : H: ""False"" [Print method-specific help message]; : InputLayout: ""1|1|300"" [The Layout of the input]; : Layout: ""DENSE|64|TANH,DENSE|TANH|64,DENSE|TANH|64,LINEAR"" [Layout of the network.]; : ErrorStrategy: ""CROSSENTROPY"" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]; : WeightInitialization: ""XAVIER"" [Weight initialization strategy]; : RandomSeed: ""0"" [Random seed used for weight initialization and batch shuffling]; : Architecture: ""CPU"" [Which architecture to perform the training on.]; : TrainingStrategy: ""LearningRate=1e-3,Momentum=0.0,Repetitions=1,ConvergenceSteps=",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:6649,Modifiability,variab,variables,6649,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:12646,Modifiability,variab,variable,12646,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:12722,Modifiability,variab,variable,12722,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:12798,Modifiability,variab,variable,12798,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:12874,Modifiability,variab,variable,12874,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:12950,Modifiability,variab,variable,12950,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13026,Modifiability,variab,variable,13026,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13102,Modifiability,variab,variable,13102,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13178,Modifiability,variab,variable,13178,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13254,Modifiability,variab,variable,13254,,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13330,Modifiability,variab,variable,13330,ing method: ␛[1mBDTG␛[0m; : ; : the option NegWeightTreatment=InverseBoostNegWeights does not exist for BoostType=Grad; : --> change to new default NegWeightTreatment=Pray; : Rebuilding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sgn; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : ,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13511,Modifiability,variab,variable,13511,ding Dataset dataset; : Building event vectors for type 2 Signal; : Dataset[dataset] : create input formulas for tree sgn; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events ,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13587,Modifiability,variab,variable,13587,dataset] : create input formulas for tree sgn; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- traini,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13663,Modifiability,variab,variable,13663,] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- trai,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13739,Modifiability,variab,variable,13739,] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Fact,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13815,Modifiability,variab,variable,13815,] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural,MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13891,Modifiability,variab,variable,13891,"] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning N",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:13967,Modifiability,variab,variable,13967,"] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch siz",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:14043,Modifiability,variab,variable,14043,"] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NT",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:14119,Modifiability,variab,variable,14119,"] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 1",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:14195,Modifiability,variab,variable,14195,"] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; : Building event vectors for type 2 Background; : Dataset[dataset] : create input formulas for tree bkg; : Using variable vars_time0[0] from array expression vars_time0 of size 30; : Using variable vars_time1[0] from array expression vars_time1 of size 30; : Using variable vars_time2[0] from array expression vars_time2 of size 30; : Using variable vars_time3[0] from array expression vars_time3 of size 30; : Using variable vars_time4[0] from array expression vars_time4 of size 30; : Using variable vars_time5[0] from array expression vars_time5 of size 30; : Using variable vars_time6[0] from array expression vars_time6 of size 30; : Using variable vars_time7[0] from array expression vars_time7 of size 30; : Using variable vars_time8[0] from array expression vars_time8 of size 30; : Using variable vars_time9[0] from array expression vars_time9 of size 30; DataSetFactory : [dataset] : Number of events in input trees; : ; : ; : Number of training and testing events; : ---------------------------------------------------------------------------; : Signal -- training events : 1600; : Signal -- testing events : 400; : Signal -- training and testing events: 2000; : Background -- training events : 1600; : Background -- testing events : 400; : Background -- training and testing events: 2000; : ; Factory : ␛[1mTrain all methods␛[0m; Factory : Train method: TMVA_LSTM for Classification; : ; : Start of deep neural network training on CPU using MT, nthreads = 4; : ; : ***** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , W",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:15887,Modifiability,config,configuration,15887,"** Deep Learning Network *****; DEEP NEURAL NETWORK: Depth = 4 Input = ( 10, 1, 30 ) Batch size = 100 Loss function = C; Layer 0 LSTM Layer: (NInput = 30, NState = 10, NTime = 10 ) Output = ( 100 , 10 , 10 ); Layer 1 RESHAPE Layer Input = ( 1 , 10 , 10 ) Output = ( 1 , 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] ",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
https://root.cern/doc/master/TMVA__RNN__Classification_8py.html:16157,Modifiability,config,configuration,16157,", 100 , 100 ) ; Layer 2 DENSE Layer: ( Input = 100 , Width = 64 ) Output = ( 1 , 100 , 64 ) Activation Function = Tanh; Layer 3 DENSE Layer: ( Input = 64 , Width = 1 ) Output = ( 1 , 100 , 1 ) Activation Function = Identity; : Using 2560 events for training and 640 for testing; : Compute initial loss on the validation data ; : Training phase 1 of 1: Optimizer ADAM (beta1=0.9,beta2=0.999,eps=1e-07) Learning rate = 0.001 regularization 0 minimum error = 0.70799; : --------------------------------------------------------------; : Epoch | Train Err. Val. Err. t(s)/epoch t(s)/Loss nEvents/s Conv. Steps; : --------------------------------------------------------------; : Start epoch iteration ...; : 1 Minimum Test error found - save the configuration ; : 1 | 0.70053 0.692869 0.625222 0.0421632 4287.73 0; : 2 | 0.694153 0.694394 0.638111 0.0419725 4193.66 1; : 3 | 0.689625 0.697522 0.634816 0.0412563 4211.88 2; : 4 | 0.681203 0.694822 0.628686 0.0407413 4252.1 3; : 5 Minimum Test error found - save the configuration ; : 5 | 0.673678 0.681248 0.619972 0.0411822 4319.36 0; : 6 Minimum Test error found - save the configuration ; : 6 | 0.660284 0.667264 0.614022 0.0406714 4360.33 0; : 7 Minimum Test error found - save the configuration ; : 7 | 0.640131 0.636981 0.602694 0.0404119 4446.16 0; : 8 | 0.616397 0.638352 0.610274 0.0409979 4391.54 1; : 9 Minimum Test error found - save the configuration ; : 9 | 0.599679 0.608826 0.618875 0.0405848 4323.09 0; : 10 Minimum Test error found - save the configuration ; : 10 | 0.585295 0.60026 0.603058 0.0404042 4443.23 0; : ; : Elapsed time for training with 3200 events: 6.25 sec ; : Evaluate deep neural network on CPU using batches with size = 100; : ; TMVA_LSTM : [dataset] : Evaluation of TMVA_LSTM on training sample (3200 events); : Elapsed time for evaluation of 3200 events: 0.214 sec ; : Creating xml weight file: ␛[0;36mdataset/weights/TMVAClassification_TMVA_LSTM.weights.xml␛[0m; : Creating standalone class: ␛[0;36mdataset/weights/TM",MatchSource.WIKI,doc/master/TMVA__RNN__Classification_8py.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/TMVA__RNN__Classification_8py.html
